\section{Remote Access and Cluster Computing with SSH} 

  This is about remote accessing, mainly with SSH. 

  \begin{definition}[SSH Configuration File]
    In \texttt{\$HOME/.ssh/}, you will see some public-private key files. You may be logging into remote servers as such. 

    \begin{lstlisting}
      > ssh username@123.123.123.123 
      > ssh username@login.duke.edu
    \end{lstlisting}

    This can be quite tedious, so you can create a \texttt{\$HOME/.ssh/config} file to store shortcuts. 
    \begin{lstlisting}
      Host server1
        HostName 123.123.123.123
        User username
    \end{lstlisting}
    and then you can connect as such. 
    \begin{lstlisting}
      > ssh server1
    \end{lstlisting}
  \end{definition}

  \begin{definition}[Installing Packages without Sudo]
    Most likely when you are SSHing into a remote server, it is a computing cluster that you do not have admin access to, and you cannot thus include global packages. Therefore, the default method of installing binaries is by building them from source. You should therefore have a designated directory where you put all of your binaries in (I use \texttt{\$HOME/.local/bin}) and include it in \texttt{\$PATH}. 
  \end{definition}

\subsection{Clipboard}

  Clipboards have been a pain in the ass for me. Say that you connect to a remote server from your local machine, and then you open neovim in your remote machine. You can use \texttt{y} or \texttt{d} to copy/cut things and \texttt{p} to paste them in other buffers within the server, but you cannot easily copy/paste between your local machine and remote server. This becomes a pain when you need to paste some code on the remote server to GPT or when you take a code snippet from github and try to paste it on the buffer in the remote session. To fix this, you need a clipboard provider on the remote server. Do the following. I did this when my local machine has MacOS with ARM64 architecture, connecting to a remote machine running Ubuntu 22.04 with x86. Given the differences, the following steps should work. 

  \begin{enumerate}
    \item \textit{You may not have to do this step, but I did this while troubleshooting}. Install \texttt{xclip} on the remote server. If you don't have sudo access, then just build it from source. Then add the binary to somewhere in \texttt{\$PATH}. 

    \item Install \texttt{lemonade} on both your local machine and the remote server. Make sure they are both in your \texttt{\$PATH}. 

    \item Make a script on your local machine for sanity checking, call it \texttt{remote} or something, and add it to path. It should have the following. Note that the \texttt{-R}flag is important.\footnote{https://gist.github.com/bketelsen/27c2cd5b1376e72e240321baa0fbc81a} 

    \begin{lstlisting}
      #!/bin/bash
      ps cax | grep lemonade> /dev/null
      if [ $? -eq 0 ]; then
        echo "lemonade is running."
      else
        echo "lemonade is not running."
        nohup lemonade server &
      fi
      ssh -R 2489:127.0.0.1:2489 mb625@123.123.123.123
    \end{lstlisting}

    \item Make sure to put the following in your \texttt{config} file. I put it in both your local and remote.\footnote{https://github.com/neovim/neovim/issues/8028} 

    \begin{lstlisting}
      ForwardX11 yes
      ForwardX11Trusted yes
    \end{lstlisting}

    \item You may also have to set \texttt{clipboard=unnamedplus}. I had this on by default. 

    \item Make sure that you do not have the keymaps \texttt{y} to \texttt{+y} enabled. 
  \end{enumerate}

  We are done. Run \texttt{:checkhealth} on neovim in your remote server and confirm that lemonade is detected. 

\subsection{High Performance Computing with Slurm}

  You will most likely be remotely connecting to a server to access a compute cluster. This is a giant server consisting of hundreds or thousands of processing units, called a \textbf{cluster}. A cluster has different \textbf{partitions}, and within each partition contains \textbf{nodes}. In each node, there is a specific number of CPUs and GPUs available. There is a natural queuing system that connects users to nodes, through an open-source workload manager called \textbf{slurm}. 

  As soon as you ssh into the cluster, you will most likely be in a \textbf{login node}.
  \begin{lstlisting}
    >>> hostname         
    compsci-login-03 
  \end{lstlisting} 

  While it does have CPUs, you generally don't want to use this for computation. It usually contains a few CPUs. 

  \begin{figure}[H]
    \centering 
    \begin{lstlisting}
      >>> lscpu 
      Architecture:             x86_64
        CPU op-mode(s):         32-bit, 64-bit
        Address sizes:          42 bits physical, 48 bits virtual
        Byte Order:             Little Endian
      CPU(s):                   6
        On-line CPU(s) list:    0-5
      Vendor ID:                GenuineIntel
        Model name:             Intel(R) Xeon(R) Gold 6248R CPU @ 3.00GHz    
        ...
    \end{lstlisting}
    \caption{There are only 6 CPUs on this list. } 
  \end{figure}

  You want to switch to a \textbf{compute node} for heavy computational tasks. 

\subsubsection{Exploring Specs and Status with sinfo} 

  Before you even connect to a compute node, you want to see which nodes are available. In here, the \textbf{sinfo} command is your friend.\footnote{The man pages is a 5 min read and explains everything in this subsection.} Just running it in your login node allows you to list the partitions on the cluster and see their availability. Note that given some partition, e.g. \texttt{compsci}, it consists of the nodes \texttt{linux[1-40]}. 

  \begin{lstlisting}
    >>> sinfo
    PARTITION    AVAIL  TIMELIMIT  NODES  STATE NODELIST
    compsci         up 90-00:00:0      1 drain* linux23
    compsci         up 90-00:00:0      4    mix linux[31,35-37]
    compsci         up 90-00:00:0     21  alloc linux[1-7,9-10,21-22,24-30,32-34]
    compsci         up 90-00:00:0     13   idle linux[11-20,38-40]
    compsci-gpu*    up 90-00:00:0      3   mix- compsci-cluster-fitz-[06,08,20]
    compsci-gpu*    up 90-00:00:0      1  down* gpu-compute7
    compsci-gpu*    up 90-00:00:0      7   drng compsci-cluster-fitz-[04,19,29],...
    compsci-gpu*    up 90-00:00:0     40  drain compsci-cluster-fitz-[01-03,09-18,...],...
    compsci-gpu*    up 90-00:00:0      4    mix compsci-cluster-fitz-[05,07],...
    compsci-gpu*    up 90-00:00:0      3  alloc linux[41-43]
    grisman         up 60-00:00:0      1   plnd grisman-37
    grisman         up 60-00:00:0      2    mix fennario-[02,04]
    grisman         up 60-00:00:0      5  alloc grisman-40,jerry[1,3,6-7]
    grisman         up 60-00:00:0      7   idle fennario-[01,03,05-06],jerry[2,4-5]
    wiseman         up 60-00:00:0      1    mix wiseman-01
    nlplab          up 60-00:00:0      1    mix nlplab-01
    nlplab-core     up 90-00:00:0      1    mix nlplab-core-01
    bhuwan          up 90-00:00:0      1    mix bhuwan-01
    rudin           up 90-00:00:0      1   mix- rudin-01
    skynet          up 90-00:00:0      1   idle skynet-02
    wills           up 90-00:00:0      1   idle olympus 
  \end{lstlisting}
  
  One of them is in drain state, 4 are in mix, 21 in alloc, and 13 idle. To clarify what each of these statuses mean, lets properly define them. 

  \begin{definition}[Compute Node Status Codes]
    \begin{enumerate}
      \item \textbf{idle} - Node is completely free and available for new jobs.
      \item \textbf{alloc} - Node is fully allocated/occupied by running jobs.
      \item \textbf{mix} - Node is partially allocated (some CPUs/GPUs free, some busy). You can potentially get resources on these nodes.
      \item \textbf{mix-} - Mixed state with some issue (the minus indicates a problem). Node has available resources but may have hardware issues.
      \item \textbf{drain} - Node is being drained (no new jobs accepted, finishing current jobs). Jobs can finish but no new jobs will start.
      \item \textbf{drng} - Node is currently draining (actively finishing jobs before maintenance).
      \item \textbf{down*} - Node is offline/broken (asterisk indicates it's not responding).
      \item \textbf{drain*} - Node is drained with issues (asterisk indicates problems).
      \item \textbf{plnd} - Node is in planned maintenance.
    \end{enumerate}
  \end{definition} 

  Sometimes, you may want to focus on the nodes rather than the partitions, since the nodes are what you will be connecting to. Therefore, you can list the nodes first and detail their specs. The \texttt{-N} flag makes it node-centric. 

  \begin{lstlisting}
    >>>sinfo -N              
    NODELIST                 NODES    PARTITION STATE 
    bhuwan-01                    1       bhuwan mix   
    compsci-cluster-fitz-01      1 compsci-gpu* mix   
    compsci-cluster-fitz-02      1 compsci-gpu* idle  
    compsci-cluster-fitz-03      1 compsci-gpu* idle  
    compsci-cluster-fitz-04      1 compsci-gpu* mix   
    compsci-cluster-fitz-05      1 compsci-gpu* mix   
    compsci-cluster-fitz-06      1 compsci-gpu* mix   
  \end{lstlisting}

  However, it only shows the node name, partition, and state. If we want to look at more detailed specs, we can put the \texttt{long} tag, which shows more detail on CPUs and the memory. 

  \begin{lstlisting}
    >>> sinfo -N --long
    NODELIST                    PARTITION       STATE CPUS    S:C:T MEMORY      WEIGHT AVAIL_FE
    bhuwan-01                      bhuwan       mixed 64     2:32:1   101095        1    (null)
    compsci-cluster-fitz-01  compsci-gpu*       mixed 48     2:12:2   768414        49    a5000
    compsci-cluster-fitz-02  compsci-gpu*        idle 48     2:12:2   768414        49    a5000
    compsci-cluster-fitz-03  compsci-gpu*        idle 48     2:12:2   768414        49    a5000
    compsci-cluster-fitz-04  compsci-gpu*       mixed 48     2:12:2   768414        49    a5000
    compsci-cluster-fitz-05  compsci-gpu*       mixed 48     2:12:2   768414        99    a6000
    compsci-cluster-fitz-06  compsci-gpu*       mixed 48     2:12:2   768414        99    a6000
  \end{lstlisting} 

  Let's clarify what the additional columns mean. 
  \begin{enumerate}
    \item The \texttt{CPUS} refer to the number of CPUs on the node. This number should be equal to the product of the \texttt{S:C:T}. 
    \item The \texttt{S:C:T} refers to the Socket:Core:Thread configuration. The socket is the number of physical CPU packages (chips) installed on the motherboard. Each socket contains one complete processor. The core is the number of physical processing cores per socket. Each core can execute instructions independently. The thread is number of hardware threads per core, enabled by technologies like Intel's Hyperthreading or AMD's SMT (Simultaneous Multithreading)
    \item The \texttt{MEMORY} is the available RAM in MB. 
    \item The \texttt{WEIGHT} means priority weight for job scheduling. 
    \item \texttt{AVAIL\_FEATURES} provides simple text labels like \texttt{a6000} that can be used as job constraints to target specific node types. 
  \end{enumerate}

  We can see CPU information, but for most cases we would like to focus on the GPU specs. This is usually stored in the \texttt{GRES} (generic resources) column, which specifies the actual GPU hardware installed in each node. It is in the format \texttt{gpu:type:count} (e.g. \texttt{gpu:a6000:4} for four A6000 GPUs). Unfortunately, it is not contained in the long tag. To inspect this, we will need to specify the format using the \texttt{-o} tag. 

  \begin{lstlisting}
    sinfo -N -o "%.25N %.6t %.4c %.4X %.8m %.20G %.15f"
                   NODELIST  STATE CPUS SOCK   MEMORY                 GRES  AVAIL_FEATURES
                  bhuwan-01    mix   64    2  1010955   gpu:a6000:8(S:0-1)          (null)
    compsci-cluster-fitz-01    mix   48    2   768414   gpu:a5000:4(S:0-1)           a5000
    compsci-cluster-fitz-02   idle   48    2   768414   gpu:a5000:4(S:0-1)           a5000
    compsci-cluster-fitz-03   idle   48    2   768414   gpu:a5000:4(S:0-1)           a5000 
  \end{lstlisting}

  Note that there is a lot of overlap between the \texttt{GRES} and \texttt{AVAIL\_FEATURES} columns, but they are fundamentally different. While \texttt{GRES} defines what hardware exists, while \texttt{AVAIL\_FEATURES} offers convenient tags for job submission, which will be elaborated later. 

\subsubsection{Interactive Jobs with srun}

  There are two ways to submit or run jobs. In the beginning, you might prototype a lot and want to have continuous access to your code on a compute node as you debug. At this point, it is best to use an \textbf{interactive} shell. 

  In interactive mode, you are changing your host in your current terminal session from your login node to the compute node, allowing you to iterate and debug easily. You can connect to the first available compute node by running \texttt{srun} with the \texttt{-i} flag (for interactive) and \texttt{pty} (for pseudo-terminal) set to whatever shell you want. 

  \begin{lstlisting}
    >>> hostname 
    compsci-login-03
    >>> srun --pty zsh -i 
    srun: job 9601353 queued and waiting for resources
    srun: job 9601353 has been allocated resources
    >>> bin hostname
    linux54
  \end{lstlisting}

  This is the minimalistic way, but you would like more control over the session you want to connect. 

  \begin{definition}[Node Specification Options]
    Here is more detail on tags related to specifying which node you want. 
    \begin{enumerate}
      \item \texttt{--partition=compsci-gpu}. Look for nodes in the \texttt{compsci-gpu} partition. 
      \item \texttt{--gres=gpu:a5000:1}. Look for nodes that have \texttt{gpu:a5000} in the \texttt{GRES} column. This is a direct resource request. 
      \item \texttt{--constraint=a6000}. Look for nodes that have \texttt{a6000} in the \texttt{AVAL\_FEATURES} column. This is a feature-based node selection. 
      \item \texttt{-vvv} for verbosity. 
    \end{enumerate} 
  \end{definition} 


  \begin{definition}[Constraint Options]
    Another natural property to have is the ability to limit resources beyond the physical capabilities of each node. 
    \begin{enumerate}
      \item \texttt{--time=1:00:00}. Run it for 1 hour.  
      \item \texttt{--mem=16G} 
      \item \texttt{--cpus-per-task=4}
    \end{enumerate}
  \end{definition}

  Here is an example. Even better, you can use tmux so that your progress is not lost when you lose connection. 
  \begin{lstlisting}
    srun --nodes=1 --ntasks-per-node=1 --partition=compsci-gpu --time=1:00:00 --gres=gpu:a5000 --pty zsh -i -vvv
  \end{lstlisting}

\subsubsection{Batch Jobs with sbatch}

  Once you are done, you probably want to run large-scale tests. This requires you to submit some script with different arguments on not just one node, but multiple runs (perhaps over multiple combinations of hyperparameters) over different nodes. This is best done through \textbf{batch runs}, called through the \texttt{sbatch} command. 

  We will first perform the simplest sbatch script, and I heavily recommend to run this as a preliminary troubleshooting step in order to make sure everything is working properly before you even try to run your script in a batch job.

  \begin{theorem}[Batch Job to Print Hostname]
    We will connect to a compute node and simply print out the hostname to verify that we have successfully connected. This command that you run when you connect to a node can be inputted into the \texttt{--wrap} argument. The standard out and error is by default logged in \texttt{slurm-jobid.out} in the directory where you submitted the sbatch script in. 

    \begin{figure}[H]
      \centering 
      \begin{lstlisting}
        >>> ls
        >>> hostname                
        compsci-login-03
        >>> sbatch --wrap="hostname"
        Submitted batch job 9601364
        >>> ls
        slurm-9601364.out
        >>> cat slurm-9601364.out 
        linux54 
      \end{lstlisting}
      \caption{I start in the login node \texttt{compsci-login-03}. Then I call \texttt{hostname} which returns \texttt{linux54}.}
    \end{figure} 
  \end{theorem}

  Batch jobs give us more flexibility in some ways and less in others. For example, we don't need to be connected to the session if we submit a batch script.\footnote{Though we can do this with interactive runs on tmux.} Running dozens of batch jobs may be hard to keep track of, so it is recommended to make a custom name for each one. Additionally, in interactive shells, we can see standard output and error in real time. We do not have such an interface in batch runs, so we must direct it to some other file. Usually, it is a file called \texttt{jobname-jobid.out} and \texttt{jobname-jobid.err}. These are additional arguments that are a good idea to set when running batch jobs. 

  \begin{definition}[Naming and Logging Options]
    All of the tags representing 
    \begin{enumerate}
      \item \texttt{--job-name}. The job name, which can be seen under the \texttt{NAME} column when calling \texttt{squeue}. 
      \item \texttt{--output}. Where you should log standard output. 
      \item \texttt{--error}. Where you should log standard error. 
    \end{enumerate}
  \end{definition}

  Great, so we have introduced our first way to run a batch job: through the \texttt{--wrap} argument. 

  \begin{theorem}[Run Batch Jobs with Wrap]
    Here is a complete script. The \texttt{\%x} refers to the name and \texttt{\%j} refers to the job id. 
    \begin{lstlisting}
      sbatch \
          --job-name="whatever" \
          --output="logs/%x-%j.out" \
          --error="logs/%x-%j.err" \
          --mem=16G \
          --cpus-per-task=4 \
          --gres=gpu:1 \
          --time=4:00:00 \
          --wrap="
              cd /usr/project/xtmp/mb625/runs
              python run.py 
          "
    \end{lstlisting}
  \end{theorem}

  While you can put all of your commands to run on the compute node into the \texttt{--wrap} argument, this might be messy and inconvenient. A more efficient thing to do is to run it as a bash script.  

  \begin{theorem}[Run Batch Jobs as a Bash Script]
    You create a normal bash script, but you input the sbatch arguments at the top as such. After that, you can write your script. 
    \begin{lstlisting}
      >>> cat run.sh  
      #!/usr/bin/env bash
      #SBATCH --mem=40gb
      #SBATCH --time=48:00:00
      #SBATCH --partition=compsci-gpu
      eval "\$(conda shell.bash hook)"
      conda activate intnn
      python main.py 
      >>> sbatch run.sh
    \end{lstlisting}
  \end{theorem}

  \begin{theorem}[Tips and Workflow for Batch Scripts]
    I want to end with some convenient tips for debugging on batch runs. Unlike interactive runs, where you can just see where you are and debug code in real time, you have less visibility for batch runs. You should not assume that the conda environment will automatically be set up, nor that you will start in a specific directory. So before you have your scripts to run, you should probably just run a dummy script that ensures that everything up to the pre-script echo statement runs. 
    \begin{enumerate}
      \item Ensure that you are using the proper shell, with environment variables properly set (especially \texttt{PATH}). 
      \item Source and activate your conda environment. 
      \item cd to the desired directory. 
      \item Echo that you are about to start, along with relevant logs. 
      \item Run your script. 
      \item Echo that you have just finished, along with relevant logs. 
    \end{enumerate}
  \end{theorem}
 
\subsubsection{Checking Queue with squeue}

  To check the status of submitted, running, or canceled jobs, \texttt{squeue} is your friend. Just running squeue will list out all jobs from all users (?).  

  \begin{lstlisting}
    >>> squeue           
         JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
       9597293    bhuwan gpu-bash     rx55  R 1-00:12:41      1 bhuwan-01
       9597071   compsci fmri_pre    zy151  R   13:33:36      1 linux25
       9601348   compsci    alpha     tqt3  R      29:53      1 linux24
       9597328   compsci sys/dash    yw628  R   22:20:41      1 linux31
       9597434 compsci-g cheshire    xz424 PD       0:00      1 (Resources)
       9597436 compsci-g   iSB619    xz424 PD       0:00      1 (Priority)
       9597673 compsci-g cheshire    xz424 PD       0:00      1 (Priority)
       9591596 compsci-g      zsh    rt195  R 3-00:10:50      1 linux54
       9600314 compsci-g run_all_    sf349  R    3:03:14      1 linux52
     9590035_0 compsci-g sw-1q57j   cjb131  R 3-16:55:33      1 compsci-cluster-fitz-19
    9597248_47 compsci-g sn_label    dt161  R   12:45:35      1 compsci-cluster-fitz-20
    9597248_48 compsci-g sn_label    dt161  R   12:45:35      1 compsci-cluster-fitz-20
    9597248_49 compsci-g sn_label    dt161  R   12:45:35      1 compsci-cluster-fitz-20 
  \end{lstlisting} 

  Rather than looking at all queued jobs, you probably want to focus on a specific user. You can either filter the current runs from yourself, 

  \begin{lstlisting}
    >>> squeue --me      
       JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
    9601352 compsci-g      zsh    mb625  R       7:18      1 linux54 
  \end{lstlisting}

  or from a specific user. 

  \begin{lstlisting}
    >>> squeue -u cjb131
         JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
     9590035_0 compsci-g sw-1q57j   cjb131  R 3-16:59:45      1 compsci-cluster-fitz-19 
  \end{lstlisting} 

  Less frequently, you might want to focus on a specific job ID. You can input this through the \texttt{-j} (job) tag. 

  \begin{lstlisting}
    >>> squeue -j 9597376
       JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
     9597376     rudin vscode-s    dt161  R    9:02:39      1 rudin-01 
  \end{lstlisting}
  
\subsubsection{Canceling Jobs with scancel}

  To cancel a current job, \texttt{scancel} is your friend. To cancel a specific jobs, you just write it down. 

  \begin{lstlisting}
    >>> scancel 9601352   
    srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
    slurmstepd: error: *** STEP 9601352.0 ON linux54 CANCELLED AT 2025-07-12T15:19:14 ***   
  \end{lstlisting}

  To cancel all of your own jobs, run 
  \begin{lstlisting}
    >>> scancel --me          # one way
    >>> scancel -u $whoami    # another way
  \end{lstlisting} 

\subsubsection{Multi-GPU or Multi-Node Runs}

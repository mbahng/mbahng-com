An informal statement of concentration of measure is the following: \textit{If $X_1, \ldots, X_n$ are independent random variables, then the random variable $f(X_1, \ldots, X_n)$ is "close" to its mean $\mathbb{E}[f(X_1, \ldots, X_n)]$ provided that the function $f(x_1, \ldots, x_n)$ is not too "sensitive" to any of the coordinates $x_i$.} Intuitively, say that we have a bunch of independent random variables $X_i$ and sample from them, to get some values $x_i$. Calculating $f(x_1, \ldots, x_n)$, we have sampled from $f(X_1, \ldots, X_n)$. Since $f$ depends smoothly w.r.t. its arguments, to drastically change $f$, we must drastically change all the arguments. This is not likely, since all the $X_i$'s are independent. 

Most of our intuition about probability in low-dimensional spaces breaks down in high-dimensional ones (on the order of perhaps $10$ or $20$). We start off with two geometric examples in high-dimensional space. 

\begin{example}[Uniform Measure on Sphere]
Let $\mu_n$ be the uniform probability distribution on the $n$-sphere $\mathbf{S}^{n} \subset \mathbb{R}^{n+1}$. That is, let us consider any measurable set $A \subset \mathbb{S}^{n}$ such that $\mu_n (A) \geq 1/2$. Then, if we let $d(x, A)$ be the geodesic distance between $x \in \mathbb{S}^n$ and $A$ , we define the expanded set 
\[A_t = \{x \in \mathbb{S}^n \mid d(x, A) < t\}\]
and it turns out that 
\[\mu_n (A_t) \geq 1 - e^{- (n -1) t^2 / 2}\]
which states that given \textit{any} length $t > 0$, no matter how small, $A_t$ almost covers the whole space. Then, for large enough $n$, $\mu_n$ is highly concentrated around the equator. 
\end{example}

Note that the bounds decay \textit{exponentially} (or of greater order). 

\begin{example}[Uniform Measure on Cube]

\end{example}

\begin{example}[High Dimensional Gaussian]
Given iid $X_1, \ldots, X_n \sim \mathcal{N}(0, \sigma^2)$, then let $\mathbf{X}$ be the random $n$-vector of these random variables. Then, the random variable 
\[||\mathbf{X}|| = \sqrt{X_1^2 + \ldots, X_n^2}\]
has a distribution that is very concentrated around the expectation 
\[\mathbb{E}[||\mathbf{X}||] = \sqrt{\frac{n}{3}}\]
\end{example}

Naturally, this concentration phenomenon extends to random variables. 

\begin{example}
Let us have iid random variables $X_i$ with $\mathbb{P}(X_i = 1) = 1/2$ and $\mathbb{P}(X_i = -1) = 1/2$. Then, let's define $S_n = \sum_{i=1}^n X_i$. The strong law of large numbers tell us that 
\[\frac{S_n}{n} \xrightarrow{a.s.} 0\]
while the central limit theorem tells us that 
\[\frac{S_n}{\sqrt{n}} \xrightarrow{d} \mathcal{N}(0, 1)\]
since $\mathbb{E}[X_i] = 0$ and $\mathrm{Var}[X_i] = 1$. The CLT result shows us that the fluctuations (variance) of $S_n$ of are order $n$. However, note that $|S_n|$ can take values as large as $n$, so the maximum value of $S_n / n$ is of order $1$. If we measure $S_n$ using this scale, then $\frac{S_n}{n}$ is essentially $0$. The actual bound looks like 
\[\mathbb{P} \bigg( \frac{|S_n|}{n} \geq r \bigg) \leq 2 e^{-n r^2 / 2}\]
\end{example}

\begin{lemma}[Markov's Inequality]
Given any random variable $X$, we have 
\[\mathbb{P}(X \geq \alpha) \leq \frac{\mathbb{E}[X]}{\alpha}\]
\end{lemma}

\begin{lemma}[Chebyshev's Inequality]
Given $X$ with finite variance and expectation, we have 
\[\mathbb{P}(|X - \mathbb{E}[X]| \geq \alpha) \leq \frac{\Var[X]}{\alpha^2}\]
\end{lemma}

An inequality that we will use often in proofs is Jensen's inequality. 

\begin{lemma}[Jensen's Inequality]
Given a convex function $g: \mathbb{R} \rightarrow \mathbb{R}$ and random variable $X$, we have 
\[g(\mathbb{E}[X]) \leq \mathbb{E}[g(X)]\]
\end{lemma}
\begin{proof}
We will assume that $f$ is differentiable for simplicity and let $\mathbb{E}[X] = \mu$. Define the linear function centered at $\mu$ to be $l(x) \coloneqq f(\mu) + f^\prime (\mu) (x - \mu)$. Then, we know that $f(x) \geq l(x)$ for all $x$, so 
\begin{align*}
    \mathbb{E}[f(X)] & \geq \mathbb{E}[ l(X)] \\ 
    & = \mathbb{E}[f(\mu) + f^\prime (\mu) \, (X - \mu)] \\
    & = \mathbb{E}[f(\mu)] + f^\prime (\mu) ( \mathbb{E}[X] - \mu) \\
    & = \mathbb{E}[f(\mu)] \\
    & = f(\mathbb{E}[X])
\end{align*}
\end{proof}

\begin{definition}[Lipschitz Continuity]
A function $f: (X, d_X) \longrightarrow (Y, d_Y)$ is \textbf{Lipschitz continuous}, with Lipschitz constant $A$, if it satisfies 
\[d_Y \big( f(\mathbf{x}), f(\mathbf{y})\big) \leq A \, d_X (\mathbf{x}, \mathbf{y})\]
for all $\mathbf{x}, \mathbf{y} \in X$. 
\end{definition}

\section{Matrices}

\subsection{Representations of Linear Maps} 

  We now describe the construction of the matrix realization of a linear map from $V \longrightarrow U$. In order to do this, we \textit{must} define a basis for each $V$ and $U$. If $V = U$, then we usually define the same basis for both the domain and codomain. 

  Let the basis for $U$ be $\{ u_1, u_2, ..., u_n\}$ and the basis of $V$ be $\{v_1, v_2, ..., v_m\}$. In fact, the assignment of this specific basis is a linear map in of itself. That is, 
  \begin{align*}
    i: U \longrightarrow \mathbb{F}^n, & i(u_\alpha) = e_\alpha  \\
    j: V \longrightarrow \mathbb{F}^m, & j(v_\beta) = e_\beta 
  \end{align*}
  However, we do not usually include this transformation in the notation. We just denote $i(u)$ as $u$ and $j(v)$ as $v$. Every vector $u \in U$ can then be represented as a linear combination
  \begin{equation}
    u = \sum_{j=1}^n c_j u_j
  \end{equation}
  By linearity of the mapping $A: U \longrightarrow V$, 
  \begin{equation}
    A u = A \bigg( \sum_{j=1}^n c_j u_j \bigg) = \sum_{j=1}^n c_j A u_j 
  \end{equation}
  This means that $A$ can be completely, uniquely determined by defining how it maps the $n$ basis vectors $u_j \in U$, that is, by defining the values 
  \begin{equation}
    A u_1, A u_2, ..., A u_{n-1}, A u_n
  \end{equation}
  Each $A u_j$ will be an element of $V$, which means that $A u_j$ can be decomposed into the linear combination of $v_i$'s. That is, 
  \begin{equation}
    A u_j = \sum_{i=1}^m a_{i j} v_i, \; j = 1, 2, ..., n 
  \end{equation}
  We are done. Given the basis of the domain and codomain, the elements $a_{i j}$ are precisely the entries of the $m \times n$ matrix $(1 \leq i \leq m, 1 \leq j \leq n)$. 
  \begin{equation}
    v = A u \iff 
    \begin{pmatrix}
     b_1 \\ b_2 \\ \vdots \\ b_m
    \end{pmatrix}
    = \begin{pmatrix}
     a_{1 1} & a_{1 2} & \ldots & a_{1 n} \\
     a_{2 1} & a_{2 2} & \ldots & a_{2 n} \\
     \vdots & \vdots & \ddots & \vdots \\
     a_{m 1} & a_{m 2} & \ldots & a_{m n} 
    \end{pmatrix} \begin{pmatrix}
     c_1 \\ c_2 \\ c_3 \\ \vdots \\ c_n
    \end{pmatrix}
  \end{equation}

  It is important to note that the matrix is \textit{not} $A$ in of itself. In the most rigorous sense, the matrix $A$ is really just equal to the composition of mappings $ j^{-1} A i$, but for simplicity it is just written as $A$. It is just one representation of a linear map given the two bases of the domain and codomain. Furthermore, as soon as one writes down a matrix to represent a linear map, they are automatically assuming some choice of basis given by $i$ and $j$. 

  \begin{definition}
    The \textbf{algebra} of $n \times n$ matrices over field $\mathbb{F}$, denoted Mat$(n, \mathbb{F})$, is defined with regular matrix addition and multiplication. 
  \end{definition}

  Furthermore, we can define the mapping between linear operators $T: \mathbb{F}^n \longrightarrow \mathbb{F}^m$ and $m \times n$ matrices (given that there is a basis for both $\mathbb{F}^n, \mathbb{F}^m$. 

  \begin{definition}
    The linear mapping between the algebras 
    \begin{equation}
      \rho: \text{Hom}(\mathbb{F}^n, \mathbb{F}^m) \longrightarrow \text{Mat}(m \times n, \mathbb{F})
    \end{equation}
    is a multiplicative group homomorphism. This mapping that assigns abstract group elements of linear mappings to matrices is called a \textbf{representation}. 
  \end{definition}

  \begin{theorem}
    Mat$(n, \mathbb{F}) \simeq $ End$(\mathbb{F}^n)$ 
  \end{theorem}
  \begin{proof}
    A matrix is completely determined by the basis mapping $i$. By definition, a linear mapping over $\mathbb{F}$ is a basis mapping if and only if it is an element of End$(\mathbb{F}^n)$. 
  \end{proof}

  Note that the composition operation in the algebra of linear operators is realized as the operation of matrix multiplication. These are two distinct operations that are related only through the basis mappings $i$ and $j$. 

  \begin{example}
    Let $\alpha: \mathbb{R}^2 \longrightarrow \mathbb{R}^2$ be the linear transformation of the counterclockwise rotation by $\theta$ and $\beta: \mathbb{R}^2 \longrightarrow \mathbb{R}^2$ be the counterclockwise rotation of $\phi$. Then the matrix representation of $\alpha \circ \beta$ is 
    \begin{align}
        & \begin{pmatrix}
      \cos{\theta} & - \sin{\theta} \\
      \sin{\theta} & \cos{\theta}
      \end{pmatrix} \begin{pmatrix}
      \cos{\phi} & - \sin{\phi} \\
      \sin{\phi} & \cos{\phi} 
      \end{pmatrix} \\
       & = \begin{pmatrix}
      \cos{\theta} \cos{\phi} - \sin{\theta} \sin{\phi} & - \sin{\phi} \cos{\theta} - \cos{\phi} \sin{\theta} \\
      \sin{\theta} \cos{\phi} + \cos{\theta} \sin{\phi} & - \sin{\theta} \sin{\phi} + \cos{\theta} \cos{\phi}
      \end{pmatrix}
    \end{align}
    But the counterclockwise rotation by $\theta$ and then $\phi$ is really just a counterclockwise rotation by $\theta + \phi$, which has the matrix representation
    \begin{equation}
      \begin{pmatrix}
      \cos{(\theta + \phi)} & - \sin{(\theta + \phi)} \\
      \sin{(\theta + \phi)} & \cos{(\theta + \phi)}
      \end{pmatrix}
    \end{equation}
    Since both matrices must be equivalent, this produces the trigonometric identities for angle addition.
    \begin{align*}
      \sin{(\theta + \phi)} = \sin{\theta} \cos{\phi} + \cos{\theta} \sin{\phi} \\
      \cos{(\theta + \phi)} = \cos{\theta} \cos{\phi} - \sin{\theta} \sin{\phi}
    \end{align*}
  \end{example}

  \begin{theorem}
    Given mappings $A_i \in$ End$(V_i)$ for $i = 1, 2, ..., n$, the matrix representation of the induced linear mapping $A_1 \oplus A_2 \oplus ... \oplus A_n$ is the block matrix 
    \begin{equation}
      \begin{pmatrix}
      A_1 & & & \\
      & A_2 & & \\
      & & \ddots & \\
      & & & A_n
      \end{pmatrix}: \bigoplus_{i=1}^n V_i \longrightarrow \bigoplus_{i=1}^n V_i
    \end{equation}
  \end{theorem}

\subsection{Change of Basis}

  \begin{definition}[Active, Passive Transformation]
    A linear transformation $A$ that maps every vector from $U$ to a vector in $V$ is called an \textbf{active transformation}. However, a \textbf{passive transformation}, or a \textbf{change of basis transformation}, linearly transforms the set of basis vectors to another set of basis vectors within the same space. That is, a passive transformation takes the components of a vector $v$ with respect to basis $\{e_1, e_2, ..., e_n\}$ and merely represents $v$ with respect to another set of basis $\{f_1, f_2, ..., f_n\}$. 
  \end{definition}

  It is obvious that a passive transformation in $V$ is an element of End$(V)$. But note that an element of End$(V)$ could be interpreted \textit{both} as a passive and active transformation. Usually, the context will make it clear whether we are interpreting a transformation as passive or active. We now provide the construction of the change of basis. 

  Suppose ${e_1, e_2, ..., e_n}$ is a basis for vector space $V$ and ${f_1, f_2, ..., f_n}$ is another basis for $V$. So, every basis vector $f_i$ can be presented as a linear combination of the old basis vectors. 
  \begin{equation}
    f_j = \sum_{i =1}^{n} s_{i j} e_i \quad \text{for all i, j}
  \end{equation}
  A general vector $x \in V$ will transform as such
  \begin{equation} 
    \label{eq1}
    \begin{split}
      x & = \sum_{j} y_j f_j \quad \text{for} \ y_{1}, y_{2}, ... \in \mathbb{F} \\
      & = \sum_{i,j} y_j s_{i j} e_i \\
      & = \sum_{i} \Big( \sum_{j} s_{i j} y_j \Big) e_i \\
      & = \sum_{i} x_i e_i \implies x_i = \sum_{j} s_{i j} y_j
    \end{split}
  \end{equation}
  Similarly to the process of how we constructed matrix representations of linear operators, this process makes it clear that $s_{i j}$ are the entries of the $n \times n$ matrix representation of the passive mapping $S$. The final line of the equation above can be expressed, in terms of matrices, as 
  \begin{equation}
    \begin{pmatrix}x_1\\x_2\\...\\...\\x_n\end{pmatrix} = 
    \begin{pmatrix} \\ \\ & & & S & & &  \\\\\\\end{pmatrix} \begin{pmatrix}
    y_1\\y_2\\...\\...\\y_n\end{pmatrix}
  \end{equation}
  This is a change of basis, since both the coefficients $x_i$ and $y_i$ represent the same vector $x$ in $V$, but through a different basis determined by $S$. Note that $S$ must be an invertible matrix since we are mapping bases to bases. So, given that $x = S y$, if $Ax = b$ is a matrix equation, then
  \begin{equation}
    A x = b \implies A S y = S b^\prime \implies S^{-1} A S y = b^\prime
  \end{equation}
  where $b^\prime$ is the set of new coefficients for the vector with respect to the basis induced by $S$. 
  This leads to the concept of matrix similarities. We once again note that whenever we create a matrix as an $m \times n$ entry of numbers, we are intuitively fixing a basis (not necessarily orthonormal, even) for the vectors that the matrix is transforming on. For example, the matrix $A$ in $y' = Ax'$ transforms the vector $x'$ with respect to the basis which $x'$ is in, i.e. the basis ${e_1', e_2', ..., e_n'}$. This transformation is not the same if it were to act on the vector $x$, which is determined by the basis ${e_1, e_2, ..., e_n}$. Therefore, we must also "change" the matrix A acting on $x'$ in order to account for the change in basis from $x'$ to $x$. This change is 
  \begin{equation}
    A \rightarrow B = S A S^{-1}
  \end{equation}
  where matrix $A$ represents the transformation with respect to basis formed by the column vectors of $S$, and $B$ represents the same transformation with respect to the basis formed by the column vectors of $S^{-1}$. 

  \begin{definition}[Similar Matrices]
    Two matrices $A$ and $B$ are \textbf{similar} if and only if there exists an invertible matrix $S$ such that $B = S A S^{-1}$. A and B both represent the same transformation $T$ but merely in different bases. Matrix similarity is a relation that partitions the $n^2$-dimensional matrix algebra Mat$(n, \mathbb{R})$ into similarity classes. 
  \end{definition}

\subsection{Solving Systems of Equations}

  \begin{definition}[Linear System of Equations]
    Fix a field $\mathbb{F}$. A \textbf{linear equation} with variables $x_1, x_2, ..., x_n$ is in the form 
    \begin{equation}
      a_1 x_1 + a_2 x_2 + a_3 x_3 + ... + a_n x_n = b
    \end{equation}
    where the \textbf{coefficients} $a_i$ and the \textbf{free term} $b$ belong to $\mathbb{F}$. If $b = 0$, then $(3)$ is called a \textbf{homogeneous equation} and if $b \neq 0$, then it is called a \textbf{inhomogeneous equation}. 
  \end{definition}

  A system of $m$ linear equations with $n$ variables has the following general form 
  \begin{align*}
    &a_{1 1} x_1 + a_{1 2} x_2 + ... + a_{1 n} x_n = b_1 \\
    &a_{2 1} x_1 + a_{2 2} x_2 + ... + a_{2 n} x_n = b_2 \\
    &..............................................=....\\
    &a_{m 1} x_1 + a_{m 2} x_2 + ... + a_{m n} x_n = b_m
  \end{align*}
  By matrix multiplication, this system is equal to the matrix equation $A x = b$.
  \begin{equation}
    \begin{pmatrix}
     a_{1 1} & a_{1 2} & \ldots & a_{1 n} \\
     a_{2 1} & a_{2 2} & \ldots & a_{2 n} \\
     \vdots & \vdots & \ddots & \vdots \\
     a_{m 1} & a_{m 2} & \ldots & a_{m n} 
    \end{pmatrix} \begin{pmatrix}
     x_1 \\ x_2 \\ x_3 \\ \vdots \\ x_n
    \end{pmatrix} = \begin{pmatrix}
    b_1 \\ b_2 \\ \vdots \\ b_m
    \end{pmatrix}
  \end{equation}
  That is, given a linear transformation $A: \mathbb{F}^n \longrightarrow \mathbb{F}^m$ and a vector $b \in \mathbb{F}^m$, we must find the preimage of $b$ under $A$. Clearly, $x$ is a solution of this matrix equation if and only if it is a solution of the system of equations. 

  We can interpret this matrix equation in two ways. First, we introduce the \textit{hyperplane interpertation}. The solution to each linear equation of $n$ variables represents an affine hyperplane in $\mathbb{F}^n$. Therefore, the solutions to the system of $m$ linear equations is simply the intersection of the $m$ affine hyperplanes of dimension $n-1$ within $\mathbb{R}^n$. That is, $x$ is a solution of $A x = b$ if and only if  
  \begin{equation}
    x \in \bigcap_{i = 1}^m \Big\{ (x_1, x_2, ..., x_n) \; | \; \sum_{j = 1}^n a_{i j} x_j = b_i \Big\}
  \end{equation}
  The \textit{column space interpretation} presents $A x = b$ in this equivalent form. 
  \begin{equation}
    x_1 \begin{pmatrix}
    a_{1 1} \\ a_{2 1} \\ \vdots \\ a_{m 1}
    \end{pmatrix} + x_2 \begin{pmatrix}
    a_{1 2} \\ a_{2 2} \\ \vdots \\ a_{m 2}
    \end{pmatrix} + \ldots + x_n \begin{pmatrix}
    a_{1 n} \\ a_{2 n} \\ \vdots \\ a_{m n}
    \end{pmatrix} = \begin{pmatrix}
    b_1 \\ b_2 \\ \vdots \\ b_m
    \end{pmatrix}
  \end{equation}
  That is, the solutions $x_1, x_2, ..., x_n$ are precisely the coefficients of the linear combination of the column vectors of $A$ that add up to vector $b$. Equivalently, it is the realization of vector $b$ with respect to the coordinate system of the column vectors of $A$. Note that the column space need not be a basis of $\mathbb{F}^m$. It does not need to be linearly independent nor does it need to span $\mathbb{F}^n$. 

  \begin{definition}[Coefficient Matrix]
    The matrix $A$ under the system is called the \textbf{coefficient matrix} and the matrix 
    \begin{equation}
      \Tilde{A} \equiv \begin{pmatrix}
      | & | & ... & | & | \\
      a_1 & a_2 & ... & a_n & b \\
      | & | & ... & | & | 
      \end{pmatrix} \equiv \begin{pmatrix}
      a_{1 1} & a_{1 2} & \ldots& a_{1 n} & b_1\\
       a_{2 1} & a_{2 2} & \ldots & a_{2 n} & b_2\\
      \vdots & \vdots & \ddots & \vdots & \vdots\\
       a_{m 1} & a_{m 2} & \ldots & a_{m n} & b_m
      \end{pmatrix}
    \end{equation}
    is called the \textbf{extended matrix}. 
  \end{definition}

  \begin{definition}
    A system of equations is called \textbf{compatible} if it has at least one solution and \textbf{incompatible} otherwise. 
  \end{definition}

  \begin{definition}[Elementary Transformation]
    An \textbf{elementary transformation} of a system of linear equation is one of the following three types of transformations
    \begin{enumerate}
      \item adding an equation multiplied by a number to another \textit{later} equation
      \item interchanging two equations
      \item multiplying an equation by a nonzero number
    \end{enumerate}
  \end{definition}

  \begin{definition}[Elementary Row Transformation]
    An \textbf{elementary row transformation} of a matrix is one of the following three types of transformations
    \begin{enumerate}
      \item adding a row multiplied by a number to another \textit{later} row
      \item interchanging two rows
      \item multiplying a row by a nonzero number
    \end{enumerate}
  \end{definition}

  Clearly, these two definitions are equivalent since every elementary transformation of a system has a corresponding row transformation in its extended matrix. Given the $i$th row of a matrix, a "later" row means the $j$th row, where $j > i$. Defining property (i) to add to a later row does not actually restrict where we can add rows to, since property (ii) allows us to add any scalar multiple of any row to any other row. We define it this way for future convenience in defining the $L U P$ Decomposition. 

  \begin{definition}
    The elementary transformations on a $m \times n$ matrix $A$ is equivalent to left matrix multiplication by the following $m \times m$ matrices. Due to the following difficulty in presenting these matrices in a general form, we present them in the specific $4 \times 4$ case and hope that the reader can extrapolate this process to general matrices. 
    \begin{enumerate}
      \item Adding row $i$ multiplied by scalar $\alpha$ to row $j$ (where $j > i$) is denoted $E^1_{\alpha \times i + j}$. The matrix is the identity matrix with $\alpha$ in the $(j, i)$ position. 
      \begin{equation}
        E^1_{2 \times 1 + 2} = \begin{pmatrix}
        1&0&0&0 \\ 2&1&0&0 \\ 0&0&1&0 \\ 0&0&0&1
        \end{pmatrix}, \;\; E^1_{-3 \times 2 + 4} = \begin{pmatrix}
        1&0&0&0 \\ 0&1&0&0 \\ 0&0&1&0 \\ 0&-3&0&1
        \end{pmatrix}
      \end{equation}
      \item Interchanging the $i$th and $j$th row is denoted by matrix $E^2_{i j}$. Note that these are permutation matrices, or more speficially, transpositions. 
      \begin{equation}
        E^2_{2 3} = \begin{pmatrix}
        1&0&0&0 \\ 0&0&1&0 \\ 0&1&0&0 \\ 0&0&0&1
        \end{pmatrix}, \; \; E^2_{2 4} = \begin{pmatrix}
        1&0&0&0 \\ 0&0&0&1 \\ 0&0&1&0 \\ 0&1&0&0
        \end{pmatrix}
      \end{equation}

      \item Multiplying the $i$th row by a scalar $\alpha$ is denoted by matrix $E^3_{\alpha \times i}$. 
      \begin{equation}
        E^3_{3 \times 3} = \begin{pmatrix}
        1&0&0&0 \\ 0&1&0&0 \\ 0&0&3&0 \\ 0&0&0&1
        \end{pmatrix}, \;\; E^3_{7 \times 1} = \begin{pmatrix}
        7&0&0&0 \\ 0&1&0&0 \\ 0&0&1&0 \\ 0&0&0&1
        \end{pmatrix}
      \end{equation}
    \end{enumerate}
  \end{definition}

  % checkpoint

  \begin{theorem}
    Each elementary matrix is invertible and their inverses are also elementary matrices. More specifically, 
    \begin{enumerate}
      \item $(E^1_{\alpha \times i + j})^{-1} = E^1_{-\alpha \times i + j}$ (same matrix but $\alpha$ changed to -$\alpha$)
      \item $(E^2_{i j})^{-1} = E^2_{i j}$ (same matrix) 
      \item $(E^3_{\alpha \times i})^{-1} = E^{3}_{(1/\alpha) \times i}$ (same matrix but $\alpha$ changed to $1 / \alpha$)
    \end{enumerate}
  \end{theorem}

  Elementary column operations of are equivalent to right multiplication of matrices. 

  \begin{definition}[Pivot]
    The \textbf{pivot} of a row $(a_1, a_2, ..., a_n)$ is its first nonzero element. If this element is $a_k$, then $k$ is the \textbf{index} of the pivot. 
  \end{definition}

  \begin{definition}[Echelon Form]
    A matrix is in \textbf{Echelon form}, or \textbf{row Echelon form}, if 
    \begin{enumerate}
      \item the indices of the pivots of its nonzero rows form a strictly increasing sequence, like steps
      \item zero rows, if they exist, are at the bottom
    \end{enumerate}
    Thus, a matrix in Echelon form is in form
    \begin{equation}
      \begin{pmatrix}
        a_{1 j_1} & * & \ldots & \ldots & * \\
        0 & a_{2 j_2} & * & \ldots & * \\
        0& 0& \ddots & \ldots & * \\
        0& 0& 0& a_{r j_r} & \vdots \\
        0 & 0 & \ldots & 0 & 0
      \end{pmatrix}
    \end{equation}
    where $*$'s represent arbitrary numbers, $a_{i j_i}$'s are nonzero (with indices $j_i$, and the entries to the left of below them are $0$. Property $(i)$ also states that $j_1 < j_2 < ... < j_r$. Let us denote the Echelon form of matrix $A$ as ref$(A)$. 
  \end{definition}

  \begin{theorem}
    Every matrix can be reduced to step form by elementary row transformations. 
  \end{theorem}
  \begin{proof}
    The relevant algorithm used will not be shown here, but we will mention that this procedure is called \textbf{Gauss Elimination}, or \textbf{row reduction}. 
  \end{proof}

  The computational efficiency of Gauss Elimination is well known. Solving a system of $n$ equations with $n$ variables with this algorithm requires approximately $2 n^3 / 3$ operations, meaning that it has arithmetic complexity of $O(n^3)$. However, for matrices of large order, multiple problems can occur. 

  The algorithm generally does not have memory problems if the field is finite or if the coefficients are floating-point numbers. However, if the coefficients are integers or rational numbers, the intermediate entries of the algorithm can grow exponentially large, so bit complexity is exponential. However, there is a variant of Gaussian elimination, called the Bareiss algorithm, that avoids this problem, but has bit complexity of $O(n^5)$. Another problem is numerical instability, caused by the possibility of dividing by numbers very close to $0$. Any such number would have its existing error amplified. Gaussian elimination algorithm is generally known to be stable for positive-definite matrices. 

  Under the column space interpretation, Gaussian Elimination is really just an algorithm that performs a change of basis in steps. Each elementary operation simultaneously changes all of the vectors of the column space in such a way that eventually, this set of vectors will be "nice-looking" with a lot of zero entries. Under the hyperplane interpretation, it is a bit harder to visualize, but it is sufficient to say that each elementary operation either "stretches/compresses" (iii) a hyperplane or it "rotates" (i) the hyperplane around the axis where the solution exists. Either way, the intersection between the hyperplane and the set of solutions do not change. 

  \begin{definition}[Step Form]
    A system of linear equations is said to be in \textbf{step form} if its extended matrix is in Echelon form. 
  \end{definition}

  \begin{definition}
    A matrix is in \textbf{reduced row echelon form}, denoted rref$(A)$, if
    \begin{enumerate}
      \item it is in row echelon form
      \item the pivots are all equal to 1
      \item each column containing a pivot has zeros in all other entries
    \end{enumerate}
  \end{definition}

  \begin{theorem}
    Every matrix can be reduced to reduced row echelon form by elementary row operations. 
  \end{theorem}
  \begin{proof}
    We will briefly describe the method to do this. We first reduce matrix $A$ to step form. Then, we perform the algorithm known as \textbf{back substitution}, where we start with the bottom row and use elementary operations to cancel out terms in upper rows. 
  \end{proof}

  \begin{definition}
    A system of linear equations is said to be \textbf{solved} if its extended matrix is in reduced row echelon form. 
  \end{definition}

  \begin{definition}
    A matrix is called \textbf{lower triangular} if $a_{i j} = 0$ for $i < j$. It is called \textbf{upper triangular} if $a_{i j} = 0$ for $i > j$. A square matrix is \textbf{diagonal} if $a_{i j} = 0$ for $i \neq j$. 
  \end{definition}

  \begin{theorem}
    Elementary operations on either a system of linear equations or its extended matrix does not change its solutions. 
  \end{theorem}
  \begin{proof}
    It is easy to see this is true when performing the computations with the three transformations. We can prove this more abstractly (tbd): Given the system $A x = b$ with $x \in \mathbb{F}^n, b \in \mathbb{F}^m$. We see that $A \in $ Mat$(m \times n, \mathbb{F}) \implies \Tilde{A} \in$ Mat$(m \times (n+1), \mathbb{F})$. Each elementary row transformation on $\Tilde{A}$, denote it $E$, is a bijective mapping. Let us define the mapping 
    \begin{equation}
      \text{sol}: \text{Mat}\big( m \times (n+1), \mathbb{F} \big) \longrightarrow 2^{\mathbb{F}^n}, \; \text{sol} \begin{pmatrix}
      A & b
      \end{pmatrix} \equiv \{ x\in \mathbb{F}^n \; | \; A x = b\} 
    \end{equation}
    where $2^{\mathbb{F}^n}$ is the set of all subsets of $\mathbb{F}^n$. By matrix multiplication, we see that 
    \begin{equation}
       E \begin{pmatrix}
      A & b
      \end{pmatrix} = \begin{pmatrix}
      E A & E b
      \end{pmatrix}
    \end{equation}
    Since $E$ is bijective, it is invertible. So, 
    \begin{align} 
      \text{sol} \big( E \begin{pmatrix}
      A & b \end{pmatrix} \big) & = \text{sol} \begin{pmatrix}
      E A & E b \end{pmatrix} \\ 
      & = \{ x \mid E A x = E b\} \\ 
      & = \{ x \mid A x = b\} \\ 
      & = \text{sol} \begin{pmatrix} A & b \end{pmatrix}
    \end{align}
  \end{proof}

  Note the importance of this theorem. This result is the foundation behind the applications of Jordan Elimination.

  \begin{definition}
    A linear system can have either have no possible solutions (\textbf{overdetermined}), one unique solution, or multiple solutions (\textbf{underdetermined}) (infinite solutions if char$\, \mathbb{F}$ = 0). We can say with probability 1 that given a random $m \times n$ matrix $A$ with random $m$-dimensional vector $b$, the system $A x = b$ has
    \begin{enumerate}
      \item 0 solutions if $m > n$, since there are more equations than variables
      \item 1 solution if $m = n$ with the same number of equations and variables
      \item Infinite solutions if $m < n$ since there are more variables than equations
    \end{enumerate}
  \end{definition}

  \begin{definition}
    The variables corresponding to the indices of the pivots are called the \textbf{pivot variables}. The rest of the variables are called \textbf{free variables}
  \end{definition}

  Because of theorem 3.3, we can determine whether a system has 0, 1, or multiple solutions by looking at the extended matrix's Echelon form. The case for 0 solutions is easy. 

  \begin{theorem}
    The system $A x = b$ has 0 solutions if and only if ref$(\Tilde{A})$ contains a row in the form 
    \begin{equation}
      \begin{pmatrix}
      0 & 0 & ... & 0 & c
      \end{pmatrix}, \; c \neq 0
    \end{equation}
  \end{theorem}
  \begin{proof}
    The existence of this row is equivalent to the linear equation
    \begin{equation}
      0 x_1 + 0 x_2 + ... + 0 x_n = c, \; c \neq 0
    \end{equation}
    which is absurd and cannot have any solution. Under the hyperplane interpretation, we can visualize all the hyperplanes failing to have a common point. 
  \end{proof}

  \begin{corollary}
    Given $m \times n$ matrix $A$, if $m > n$ and the row vectors of $A$ are all linearly independent, then the system $A x = b$ has 0 solutions. 
  \end{corollary}

  \begin{theorem}
    The system $A x = b$ has 1 solution if and only if ref$(A)$ is diagonal. 
  \end{theorem}

  \begin{proof}
    ref$(A)$ being diagonal implies that there exists at least one solution and also implies the absence of any free variables. 
  \end{proof}

  \begin{theorem}
    The system $A x = b$ has multiple solutions if and only if ref$(A)$ has free variables. 
  \end{theorem}
  \begin{proof}
    Clear. 
  \end{proof}

  \begin{definition}[Rank]
    The number of pivots in ref$(A)$ is called the \textbf{rank} of $A$, denoted rk$(A)$. 
  \end{definition}

  \begin{theorem}
    Let $A$ be a $m \times n$ matrix. Then rk$(A) \leq$ min$\{m ,n\}$. 
  \end{theorem}
  \begin{proof}
    By definition, the number of pivots cannot exceed the number of variables nor can it exceed the number of equations. 
  \end{proof}

  \begin{definition}
    A $n \times n$ matrix $A$ is called \textbf{nonsingular} if and only if rk$(A) = n$. It is \textbf{singular} if and only if rk$(A) < n$. Clearly, rk$(A) \not> n$. 
  \end{definition}

\subsection{Four Fundamental Spaces}

  We will begin to bring over the general concepts of linear transformations and state them within the realm of matrices. We will start with the concept of dual vectors. 

  It is customary to interpret vectors in the abstract sense as a column of $n$ numbers. Given that vectors are column vectors, it is sometimes useful (but not entirely comprehensive) to interpret covectors as row vectors. That is, given a vector $v$ and covector $l$, $l$ linearly maps $v$ to a field element by left matrix multiplication. 
  \begin{equation}
    l(v) = \begin{pmatrix} l_1 & l_2 & ... & l_n \end{pmatrix} \begin{pmatrix}
    v_1 \\ v_2 \\ ... \\ v_n
    \end{pmatrix} = \sum_{i = 1}^{n} l_i v_i
  \end{equation}

  \begin{definition}[Transpose of a Matrix]
    The \textbf{transpose} of matrix $A$, denoted $A^T$, is the matrix with entries $(a^T)_{i j} = a_{i j}$. That is, it is $A$, "flipped over." 
  \end{definition}

  We illustrate why this definition of a transpose is equivalent to the abstract definition to the transpose of a linear map. Given a linear map $A: U \longrightarrow V$ with $\dim U = n, \dim V = m$, we can fix a basis on both $U$ and $V$ to define its matrix $A$. The abstract definition states that 
  \begin{equation}
    A^T: V^\ast \longrightarrow U^\ast, \; l \equiv \varphi A
  \end{equation}
  Treating $l$ and $\varphi$ as row vectors, we can see that the $m \times n$ matrix $A$ maps the $1 \times m$ covector $\varphi$ to the $1 \times n$ covector $l$. Note that this linear mapping is realized through \textit{right multiplication} of $A$ on $\varphi$. It is customary to present linear maps as \textit{left} multiplication, so by "flipping" (i.e. taking the matrix transpose) of all the elements in the equation, we get 
  \begin{equation}
    l^T \equiv A^T \varphi^T
  \end{equation}
  which presents the mapping in the more usual way of left matrix multiplication. Note that $l^T$ and $\varphi^T$ are still covectors. Just because they are now represented as column vectors, it does not mean that they are not covectors, which is why we shouldn't be too dependent on the row vector interpretation of dual vectors mentioned above.  

  Continuing the previous point, note that the way we represent vectors and linear transformation has all been arbitrarily chosen. There is nothing innate about the way we express these transformation as matrix multiplication. This last example especially shows us that the entire definition of the matrix transpose (rooting from the abstract definition) is dependent on our \textit{initial choice} to represent linear mappings as \textit{left} matrix multiplication and to represent all vectors as column vectors. 

  \begin{theorem}[Properties of the Transpose]
    Given that $A, B: U \longrightarrow V$ is linear, $c$ a constant
    \begin{enumerate}
      \item $(A^T)^T = A$. 
      \item $(A+B)^T = A^T + B^T, \; (c A)^T = c A^T$. 
      \item $(A B)^T = B^T A^T$. 
      \item If $A$ is invertible, $(A^{-1})^T = (A^T)^{-1}$ and $A$ invertible $\implies A^T$ invertible. 
      \item $x \cdot y = x^T y$. Furthermore, 
      \begin{equation}
        Ax \cdot y = (A x)^T y = x^T A^T y = x \cdot A^T y
      \end{equation}
    \end{enumerate}
  \end{theorem}

  \begin{definition}
    Matrix $A$ is a \textbf{symmetric matrix} if $A = A^T$. $A$ is \textbf{skew-symmetric}, or \textbf{anti-symmetric}, if $A^T = - A$. 
  \end{definition}

  Now we are ready to describe the four fundamental spaces of a matrix $A$: the column space, row space, nullspace, and left nullspace. All four of these spaces are subspaces, but we will not check them here. 

  \begin{definition}[Column Space]
    The \textbf{column space} of matrix $A$, denoted $C(A)$, is the span of its column vectors. That is, 
    \begin{equation}
      C(A) = \text{span}\{ a_1, a_2, ..., a_n\}
    \end{equation}
    We will denote the column vectors with lowercase $a_i$'s.
  \end{definition}

  \begin{definition}[Row Space]
    The \textbf{row space} of matrix $A$, denoted $R(A)$, is the span of its row vectors. That is, 
    \begin{equation}
      R(A) = \text{span}\{ A_1, A_2, ..., A_m\}
    \end{equation}
    We will denote the row vectors with uppercase $A_i$'s. 
  \end{definition}

  \begin{definition}[Null Space]
    The kernel of linear transformation is called the \textbf{nullspace} of its associated matrix, denoted Null$(A)$. 
  \end{definition}

  \begin{definition}
    The \textbf{left nullspace} of matrix $A$ is the nullspace of $A^T$. It is denoted Null$(A^T)$. 
  \end{definition}

  \begin{theorem}
    By the column space interpretation, it is clear that
    \begin{equation}
      C(A) = \im \, A
    \end{equation}
  \end{theorem}

  We state the matrix analogue of Theorem 2.5. 

  \begin{theorem}
    A vector is a solution to the system of equation $A x = b$ if and only if it is of the form 
    \begin{equation}
    a + \text{Null}\; (A)
    \end{equation}
    where $a$ is one solution. 
  \end{theorem}

  \begin{theorem}
    Let $A: \mathbb{F}^n \longrightarrow \mathbb{F}^m$ be a $m \times n$ matrix with rank $k$. Assuming that $\mathbb{F}^n$ and $\mathbb{F}^m$ are inner product spaces,  
    \begin{align}
      Null(A) = R(A)^\perp &\iff Null(A)^\perp = R(A) \\
      Null(A^T) = C(A)^\perp &\iff Null(A^T)^\perp = C(A)
    \end{align}
    That is, Null$(A)$ and $R(A)$ are orthogonal complements in $\mathbb{F}^n$, with $\dim R(A) = k$ and $\dim\,$Null$(A) = n - k$. Null$(A^T)$ and $C(A)$ are orthogonal complements in $\mathbb{F}^m$, with $\dim C(A) = k$ and $\dim \,$Null$(A^T) = m - k$. 
  \end{theorem}

  \begin{corollary}
    The solution to the homogeneous system $A x = 0$ is precisely Null$(A)$. 
  \end{corollary}

  \begin{definition}
    The homogeneous system $A x = 0$ always has a \textit{trivial solution} $x = 0$. 
  \end{definition}

  \begin{example}
    Given a system of linear equations 
    \begin{align*}
      x + 3 y - 2z = 5 \\
      3 x + 5 y + 6 z = 7 \\
      2 x + 4 y + 3 z = 8
    \end{align*}
    We put it into extended matrix form $A$ and perform Gauss Elimination to get rref$(A)$. 
    \begin{equation}
      \begin{pmatrix}
      1 & 3&-2&5 \\ 3&5&6&7\\ 2&4&3&8
      \end{pmatrix} \rightarrow \begin{pmatrix}
      1&3&-2&5\\ 0&1&-3&2 \\ 0&0&1&2 \end{pmatrix} \rightarrow \begin{pmatrix}
      1&0&0&-15 \\ 0&1&0&8 \\ 0&0&1&2
      \end{pmatrix}
    \end{equation}
    So, rref$(A)$ has the solution $(-15, 8, 2)$ and it is unique because there are no free variables. 
  \end{example}

  This leads to the following theorem. 

  \begin{theorem}
    The set of $n$ linear equations with $n$ variables can be expressed in the form of $A x = b$, where $A$ is an $n \times n$ matrix. 
    \begin{equation}
      A x = b \text{ has a unique solution} \iff A \text{ is nonsingular} \iff \text{rk}\,(A) = n
    \end{equation}
  \end{theorem}
  \begin{proof}
    $A$ is nonsingular is equivalent to saying that rref$(A) = I_n$, where $I_n$ is the $n \times n$ identity matrix. This clearly means that rref$(\Tilde{A})$ will always reveal unique solutions. 
  \end{proof}

  \begin{theorem}
    $n \times n$ matrix $A$ is invertible if and only if it is nonsingular. 
  \end{theorem}
  \begin{proof}
    $A$ is nonsingular $\iff A x = b$ will always have a unique solution $\iff$ $A$ is an isomorphism from $\mathbb{F}^n$ to itself $\iff$ by definition, $A$ is invertible. 
  \end{proof}

  The realization of an endomorphism of $\mathbb{F}^n$ in matrix form is a $n \times n$ matrix. The realization of an automorphism of $\mathbb{F}^n$ in matrix form is an $n \times n$ nonsingular matrix. This set is actually a multiplicative, nonabelian group denoted GL$_n(\mathbb{F})$ and is one example of a Lie Group. 

  \begin{theorem}
    There are $k$ free variables in $A$ if and only if $\dim\,$Null$(A) = k$. 
  \end{theorem}
  \begin{proof}
    We do not give a rigorous proof but we outline one. Each free variable corresponds to a free vector in the row echelon form of $A$ that are all linearly independent. Since the span of these free vectors is equal to Null$(A)$, the $k$ vectors form a basis of $A$ $\implies$ by definition, $\dim\,$Null$(A) = k$.
  \end{proof}

  \begin{theorem}
    \begin{equation}
      \text{rk}(A) = \dim \, \im \,A = \dim C(A)
    \end{equation}
  \end{theorem}
  \begin{proof}
    Let $A$ be a $m \times n$ matrix over $\mathbb{F}$. Then, let rk$(A) = k$, which implies that there are $n-k$ free variables $\implies \dim \,$ Null$(A) = n - k$. By rank nullity, 
    \begin{equation}
      \dim \im{A} = n - \dim \text{Null}(A) = n - (n -k) = k = \text{rk} A 
    \end{equation}
  \end{proof}

  This theorem establishes the consistency in definition between the rank of an abstract mapping mentioned in chapter 2 and the rank of its matrix representation. We can in fact establish strong claims on top of this. 

  \begin{theorem} 
    \begin{equation}
      \dim C(A) = \dim R(A)
    \end{equation}
  \end{theorem}
  \begin{proof}
    Let $A$ be a $m \times n$ matrix of rank $r$. There are $r$ pivots and a pivot in each nonzero row of ref$(A)$, so $\dim R(A) = r$. The previous theorem says $r = \dim C(A)$. 
  \end{proof}

  \begin{corollary}
    \begin{equation}
      C(A) \simeq R(A)
    \end{equation}
  \end{corollary}
  \begin{proof}
    While this is a direct result of the dimensions of the two subspaces being equal, it is worthwhile to mention this alternative proof. We will prove that the linear mapping $A$ is the isomorphism itself. Let rk$(A) = r$ and let $\{ v_1, v_2, ..., v_r\}$ be a basis for $R(A)$. Then, the set $\{ A v_1, A v_2, ..., A v_r\}$ are $r$ vectors in $C(A)$. They are linearly independent because 
    \begin{align*}
      \sum_{i = 1}^r c_i A v_i = A \sum_{i=1}^r c_i v_i = 0 & \implies \sum_{i=1}^r c_i v_i \in \text{Null}(A) \text{, but } \sum_{i=1}^r c_i v_i \in R(A) \\
      & \implies \sum_{i=1}^r \in \text{Null}(A) \cap R(A) = \{0\}
    \end{align*} 
    Since $\dim C(A) = r$, $\{A v_i\}$ must form a basis of $C(A)$. Therefore, $A$ is a bijection between vector spaces and is thus an isomorphism. 
  \end{proof}

  \begin{corollary}
    \begin{equation}
      \text{rk}(A) = \text{rk}(A^T)
    \end{equation}
  \end{corollary}

  \begin{theorem}
    The product of square lower triangular matrices is a lower triangular matrix. The product of square upper triangular matrices is an upper triangular matrix. 
  \end{theorem}

\subsection{LU Decomposition}

  \begin{theorem}[LU Decompositions]
    If a $m \times n$ matrix $A$ can be reduced to row echelon form using only elementary row operations $E^1$, it can be decomposed into the product of a lower triangular $m \times m$ matrix $L$ with diagonal entries equal to $1$ and an upper triangular $m \times n$ matrix $U$. 
    \begin{equation}
      A = L U
    \end{equation}
    This is called \textbf{LU decomposition}, or \textbf{LU factorization}. 
  \end{theorem}
  \begin{proof}
    We reduce $A$ to its echelon form ref$(A)$ by successively multiplying elementary matrices $E^{\gamma_i}$ representing elementary operation (i). After a finite amount of steps $r$, we will reduce it to ref$(A)$.
    \begin{equation}
      \text{ref}(A) = E^{\gamma_r} E^{\gamma_{r-1}} ... E^{\gamma_2} E^{\gamma_1} A = \bigg(\prod_{i = 0}^{r-1} E^{\gamma_{r-i}}\bigg) A
    \end{equation}
    Since each $E^{\gamma_i}$ is invertible, we multiply the product of the inverses of the elementary matrices of operation (i), which are also elementary matrices of operation (i). 
    \begin{equation}
      (E^{\gamma_1})^{-1} (E^{\gamma_2})^{-1} ... (E^{\gamma_r})^{-1} ref(A) = \bigg( \prod_{j = 1}^r (E^{\gamma_j})^{-1} \bigg) \bigg( \prod_{i=1}^{r-1} E^{\gamma_{r-i}} \bigg) A = A
    \end{equation}
    Since each $(E^{\gamma_j})^{-1}$ is an elementary row operation, it is lower diagonal, and by theorem 3.16, their product is also lower triangular. It is easy to prove that if the diagonal entries are furthermore equal to 1, then the product has diagonal entries equal to 1. Finally, it is clear that every matrix in row echelon form is upper triangular, and we are done. 
    \begin{equation}
      A = \bigg( \prod_{j = 1}^r (E^{\gamma_j})^{-1} \bigg) \text{ref}(A) = L U
    \end{equation}
  \end{proof}

  Note that the existence of the LU decomposition for a general $m \times n$ matrix is not guaranteed. It will not exist if we must switch rows in matrix $A$ in order to reduce it to its echelon form. It does not matter whether we need to use elementary operation (ii) or not. Only the necessity of elementary operation (iii) to reduce the matrix determines the existence of the LU decomposition. The decomposition is also unique. 

  Finding the LU decomposition of a matrix is useful for solving systems of linear equations. Given a system in the form of $A x = b$, if we know the LU decomposition of $A$, we can rewrite the system as 
  \begin{equation}
    L U x = b
  \end{equation}
  Setting $y = U x$, we can easily solve the system $L y = b$ using forward substitution and then we can solve the system $U x = y$ using back substitution. Therefore, knowing this decomposition beforehand greatly aids in computing the solutions to the linear system. But computing $L$ and $U$ in order to solve this system takes as much effort as solving the system using Gauss Elimination in the first place. 

  It is imperative to mention a similar decomposition for $n \times n$ matrices, known as \textbf{LUP decomposition}. 

  \begin{definition}
    An $n \times n$ permutation matrix is a matrix of $0$s and $1$s with exactly one $1$ in each row and column. The set of all $n \times n$ permutation matrices form a multiplicative matrix group of order $n!$. We can also view this group as the matrix representation of the symmetric group $S_n$. 
  \end{definition}

  \begin{example}
    The set of all $2 \times 2$ permutation matrices is 
    \begin{equation}
      S_2 = \bigg\{  \begin{pmatrix}1 & 0 \\0 & 1 \end{pmatrix}, \begin{pmatrix}1 & 0 \\0 & 1 \end{pmatrix} \bigg\}
    \end{equation}
    and the set of all $3 \times 3$ permutation matrices is
    \begin{equation}
      S_3 = \Bigg\{I_3, 
      \begin{pmatrix}1 & 0 & 0 \\0 & 0 & 1 \\0 & 1 & 0 \end{pmatrix},
      \begin{pmatrix}0 & 1 & 0 \\1 & 0 & 0 \\0 & 0 & 1 \end{pmatrix}, 
      \begin{pmatrix}0 & 1 & 0 \\0 & 0 & 1 \\1 & 0 & 0 \end{pmatrix}, 
      \begin{pmatrix}0 & 0 & 1 \\0 & 1 & 0 \\1 & 0 & 0 \end{pmatrix}, 
      \begin{pmatrix}0 & 0 & 1 \\1 & 0 & 0 \\0 & 1 & 0 \end{pmatrix} \Bigg\}
    \end{equation}
  \end{example}

  \begin{theorem}
    Every $n \times n$ matrix $A$ can be decomposed into the form $A = P L U$, where $L$ is lower triangular, $U$ is upper triangular, and $P$ is a permutation matrix. 
  \end{theorem}
  \begin{proof}
    We can modify the Gauss Elimination algorithm to do all the row interchanges in the beginning. The permutation matrices form a group, so the product of all the initial row changes is a permutation matrix. Call it $P^\prime$. The previous theorem states that we can do LU decomposition on $P^\prime A$. 
    \begin{equation}
      P^\prime A = LU \implies A = P^{\prime -1} L U = P L U
    \end{equation}
    Since $P^{\prime -1}$ is also in the symmetric group of permutations, we can denote it as $P$. 
  \end{proof}

  \begin{corollary}
    Every $n \times n$ matrix $A$ can be decomposed into the form $L U P$. That is, in the form
    \begin{equation}
      A = L U P = 
      \begin{pmatrix}
      1 & 0 & 0 & \ldots & 0\\
      * & 1 & 0 & \ldots & 0\\
      * & * & 1 & \ldots & \vdots\\
      \vdots & \vdots & \vdots & \ddots & 0\\
      * & * & ... & * & 1 
      \end{pmatrix}
      \begin{pmatrix}
      u_{11} & * & * & \ldots & *\\
      0 & u_{22} & * & \ldots & *\\
      0 & 0 & u_{33} & \ldots & \vdots\\
      \vdots & \vdots & \vdots & \ddots & * \\
      0 & 0 & \ldots & 0 & u_{n n} 
      \end{pmatrix}
      \begin{pmatrix}
      \\
      \\
       & & & P & & & \\
      \\
       & 
      \end{pmatrix}
    \end{equation}
  \end{corollary}
  \begin{proof}
    We decompose $A^T = P_0 L_0 U_0$, where $P_0$ is a permutation matrix, $L_0$ lower triangular, $U_0$ upper triangular. This implies that 
    \begin{equation}
      A = A^{T T} = U_0^T L_0^T P_0^T = L U P
    \end{equation}
    since $U_0^T$ is lower triangular and $L_0^T$ is upper triangular. Note that $L$ is unique, but $U$ is not unique, so this decomposition is not unique. 
  \end{proof}

  This decomposition can also be used to solve matrix equations
  \begin{equation}
    A X = B
  \end{equation}
  Since this equation can be expressed in the form
  \begin{equation}
    A \begin{pmatrix}
    | & & | \\ x_1 & ... & x_n \\ | & & | 
    \end{pmatrix} = \begin{pmatrix}
    | & & | \\ A x_1 & ... & A x_n \\ | & & | 
    \end{pmatrix} = \begin{pmatrix}
    | & & | \\ b_1 & ... & b_n \\ | & & | 
    \end{pmatrix}
  \end{equation}
  solving this matrix is equivalent to solving the system of systems of linear equations 
  \begin{equation}
    Ax_1 = b_1, Ax_2 = b_2, ..., Ax_n = b_n
  \end{equation}
  i.e. by solving one column at a time. This method can also be used to solve 
  \begin{equation}
    A X = I
  \end{equation}
  to find $X = A^{-1}$. Equivalently, we can left multiply elementary matrices to reduce $A$ to rref$(A)$. 
  \begin{equation}
    E^{\gamma_r} E^{\gamma_{r-1}} ... E^{\gamma_2} E^{\gamma_1} A X = \text{rref}(A) X = E^{\gamma_r} E^{\gamma_{r-1}} ... E^{\gamma_2} E^{\gamma_1} I = \prod_{i = 0}^{r-1} E^{\gamma_{r-i}}
  \end{equation}
  If rref$(A)= I$, then 
  \begin{equation}
    A^{-1} = \prod_{i = 0}^{r-1} E^{\gamma_{r-i}}
  \end{equation}
  and if rref$(A) \neq I$, then $A^{-1}$ does not exist. This is in fact precisely the method of finding the inverse where we do Gauss Elimination on the extended matrix
  \begin{equation}
    \begin{pmatrix}
    & &| & & \\ & A &| & I & \\ & &| & & 
    \end{pmatrix} \longrightarrow \begin{pmatrix}
    & &| & & \\ & I &| & A^{-1} & \\ & &| & & 
    \end{pmatrix}
  \end{equation}

\subsection{Strassen Algorithm}

  When computing the product two $n \times n$ matrices $A$ and $B$ to another $n \times n$ matrix $C$, since each entry of $C$ is the product of a row of $A$ with a column of $B$, and since $C$ has $n^2$ entries, we need $n^3$ scalar multiplications to compute (as well as $n^3 - n^2$ additions). In order words, the computing efficiency of the algorithm is at $O(n^3)$. However, there are faster algorithms than this. This is algorithm is known as the \textbf{Strassen Algorithm} (however, there do exist faster algorithms). 

  \begin{theorem}[Strassen Algorithm]
    Let $A, B$ be $2 \times 2$ matrices such that $AB = C$. That is, component-wise,
    \begin{equation}
      \begin{pmatrix}
      a_{11} & a_{12} \\ a_{21} & a_{22}
      \end{pmatrix} \begin{pmatrix}
      b_{11} & b_{12} \\ b_{21} & b_{22}
      \end{pmatrix}
       = \begin{pmatrix}
        c_{11} & c_{12} \\ c_{21} & c_{22}
       \end{pmatrix}
    \end{equation}
    where for $i, j = 1, 2$, 
    \begin{equation}
      c_{ij} = a_{i1} b_{1j} + a_{i2} + b_{2j}
    \end{equation}
    Then, let us define 
    \begin{align*}
      P_1 &= (a_{11} + a_{22}) (b_{11} + b_{22}) \\
      P_2 &= (a_{21} + a_{22}) b_{11} \\
      P_3 &= a_{11} (b_{12} - b_{22}) \\
      P_4 &= a_{22} (b_{21} - b_{11} \\
      P_5 &= (a_{11} + a_{12}) b_{22} \\
      P_6 &= (a_{21} - a_{11}) (b_{11} + b_{12}) \\
      P_7 &= (a_{12} - a_{22}) (b_{21} + b_{22}) 
    \end{align*}
    Then, the theorem states that we the entries of $C$ are 
    \begin{align*}
      c_{11} &= P_1 + P_4 - P_5 + P_7 \\
      c_{12} &= P_3 + P_5 \\
      c_{21} &= P_2 + P_4 \\
      c_{22} &= P_1 + P_3 - P_2 + P_6
    \end{align*}
  \end{theorem}

  This algorithm for multiplying $2\times 2$ matrices requires $7$ scalar multiplications, while regular multiplication requires $8$. Using block multiplication, we can use this algorithm to calculate any matrix of order $2^k$. That is, to calculate $2^k \times 2^k$ matrices, we have to perform seven multiplications of blocks of size $2^{k-1} \times 2^{k-1}$, and doing this recursively, it reduces it down to 
  \begin{equation}
    7^k = 2^{k \log_2{7}} = n^{\log_2{7}}
  \end{equation}
  where $n$ is the order of the matrices being multiplied. 

  Additionally, the number of scalar additions or subtractions needed is bounded by 
  \begin{equation}
    6 \times 7^k = 6 \times 2^{k \log_2{7}} = 6 n^{\log_2{7}}
  \end{equation}
  Since $\log_2{7} \approx 2.807 < 3$, this algorithm does indeed have more computational efficiency. Note that matrices whose order is not a power of $2$ can be turned into one by adjoining a suitable number of $1$s on the diagonal. 

  \begin{theorem}[Conjecture]
    For any positive number $\varepsilon$, there is an algorithm that computes the product of two $n \times n$ matrices with computational efficiency of $O(n^{2 + \varepsilon})$. 
  \end{theorem}


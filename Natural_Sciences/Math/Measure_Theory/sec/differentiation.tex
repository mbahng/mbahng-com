\section{Differentiation}

  Now, we will establish differentiation and culminate in the fundamental theorem of calculus. Monotone functions are a nice class of functions to study for differentiation and for constructing more general measures. 

  \begin{theorem}
    Suppose $f$ is monotone, increasing on $[a, b]$. Then, the set of discontinuities of $f$ at most countable. 
  \end{theorem}
  \begin{proof}
    Let $x_k$ be any point of discontinuity. Note that 
    \begin{equation}
      \lim_{x \to x_k^-} f(x), \qquad \lim_{x \to x_k^+} f(x)
    \end{equation}
    both exist by monotonicity, but since there is a discontinuity, we have 
    \begin{equation}
      L_k^- = \lim_{x \to x_k^-} f(x) < \lim_{x \to x_k^+} f(x) = L_k^+
    \end{equation}
    Then, $L_k^+ - L_k^-$ is a jump of $f$ at $x_k$. These intervals $[L_k^-, L_k^+]$ are disjoint due to monotonicity, and each interval contains a rational number. So there can only be at most countable intervals. 
  \end{proof}

  One piece of info from this trick. 

  \begin{definition}
    A point $x$ is a discontinuity of the first kind of $f(x)$ if both one-sided limits exist. 
  \end{definition}

  Now here's a generalization for not necessarily monotone functions. 

  \begin{theorem}[Detour]
    The set of discontinuities of the first kind is countable. 
  \end{theorem}
  \begin{proof}
    Idea of the proof. Look at some jump discontinuity and record the jump $\eta > 0$. Then, find $\delta > 0$ s.t. if $0 < y - x < \delta$, then 
    \begin{equation}
      \big| f(x)  - \lim_{y \to x^+} f(y) \big|  < \frac{\eta}{10}
    \end{equation}
    Then look at the rectangle on the graph associated with each jump. Because the limits exist, you can pick the rectangles so small that they are completely disjoint. Look at picture. 
  \end{proof}

  Now back to monotone functions. 

  \begin{theorem}
    For any countable set $C \subset (a, b)$ (where the interval doesn't need to be bounded), there exists monotonically increasing $f$ with a jump at each $x \in C$ and continuous at every $x \not\in C$. 
  \end{theorem}
  \begin{proof}
    Let $x_1, x_2, \ldots$ be $C$, and define 
    \begin{equation}
      f(x) = \sum_{x_k \leq x, x \in C} 2^{-k}
    \end{equation}
    The sum is increasing and convergent (since it's dominated by geometric series). $f$ also has a jump of $2^{-k}$ at every $x_k$. 

    Now we prove continuity. Suppose $x \not\in C$. Take $N \in \mathbb{N}$. Find $\delta_N > 0$ s.t. 
    \begin{equation}
      x_1, x_2, \ldots, x_N \not\in (x - \delta_N, x + \delta_N)
    \end{equation}
    which is possible since this is a finite set. The remaining sum can only add up to $2^{-N}$, and so $f(x + \delta_N) - f(x - \delta_N) \leq 2^{-N}$. 
  \end{proof}


\subsection{Oct 22} 

  \begin{theorem}[Fundamental Theorem of Calculus of Absolutely Continuous Functions]
    Suppose $f$ is AC on $[a, b]$.\footnote{Note that this is bounded and closed.} Then $f$ is differentiable a.e. on $(a, b)$, and $f^\prime$ is integrable on $[a, b]$, and 
    \begin{equation}
      \int_a^b f^\prime = f(b) - f(a)
    \end{equation}
  \end{theorem}

  An immediate corollary is that we can do 
  \begin{equation}
    \int_a^x = f(x) - f(a) \quad \forall x \in [a, b]
  \end{equation}
  If $f$ is AC on $[a, b]$, then $f$ is AC on $[a, x]$.  

  \begin{corollary}
    $f$ is AC on $[a, b]$ if and only if 
    \begin{equation}
      f(x) = f(a) + \int_a^x g(y) \,dy \quad \forall x \in [a, b]
    \end{equation}
    for some integrable $g$. 
  \end{corollary}
  \begin{proof}
    We prove bidirectionally. 
    \begin{enumerate}
      \item The forward implication is quite straightforward using the fundamental theorem. 
      \item $(\leftarrow)$ We just need to prove that the antiderivative is absolutely continuous. Take $\{(a_k, b_k)\}_{k=1}^n$ disjoint. We estimate the total variation,
        \begin{equation}
          \sum_{k=1}^n |f(b_k) - f(a_k)| 
        \end{equation}
        and try to make this small if the measure of the unions of the intervals is small. Just using the definition that $f$ is the antiderivative, the sum can be bounded by 
        \begin{equation}
          \sum_{k=1}^n |f(b_k) - f(a_k)| \leq \sum_{k=1}^n \int_{a_k}^{b_k} |g| \,dx = \int_{\cup (a_k, b_k)} |g| \,dx 
        \end{equation}
        using the triangle inequality, and then using additivity. The rest is just $\epsilon$-$\delta$ language. $\forall \epsilon > 0$, $\exists \delta > 0$ s.t. 
        \begin{equation}
          m(\bigcup (a_k, b_k)) < \delta \implies \sum_{k=1}^n |f(b_k) - f(a_k)| < \epsilon
        \end{equation}
        This is true since $g$ is integrable, and whenever the measure of the region that you are integrating on is less than $\delta$, your integral will be less than $\epsilon$. So $\exists \delta > 0$ s.t. for all measurable $E$, 
        \begin{equation}
          m(E) < \delta \implies \int_E |g| \,dx < \epsilon
        \end{equation}
        This is just the definition of integrability. This implies that 
        \begin{equation}
          \sum_{k=1}^n |a_k - b_k| < \delta \implies \sum_{k=1}^n |f(b_k) - f(a_k)| < \epsilon 
        \end{equation}
        Note that we are using the bounded 
    \end{enumerate}
  \end{proof}

  Another corollary is for monotone functions, and how we can determine whether they are AC or not. 

  \begin{corollary}[AC of Monotone Functions]
    Let $f$ be monotone on $[a, b]$. Then $f$ is AC if and only if 
    \begin{equation}
      \int_a^b f^\prime \,dx = f(b) - f(a)
    \end{equation}
    If we have montone function, note that derivative should exist a.e., and the derivative is integrable for monotone functions. Note that in the previous corollary, we need to check for all $x \in [a, b]$, but in here, we only need to check at the endpoints $a$ and $b$. 
  \end{corollary}
  \begin{proof}
    Bidirectional. 
    \begin{enumerate}
      \item $(\rightarrow)$. 
      \item $(\leftarrow)$. Let $x \in [a, b]$. We know from assumption---by rearranging the terms---that
      \begin{equation}
        0 = \int_a^b f^\prime - \big( f(b) - f(a) \big)
      \end{equation}
      But by additivity of the integral, we have 
      \begin{equation}
        = \underbrace{\int_a^x f^\prime - \big( f(x) - f(a) \big)}_{\leq 0} + \underbrace{\int_x^b f^\prime - \big( f(b) - f(x) \big)}_{\leq 0}
      \end{equation}
      But we know that WLOG, $f$ is increasing. If $f$ is increasing, then we know that both integrals should be positive, since the only type of discontinuities can be jump discontinuities. 
      \begin{equation}
        \int_a^x f^\prime - \big( f(x) - f(a) \big) + \int_x^b f^\prime - \big( f(b) - f(x) \big)
      \end{equation}

    \end{enumerate}
  \end{proof}

  This is another corollary, also sometimes known as the fundamental theorem for absolutely continuous functions. 

  \begin{lemma} 
    Le $f$ be integrable over $[a, b]$, with 
    \begin{equation}
      \int_{x_1}^{x_2} f\, dx = 0 \quad \forall (x_1, x_2) \subset [a, b]
    \end{equation}
    Then, $f = 0$ a.e. $[a, b]$. 
  \end{lemma}
  \begin{proof}
    Note that if we add the constraint that $f \geq 0$, then this is true. But the potential problem is that $f$ might change signs, which may cancel out. So starting from the assumption, we know that for any open $O$, 
    \begin{equation}
      \int_O f \,dx = 0 \quad \forall O \text{ open} 
    \end{equation}
    Since $G_\delta$ sets can be written as a decreasing sequence of open sets, by continuity of measure, we can write 
    \begin{equation}
      \int_G f \,dx = 0 \quad \forall G \text{ } G_\delta
    \end{equation}
    Since any measurable set $E$ can be written as $E = G \setminus E_0$ with $m(E_0) = 0$, we have 
    \begin{equation}
      \int_E f \,dx = 0 \quad \forall E \text{ measurable}
    \end{equation}
    So let $E^+ = \{ x \in [a, b] \mid f(x) > 0\}$ and $E^- = \{ x \in [a, b] \mid f(x) < 0\}$. So we have the first equalities
    \begin{align}
      0 & = \int_{E^+} f = \int_a^b f^+ \\
      0 & = \int_{E^-} f = - \int_a^b f^-  
    \end{align}
    So this means that $f^+ = f^- = 0$ a.e. 
  \end{proof}

  \begin{corollary}
    Let $f$ be integrable over bounded, closed interval $[a, b]$. Then, 
    \begin{equation}
      \frac{d}{dx} \bigg[ \int_a^x f \,dt \bigg] = f(x) \quad \text{ for a.e. } x \in (a, b)
    \end{equation}
    So basically, the derivative of the antiderivative is the function itself. 
  \end{corollary}
  \begin{proof}
    We know that $\int_a^x f \,dt$ is AC, so by two corollaries ago, its derivative must exist a.e. Let's call it $F(x) = \int_a^x f \, dt$, and so $F^\prime$ is integrable. So we need to compare $F^\prime$ and $f$. For any $(x_1, x_2) \subset [a, b]$ by linearity, we have 
    \begin{equation}
      \int_{x_1}^{x_2} [ F^\prime - f ] = \int_{x_1}^{x_2} F^\prime - \int_{x_1}^{x_2} f 
    \end{equation}
    where the integral of the first term is $F(x_2) - F(x_1)$, since it is AC. For the second term, we know that $F = \int_a^x f \,dt$, so can can split it 
    \begin{equation}
      = F(x_2) - F(x_1) - \underbrace{\int_a^{x_2}}_{F(x_2)} + \underbrace{\int_{a}^{x_1}}_{F(x_1)} = 0
    \end{equation}
    So this is true for any open interval in $[a, b]$. Then by invoking the previous lemma, $f = F^\prime$ a.e. 
  \end{proof}

  This is the end of absolutely continuous functions. 

\subsection{Oct 22. Convex Functions} 

  \begin{definition}[Convex Function]
    $\varphi$ is convex on $(a, b) \subset \mathbb{R}$ if $\forall x_1, x_2 \in (a, b)$, $\forall \lambda \in [0, 1]$, the linear interpolation 
    \begin{equation}
      \varphi (\lambda x_1 + (1 - \lambda) x_2) \leq \lambda \varphi(x_1) + (1 - \lambda) \varphi(x_2)
    \end{equation}
  \end{definition}

  Note that if we have $x = \lambda x_1 + (1 - \lambda) x_2$, then $\lambda = \frac{x_2 - x}{x_2 - x_1}$, and so the definition can be rewritten as 
  \begin{equation}
    \varphi(x) \leq \frac{x_2 - x}{x_2 - x_1} \varphi(x_1) + \frac{x - x_1}{x_2 - x_1} \varphi(x_2) \quad \forall x \in [x_1, x_2]
  \end{equation}
  Note that the two fraction coefficients add up to $1$. So we can write 
  \begin{equation}
    \frac{x_2 - x}{x_2 - x_1} \varphi(x) + \frac{x - x_1}{x_2 - x_1} \varphi(x)  \leq \frac{x_2 - x}{x_2 - x_1} \varphi(x_1) + \frac{x - x_1}{x_2 - x_1} \varphi(x_2) \quad \forall x \in (x_1, x_2)
  \end{equation}
  and rearranging, we get 
  \begin{equation}
    \frac{x_2 - x}{x_2 - x_1} \big( \varphi(x) - \varphi(x_1) \big) = \frac{x - x_1}{x_2 - x_1} \big( \varphi(x_2) - \varphi(x) \big)
  \end{equation} 
  Cancel out the common denominator to get 
  \begin{equation}
    \frac{\varphi(x) - \varphi(x_1)}{x - x_1} \leq \frac{\varphi(x_2) - \varphi(x)}{x_2 - x} \qquad \forall x \in (x_1, x_2)
  \end{equation}
  This is an if and only if derivation, so this is an equivalent. 

  \begin{theorem}
    If $\varphi$ is differentiable on $(a, b)$ with $\varphi^\prime$ increaseing, then $\varphi$ is convex. 
  \end{theorem}
  \begin{proof}
    Note that the LHS $= \varphi^\prime (c_1)$, RHS $= \varphi^\prime (c_2)$. Therefore, 
    \begin{equation}
      \varphi^\prime (c_1) \leq \varphi^\prime (c_2)
    \end{equation}
  \end{proof}

  \begin{example}
    $x^p$ for $p \geq 1$ is convex on $(0, \infty)$. Also, $e^{\alpha x}$ for $\alpha > 1$ is convex on $\mathbb{R}$. 
  \end{example}

  \begin{lemma}[Chorded Slope Lemma]
    Let $\varphi$ be convex on $(a, b)$, with $x_1 < x < x_2$ belonging to $a, b$. Then, 
    \begin{equation}
      \frac{\varphi(x) - \varphi(x_1)}{x - x_1} \leq \frac{\varphi(x_2) - \varphi(x_1)}{x_2 - x_1} \leq \frac{\varphi(x_2) - \varphi(x)}{x_2 - x} 
    \end{equation}
  \end{lemma}
  \begin{proof}
    Note that we can just write the second term as an interpolation of the first and third terms. 
    \begin{equation}
      \frac{\varphi(x_2) - \varphi(x_1)}{x_2 - x_1} = \frac{\varphi(x) - \varphi(x_1)}{x - x_1} \frac{x - x_1}{x_2 - x_1} + \frac{\varphi(x_2) - \varphi(x)}{x_2 - x} \frac{x_2 - x}{x_2 - x_1}
    \end{equation}
  \end{proof}

  \begin{theorem}
    Let $\varphi$ be convex on $(a, b)$. Then $\varphi$ has left and right derivatives at each point $x \in (a, b)$. Moreover, if $u < v$ on $(a, b)$, then 
    \begin{equation}
      \varphi^\prime (u^-) \leq \varphi^\prime (u^+) \leq \frac{\varphi(v) - \varphi(u)}{v - u} \leq \varphi^\prime (v) \leq \varphi^\prime (v^\prime)
    \end{equation}
  \end{theorem}
  \begin{proof}
    From lemma, $\varphi(u^-)$ exists since $\frac{\varphi(u) - \varphi(w)}{u - w}$ is monotonically incraesing in $w$. It is also bounded from above by $\frac{\varphi(v) - \varphi(u)}{v - u}$. So $\varphi^\prime(u^-)$ exists. 
  \end{proof}



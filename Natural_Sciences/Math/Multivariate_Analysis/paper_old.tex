\documentclass{article}
  \usepackage[a4paper, top=1in, bottom=1in, left=1in, right=1in]{geometry}
  \usepackage[utf8]{inputenc}
  \usepackage[english]{babel}

  \usepackage{tikz-cd, lipsum, bm, dcolumn}
  \usetikzlibrary{arrows}
  \usepackage{amsmath, amssymb, amsthm, mathrsfs, mathtools, centernot, hyperref, fancyhdr, lastpage}
  \usepackage{extarrows, esvect, esint, pgfplots}
  \pgfplotsset{compat=1.18}

  \setlength{\parindent}{0pt} % set no indent
  \hfuzz=5.0pt % ignore overfull hbox badness warnings below this limit


\renewcommand{\thispagestyle}[1]{}

\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\Sym}{Sym}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\im}{Im}
\DeclareMathOperator{\Div}{div}
\DeclareMathOperator{\curl}{curl}
\DeclareMathOperator{\GL}{GL}
\DeclareMathOperator{\SL}{SL}
\DeclareMathOperator{\GA}{GA}
\DeclareMathOperator{\std}{std}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Corr}{Corr}
\DeclareMathOperator{\Int}{Int}
\DeclareMathOperator{\Id}{Id}
\DeclareMathOperator{\Lie}{Lie}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\Alt}{Alt}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\conv}{conv}
\DeclareMathOperator{\aff}{aff}
\DeclareMathOperator{\arccot}{arccot}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{example}{Example}[section]
\newtheorem{corollary}{Corollary}[theorem]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\renewcommand{\qed}{\hfill$\blacksquare$}
\renewcommand{\footrulewidth}{0.4pt}% default is 0pt


\begin{document}
\pagestyle{fancy}

\lhead{Multivariate Calculus}
\chead{Muchang Bahng}
\rhead{\date{August 2021}}
\cfoot{\thepage / \pageref{LastPage}}

\title{Multivariate Calculus}
\author{Muchang Bahng}

\maketitle

\section{Integration}

\subsection{Reduction to Iterated Integrals}

We first state a basic condition of integration. 

\begin{theorem}
Any function $f: \mathbb{R}^n \longrightarrow \mathbb{R}$ that is continuous over a certain region $B \subset \mathbb{R}^n$ can be integrated over $B$. 
\end{theorem}
That is, if $f$ is discontinuous at a certain subset $D \subset B$, then the infinitesimal neighborhoods around each point $d \in D$ is not well defined, since they would always contain two values of $f$ that do not converge to each other at $d$. 

However, there are some discontinuous functions that are in fact integrable. Assuming $B \subset \mathbb{R}^n$ is the region that we are integrating over, 
\begin{enumerate}
    \item Given that there is a subset $N$ in $B$ with volume $0$ over which $f$ is not defined, we can integrate over $B \setminus N$. In the one and two dimensional cases, 
    \[\int_{B \setminus N} f(x) dx \text{ and } \iint_{B\setminus N} f(x) dA\]
    are well-defined. Visually, 
    \begin{center}
        \includegraphics[scale=0.2]{img/Integrable_Hole_Function.jpg}
    \end{center}
    \item The function is defined for all values in the region, but there is a jump in the value of the function. 
    \begin{center}
        \includegraphics[scale=0.23]{img/Integrable_Jump_Function.PNG}
    \end{center}
\end{enumerate}
Informally, if we can visualize the Riemann sum converging to a well-defined area as the rectangles get thinner and thinner, then a discontinuous function is integrable. Indeed, all continuous functions (over a bounded set) are integrable since their Riemann sums are well defined. 

\subsubsection{Integration over Intervals, Rectangles, Boxes}
The simplest region that we can integrate over is a single interval 
\[B \equiv [a,b] \subset \mathbb{R}\]
a rectangle 
\[B \equiv [a, b] \times [c,d] \subset \mathbb{R}^2\]
and a box 
\[B \equiv [a,b] \times [c,d] \times [e,f] \subset \mathbb{R}^3\]
Clearly, this extends to integration over any dimension. 
\[B \equiv \prod_{i=1}^n [\alpha_i, \beta_i] \subset \mathbb{R}^n\]

Solving these integrals are quite simple. However, to rigorously define the methodology, we must use the following theorems. 

\begin{theorem}[Cavalieri's Principle]
Let $S$ be a bounded $n$-dimensional solid in $\mathbb{R}^n$ (note that $S$ can be an interval in $\mathbb{R}$). Define an $n-1$ subspace $P$ in $\mathbb{R}^n$ and given the quotient space $\mathbb{R}^n / P$ with elements $P_x$, let 
\[S \subset \bigcap_{a \leq x \leq b} P_x\]
That is, $S$ is "in between" affine subspaces $P_a$ and $P_b$. The cross section of $S$ cut by $P_x$ is the intersection of it with $S$
\[\text{Cross Section at } P_x \equiv P_x \cap S\]
Denote the area of this cross section as $A(x)$. Then, 
\[\text{Volume of } S = \int_a^b A(x) \; d x\]
\end{theorem}
This theorem basically says that the volume of $S$ is the sum of the areas of its infinitesimal cross sections. 
\begin{center}
    \includegraphics[scale=0.27]{img/Cavalieri_Principle .PNG}
\end{center}
This clearly works for an interval in $\mathbb{R}$, which is computed by the sum of all its "points" (rigorously speaking, infinitesimally thin intervals). The integral works for a shape in $\mathbb{R}^2$, which is computed by the sum of its "line segments" (rigorously speaking, infinitesimally thin rectangles) that add up to the shape. In $\mathbb{R}^3$, the solid is computed by the sum of its cross sections (infinitesimally thin "molded" cylinders). This analogy continues into higher dimensions. 

Given a solid $S \subset \mathbb{R}^n$, it is easy to see that no matter what subspace $P$ we choose–that is, no matter what orientation we choose to "cut" the solid– the sum of all of its cross sections should be equal to the true volume of $S$. In the case when $S$ is a box in $\mathbb{R}^n$, Fubini's theorem states that whether we cut $S$ up along the $x_1$-axis, $x_2$-axis, ..., or the $x_n$-axis, the symmetry in volume is always preserved. This theorem is really just a specific case of this general symmetry in volume. 

\begin{theorem}[Fubini's Theorem]
Given a function $f: \mathbb{R}^n \longrightarrow \mathbb{R}$, let 
\[B \equiv \prod_{i=1}^n [\alpha_i, \beta_i]\]
and let 
$p$ be any permutation of the elements $\{x_1, x_2, ..., x_n\}$. Then 
\begin{align*}
    \int_B f \; d V & = \int_{\alpha_n}^{\beta_n} ... \int_{\alpha_1}^{\beta_1} f(x_1,x_2,...,x_n) \; d x_1 ... d x_n \\
    & = \int_{p(\alpha_n)}^{p(\beta_n)} ... \int_{p(\alpha_1)}^{p(\beta_1)} f(x_1,x_2, ..., x_n) \; d p(x_1) ... d p(x_n) 
\end{align*}
In the two dimensional case, we have
\begin{align*}
    \iint_B f \; d A & = \int_c^d \int_a^b f(x, y) \; d x \, d y = \int_a^b \int_c^d f(x, y) \; d y \, d x 
\end{align*}
\begin{center}
    \includegraphics[scale=0.27]{img/Fubini_Theorem.PNG}
\end{center}
In the three dimensional case, we have
\begin{align*}
    \iiint_B f \; d V & 
    = \int_e^f \int_c^d \int_a^b f(x, y, z) \; d x \, d y \, d z = \int_e^f \int_a^b \int_c^d f(x, y, z) \; d y \, d x \, d z \\
    & = \int_c^d \int_a^b \int_e^f f(x, y, z) \; d z \, d x \, d y = \int_c^d \int_e^f \int_a^b f(x, y, z) \; d x \, d z \, d y \\
    & = \int_a^b \int_e^f \int_c^d f(x, y, z) \; d y \, d z \, d x = \int_a^b \int_c^d \int_e^f f(x, y, z) \; d z \, d y \, d x 
\end{align*}
\end{theorem}

Computation of these integrals is simple. You do the innermost integral first with respect to the corresponding variable, while treating the rest of the variables constant. Evaluating each integral outputs a formula for a higher dimensional cross section of the solid $S$. It is clear that computing iterated integrals is really just doing Cavalieri's principle repeatedly. 

\subsubsection{Integration over Solids Bounded by Curves}
We must first define the different types of \textit{elementary regions} first. 
\begin{definition}
A bounded region $D$ in $\mathbb{R}^n$ is said to be $x_i$-simple if it is bounded by the graphs of two continuous functions $u_1, u_2: \mathbb{R}^{n-1} \longrightarrow \mathbb{R}$ of the variables 
\[x_1, x_2, ..., x_{i-1}, x_{i+1}, ..., x_n\]
That is, $D$ can be expressed in the form 
\[\{ x \in \mathbb{R}^n \; | \; u_1 (x_1,..., x_{i-1}, x_{i+1}, ... , x_n) \leq x_i \leq u_2 (x_1, ..., x_{i-1}, x_{i+1}, ..., x_n)\}\]
If a region is simple in all of its variables, it is simply called \textit{simple}. Note that $n$-dimensional boxes are simple regions. 
\end{definition}

\begin{example}
In $\mathbb{R}^2$, the region on the left graph is an $y$-simple region and the region on the right is a $x$-simple region. 
\begin{center}
\begin{tikzpicture}[scale=0.8]
  \draw[<->] (-1,0)--(5,0);
  \draw[<->] (0,-1)--(0,5);
  \draw[<->] (6,0)--(12,0);
  \draw[<->] (7,-1)--(7,5);
  \draw plot [smooth] coordinates {(0.6, 1.2) (1,1) (2,1.4) (3,1.3) (4,1.5) (4.3,1.7)};
  \draw plot [smooth] coordinates {(0.6, 3.9) (1,4.1) (2,4) (3,4.3) (4,4.2) (4.3,4.1)};
  \draw[dashed] (0.6,1.2)--(0.6,3.9);
  \draw[dashed] (4.3,1.7)--(4.3,4.1);
  \draw plot [smooth] coordinates {(8.2,0.6) (8,1) (8.4,2) (8.3,3) (8.5,4) (8.7,4.3)};
  \draw plot [smooth] coordinates {(10.9,0.6) (11.1,1) (11,2) (11.3,3) (11.2,4) (11.1,4.3)};
  \draw[dashed] (8.2,0.6)--(10.9,0.6);
  \draw[dashed] (8.7,4.3)--(11.1,4.3);
  \node[below] at (4.8,0) {$x$};
  \node[below] at (11.8,0) {$x$};
  \node[left] at (0,4.8) {$y$};
  \node[left] at (7,4.8) {$y$};
  \node[above] at (3,4.2) {$u_1$};
  \node[above] at (3,1.3) {$u_2$};
  \node[left] at (8.3, 3) {$v_1$};
  \node[right] at (11.3, 3) {$v_2$};
  \draw[fill] (0.6,0) circle (0.05);
  \node[below] at (0.6,0) {$a$};
  \draw[fill] (4.3,0) circle (0.05);
  \node[below left] at (4.3,0) {$b$};
  \draw[fill] (7,0.6) circle (0.05);
  \draw[fill] (7,4.3) circle (0.05);
  \node[left] at (7,0.6) {$c$};
  \node[left] at (7,4.3) {$d$};
\end{tikzpicture}
\end{center}
\end{example}
We now describe the method of calculating double integrals over elementary regions. 
\begin{theorem}
The double integral over a $y$-simple region $D$ bounded by functions $u_1$ and $u_2$ in $\mathbb{R}^2$ and the $x$-values $a$ and $b$ (as shown in the left graph of example 2.1) is
\[\iint_D f(x, y) = \int_a^b \int_{u_2 (x)}^{u_1 (x)} f(x, y) \, dy \, dx\]
The double integral over an $x$-simple region $D$ bounded by functions $v_1$ and $v_2$ in $\mathbb{R}^2$ and the $y$-values $c$ and $d$ (shown in the right of graph of example 2.1) is 
\[\iint_D f(x, y) = \int_c^d \int_{v_2 (y)}^{v_1 (y)} f(x, y) \, dx \, dy\]
\end{theorem}

\begin{example}
Integrating $f(x, y)$ over the unit disk would have the form
\[\int_{-1}^1 \int_{-\sqrt{1-x^2}}^{\sqrt{1-x^2}} f(x,y) \, dy\, dx \text{ or } \int_{-1}^1 \int_{-\sqrt{1-y^2}}^{\sqrt{1-y^2}} f(x,y) \, dx\, dy \]
Note that the unit disk is both $x$ and $y$ simple. 
\end{example}
\subsection{Change of Basis}
Sometimes, integrating a region over a different basis would make the integral computation much more simpler. In this case, we may be able to transform more complicated regions into elementary regions. We first introduce a change of basis in 2 dimensions and then generalize it into higher dimensions. 
\\
Let $\mathbb{R}^2$ have the standard orthonomal basis $e_1, e_2$, commonly known as the $x, y$ basis. Now, let us construct new basis vectors of $\mathbb{R}^2$, denoted $f_1, f_2$ such that $f_1, f_2$ are functions of $e_1, e_2$. Since they are both bases that span $\mathbb{R}^2$, we can equally represent $e_1, e_2$ as functions of $f_1, f_2$. 
\begin{align*}
    &e_1 = g(f_1, f_2)\\
    &e_2 = h(f_1, f_2) 
\end{align*}
Note that this change of basis does not necessarily have to be linear, as in the context of passive transformation in linear algebra. Then, every point $(x,y)$ in the $(e_1, e_2)$-basis can be rewritten as
\begin{align*}
    (x, y) & = x e_1 + y e_2 \\
    & = x \, g(f_1, f_2) + y \, h(f_1, f_2) \\
    & = u f_1 + v f_2
\end{align*}
Note that it is customary to denote $x, y$ as the coefficients in the $e_1, e_2$ basis and $u, v$ as the coefficients in the new $f_1, f_2$ basis. This way, we can not only write $e_1$ and $e_2$ as functions of $f_1$ and $f_2$, but we can also write the coefficents $x, y$ as functions of the coeffiecents $u, v$! That is, 
\begin{align*}
    & x = x(u, v) \\
    & y = y(u, v)
\end{align*}
which is really just a function 
\[B: \mathbb{R}^2 \longrightarrow \mathbb{R}^2, \;\; B(u, v) = \begin{pmatrix} x(u, v) \\ y(u, v) \end{pmatrix}\]
Notice that $B$ changes the $u, v$ coordinates to the $x, y$ coordinates, and $B^{-1}$ changes the $x, y$ coordinates to the $u, v$ coordinates. 
\[B^{-1}: \mathbb{R}^2 \longrightarrow \mathbb{R}^2, \;\; B^{-1} (x, y) = \begin{pmatrix} u (x, y) \\ v (x, y) \end{pmatrix}\]
Note that these coefficients actually change \textit{contravariantly}, that is, they change inversely with respect to how the basis vectors are changed. In vector calculus, it is conventional to represent a change of basis with functions that relate the coefficients $x, y$ with $u, v$, rather than the bases $f_1, f_2$ with $e_1, e_2$. 

\begin{theorem}[Integration over Change of Bases in $\mathbb{R}^2$]
Let $\mathbb{R}^2$ have the standard orthonomal basis $e_1, e_2$. Now, let us construct new basis vectors of $\mathbb{R}^2$, denoted $f_1, f_2$ such that the coefficients of the vectors in $\mathbb{R}^2$ are related by the change of basis function 
\[B = \begin{pmatrix} x \\ y \end{pmatrix} \implies B(u, v) = \begin{pmatrix} x(u, v) \\ y(u, v) \end{pmatrix}\]
Given region $D \subset \mathbb{R}^2$ and $S = B(D)$ is the region transformed by $B$, the integral of function $f(x, y)$ over region $D$ can be expressed as 
\[\iint_D f(x, y) \, dA = \iint_S f \big( x(u, v), y(u, v) \big) \, \big| J B(u, v) \big| \, d \bar{A}\]
where $\big| J B(u, v) \big|$ is the determinant of the Jacobian matrix of $B$. Expanding the Facobian determinant gives
\[\big| J B(u, v) \big| = \frac{\partial x}{\partial u} \frac{\partial y}{\partial v} - \frac{\partial x}{\partial v} \frac{\partial y}{\partial u}\]
\end{theorem}

\begin{theorem}[Integration over Change of Bases in $\mathbb{R}^3$]
Given that we have the change of basis function 
\[B: \mathbb{R}^3 \longrightarrow \mathbb{R}^3, \;\;\; B(u, v, w) = \begin{pmatrix} x(u, v, w) \\ y(u, v, w) \\ z(u, v, w) \end{pmatrix}\]
a region $D \in \mathbb{R}^3$ and $S = B(D)$, the region transformed by $B$, the integral of $f(x, y, z)$ over region $D$ can be expressed as 
\[\iiint_D f(x, y, z)\, dV = \iiint_S f\big( x(u, v, w), y(u, v ,w), z(u, v, w) \big) \big| J B (u, v, w)\big| \, d \bar{V}\]
where $\big| J B (u, v, w)\big|$ is the Jacobian determinant of $B$. 
\end{theorem}

\begin{example}
Given a real-valued function $f$ defined over the region $D \subset \mathbb{R}^2$, we can perform a change of basis of the $x, y$ coordinates into polar ones within a new region $S$. The change of basis 
\begin{align*}
    & x = r \cos{\theta} \\
    & y = r \sin{\theta} 
\end{align*}
\begin{center}
\begin{tikzpicture}
    \draw[thick, fill=lightgray] (7.5,2) circle (1.5);
    \draw[<->] (-1,0)--(3,0);
    \draw[<->] (0,-1)--(0,5);
    \draw[thick, fill=lightgray] (0,0) rectangle (2,4);
    \node at (1,2) {$S$};
    \draw[fill] (0,4) circle (0.05);
    \draw[fill] (2,0) circle (0.05);
    \node[below] at (2,0) {$1$};
    \node[left] at (0,4) {$2 \pi$};
    \node[above] at (3,0) {$r$};
    \node[right] at (0,5) {$\theta$};
    \draw[->] (2.5, 2)--(5,2);
    \node[above] at (4,2.5) {$T: (r, \theta) \mapsto$};
    \node[above] at (4,2) {$ (r \cos{\theta}, r \sin{\theta})$};
    \draw[<->] (5.5,2)--(9.5,2);
    \draw[<->] (7.5,0)--(7.5,4);
    \node at (8,2.5) {$D$};
\end{tikzpicture}  
\end{center}
\end{example}

\begin{theorem}[Integration over Change of Bases in $\mathbb{R}^n$]
Let $\mathbb{R}^n$ have the standard orthonormal basis $e_1, e_2, ..., e_n$, and let us construct a new basis $f_1, f_2, ..., f_n$ such that the coefficients of the vectors in $\mathbb{R}^n$ are related with the functions
\[B: \mathbb{R}^n \longrightarrow \mathbb{R}^n, \;\;\;\; B(u_1, u_2, \ldots, u_n) = \begin{pmatrix}
x_1 (u_1, \ldots, u_n) \\x_2 (u_1, \ldots, u_n) \\ \vdots \\ x_n (u_1, \ldots, u_n)
\end{pmatrix}\]
Given that the region $D \subset \mathbb{R}^n$ is transformed into a new region $S = B(D) \subset \mathbb{R}^n$ under this basis transformation, the integral of function $f(x_1, \ldots, x_n)$ over region $D$ can be expressed as 
\[\int_D f(x) \, dH = \int_S f \big( x_1(u), x_2(u), ..., x_n (u) \big) \big| J B(u_1, \ldots, u_n)\big| \, d \bar{H}\]
where the integral on both the left and right hand side represents integration over an $n$-dimensional region, $x$ represents the $n$-tuple $(x_1, \ldots, x_n)$, $u$ represents the $n$-tuple $(u_1, \ldots, u_n)$, and $\big| J B(u_1, \ldots, u_n)\big|$ represents the Jacobian determinant of function $B$. 
\end{theorem}

We now describe some common change of basis formulas for polar, cylindrical, and spherical coordinates. 

\begin{theorem}[Integration in Polar Coordinates]
\[\iint_{D} f(x, y) \, dx \,dy = \iint_S f(r \cos{\theta}, r \sin{\theta}) r \, dr \, d\theta\]
\end{theorem}

\begin{definition}[Cylindrical, Spherical Coordinates]
In $\mathbb{R}^3$, \textit{cylindrical coordinates} have the following relation to rectangular coordinates. 
\begin{align*}
    & x = r \cos{\theta} \\
    & y = r \sin{\theta} \\
    & z = z
\end{align*}
In $\mathbb{R}^3$, \textit{spherical coordinates} have the following relation to rectangular coordinates. 
\begin{align*}
    & x = \rho \sin{\phi} \cos{\theta} \\
    & y = \rho \sin{\phi} \sin{\theta} \\
    & z = \rho \cos{\phi}
\end{align*}
\end{definition}

\begin{corollary}[Integration in Cylindrical Coordinates]
\[\iiint_D f(x, y, z) \, dx \, dy \, dz = \iiint_S f( r \cos{\theta}, r \sin{\theta}, z) r \, dr \, d\theta \, dz\]
\end{corollary}

\begin{corollary}[Integration in Spherical Coordinates]
\[\iiint_D f(x, y, z) \,dx\,dy\,dz = \iiint_S f(\rho \sin{\phi} \cos{\theta}, \rho \sin{\phi} \sin{\theta}, \rho \cos{\phi}) \rho^2 \sin{\theta} \, d\rho \, d\theta \, d\phi\]
\end{corollary}

\begin{example}[Gaussian Integral]
The following is the (un-normalized) probability distribution function of the Gaussian distribution. 
\[\int_{-\infty}^{\infty} e^{-x^2} \, dx = \sqrt{\pi}\]
\end{example}

\subsection{Improper Integrals}
There are generally two types of improper integrals. 
\begin{enumerate}
    \item The region $D$ integrated over is unbounded. 
    \item The function $f$ that is integrated is unbounded within the region $D$.
\end{enumerate}
\subsubsection{Single Variable Improper Integrals}
These types of improper integrals are usually evaluated using a limiting process. When the interval $I$ is unbounded, say $(1, \infty)$, the integral can be evaluated as 
\[\int_1^\infty \frac{1}{x^2} \,dx = \lim_{b \rightarrow \infty} \int_1^b \frac{1}{x^2} \, dx = \lim_{b\rightarrow \infty} \bigg( 1 - \frac{1}{b} \bigg) = 1\]
In case 2, we can add a limit at the point where the function $f$ diverges as such. 
\[\int_0^1 \frac{1}{\sqrt{x}} \, dx = \lim_{a \rightarrow 0} \int_a^1 \frac{1}{\sqrt{x}} \, dx = \lim_{a \rightarrow 0} (2 - 2\sqrt{a}) = 2\]
We now describe how to integrate over a certain path $p$ embedded in a higher dimensional space $\mathbb{R}^n$, possibly with a scalar or vector field $f$. We must first go over oriented paths. 

\subsubsection{Two Variable Improper Integrals}
Extending the previous case, we use a multivariate limiting process in $\mathbb{R}^2$. We will first work with case 2, when $f$ is unbounded within the region $D$. Let us define an elementary region $D$ in $\mathbb{R}^2$; without loss of generality, we will make it $y$-simple, meaning that $D$ can be expressed as
  \[D \equiv \{ (x, y) \in \mathbb{R}^2 \; | \; a \leq x \leq b, \; \phi_1 (x) \leq y \leq \phi_2 (x)\}\]
We can actually assume that the region in which $f$ is unbounded lies in the boundary $\partial D$. This is because if it lied in the interior of $D$, we could split $D$ into pieces across a path that intersects this region with divergent values, evaluate the integrals over the pieces separately, and then sum the integrals. For example, in the rectangular region below, let the dashed line represent the values where the function $f$ diverges. Then, we can split the region into two rectangular regions shown in the right. 
\begin{center}
\begin{tikzpicture}
  \draw (0,0) rectangle (3,2);
  \draw[dashed] rectangle (2,0)--(2,2);
  \draw[->] (3.5,1)--(5,1);
  \draw (8,0)--(6,0)--(6,2)--(8,2);
  \draw[dashed] (8,0)--(8,2);
  \draw[dashed] (9,0)--(9,2);
  \draw (9,0)--(10,0)--(10,2)--(9,2);
\end{tikzpicture}
\end{center}
Therefore, assuming that $f$ is unbounded in $\partial D$, we can construct a new region 
\[D_{\eta, \delta} \equiv \{(x, y) \in \mathbb{R}^2 \; | \; a + \eta \leq x \leq b - \eta, \; \phi_1 (x) + \delta \leq y \leq \phi_2 (x) - \delta\}\]
for some arbitrarily small numbers $\eta, \delta >0$, meaning that the integral (reduced to iterated integrals using Fubini's theorem) 
\[F(\eta, \delta) \equiv \iint_{D_{\eta, \delta}} f(x, y) \, dA = \int_{a + \eta}^{b - \eta} \int_{\phi_1 (x) + \delta}^{\phi_2 (x) - \delta} f(x, y) \, dy\,dx\]
is well defined. 
\begin{center}
\begin{tikzpicture}
    \draw[<->] (-0.5,0)--(5,0);
    \draw[<->] (0,-0.5)--(0,5);
    \draw (1,1.5)--(1,3);
    \draw plot [smooth] coordinates {(1,3) (1.5,3.3) (2,3.1) (3,3.8) (4,4.2) (4.5,4)};
    \draw (4.5, 4)--(4.5,1);
    \draw plot [smooth] coordinates {(1,1.5) (2,1.7) (3, 1.6) (3.7, 1.3) ( 4.5,1)};
    \draw[dashed] (1.3,1.85)--(1.3,2.9);
    \draw[dashed] (4.2, 3.8)--(4.2,1.4);
    \draw[dashed] plot [smooth] coordinates {(1.3,2.9) (1.5,3) (2,2.8) (3,3.5) (4,3.9) (4.2,3.8)};
    \draw[dashed] plot [smooth] coordinates {(1.3,1.85) (2,2) (3, 1.9) (3.7, 1.6) ( 4.2,1.4)};
    \node at (3,2.5) {$D_{\eta, \delta}$};
    \node[below left] at (2,1) {$D$};
    \draw[->] (2,1)--(2.5,1.85);
\end{tikzpicture}
\end{center}
Clearly, the function $F( \eta, \delta)$ is a function of two variables $\eta$ and $\delta$. So, if the limit 
\[\lim_{(\eta, \delta) \rightarrow (0, 0)} F(\eta, \delta)\]
is well defined, then so is the improper integral. For it to exist, the iterated limits must both equal to a well-defined real number $\mathcal{L}$ (and to each other). That is, 
\[\lim_{\eta \rightarrow 0} \lim_{\delta \rightarrow 0} F(\eta, \delta) = \lim_{\delta \rightarrow 0} \lim_{\eta \rightarrow 0} F(\eta, \delta) = \mathcal{L} \implies \lim_{(\eta, \delta) \rightarrow (0,0)} F(\eta, \delta) = \mathcal{L}\]


It is also worthwhile to note that functions unbounded at isolated points can be evaluated using the methods above using a change of basis. Consider the example below. 

\begin{example}
In the unit disk $D \subset \mathbb{R}^2$, let the function $f$ be defined as 
\[f(x, y) \equiv \frac{1}{\sqrt{x^2 + y^2} }\]
Clearly, $f$ is continuous at every point except $0= (0,0)$, meaning that 
\[\iint_{D \setminus \{0\}} f(x, y)\, dA\]
is well-defined. In order to solve the integral over the entire disk, we convert to polar coordinates and evaluate the limit
\[\iint_{D \setminus \{0\}} f(x, y) \, dA = \lim_{\delta \rightarrow 0} \int_{\delta}^1 \int_0^{2 \pi} r \, f( r \cos{\theta}, r \sin{\theta}) \, d\theta \,dr\]
\end{example}
\begin{center}
\begin{tikzpicture}
    \draw[thick, fill=lightgray] (0,0) circle (1.5); 
    \draw[<->] (-2,0)--(2,0);
    \draw[<->] (0,-2)--(0,2);
    \draw[fill=white] (0,0) circle (0.08);
    \draw[->, thick] (2.5,0.5)--(4, 0.5); 
    \draw[<->] (4.5,-1)--(7,-1);
    \draw[<->] (5,-1.5)--(5,2);
    \draw[white, fill=lightgray] (5,-1) rectangle (6.5,1.5);
    \draw[thick] (5,-1)--(6.5,-1)--(6.5,1.5)--(5,1.5);
    \draw[thick, dashed] (5,-1)--(5,1.5);
    \draw[fill] (6.5,-1) circle (0.03);
    \node[below] at (6.5,-1) {$1$};
    \node[left] at (5,1.5) {$2 \pi$};
    \draw[fill] (5, 1.5) circle (0.03);
    \draw[fill] (1.5,0) circle (0.03);
    \draw[fill] (0,1.5) circle (0.03);
    \node[below right] at (1.5,0) {$1$};
    \node[above right] at (0,1.5) {$1$};
\end{tikzpicture}
\end{center}

If we are given an unbounded region $D \subset \mathbb{R}^2$, we can first create a bounded region and expand that region using a limit to cover all of $D$. 

\subsection{Line Integrals}
\begin{definition}[Orientations, Simple Curves, Closed Curves]
A path function $p: [a,b]\subset \mathbb{R} \longrightarrow \mathbb{R}^n$ determines a curve in $\mathbb{R}^n$ with endpoints $p(a)$ and $p(b)$. The direction the curve $p$ takes, that is from $p(a)$ to $p(b)$ in $\mathbb{R}^n$ is called the \textit{orientation} of $p$. A path or a curve with a defined orientation is called an \textit{oriented curve}. 

A \textit{simple curve} $C$ to be the image of an injective piecewise $C^1$ map $c: I \subset \mathbb{R} \longrightarrow \mathbb{R}^3$. Since it is inejctive, it does not intersect itself, and $C$ is piecewise smooth in $\mathbb{R}^n$. If $I = [a,b]$, then $c(a)$ and $c(b)$ are the endpoints of the curve. A simple curve with an orientation is called an \textit{oriented simple curve}. 

A closed curve $C$ is the image of piecewise $C^1$ map $c: [a,b] \longrightarrow \mathbb{R}^n$ such that $c(a) = c(b)$. That is, the endpoints of $C$ are equal. A \textit{simple closed curve} is a closed curve that is injective over the interval $[a,b)$. Note that a closed curve has two possible orientations. 
\end{definition}

If $C$ is an oriented simple curve or an oriented simple closed curve, then we can unambiguously define line integrals along them. 

\begin{definition}
Let $h$ be an injective function that takes $[\alpha,\beta] \subset \mathbb{R}$ to the interval $[a, b] \subset \mathbb{R}$. Given an oriented simple path function $p: [a,b]\subset \mathbb{R} \longrightarrow \mathbb{R}^n$, the composition
\[\rho = p \circ h: [\alpha, \beta] \longrightarrow \mathbb{R}^n\]
is called a \textit{reparamaterization} of $p$. Note that since $h$ is injective, it takes endpoints to endpoints. If $h$ preserves the direction in which the path travels, that is, if 
\[(p \circ h)(\alpha) = a \text{ and } (p \circ h)(\beta) = b\]
then $h$ is \textit{orientation preserving}. If
\[(p \circ h)(\alpha) = b \text{ and } (p \circ h)(\beta) = a\]
then $h$ is \textit{orientation reversing}. Note that a path $c$ having the same image as $p$ does not imply that $c$ is a reparamaterization of $p$, since $c$ may not be injective. 
\begin{center}
    \includegraphics[scale=0.25]{img/Orientation_Preserving_Reversing.PNG}
\end{center}
\end{definition}

\begin{definition}[Scalar Line Integral]
Let $f: \mathbb{R}^n \longrightarrow \mathbb{R}$, which can be interpreted as a scalar field. Now define a $C^1$ path function 
\[c: [a,b] \subset \mathbb{R} \longrightarrow \mathbb{R}^n \]
such that the composition of functions
\[f \circ c: [a, b] \subset \mathbb{R} \longrightarrow \mathbb{R}^n\]
is continuous. Then, the \textit{path integral}, or \textit{scalar line integral}, of $f$ along the path $c$. is defined
\begin{align*}
    \int_c f \;d s & = \int_a^b f\big(c(t)\big) ||c^\prime (t)|| \;d t \\
    & = \int_a^b f\big( x_1 (t), x_2 (t), ..., x_n (t)\big) ||c^\prime (t)|| \; d t
\end{align*}
If $c(t)$ is only piece-wise $C^1$, we can define the path integral by breaking $[a,b]$ into pieces over with $f\big( c(t)\big) ||c^\prime (t)||$ is continuous and then summing the integrals over the pieces. 
That is, 
\[\int_a^b f\big(c(t)\big) ||c^\prime (t)|| \;d t = \sum_{i = 0}^{n-1} \int_{\alpha_i}^{\alpha_{i+1}} f\big(c(t)\big) ||c^\prime (t)|| \; d t\]
\end{definition}
Note that since $f$ is a scalar-valued function, we can interpret a path integral as the sum of infinitesmal segments of the path $c$ having a weight determined by $f$ at each section. 
If $f$ is a constant function outputting $1$ at every point, then the path integral just outputs the length of the path $c$ in $\mathbb{R}^n$. 
\[L = \int_a^b f\big( c(t)\big) ||c^\prime (t)|| \; d t = \int_a^b ||c^\prime (t)|| \; d t\]

\begin{definition}[Vector Line Integral]
Let $F: \mathbb{R}^n \longrightarrow \mathbb{R}^n$ be a vector field on $\mathbb{R}^n$ that is continuous on the $C^1$ oriented path $c: [a, b] \subset \mathbb{R} \longrightarrow \mathbb{R}^n$. The \textit{line integral} of $F$ along $c$ is defined by the formula 
\[\int_c F \cdot d s = \int_a^b F\big( c(t)\big) \cdot c^\prime (t) \; d t\]
where $\cdot$ represents the dot product of $F$ with $c^\prime$ over the interval $[a,b]$. It is also commonly written in differential notation, 
\[\int_c F \cdot ds = \int_c F \cdot (dx_1, \ldots, d x_n) = \int_c F_1 dx_1 + F_2 dx_2 + \ldots F_n dx_n\]
\begin{center}
    \includegraphics[scale=0.27]{img/Vector_Line_Integral.PNG}
\end{center}
 Similarly with path integrals, we can also define line integrals as the sum of integrals over piece-wise continuous sections of $c$. That is, given an oriented curve $C$ made up of several oriented component curves $C_i$, $i = 1, 2, ..., k$, we can paramaterize $C$ by paramaterizing the pieces $C_i$'s separately. Thus, we can treat $C = C_1 + ... C_k$ and get
\[\int_C F \cdot d s = \sum_{i = 1}^k \int_{C_i} F \cdot d s\]
Note that a vector line integral is a generalization of scalar line integrals, so any results holding for vector line integrals also holds for their scalar counterpart. 
\end{definition}

\begin{example}[Work]
In mechanics, work $W$ is defined as 
\[W = F \cdot d\]
where $F$ is force and $d$ is displacement. With this knowledge, the reader can easily see that the work done by vector field $F$ on a particle traveling along a path $c$ from time $a$ to time $b$ can be calculated by the line integral
\begin{align*}
    W & = \int_a^b F\big( c(t)\big) \cdot c^\prime (t) \; d t \\
    & = \int_c F_1 dx + F_2 dy + F_3 dz
\end{align*}
\end{example}

\begin{theorem}[Invariance of Path Paramaterizations on Vector Line Integrals]
Let $F$ be a vector field and $f$ be a scalar field, both continuous on the $C^1$ path function $p: [a,b] \longrightarrow \mathbb{R}^n$ and let $q: [\alpha, \beta] \longrightarrow \mathbb{R}^n$ be a reparamaterization of $p$. Then, 
\begin{align*}
    q \text{ is orientation preserving} & \implies \int_p F \cdot d s = \int_q F \cdot d s \\
    q \text{ is orientation reversing} & \implies \int_p F \cdot d s = - \int_q F \cdot d s
\end{align*}
\end{theorem}

\subsubsection{Conservative Vector Fields}
We now introduce a fundamental theorem about line integrals over gradient fields. Recall the fundamental theorem of calculus and it's equivalent form. 

\begin{theorem}[Fundamental Theorem of Single Variable Calculus]
Let function $\nabla g: \mathbb{R} \longrightarrow \mathbb{R}$ be the gradient of the single variable $C^1$ function $g: \mathbb{R} \longrightarrow \mathbb{R}$; that is, $\nabla g$ is a conservative vector field on $\mathbb{R}$. Then, 
\[\int_a^b \nabla g (x) \,dx = g(b) - g(a)\]
Note that in the single variable case, 
\[\frac{d}{dx} g(x) = \nabla g(x)\]
This means that the value of the integral of $\nabla g$ only depends on the value of $g$ at the endpoints of the interval $[a,b]$. 
\end{theorem}

We can extend this to line integrals for functions mapping $\mathbb{R}^n$ to $\mathbb{R}$. 

\begin{theorem}[Invariance of Line Integrals in Conservative Vector Fields]
Given that $F: \mathbb{R}^n \longrightarrow \mathbb{R}^n$ is a $C^1$ conservative vector field with $\nabla f = F$ for $C^2$ function $f: \mathbb{R}^n \longrightarrow \mathbb{R}$ and path function $p: [a,b] \longrightarrow \mathbb{R}^n$ is a piecewise $C^1$ path, then 
\[\int_p F \cdot d s = \int_p \nabla f \cdot d s = f\big(p(b)\big) - f\big(p(a)\big)\]
That is, the line integral of any path in a conservative vector field is dependent on the value of $f$ at the endpoints $p(a)$ and $p(b)$. 
\begin{center}
    \includegraphics[scale=0.2]{img/Line_Integral_Independence_of_path.PNG}
\end{center}
\end{theorem}

In physics, calculating the work done by a force represented by a vector field requires us to know the path that it travels through. 
\[W = \int_p F \cdot ds\]
However, in many cases $F$ is assumed to be conservative, so it is only necessary that we find the displacement of the particle from its endpoints, resulting in the simplification of the formula.  
\[W = \int_p \nabla f \cdot ds = f\big( p(b)\big) - f \big(p(a)\big)\]

\begin{corollary}[Equivalent Conditions for Vector Field to be Conservative]
The following conditions are equivalent: 
\begin{enumerate}
    \item $F: \mathbb{R}^n \longrightarrow \mathbb{R}^n$ is a conservative vector field. 
    \item The line integral of $F: \mathbb{R}^n \longrightarrow \mathbb{R}^n$ in curve $C$ is path independent; that is, if $C_1$ and $C_2$ are two paramaterizations of $C$, 
    \[\int_{C_1} F \cdot ds = \int_{C_2} F \cdot ds\]
    \item Given that $C$ is a closed loop, the line integral of $F: \mathbb{R}^n \longrightarrow \mathbb{R}^n$ across $C$ is $0$. 
    \[\oint_C F \cdot ds = 0\]
    \item The curl of $F: \mathbb{R}^3 \longrightarrow \mathbb{R}^3$ vanishes
    \[\curl{F} = \nabla \times F = \begin{pmatrix}
    \frac{\partial F}{\partial x} \\\frac{\partial F}{\partial y} \\\frac{\partial F}{\partial z} 
    \end{pmatrix} \times \begin{pmatrix}
    F_1\\F_2\\F_3
    \end{pmatrix}= 0\]
    \item The following partial derivatives of $F: \mathbb{R}^2 \longrightarrow \mathbb{R}^2$ are equal
    \[\frac{\partial F_1}{\partial y} = \frac{\partial F_2}{\partial x}\]
\end{enumerate}
\end{corollary}

We can develop a bit of intuition to determine whether a vector field is conservative or not. If vector field $F$ is conservative, then there exists a smooth scalar field $f$ such that $\nabla f = F$. For each latitude and longitude on a certain map, we can give it an altitude as a function of those coordinates (picture a map with a bunch of hills and valleys). The gradient and thus the vector field is all the vectors that point in the direction of highest ascent. he vector field is all the vectors that point in the direction of highest ascent. Extending the metaphor the path integral is like starting on at a point and climbing the hills and valleys, creating work as you go up a hill (proportional to the steepness and thus the dot product of your motion vector with the gradient vector field in the path integral) and decreasing the work you put in by going down a hill. Since the path is closed, it is like you are going up and down the same amount overall, so the path integral is zero. Following this analogy, the vector field determined by this function (marked as arrows in the $x, y$ plane) is conservative. 
\begin{center}
    \includegraphics[scale=0.28]{img/Conservative_Vector_Field.jpg}
\end{center}
If we can construct a closed loop around $F$ where the line integral is nonzero, then it means that we have ended up at a "higher" or "lower" (altitude) at the same point. This means that rather than being a certain landscape, there exist different "levels" of values at one point, like a spiraling staircase. For example, look at the solenoidal vector field below, where we can construct a closed loop (a circle going around the origin counterclockwise). There is no "surface" that can be defined such that it contains the solenoid. 
\begin{center}
    \includegraphics[scale=0.28]{img/Solenoid_nonconservative.jpg}
\end{center}
Clearly, as a particle travels through the vector field along the path, it does positive work while it has zero displacement, and clearly, there exists no function that can output both these values as determined by vector field $F$. 

\begin{theorem}[Helmholtz Decomposition]
Let $F: \mathbb{R}^3 \longrightarrow \mathbb{R}^3$ be a $C^2$ vector field. Then, $F$ can be decomposed into a curl-free component and a divergence-free component. That is, there exists vector fields $A$ and $\Phi$
\[F = - \nabla \cdot \Phi + \nabla \times A\]
\end{theorem}

\subsubsection{Curvature}
\begin{definition}[Curvature at a Point]
Let $c: [a, b] \longrightarrow C \subset \mathbb{R}^3$ be a unit-speed paramaterization of $C$, meaning that $||c^\prime (t)|| = 1$ for all $t \in [a,b]$, and let $p = c(t_0)$ be a point in $C$. The \textit{curvature} $\kappa(p)$ at $p$ is a mapping defined
\[\kappa: C \longrightarrow \mathbb{R}, \;\; \kappa(p) \equiv ||c^{\prime \prime} (t_0)||\]
Notice that since we require a unit speed paramaterization of $C$, we do not need to worry about how a given curve is paramaterized. 
\end{definition}

Since the curvature is defined pointwise for each point in curve $C$, we can integrate over all the curvatures in $C$ to define the total curvature. 

\begin{definition}[Total Curvature]
The \textit{total curvature} of a curve $c: [a,b] \longrightarrow C \subset \mathbb{R}^3$ is the scalar line integral 
\[\int_C \kappa \, ds\]
\end{definition}

We now present an important theorem in differential geometry. 
\begin{theorem}[Fary-Milnor Theorem]
Given a unit speed paramaterization $c: [a,b] \longrightarrow C \subset \mathbb{R}^3$, if $C$ is closed (that is, $c(a)=c(b)$), then 
\[\oint_C \kappa\, ds \geq 2 \pi\]
and equals $2\pi$ only when $C$ is a circle. Furthermore, if $C$ is a closed space curve with 
\[\oint_C \kappa\, ds \leq 4\pi\]
then $C$ is "unknotted." That is, $C$ can be continuously deformed without every intersecting itself into a planar circle. Therefore, for knotted curves $C$, we have
\[\oint_C \kappa \, ds > 4\pi\]
\end{theorem}

\subsection{Surface Integrals}
Surface integrals are the $2$-dimensional analogue, or the double integral version, of line integrals. It is the integration of surfaces. 

\subsubsection{2-Dimensional Paramaterizations of Surfaces}
Just like how we create path functions using a paramaterization function $p: [a, b] \subset \mathbb{R} \longrightarrow \mathbb{R}^n$, we can parameterize surfaces by defining a function 
\[\varphi: D \subset \mathbb{R}^2 \longrightarrow \mathbb{R}^n, \;\;\; \varphi (u, v) \equiv \begin{pmatrix} x_1 (u, v) \\ \vdots \\ x_n (u, v) \end{pmatrix}\]
The surface 
\[S = \varphi(D)\]
corresponding to the function $\varphi$ is its image. If $\varphi$ is differentiable or is of class $C^1$, then we call $S$ a \textit{differentiable} or $C^1$ surface, respectively. 

For those that are familiar with differential geometry, this makes every paramaterized surface a 2-manifold induced by the single homeormophism $\varphi$. In fact, it is more than just locally homeomorphic; it is \textit{globally} homeomorphic. 


\begin{definition}[Tangent Vectors of Surfaces Embedded in $\mathbb{R}^3$]
Given surface paramaterization 
\[\varphi: \mathbb{R}^2 \longrightarrow \mathbb{R}^3, \;\;\; \varphi(u, v) \equiv \begin{pmatrix}
x (u, v) \\ y(u, v) \\ z(u, v) 
\end{pmatrix}\]
it is visually clear that there can be up to two linearly independent tangent vectors at a point on the surface $S$. We can calculate these two vectors by embedding two nonparallel paths in $D \subset \mathbb{R}^2$ and taking the derivative with respect to a point traveling through these paths, which would give us a tangent vector on $S$. To keep things simple, we take the partial derivatives with respect to $u$ and $v$. 
\begin{center}
    \includegraphics[scale=0.28]{img/Partial_Derivatives_with_respect_to_U_V.PNG}
\end{center}
Clearly, these paths are functions 
\begin{align*}
    \frac{\partial \varphi}{\partial u} \equiv \begin{pmatrix}
     \frac{\partial x}{\partial u} \\ \frac{\partial y}{\partial u} \\ \frac{\partial z}{\partial u}
    \end{pmatrix} : \mathbb{R}^2 \longrightarrow \mathbb{R}^3\\
    \frac{\partial \varphi}{\partial v} \equiv \begin{pmatrix}
     \frac{\partial x}{\partial v} \\ \frac{\partial y}{\partial v} \\ \frac{\partial z}{\partial v}
    \end{pmatrix} : \mathbb{R}^2 \longrightarrow \mathbb{R}^3
\end{align*}
where 
\[\frac{\partial \varphi}{\partial u} (u_0 ,v_0), \; \frac{\partial \varphi}{\partial v} (u_0, v_0)\]
represent two vectors in $\mathbb{R}^3$ that are tangent to $S$ at the point $\varphi(u_0, v_0) \in \mathbb{R}^3$. 
\end{definition}

We must make sure that the surface $S$ is smooth in the sense that (informally) there aren't any wrinkles, points, folds, or self-intersections in such a way that the tangent plane to the surface is not well-defined. 

\begin{definition}[Regular Surfaces]
To formalize this concept, we say that $S$ is \textit{regular}, or \textit{smooth}, at point $(u_0, v_0)$ if
\[\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v} \neq 0\]
where $\times$ is the Euclidean cross product. That is, if the vector that is orthogonal to the two tangent vectors is well defined at a point, the surface is said to be smooth at that point. Note that $\frac{\partial \varphi}{\partial u}$ is parallel to $\frac{\partial \varphi}{\partial v}$ if and only if their cross product is $0$. 
\begin{center}
    \includegraphics[scale=0.3]{img/Cross_Product_Regular_Surfaces.PNG}
\end{center}
It is quite clear that $(\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v})(u_0, v_0) \neq 0 \implies \frac{\partial \varphi}{\partial u}$ and $\frac{\partial \varphi}{\partial v}$ are linearly independent. This means that an entire span of tangent vectors, i.e. a tangent plane, of the surface $S$ at $\varphi(u_0, v_0)$ exists. 
$S$ is said to be \textit{regular} if it is regular at all points $\varphi(u_0, v_0) \in S$. 
\end{definition}

In fact, the tangent plane at $\varphi(u_0, v_0)$ is the set of points 
\[\{\varphi(u_0, v_0) + \frac{\partial \varphi}{\partial u} (u_0, v_0) c_1 + \frac{\partial \varphi}{\partial v} (u_0, v_0) c_2 \; | \; c_1, c_2 \in \mathbb{R} \}\]
which is precisely the affine tangent plane spanned by $T_u$ and $T_v$. Note also that the vector $T_u \times T_v$, if nonzero, is normal to this plane, which leads to this equivalent definition. 

\begin{definition}[Tangent Planes of Surfaces]
Given a paramaterized surface $\varphi: D \subset \mathbb{R}^2 \longrightarrow \mathbb{R}^3$ that is regular at $\varphi(u_0, v_0)$, the tangent plane of the surface $S$ at $\varphi(u_0, v_0) = (x_0, y_0, z_0)$ is defined
\[\{(x, y, z) \in \mathbb{R}^3 \;|\; (x-x_0, y-y_0, z-z_0) \cdot n = 0\}\]
where $n = (\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v})(u_0, v_0)$. 
\end{definition}

We finally construct the concept of signed areas before defining surface integration. 
We have all the tools we need to calculate surface areas, but remember that integration also covers the concept of \textit{signed areas}, which could be negative. In order to define this, we define the concept of orientation on surfaces. 

\subsubsection{Orientation of Surfaces}

\begin{definition}[Oriented Surfaces]
An \textit{oriented surface} is a two-sided surface with one side specified as the \textit{outside/positive} side and the other side as the \textit{inside/negative} side. Note that an oriented surface is not guaranteed to have two sides (e.g. a Mobius strip). To ensure that there exist two sides, $S$ must be regular. 

Surprisingly, a paramaterization does not have an intrinsic orientation. Rather, we determine the orientation ourselves by choosing a unit vector that generally points towards the outside of the surface $S$. Again, this choice is arbitrary, but it is customary to choose a vector that generally points "out." Either way, the orientation (unit) vector at every point $\varphi(u, v) \in S$, denoted as $n$, is 
\[n\big(\varphi(u, v)\big) = \pm \frac{\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v}}{\big|\big|\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v}\big|\big|}\]
which can be visually calculated using the right hand rule. 
\begin{center}
    \includegraphics[scale=0.23]{img/Orientation_Unit_Vector.PNG}
\end{center}
\end{definition}

\begin{definition}[Orientation Preserving, Reversing Paramaterizations]
Given an oriented surface $S$ with its positive side determined by the direction of unit vector $n\big( \varphi(u,v)\big)$, the paramaterization $\varphi$ is said to be \textit{orientation preserving} if 
\[n \big( \varphi(u, v)\big) = \frac{\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v}}{\big|\big|\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v}\big|\big|}\]
and \textit{orientation reversing} if
\[n \big( \varphi(u, v)\big) = - \frac{\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v}}{\big|\big|\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v}\big|\big|}\]
\end{definition}

So, to find whether a paramaterization is orientation preserving or reversing, it suffices to find the cross product $T_u \times T_v$ and see if it points in the same direction of the normal vector $n$ (which should have already been determined when deciding the orientation of $S$). 

Given a paramaterization $\varphi$ and an un-oriented surface $S$, we can also just construct $\varphi$ to be orientation-preserving (or reversing) by \textit{defining} the normal vector $n$ to be 
\[n\big( \varphi(u, v)\big) = \frac{\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v}}{\big|\big|\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v}\big|\big|} \;\; \bigg( \text{or } n\big( \varphi(u, v)\big) = - \frac{\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v}}{\big|\big|\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v}\big|\big|} \bigg)\]
So rather than finding out whether a paramaterization $\varphi$ is orientation preserving or reversing by comparing $T_u \times T_v$ with $n$, we have defined $n$ in a way such that $\varphi$ must be orientation preserving (or reversing). We can utilize these tools of paramaterization to now define the surface integral. 

\subsubsection{Scalar, Vector Surface Integrals}

A physical interpretation of a scalar surface integral is the weighted surface area of a certain surface. 

\begin{definition}[Scalar Surface Integrals]
Let $f: \mathbb{R}^3 \longrightarrow \mathbb{R}$ be a $C^1$ scalar field defined on a paramaterized surface $S \subset \mathbb{R}^3$ with paramaterization $\varphi: D \subset \mathbb{R}^2 \longrightarrow \mathbb{R}^3$. That is, $\varphi(D) = S$. We define the integral $f$ over $S$ to be
\begin{align*}
    \iint_S f \; dS & = \iint_S f(x, y, z) \; dS \\
    & = \iint_D f\big( \varphi(u, v)\big) \bigg|\bigg|\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v}\bigg|\bigg| \; du \,dv
\end{align*}
Note that this will require us to transform $f$, a function of $x, y, z$, into the function $f \circ \varphi$ of $u, v$. Additionally, if the paramaterization of the surface $S$ is not defined, then it one must be constructed. It is also clear that if $S$ is a union of surfaces $S_i$, then its surface integral is the sum of the surface integrals of the $S_i$'s. 
\end{definition}

Letting the scalar field $f$ be the constant field equal to $1$, the scalar surface integral measures the surface area of $S$. 
\[A(S) = \iint_S \; dS = \iint_D \Big|\Big|\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v}\Big|\Big| \; du\, dv\]
It is easy to see that the orientation of the paramaterization $\varphi$ does not affect scalar surface integrals, since the sign of the orientation gets nullified by the absolute value sign over $||\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v}||$. 

Its physical interpretation is to measure the rate at which a fluid (determined by a vector field $F$) is crossing a given surface $S$. It also has many applications in electromagnetism. 

\begin{definition}[Vector Surface Integrals]
Let $F$ be a vector field defined on surface $S$, the image of a paramaterized surface $\varphi$. The \textit{surface integral} of $F$ over $S$ is defined below, which is equivalent to summing up the dot product of the vector field and the normal vector to the surface. 
\begin{center}
    \includegraphics[scale=0.3]{img/Vector_Surface_Integral.jpg}
\end{center}
It can be calculated with the following formulas by converting it into a scalar surface integral where the scalar field is the value of the dot product of the vector field with the normal vectors of the surface. 
\begin{align*}
    \iint_{S} F \cdot d S & = \iint_S (F \cdot n) \; dS \\
    & = \iint_D \Bigg( F\big( \varphi(u, v)\big) \cdot \frac{\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v}}{\Big|\Big|\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v} \Big|\Big|} \Bigg) \, \bigg|\bigg|\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v}\bigg|\bigg|\; du\,dv \\
    & = \iint_D F\big(\varphi(u, v)\big) \cdot \bigg( \frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v}\bigg) \; du\,dv
\end{align*}
\end{definition}

Since we are now talking about vector fields, the orientation of the paramaterization is now significant. Visually, if the orientation of the surface $S$ generally aligns with the vector field $F$, then the integral will be positive (since two vectors $\alpha, \beta$ generally pointing in the same direction implies that $\alpha \cdot \beta > 0$). The orientation of the paramaterization, which is dependent on $\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v}$, determines the direction of the normal vector $n$ (since it is defined to be $(\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v}) / \big|\big|\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v}\big|\big|$. Therefore, changing the orientation of $\varphi$ will reverse the direction of $n$, which will then reverse the sign of the integral since $n$ now points in the opposite direction of the vector field $F$ than it previously did (by reversing the sign of the dot products). This is formalized in the theorem below. 

\begin{theorem}[Invariance of Surface Paramaterizations on Vector Surface Integrals]
Let $S$ be an oriented surface and let $\varphi_1$ and $\varphi_2$ be two regular paramaterizations with $F$ a continuous vector field defined on $S$. Then, assuming $\varphi_1$ is orientation preserving, 
\begin{align*}
    \varphi_2 \text{ is orientation preserving } & \implies \iint_{\varphi_1} F \cdot d S = \iint_{\varphi_2} F \cdot d S \\
    \varphi_2 \text{ is orientation reversing } & \implies - \iint_{\varphi_1} F \cdot d S = \iint_{\varphi_2} F \cdot d S 
\end{align*}
\end{theorem}

\subsubsection{Surface Integrals over Graphs}
Given that we have the graph of a function $g: \mathbb{R}^2 \longrightarrow \mathbb{R}$ rather than a general surface, we can paramaterize it simply as
\[\varphi(u, v) \equiv \big(u, v, g(u, v) \big)\]
\begin{center}
    \includegraphics[scale=0.25]{img/Paramaterize_Surfaces_as_Graphs.PNG}
\end{center}
This means that 
\[\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v} = 
\begin{pmatrix}
-\frac{\partial g}{\partial u} \\ -\frac{\partial g}{\partial v} \\ 1
\end{pmatrix} \implies \bigg|\bigg|\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v} \bigg|\bigg| = \sqrt{1 + \Big(\frac{\partial g}{\partial u}\Big)^2 + \Big( \frac{\partial g}{\partial v}\Big)^2}\]
So we can simplify the equation for the surface area $S$ of the graph of $g$ over the region $D$ in the $x y$-plane, as 
\begin{align*}
    A(S) & = \iint_S \; d S = \iint_D \bigg|\bigg|\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v} \bigg|\bigg| \; d A \\
    & = \iint_D \sqrt{1 + \Big(\frac{\partial g}{\partial u}\Big)^2 + \Big( \frac{\partial g}{\partial v}\Big)^2} \; d u \, d v
\end{align*}
With the same $g$, we can find the weighed surface area of $S$ over the scalar function $f: \mathbb{R}^3 \longrightarrow \mathbb{R}$ with the formula
\[\iint_S f \; d S = \iint_D f\big(u, v, g(u, v)\big) \sqrt{1 + \Big(\frac{\partial g}{\partial u}\Big)^2 + \Big( \frac{\partial g}{\partial v}\Big)^2} \; d u \, d v\]
Finally, with the same graph $g$, the surface integral over the vector field $F$ is
\begin{align*}
    \iint_S F \cdot d S & = \iint_D F\big(\varphi(u, v)\big) \cdot \bigg( \frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v}\bigg) \; d u \, d v \\
    & = \iint_D \bigg( F_1(u, v) \Big(- \frac{\partial g}{\partial u}\Big) + F_2 (u, v) \Big( - \frac{\partial g}{\partial v} \Big) + F_3 (u, v) \bigg) \; d u \, d v
\end{align*}

\subsection{Integral Theorems}
Recall the differential notation for writing line integrals. For 2 and 3 dimensions, it is written as
\begin{align*}
    & \int_C F \cdot d s = \int_C F \cdot (d x, d y) = \int_C F_1 \, d x + F_2 \, d y \\
    & \int_C F \cdot d s = \int_C F \cdot (d x, d y, d z) = \int_C F_1 \, d x + F_2 \, d y + F_3 \, d z 
\end{align*}
\subsubsection{Green's Theorem}
Green's Theorem gives the relationship between a line integral around a simple closed curve $C$ and a double integral over the plane region $D$ bounded by $C$. 

\begin{theorem}[Green's Theorem in $\mathbb{R}^2$]
Let there be a 2-dimensional $C^1$ vector field $F$ on $\mathbb{R}^2$ defined on a simple oriented closed piecewise-smooth curve $C$ and its bounded region $D \subset \mathbb{R}^2$ (that is, $C = \partial D$). Let the orientation of the path of $C$ be such that it is traveling \textit{counterclockwise}, i.e. a point traveling through $C$ would see the region $D$ to its \textit{left}, denoted as $C^+$ and the clockwise orientation as $C^-$.  Then, 
\[\oint_{C^+} F_1 \, d x + F_2 \, d y = \iint_D \bigg( \frac{\partial F_2}{\partial x} - \frac{\partial F_1}{\partial y} \bigg) \; d x \, d y\]
By reversing the orientation, it is clear that we have
\[\oint_{C^-} F_1 \, d x + F_2 \, d y = - \iint_D \bigg( \frac{\partial F_2}{\partial x} - \frac{\partial F_1}{\partial y} \bigg) \; d x \, d y\]
Note that this theorem is expressed in terms of the components of the vector field $F$. 
\begin{center}
\begin{tikzpicture}
    \draw[fill=lightgray] plot [smooth cycle] coordinates {(2,3) (3,4) (5,4) (5,3) (3,2)};
    \node [above left] at (4,3) {D};
    \begin{axis}[view={0}{90}]
    \addplot3 [lightgray,-stealth,samples=10,
        quiver={
            u={3*x/pow(x^2 + y^2,1/2)},
            v={-2*y/pow(x^2 + y^2,1/2)},
            scale arrows=0.2,
        },] { 1};
    \end{axis}
    \node at (2,1) {$F = (F_1, F_2)$};
    \node at (5,4.7) {$C^+ = \partial D$};
    \draw (4.3,2.4)--(4.45,2.6)--(4.2,2.6);
\end{tikzpicture}
\end{center}
\end{theorem}

Green's theorem has many applications in physics. For example, in order to solve two-dimensional flow integrals measuring the sum of fluid outflowing from a volume, Green's theorem allows us to calculate the total outflow summed about an enclosing area . 

\begin{corollary}
Let $D$ be a region for which Green's theorem applies with positively oriented boundary $\partial D$. Then, the area of $D$ can be computed with the formula
\[A(D) = \frac{1}{2} \oint_{\partial D} x \, d y - y \, d x\]
\end{corollary}

Green's theorem can be used to determine the area of centroid of plane figures solely by integrating over the perimeter. 

\subsubsection{Stokes' Theorem}
Green's theorem relates line integrals to double integrals. Stokes' theorem generalizes Green's theorem by relating line integrals to surface integrals of 2-dimensional surfaces embedded in $\mathbb{R}^3$. 

\begin{theorem}[Stokes' Theorem]
Let $S$ be an oriented regular surface defined by paramaterization $\varphi: D \subset \mathbb{R}^2 \longrightarrow \mathbb{R}^3$, and let the image of the boundary $\partial D$ under $\varphi$ be the boundary $\partial S$ of $S$. We can interpret $\partial S$ as a path mapping from $\mathbb{R} \longrightarrow S \subset \mathbb{R}^3$. 
\begin{center}
  \includegraphics[scale=0.27]{img/Boundary_Mapping.PNG}
\end{center}
The orientation unit vector $n$ of $S$ induces the positive orientation of $\partial S$, denoted $\partial S^+$. Visually, if you are walking along the curve with your head is pointing in the same direction as the unit normal vectors while the surface is on the left then you are walking in the positive direction on $\partial S$. 
\begin{center}
    \includegraphics[scale=0.8]{img/Stokes_Theorem_Orientation.png}
\end{center}
Given that $F$ is a $C^1$ vector field defined on $S$, then
\[\iint_S \curl{F} \cdot dS = \iint_S \big( \nabla \times F \big) \cdot d S = \oint_{\partial S^+} F \cdot d s\]
If $S$ has no boundary, that is, if the image of $p^\prime = \partial S$ is not a simple closed curve, then the integral is $0$. 
\end{theorem}

The above theorem implies that the vector surface integral of a surface without a boundary (i.e. a closed graph, such as a sphere) is always $0$ along the curl of any $C^1$ field. Geometrically, this means that given a closed solid $S$ with field $\nabla \times F$, the rate of flow of the vector field into $S$ is equal to the flow out of $S$. 

\subsubsection{Gauss' Theorem}
The divergence theorem relates the flux of a vector field through a closed surface to the divergence of the field in the volume enclosed. 

\begin{theorem}[Gauss' Divergence Theorem]
Let $V$ be a subset of $\mathbb{R}^3$. Denote by $\partial V$ the oriented closed surface that bounds $V$ (with outward pointing normal orientation vectors), and let $F$ be a $C^1$ vector field defined on a neighborhood of $V$. Then, 
\[\iiint_V \Div{F} \; d V = \iiint_V (\nabla \cdot F) \; d V = \oiint_{\partial V} F \cdot d S = \oiint_{\partial V} (F \cdot n) \; dS\]
where the two left-most integrals are volume integrals, and the two right-most integrals are surface integrals. Intuitively, this makes sense; the volume integrals represent the total of the sources in volume $V$, and the right hand side represents the total flow across the boundary $\partial V$. 
\begin{center}
    \includegraphics[scale=0.35]{img/Gauss_Theorem_Volume.png}
\end{center}
\end{theorem}



\end{document}


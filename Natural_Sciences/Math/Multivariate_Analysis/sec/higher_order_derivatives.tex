\section{Higher Order Derivatives} 

  So far, we have talked about derivatives as linear maps. However, this language is a bit too restrictive because it doesn't allow us to represent the derivatives of higher order using matrices. It turns out that these higher order derivatives are tensors, so let's generalize our definitions. Recall from \hyperref[la-linmaps_are_tensors]{linear algebra} that a linear map $f: V \to W$ is an element of the tensor product space $V^\dagger \otimes W$, where $V^\dagger$ represents the dual space of $V$. That is, 
  \begin{equation}
    \mathrm{Hom}(V, W) \simeq V^\dagger \otimes W
  \end{equation}

  \begin{definition}[Frechet Derivative]
    A function $f: V \longrightarrow W$ is \textbf{differentiable} at $x \in V$ if there exists a unique tensor $D f_x \in V^\dagger \otimes W$---called the \textbf{total derivative} or \textbf{Frechet derivative}---such that 
    \begin{equation}
      \lim_{h \to 0} \frac{\| f(x + h) - f(x) - D f_x h \|}{\| h \|} = 0 
    \end{equation} 
  \end{definition}

  It's a simple---almost trivial---change, but extremely powerful. Note that if $f$ is differentiable over $V$, then the transformation $Df: V \to (V^\dagger \otimes W)$ that maps $x_1$ to its Frechet derivative $Df_{x_1}$ can also be analyzed. Supposing that it is differentiable, the derivative of it at $x$ is 
  \begin{equation}
    D^2 f_x \coloneqq D(Df)_x \in V^\dagger \otimes (V^\dagger \otimes W) \simeq (V^\dagger)^{\otimes 2} \otimes W
  \end{equation}

  by the definition above. Intuitively this also make sense: if we want to take the second derivative of $f$, we must first specify the point in the domain of $Df$ (our velocity) plus the point in the domain of $f$ itself (our position) to compute it (the acceleration). Continuing this pattern, we get the following recursive definition. 

  \begin{definition}[$k$th Frechet Derivative]
    For $k > 1$, a function $f: V \longrightarrow W$ is \textbf{$k$-times differentiable} at $x \in V$ if 
    \begin{enumerate}
      \item $f$ is $k-1$ times differentiable at $x \in V$, and 
      \item  there exists a unique rank $(k, 1)$ tensor $D^k f_x \in (V^\dagger)^{\otimes k} \otimes W$---called the \textbf{$k$th total derivative} or \textbf{$k$th Frechet derivative}---such that 
      \begin{equation}
        \lim_{h \to 0} \frac{\| D^{k-1} f_x (x + h) - D^{k-1} f_x (x) - D^k f_x h \|}{\| h \|} = 0 
      \end{equation} 
      where $D^{k-1} f_x$ is the $(k-1)$th derivative of $f$ at $x$.
    \end{enumerate}
  \end{definition}

  Generally, multivariate calculus courses do not go into tensor algebra, so these higher order derivatives are omitted; you only work with the Hessian for scalar-valued functions. But we won't be scared off and will present this in full generality, and this gives us two major benefits: 
  \begin{enumerate}
    \item We can compute higher order derivative as we have seen. 
    \item We can compute functions that also input or output tensors, such as matrix-valued functions. This knowledge becomes very useful for optimization, e.g. when you optimize a neural network with respect to matrices (in fully-connected layers) and 3-tensors (e.g. convolutions) and is a must-know for building autograd libraries.  
  \end{enumerate} 

  \begin{definition}[$k$th Partial Derivative]
    
  \end{definition}

  Directional derivatives lose meaning in these higher order regimes, and we just work with iterated partials. 

  \begin{definition}[Hessian]
    Given $f: E \subset V \to \mathbb{R}$ twice-differentiable at $x$, the \textbf{Hessian} of $f$ at $x$ is the matrix realization of the total derivative, denoted $Hf_x$. 
  \end{definition}

  \begin{theorem}[Entries of Hessian are 2nd Partials]
    The entries of the Hessian matrix are precisely the partials. 
    \begin{equation}
      H f_{x} \coloneqq \begin{pmatrix} 
        \partial_{x_1 x_1} (\mathbf{a}) & \ldots & \partial_{x_1 x_n} (\mathbf{a}) \\
        \vdots & \ddots & \vdots \\
        \partial_{x_n x_1} (\mathbf{a}) & \ldots & \partial_{x_n x_n} (\mathbf{a}) \\
        \end{pmatrix} \text{ and } H f \coloneqq \begin{pmatrix} 
        \partial_{x_1 x_1} & \ldots & \partial_{x_1 x_n} \\
        \vdots & \ddots & \vdots \\
        \partial_{x_n x_1} & \ldots & \partial_{x_n x_n} \\
      \end{pmatrix}
    \end{equation}
  \end{theorem}
  \begin{proof}
    
  \end{proof}

\subsection{k-Times Continuously Differentiable Functions}

  \begin{definition}[$C^k$ Functions]
    A function $f: D \subset \mathbb{R}^n \longrightarrow \mathbb{R}$ is said to be a $C^k$ function if all $k$-times iterated partial derivatives 
    \begin{equation}
      \partial_{x_{i_1} x_{i_2} \ldots x_{i_k}} f
    \end{equation}
    exist and are continuous. The vector space of all $C^k$ functions is denoted $C^k (D; \mathbb{R})$, or $C^k(D)$. 
  \end{definition}

  Whenever we want to get the $k$th iterated partial derivative of $f$, we will assume that $f \in C^k$. Again, this is overkill, but it is conventional since we don't really work with the set of functions with existing partial derivatives. Note that mathematicians throw around the word "smooth" a lot. Usually, it means one of three things: it is of class $C^1$, $C^\infty$, or $C^k$ where $k$ is however high it needs to be to satisfy our assumptions. For example, if I say let us differentiate smooth $f$ two times, then I am assuming that $f \in C^2 (\mathbb{R}^n)$. 

  Visualizing $C^k$-functions is easy for low orders. A $C^0$ function produces a graph that isn't "ripped" or "punctured," since this is exactly what a discontinuity would look like. A $C^1$ function requires the surface to be smooth in such a way that there is a well defined affine tangent subspace at every point. This means that there cannot be any sharp "points" or "edges" on the graph since a tangent subspace cannot be well defined. 

  \begin{theorem}[Clairut's Theorem]
    Given $f \in C^2$ at point $\mathbf{a}$, its second iterated partials are equal. 
    \begin{equation}
      \partial_{x_i x_j} f (\mathbf{a})= \partial_{x_j x_i} f (\mathbf{a})\text{ for } i, j = 1, 2, \ldots, n
    \end{equation}
  \end{theorem}
  \begin{proof}
    For clarity, denote $x_i, x_j$ as $x, y$ and ignore the rest of the variables. Then, the partial derivatives $\partial_{x y} f$ and $\partial_{y x} f$ at a point $(x_0, y_0)$ can be expressed as double limits: 
    \begin{equation}
      \partial_{x y} f (x_0, y_0) = \lim_{y \rightarrow y_0} \frac{\partial_x f (x_0, y) - \partial_x f (x_0, y_0)}{y - y_0}
    \end{equation}
    where $\partial_x f: D \subset \mathbb{R}^n \longrightarrow \mathbb{R}$. We can use the two limit definitions of partial derivatives
    \begin{equation}
      \partial_x f (x_0, y) = \lim_{x \rightarrow x_0} \frac{f(x, y) - f(x_0, y)}{x-x_0} \text{ and } \partial_x f (x_0, y_0) = \lim_{x \rightarrow x_0} \frac{f(x, y_0) - f(x_0, y_0)}{x-x_0}
    \end{equation}
    and substitute them to get the two partials
    \begin{align}
      \partial_{xy} f (x_0, y_0) & = \lim_{y \rightarrow y_0} \frac{ \lim_{x \rightarrow x_0} \frac{f(x, y) - f(x_0, y)}{x-x_0} - \lim_{y \rightarrow y_0} \frac{f(x, y_0) - f(x_0, y_0)}{x-x_0}}{y - y_0} \\
      & =  \lim_{y \rightarrow y_0} \lim_{x \rightarrow x_0} \bigg( \frac{f(x, y) - f(x_0, y) - f(x, y_0) + f(x_0, y_0)}{(x - x_0) (y - y_0)} \bigg) \\
      \partial_{yx} f (x_0, y_0) & = \lim_{x \rightarrow x_0} \frac{ \lim_{y \rightarrow y_0} \frac{f(x, y) - f(x, y_0)}{y-y_0} - \lim_{y \rightarrow y_0} \frac{f(x_0, y) - f(x_0, y_0)}{y-y_0}}{x - x_0} \\
      & = \lim_{x \rightarrow x_0} \lim_{y \rightarrow y_0} \bigg( \frac{f(x, y) - f(x, y_0) - f(x_0, y) + f(x_0, y_0)}{(y-y_0) (x-x_0)} \bigg)
    \end{align}
    Now invoking our assumption that $f$ is $C^2$, the two limits, which approach $(x_0, y_0)$ along different paths, both exist and are equal to 
    \begin{equation}
      \lim_{(x, y) \rightarrow (x_0, y_0)} \frac{f(x, y) - f(x_0, y) - f(x, y_0) + f(x_0, y_0)}{(x - x_0) (y - y_0)}
    \end{equation}
    and therefore $\partial_{x y} f = \partial_{y x} f$. 
  \end{proof}

  \begin{corollary}[Symmetry in $k$th Iterated Partials]
    Given $f \in C^k$, its $k$th iterated partials are equal. That is, given any permutation $\sigma$, 
    \begin{equation}
      \partial_{x_{i_1} x_{i_2} \ldots x_{i_k}} f = \partial_{x_{\sigma(i_1)} x_{\sigma(i_2)} \ldots x_{\sigma(i_k)}} f \text{ for } i_1, \ldots, i_k = 1, \ldots, n
    \end{equation}
  \end{corollary}
  \begin{proof}
    By induction. 
  \end{proof}

  \begin{corollary}
    The Hessian of a $C^2$ function is symmetric. 
  \end{corollary}

  Let's talk more about this. By the spectral theorem, we can eigendecompose it. 

\subsection{Taylor Series}

  To talk about convergence, the big-O notation is very useful. 

  \begin{definition}[Classes of Infinitesimal Functions]
  A function $\alpha: \mathbb{R}^n \longrightarrow \mathbb{R}$ is \textbf{infinitesimal} if $\alpha \rightarrow 0$ as $\mathbf{x} \rightarrow \mathbf{x}_0$. There are multiple "levels" of infinitesimal functions, i.e. how fast they converge to $0$. We can classify them by comparing their limits to polynomials. 
  \begin{enumerate}
    \item $\alpha$ is of class $O(1)$ if 
    \[\lim_{\mathbf{x} \rightarrow \mathbf{x}_0} \frac{\alpha(\mathbf{x})}{1} = 0\]
    This means that $\alpha(\mathbf{x})$ tends to $0$ infinitely faster than $1$ (which just means that it tends to $0$). 
    
    \item $\alpha$ is of class $O(h)$ if 
    \[\lim_{\mathbf{x} \rightarrow \mathbf{x}_0}
    \frac{\alpha(\mathbf{x})}{||\mathbf{x} - \mathbf{x}_0||} = 0\]
    This means that $\alpha(\mathbf{x})$ tends to $0$ infinitely faster than the linear $||\mathbf{h}||$, where $\mathbf{h} = \mathbf{x} - \mathbf{x}_0$. 
    
    \item $\alpha$ is of class $O(h^2)$ if 
    \[\lim_{\mathbf{x} \rightarrow \mathbf{x}_0} \frac{\alpha(\mathbf{x})}{||\mathbf{x} - \mathbf{x}_0||^2} = 0\]
    This means that $\alpha(\mathbf{x})$ tends to $0$ infinitely faster than the quadratic $||\mathbf{h}||^2$, where $\mathbf{h} = \mathbf{x} - \mathbf{x}_0$. 
    
    \item $\alpha$ is of class $O(h^k)$ if 
    \[\lim_{\mathbf{x} \rightarrow \mathbf{x}_0} \frac{\alpha(\mathbf{x})}{||\mathbf{x} - \mathbf{x}_0||^k} = 0\]
    This means that $\alpha(\mathbf{x})$ tends to $0$ infinitely faster than the $k$th-order $||\mathbf{h}||^k$, where $\mathbf{h} = \mathbf{x} - \mathbf{x}_0$. 
  \end{enumerate}
  Clearly, $O(h^k) \supset O(h^{k+1})$. 
  \end{definition}

  Now given a $f: D \subset \mathbb{R}^n \longrightarrow \mathbb{R}$, we present some polynomial approximations of $f$ at $\mathbf{x}_0 \in D$: 
  \begin{enumerate}
    \item If $f \in C^0$, the zeroth (constant) approximation is just 
    \[P_0 (\mathbf{x}) = f(\mathbf{x}_0)\]
    This is not interesting at all, since it is just constant. Furthermore, the error term $\epsilon_0 (\mathbf{x}) = f(\mathbf{x}) - P_0 (\mathbf{x})$ is an infinitesimal function as $\mathbf{x} \rightarrow \mathbf{x}_0$ and is of class $O(1)$, since 
    \[\lim_{\mathbf{x} \rightarrow \mathbf{x_0}} \frac{f(\mathbf{x}) - P_0 (\mathbf{x})}{1} = 0\]
    
    \item If $f \in C^1$, the first (linear) approximation requires us to use our total derivative: 
    \[P_1 (\mathbf{x}) = f(\mathbf{x}_0) + D f_{\mathbf{x}_0} (\mathbf{x} - \mathbf{x}_0)\]
    and we know that the error $\epsilon_1 (\mathbf{x}) = f(\mathbf{x}) - P_1 (\mathbf{x})$ is infinitesimal as $\mathbf{x} \rightarrow \mathbf{x}_0$ and is of class $O(h)$, since 
    \[\lim_{\mathbf{x} \rightarrow \mathbf{x}_0} \frac{f(\mathbf{x}) - P_1(\mathbf{x})}{||\mathbf{x} - \mathbf{x}_0||} = 0\]
    
    \item If $f \in C^2$, the second (quadratic) approximation requires us to use a quadratic term (i.e. a bilinear form of $\mathbf{h} = \mathbf{x} - \mathbf{x}_0$) centered at $\mathbf{x}_0$. Call it $H_{\mathbf{x}_0}: \mathbb{R}^n \times \mathbb{R}^n \longrightarrow \mathbb{R}$, and our estimation is 
    \[P_2 (\mathbf{x}) = f(\mathbf{x}_0) + D f_{\mathbf{x}_0} (\mathbf{x} - \mathbf{x}_0) + \frac{1}{2} H (\mathbf{x} - \mathbf{x}_0, \mathbf{x} - \mathbf{x}_0)\]
    which we would like the error term $\epsilon_2 (\mathbf{x}) = f(\mathbf{x}) - P_2 (\mathbf{x})$ to be $O(h^2)$, or in limit terms, $P_2$ must satisfy 
    \[\lim_{\mathbf{x} \rightarrow \mathbf{x}_0} \frac{f(\mathbf{x}) - P_2 (\mathbf{x})}{||\mathbf{x} - \mathbf{x}_0||^2} = 0\]
    We show that this form $H$ is precisely the Hessian matrix. 
  \end{enumerate}

  \begin{theorem}[Hessian]
  The second order approximation of a $C^2$-differentiable function $f$ about a point $\mathbf{x}_0$ is 
  \[f (\mathbf{x}) = f(\mathbf{x}_0) + D f_{\mathbf{x}_0} (\mathbf{x} - \mathbf{x}_0) + \frac{1}{2} (\mathbf{x} - \mathbf{x}_0)^T H f_{\mathbf{x}_0} (\mathbf{x} - \mathbf{x}_0) + O(h^2)\]
  where $D f_{\mathbf{x}_0}$ is the total derivative at $\mathbf{x}_0$ and $H f_{\mathbf{x}_0}$ is the Hessian matrix at $\mathbf{x}_0$. 
  \end{theorem}

\subsection{Matrix Calculus}

  Now we will take a look at functions that have either an input or output as matrices. Essentially, matrices are also vectors, so there is nothing new here to learn, but having a concrete set of notation is useful. First, note that when we talk about a total derivative $D \mathbf{f}_\mathbf{a}$, we can interpret this as a linear map that takes in some small perturbation $\mathbf{h}$ and gives us the result $D \mathbf{f}_\mathbf{a} (\mathbf{h})$. In our column-vector setting, this just corresponded to left matrix multiplication: 
  \begin{equation}
    D \mathbf{f}_\mathbf{a} (\mathbf{h}) = D \mathbf{f}_\mathbf{a} \mathbf{h}
  \end{equation}

  This is not the case in the matrix setting. Let us compare the following: 
  \begin{enumerate}
    \item The derivative of $f: \mathbb{R} \rightarrow \mathbb{R}$ at $a$ is a linear function $D f_a: \mathbb{R} \longrightarrow \mathbb{R}$ satisfying $f(a + h) \approx f(a) + D f_a (h) + O(h^2)$. But linearity reduces $D f_a$ to simply a scalar, and so our condition reduces to 
    
    \[f(a + h) \approx f(a) + D f_a h + O(h^2)\]
    
    \item The derivative of a path function $\mathbf{f}: \mathbb{R} \rightarrow \mathbb{R}^m$ is a linear function $D \mathbf{f}_a : \mathbb{R} \longrightarrow \mathbb{R}^m$ satisfying $\mathbf{f}(a + h) = \mathbf{f}(a) + D \mathbf{f}_a (h) + O(h^2)$. Linearity implies that $D \mathbf{f}_a$ is a rank-1 linear map, which reduces to it being a column vector, and our condition reduces to 
    \[\underbrace{\mathbf{f}(a + h)}_{m \times 1} \approx \underbrace{\mathbf{f}(a)}_{m \times 1} + \underbrace{D \mathbf{f}_a}_{m \times 1} \underbrace{h}_{1 \times 1}\]
    
    \item The derivative of a matrix function $\mathbf{F}: \mathbb{R} \rightarrow \mathbb{R}^{m \times n}$ is a linear map $D \mathbf{F}_a: \mathbb{R} \longrightarrow \mathbb{R}^{m \times n}$ satisfying $\mathbf{F}(a + h) \approx \mathbf{F}(a) + D \mathbf{F}_a (h)$. This time, linearity does not reduce it to simple left-hand matrix multiplication. We could just say that this is a left-hand scalar multiplication, but this doesn't generalize well, so we are stuck with just saying that $D \mathbf{F}_a$ is a linear map. 
    \[\underbrace{\mathbf{F}(a + h)}_{m \times n} \approx \underbrace{\mathbf{F}(a)}_{m \times n} + \underbrace{D \mathbf{F}_a (h)}_{m \times n}\]
  \end{enumerate}
  Now let us take a look at when we have matrix inputs. 
  \begin{enumerate}
    \item The derivative of $f: \mathbb{R}^{m \times n} \longrightarrow \mathbb{R}$ is a linear function $D f_{\mathbf{A}} : \mathbb{R}^{m \times n} \longrightarrow \mathbb{R}$ satisfying $f(\mathbf{A} + \mathbf{H}) \approx f(\mathbf{A}) + D f_{\mathbf{A}} (\mathbf{H})$. We could let $D f_{\mathbf{A}}$ be some linear map like $\mathbf{M} \mapsto \mathbf{v}^T \mathbf{M} \mathbf{u}$, where $\mathbf{v}, \mathbf{u}$ is fixed. But in generality, we just have the condition 
    \[\underbrace{f(\mathbf{A} + \mathbf{H})}_{1 \times 1} \approx \underbrace{f(\mathbf{A})}_{1 \times 1} + \underbrace{D f_{\mathbf{A}} (\mathbf{H})}_{1 \times 1}\]
    
    \item The derivative of $\mathbf{f}: \mathbf{R}^{m \times n} \longrightarrow \mathbb{R}^d$ is some linear map $D \mathbf{f}_\mathbf{A} : \mathbf{R}^{m \times n} \longrightarrow \mathbb{R}^d$ satisfying $\mathbf{f} (\mathbf{A} + \mathbf{H}) \approx \mathbf{f}(\mathbf{A}) + D \mathbf{f}_\mathbf{A} (\mathbf{H})$. Again, we could construct some form that would give us a linear map in terms of some matrix multiplication, but in generality, we have the condition 
    \[\underbrace{\mathbf{f} (\mathbf{A} + \mathbf{H})}_{d \times 1} \approx \underbrace{\mathbf{f}(\mathbf{A})}_{d \times 1} + \underbrace{D \mathbf{f}_\mathbf{A} (\mathbf{H})}_{d \times 1}\]
  \end{enumerate}

  Now we present some theorems on basic differentiation. Proving these just requires us to expand the function and compute the derivatives component-wise. 

  \begin{theorem}[Derivative of Affine Map]
  Given $f: \mathbb{R}^n \longrightarrow \mathbb{R}^m$ defined $f(\mathbf{x}) = \mathbf{A} \mathbf{x} + \mathbf{b}$ (where $\mathbf{A}, \mathbf{b}$ is not dependent on $\mathbf{x}$), its derivative is 
  \[D \mathbf{f} = \mathbf{A}\]
  \end{theorem}

  \begin{theorem}
  Given the scalar $\alpha$ defined by 
  \[\alpha = \mathbf{y}^T \mathbf{A} \mathbf{x}\]
  where $\mathbf{y} \in \mathbb{R}^{m \times 1}, \mathbf{A} \in \mathbb{R}^{m \times n}$, and $\mathbf{x} \in \mathbb{R}^{n \times 1}$, then 
  \[\frac{\partial \alpha}{\partial \mathbf{x}} = \mathbf{y}^T \mathbf{A} : \mathbb{R}^n \longrightarrow \mathbb{R}\]
  and 
  \[\frac{\partial \alpha}{\partial \mathbf{y}} = \mathbf{x}^T \mathbf{A}^T : \mathbb{R}^m \longrightarrow \mathbb{R}\]
  Rewritten in the total derivative notation, we can interpret $\alpha$ as a function of both $\mathbf{x}$ and $\mathbf{y}$ and write 
  \[D \alpha_{(\mathbf{x}, \mathbf{y})} = \begin{pmatrix} \mathbf{y}^T \mathbf{A} \\ \mathbf{x}^T \mathbf{A}^T \end{pmatrix} : \mathbb{R}^{n + m} \longrightarrow \mathbb{R}\]
  \end{theorem}

  \begin{theorem}
    Given the scalar $\alpha$ defined by the quadratic form 
    \begin{equation}
      \alpha = \mathbf{x}^T \mathbf{A} \mathbf{x}
    \end{equation}
    where $\mathbf{x} \in \mathbb{R}^{n \times 1}$ and $\mathbf{A} \in \mathbb{R}^{n \times n}$, then 
    \begin{equation}
      \frac{\partial \alpha}{\partial \mathbf{x}} = \mathbf{x}^T \big( \mathbf{A} + \mathbf{A}^T \big)
    \end{equation}
    or in the total derivative notation, 
    \begin{equation}
      D \alpha_\mathbf{a} = \mathbf{a}^T \big( \mathbf{A} + \mathbf{A}^T \big) : \mathbb{R}^n \longrightarrow \mathbb{R}
    \end{equation}
  \end{theorem}


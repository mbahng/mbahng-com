\section{Higher Order Derivatives} 

  Redefine derivatives by talking about tensors. 

  \begin{definition}
    Definition of derivatives again with tensors. 
  \end{definition}

  Since $\nabla_\mathbf{v} f: D \subset \mathbb{R}^n \longrightarrow \mathbb{R}$, we can take the directional derivative (assuming it exists) of it again in direction $\mathbf{u}$ to get a second derivative $\nabla_\mathbf{u} \nabla_\mathbf{v} f$. We usually work with iterated partial derivatives, and we can compute derivatives as many times as we want, given that they exist. Therefore, the second-order iterated partial derivatives of $f$ are 
  \[\partial_{x_i x_j} \coloneqq \partial_{x_j} \partial_{x_i} f \text{ for } i, j = 1, \ldots n\]

  \begin{definition}[$C^k$ Functions]
  A function $f: D \subset \mathbb{R}^n \longrightarrow \mathbb{R}$ is said to be a $C^k$ function if all $k$-times iterated partial derivatives 
  \[\partial_{x_{i_1} x_{i_2} \ldots x_{i_k}} f\]
  exist and are continuous. The vector space of all $C^k$ functions is denoted $C^k (D; \mathbb{R})$, or $C^k(D)$. 
  \end{definition}

  Whenever we want to get the $k$th iterated partial derivative of $f$, we will assume that $f \in C^k$. Again, this is overkill, but it is conventional since we don't really work with the set of functions with existing partial derivatives. 

  \begin{theorem}[Nested $C^k$ and $\mathcal{D}^k$ Function Spaces]
  Let the space of all $k$-times differentiable functions over $\mathbb{R}^n$ be denoted $\mathcal{D}(\mathbb{R}^n)$. Then, 
  \[C^0(\mathbb{R}^n) \supset \mathcal{D}^1 (\mathbb{R}^n) \supset C^1 (\mathbb{R}^n) \supset \mathcal{D}^2 (\mathbb{R}^n) \supset C^2 (\mathbb{R}^n) \ldots \mathcal{D}^k (\mathbb{R}^n) \supset C^k (\mathbb{R}^n) \ldots C^\infty (\mathbb{R}^n) \]
  \end{theorem}

  Note that mathematicians throw around the word "smooth" a lot. Usually, it means one of three things
  \begin{enumerate}
      \item it is of class $C^1$ 
      \item it is of class $C^\infty$
      \item it is of class $C^k$, where $k$ is however high it needs to be to satisfy our assumptions. For example, if I say let us differentiate smooth $f$ two times, then I am assuming that $f \in C^2 (\mathbb{R}^n)$. 
  \end{enumerate}
  Visualizing $C^k$-functions is easy for low orders. A $C^0$ function produces a graph that isn't "ripped" or "punctured," since this is exactly what a discontinuity would look like. A $C^1$ function requires the surface to be smooth in such a way that there is a well defined affine tangent subspace at every point. This means that there cannot be any sharp "points" or "edges" on the graph since a tangent subspace cannot be well defined. 

  \begin{theorem}[Clairut's Theorem]
    Given $f \in C^2$ at point $\mathbf{a}$, its second iterated partials are equal. 
    \[\partial_{x_i x_j} f (\mathbf{a})= \partial_{x_j x_i} f (\mathbf{a})\text{ for } i, j = 1, 2, \ldots, n\]
  \end{theorem}
  \begin{proof}
    For clarity, denote $x_i, x_j$ as $x, y$ and ignore the rest of the variables. Then, the partial derivatives $\partial_{x y} f$ and $\partial_{y x} f$ at a point $(x_0, y_0)$ can be expressed as double limits: 
    \[\partial_{x y} f (x_0, y_0) = \lim_{y \rightarrow y_0} \frac{\partial_x f (x_0, y) - \partial_x f (x_0, y_0)}{y - y_0}\]
    where $\partial_x f: D \subset \mathbb{R}^n \longrightarrow \mathbb{R}$. We can use the two limit definitions of partial derivatives
    \[\partial_x f (x_0, y) = \lim_{x \rightarrow x_0} \frac{f(x, y) - f(x_0, y)}{x-x_0} \text{ and } \partial_x f (x_0, y_0) = \lim_{x \rightarrow x_0} \frac{f(x, y_0) - f(x_0, y_0)}{x-x_0}\]
    and substitute them to get the two partials
    \begin{align*}
        \partial_{xy} f (x_0, y_0) & = \lim_{y \rightarrow y_0} \frac{ \lim_{x \rightarrow x_0} \frac{f(x, y) - f(x_0, y)}{x-x_0} - \lim_{y \rightarrow y_0} \frac{f(x, y_0) - f(x_0, y_0)}{x-x_0}}{y - y_0} \\
        & =  \lim_{y \rightarrow y_0} \lim_{x \rightarrow x_0} \bigg( \frac{f(x, y) - f(x_0, y) - f(x, y_0) + f(x_0, y_0)}{(x - x_0) (y - y_0)} \bigg) \\
        \partial_{yx} f (x_0, y_0) & = \lim_{x \rightarrow x_0} \frac{ \lim_{y \rightarrow y_0} \frac{f(x, y) - f(x, y_0)}{y-y_0} - \lim_{y \rightarrow y_0} \frac{f(x_0, y) - f(x_0, y_0)}{y-y_0}}{x - x_0} \\
        & = \lim_{x \rightarrow x_0} \lim_{y \rightarrow y_0} \bigg( \frac{f(x, y) - f(x, y_0) - f(x_0, y) + f(x_0, y_0)}{(y-y_0) (x-x_0)} \bigg)
    \end{align*}
    Now invoking our assumption that $f$ is $C^2$, the two limits, which approach $(x_0, y_0)$ along different paths, both exist and are equal to 
    \[\lim_{(x, y) \rightarrow (x_0, y_0)} \frac{f(x, y) - f(x_0, y) - f(x, y_0) + f(x_0, y_0)}{(x - x_0) (y - y_0)}\]
    and therefore $\partial_{x y} f = \partial_{y x} f$. 
  \end{proof}

  \begin{corollary}
    Given $f \in C^k$, its $k$th iterated partials are equal. That is, given any permutation $\sigma$, 
    \[\partial_{x_{i_1} x_{i_2} \ldots x_{i_k}} f = \partial_{x_{\sigma(i_1)} x_{\sigma(i_2)} \ldots x_{\sigma(i_k)}} f \text{ for } i_1, \ldots, i_k = 1, \ldots, n\]
  \end{corollary}

  \begin{corollary}
    Higher order derivatives live in the exterior algebra. 
  \end{corollary}

\subsection{Second Order Derivatives}

  \begin{definition}[Hessian Matrix]
    The $n \times n$ matrix of second iterated partials of $f \in C^2$ at $\mathbf{a}$ is called the \textbf{Hessian matrix}. 
    \[H f_\mathbf{a} \coloneqq \begin{pmatrix} 
    \partial_{x_1 x_1} (\mathbf{a}) & \ldots & \partial_{x_1 x_n} (\mathbf{a}) \\
    \vdots & \ddots & \vdots \\
    \partial_{x_n x_1} (\mathbf{a}) & \ldots & \partial_{x_n x_n} (\mathbf{a}) \\
    \end{pmatrix} \text{ and } H f \coloneqq \begin{pmatrix} 
    \partial_{x_1 x_1} & \ldots & \partial_{x_1 x_n} \\
    \vdots & \ddots & \vdots \\
    \partial_{x_n x_1} & \ldots & \partial_{x_n x_n} \\
    \end{pmatrix}\]
    By equality of mixed partials, it is symmetric. 
  \end{definition}

  \begin{theorem}
    The Hessian matrix of a function $f: D \subset \mathbb{R}^n \longrightarrow \mathbb{R}$ is the Jacobian matrix of the gradient of $f$. That is, interpreting $\nabla f: D \subset \mathbb{R}^n \longrightarrow \mathbb{R}^n$, we have
    \[H f_\mathbf{a} = D \nabla f_\mathbf{a} \text{ for all } \mathbf{a} \in D\]
    which we can also write as $H f = D \nabla f$. This theorem is very useful, especially for optimization and sampling methods, since we can now interpret the Hessian as the rate of change of the gradient of $f$. 
  \end{theorem}

\subsection{Taylor Series}

  To talk about convergence, the big-O notation is very useful. 

  \begin{definition}[Classes of Infinitesimal Functions]
  A function $\alpha: \mathbb{R}^n \longrightarrow \mathbb{R}$ is \textbf{infinitesimal} if $\alpha \rightarrow 0$ as $\mathbf{x} \rightarrow \mathbf{x}_0$. There are multiple "levels" of infinitesimal functions, i.e. how fast they converge to $0$. We can classify them by comparing their limits to polynomials. 
  \begin{enumerate}
      \item $\alpha$ is of class $O(1)$ if 
      \[\lim_{\mathbf{x} \rightarrow \mathbf{x}_0} \frac{\alpha(\mathbf{x})}{1} = 0\]
      This means that $\alpha(\mathbf{x})$ tends to $0$ infinitely faster than $1$ (which just means that it tends to $0$). 
      
      \item $\alpha$ is of class $O(h)$ if 
      \[\lim_{\mathbf{x} \rightarrow \mathbf{x}_0}
      \frac{\alpha(\mathbf{x})}{||\mathbf{x} - \mathbf{x}_0||} = 0\]
      This means that $\alpha(\mathbf{x})$ tends to $0$ infinitely faster than the linear $||\mathbf{h}||$, where $\mathbf{h} = \mathbf{x} - \mathbf{x}_0$. 
      
      \item $\alpha$ is of class $O(h^2)$ if 
      \[\lim_{\mathbf{x} \rightarrow \mathbf{x}_0} \frac{\alpha(\mathbf{x})}{||\mathbf{x} - \mathbf{x}_0||^2} = 0\]
      This means that $\alpha(\mathbf{x})$ tends to $0$ infinitely faster than the quadratic $||\mathbf{h}||^2$, where $\mathbf{h} = \mathbf{x} - \mathbf{x}_0$. 
      
      \item $\alpha$ is of class $O(h^k)$ if 
      \[\lim_{\mathbf{x} \rightarrow \mathbf{x}_0} \frac{\alpha(\mathbf{x})}{||\mathbf{x} - \mathbf{x}_0||^k} = 0\]
      This means that $\alpha(\mathbf{x})$ tends to $0$ infinitely faster than the $k$th-order $||\mathbf{h}||^k$, where $\mathbf{h} = \mathbf{x} - \mathbf{x}_0$. 
  \end{enumerate}
  Clearly, $O(h^k) \supset O(h^{k+1})$. 
  \end{definition}

  Now given a $f: D \subset \mathbb{R}^n \longrightarrow \mathbb{R}$, we present some polynomial approximations of $f$ at $\mathbf{x}_0 \in D$: 
  \begin{enumerate}
      \item If $f \in C^0$, the zeroth (constant) approximation is just 
      \[P_0 (\mathbf{x}) = f(\mathbf{x}_0)\]
      This is not interesting at all, since it is just constant. Furthermore, the error term $\epsilon_0 (\mathbf{x}) = f(\mathbf{x}) - P_0 (\mathbf{x})$ is an infinitesimal function as $\mathbf{x} \rightarrow \mathbf{x}_0$ and is of class $O(1)$, since 
      \[\lim_{\mathbf{x} \rightarrow \mathbf{x_0}} \frac{f(\mathbf{x}) - P_0 (\mathbf{x})}{1} = 0\]
      
      \item If $f \in C^1$, the first (linear) approximation requires us to use our total derivative: 
      \[P_1 (\mathbf{x}) = f(\mathbf{x}_0) + D f_{\mathbf{x}_0} (\mathbf{x} - \mathbf{x}_0)\]
      and we know that the error $\epsilon_1 (\mathbf{x}) = f(\mathbf{x}) - P_1 (\mathbf{x})$ is infinitesimal as $\mathbf{x} \rightarrow \mathbf{x}_0$ and is of class $O(h)$, since 
      \[\lim_{\mathbf{x} \rightarrow \mathbf{x}_0} \frac{f(\mathbf{x}) - P_1(\mathbf{x})}{||\mathbf{x} - \mathbf{x}_0||} = 0\]
      
      \item If $f \in C^2$, the second (quadratic) approximation requires us to use a quadratic term (i.e. a bilinear form of $\mathbf{h} = \mathbf{x} - \mathbf{x}_0$) centered at $\mathbf{x}_0$. Call it $H_{\mathbf{x}_0}: \mathbb{R}^n \times \mathbb{R}^n \longrightarrow \mathbb{R}$, and our estimation is 
      \[P_2 (\mathbf{x}) = f(\mathbf{x}_0) + D f_{\mathbf{x}_0} (\mathbf{x} - \mathbf{x}_0) + \frac{1}{2} H (\mathbf{x} - \mathbf{x}_0, \mathbf{x} - \mathbf{x}_0)\]
      which we would like the error term $\epsilon_2 (\mathbf{x}) = f(\mathbf{x}) - P_2 (\mathbf{x})$ to be $O(h^2)$, or in limit terms, $P_2$ must satisfy 
      \[\lim_{\mathbf{x} \rightarrow \mathbf{x}_0} \frac{f(\mathbf{x}) - P_2 (\mathbf{x})}{||\mathbf{x} - \mathbf{x}_0||^2} = 0\]
      We show that this form $H$ is precisely the Hessian matrix. 
  \end{enumerate}

  \begin{theorem}[Hessian]
  The second order approximation of a $C^2$-differentiable function $f$ about a point $\mathbf{x}_0$ is 
  \[f (\mathbf{x}) = f(\mathbf{x}_0) + D f_{\mathbf{x}_0} (\mathbf{x} - \mathbf{x}_0) + \frac{1}{2} (\mathbf{x} - \mathbf{x}_0)^T H f_{\mathbf{x}_0} (\mathbf{x} - \mathbf{x}_0) + O(h^2)\]
  where $D f_{\mathbf{x}_0}$ is the total derivative at $\mathbf{x}_0$ and $H f_{\mathbf{x}_0}$ is the Hessian matrix at $\mathbf{x}_0$. 
  \end{theorem}

\section{Matrix Calculus}

  Now we will take a look at functions that have either an input or output as matrices. Essentially, matrices are also vectors, so there is nothing new here to learn, but having a concrete set of notation is useful. First, note that when we talk about a total derivative $D \mathbf{f}_\mathbf{a}$, we can interpret this as a linear map that takes in some small perturbation $\mathbf{h}$ and gives us the result $D \mathbf{f}_\mathbf{a} (\mathbf{h})$. In our column-vector setting, this just corresponded to left matrix multiplication: 
  \[D \mathbf{f}_\mathbf{a} (\mathbf{h}) = D \mathbf{f}_\mathbf{a} \mathbf{h}\]
  This is not the case in the matrix setting. Let us compare the following: 
  \begin{enumerate}
      \item The derivative of $f: \mathbb{R} \rightarrow \mathbb{R}$ at $a$ is a linear function $D f_a: \mathbb{R} \longrightarrow \mathbb{R}$ satisfying $f(a + h) \approx f(a) + D f_a (h) + O(h^2)$. But linearity reduces $D f_a$ to simply a scalar, and so our condition reduces to 
      
      \[f(a + h) \approx f(a) + D f_a h + O(h^2)\]
      
      \item The derivative of a path function $\mathbf{f}: \mathbb{R} \rightarrow \mathbb{R}^m$ is a linear function $D \mathbf{f}_a : \mathbb{R} \longrightarrow \mathbb{R}^m$ satisfying $\mathbf{f}(a + h) = \mathbf{f}(a) + D \mathbf{f}_a (h) + O(h^2)$. Linearity implies that $D \mathbf{f}_a$ is a rank-1 linear map, which reduces to it being a column vector, and our condition reduces to 
      \[\underbrace{\mathbf{f}(a + h)}_{m \times 1} \approx \underbrace{\mathbf{f}(a)}_{m \times 1} + \underbrace{D \mathbf{f}_a}_{m \times 1} \underbrace{h}_{1 \times 1}\]
      
      \item The derivative of a matrix function $\mathbf{F}: \mathbb{R} \rightarrow \mathbb{R}^{m \times n}$ is a linear map $D \mathbf{F}_a: \mathbb{R} \longrightarrow \mathbb{R}^{m \times n}$ satisfying $\mathbf{F}(a + h) \approx \mathbf{F}(a) + D \mathbf{F}_a (h)$. This time, linearity does not reduce it to simple left-hand matrix multiplication. We could just say that this is a left-hand scalar multiplication, but this doesn't generalize well, so we are stuck with just saying that $D \mathbf{F}_a$ is a linear map. 
      \[\underbrace{\mathbf{F}(a + h)}_{m \times n} \approx \underbrace{\mathbf{F}(a)}_{m \times n} + \underbrace{D \mathbf{F}_a (h)}_{m \times n}\]
  \end{enumerate}
  Now let us take a look at when we have matrix inputs. 
  \begin{enumerate}
      \item The derivative of $f: \mathbb{R}^{m \times n} \longrightarrow \mathbb{R}$ is a linear function $D f_{\mathbf{A}} : \mathbb{R}^{m \times n} \longrightarrow \mathbb{R}$ satisfying $f(\mathbf{A} + \mathbf{H}) \approx f(\mathbf{A}) + D f_{\mathbf{A}} (\mathbf{H})$. We could let $D f_{\mathbf{A}}$ be some linear map like $\mathbf{M} \mapsto \mathbf{v}^T \mathbf{M} \mathbf{u}$, where $\mathbf{v}, \mathbf{u}$ is fixed. But in generality, we just have the condition 
      \[\underbrace{f(\mathbf{A} + \mathbf{H})}_{1 \times 1} \approx \underbrace{f(\mathbf{A})}_{1 \times 1} + \underbrace{D f_{\mathbf{A}} (\mathbf{H})}_{1 \times 1}\]
      
      \item The derivative of $\mathbf{f}: \mathbf{R}^{m \times n} \longrightarrow \mathbb{R}^d$ is some linear map $D \mathbf{f}_\mathbf{A} : \mathbf{R}^{m \times n} \longrightarrow \mathbb{R}^d$ satisfying $\mathbf{f} (\mathbf{A} + \mathbf{H}) \approx \mathbf{f}(\mathbf{A}) + D \mathbf{f}_\mathbf{A} (\mathbf{H})$. Again, we could construct some form that would give us a linear map in terms of some matrix multiplication, but in generality, we have the condition 
      \[\underbrace{\mathbf{f} (\mathbf{A} + \mathbf{H})}_{d \times 1} \approx \underbrace{\mathbf{f}(\mathbf{A})}_{d \times 1} + \underbrace{D \mathbf{f}_\mathbf{A} (\mathbf{H})}_{d \times 1}\]
  \end{enumerate}

  \subsection{Simple Differentiation Rules}

  Now we present some theorems on basic differentiation. Proving these just requires us to expand the function and compute the derivatives component-wise. 

  \begin{theorem}[Derivative of Affine Map]
  Given $f: \mathbb{R}^n \longrightarrow \mathbb{R}^m$ defined $f(\mathbf{x}) = \mathbf{A} \mathbf{x} + \mathbf{b}$ (where $\mathbf{A}, \mathbf{b}$ is not dependent on $\mathbf{x}$), its derivative is 
  \[D \mathbf{f} = \mathbf{A}\]
  \end{theorem}

  \begin{theorem}
  Given the scalar $\alpha$ defined by 
  \[\alpha = \mathbf{y}^T \mathbf{A} \mathbf{x}\]
  where $\mathbf{y} \in \mathbb{R}^{m \times 1}, \mathbf{A} \in \mathbb{R}^{m \times n}$, and $\mathbf{x} \in \mathbb{R}^{n \times 1}$, then 
  \[\frac{\partial \alpha}{\partial \mathbf{x}} = \mathbf{y}^T \mathbf{A} : \mathbb{R}^n \longrightarrow \mathbb{R}\]
  and 
  \[\frac{\partial \alpha}{\partial \mathbf{y}} = \mathbf{x}^T \mathbf{A}^T : \mathbb{R}^m \longrightarrow \mathbb{R}\]
  Rewritten in the total derivative notation, we can interpret $\alpha$ as a function of both $\mathbf{x}$ and $\mathbf{y}$ and write 
  \[D \alpha_{(\mathbf{x}, \mathbf{y})} = \begin{pmatrix} \mathbf{y}^T \mathbf{A} \\ \mathbf{x}^T \mathbf{A}^T \end{pmatrix} : \mathbb{R}^{n + m} \longrightarrow \mathbb{R}\]
  \end{theorem}

  \begin{theorem}
  Given the scalar $\alpha$ defined by the quadratic form 
  \[\alpha = \mathbf{x}^T \mathbf{A} \mathbf{x}\]
  where $\mathbf{x} \in \mathbb{R}^{n \times 1}$ and $\mathbf{A} \in \mathbb{R}^{n \times n}$, then 
  \[\frac{\partial \alpha}{\partial \mathbf{x}} = \mathbf{x}^T \big( \mathbf{A} + \mathbf{A}^T \big)\]
  or in the total derivative notation, 
  \[D \alpha_\mathbf{a} = \mathbf{a}^T \big( \mathbf{A} + \mathbf{A}^T \big) : \mathbb{R}^n \longrightarrow \mathbb{R}\]
  \end{theorem}


\section{Central Limit Theorems}

  By the law of large numbers, the sample averages converge almost surely (and therefore converge in probability) to the expected value $\mu$ as $n \rightarrow \infty$. The CLT describes the size and the distributional form of the stochastic fluctuations around $\mu$ during this convergence. That is, it states that as $n$ gets larger, the distribution of the difference $\overline{X}_n - \mu$ approximates a $\mathcal{N}(0, \sigma^2 / n)$ distribution, where $\sigma^2$ is the variance of $X$. 


  Roughly speaking, the (weak) law of large numbers says that 
  \begin{equation}
    \frac{S_n - n \mathbb{E}[X]}{n} \xrightarrow{i.p.} 0
  \end{equation}
  That is, if we consider the sequence of functions $\{S_n - n \mathbb{E}[x]\}_{n \in \mathbb{N}}$, this sequence is sublinear (i.e. is $o(n)$). CLT does two things: 
  \begin{enumerate}
    \item It specifically quantifies this fluctuation $\{S_n - n \mathbb{E}[X]$ by saying that it is approximately of order $\sqrt{n}$. 
    \item Furthermore, this fluctuation, when divided by $\sqrt{n}$ converges in distribution to a Gaussian. 
    \begin{equation}
      \frac{S_n - n \mathbb{E}[X]}{\sqrt{n}} \xrightarrow{D} \mathcal{N}(0, \sigma_X^2)
    \end{equation}
  \end{enumerate}

  \begin{theorem}[Central Limit Theorem]
    Let $X_1, X_2, X_3, ...$ be a sequence of iid random variables, with mean $\mu = \mathbb{E}[X]$ and with variance $\Var(X) = \sigma^2 < \infty$. Then, the sequence of random variables $\{\overline{X}_n\}_{n \in \mathbb{N}}$ converges in distribution to a Gaussian $\mathcal{N}(\mu, \sigma^2 / n)$. That is, 
    \begin{equation}
      \frac{\overline{X}_n - \mu}{\sigma \sqrt{n}} \xrightarrow{D} \mathcal{N}(0, 1)
    \end{equation}
  \end{theorem}
  \begin{proof}
    Let $Z_i = \frac{X_i - \mu}{\sigma}$ and let $U_n = \frac{1}{\sqrt{n}} \sum_{i=1}^n Z_i$ (we can normalize the $X_i$'s since they have finite mean and variance). Note that since we have finite second moments 
    \begin{align*}
      & \mathbb{E}[Z_i] = 0 < \infty \\
      & \mathrm{Var}[Z_i] = \mathbb{E}[ (Z_i - \mathbb{E}[Z_i])^2] = \mathbb{E}[Z_i^2] = 1 < \infty 
    \end{align*}
    we can Taylor expand the characteristic function $\varphi_{Z_i} (t)$ up to at least the second order (from moment generating property theorem). So, we have 
    \begin{align*}
      \varphi_{Z_i} (t) & = 1 + \frac{\mathbb{E}[Z_i]}{1!} (i t)^1 + \frac{\mathbb{E}[Z_i^2]}{2!} (i t)^2 + o (t^2) \\
      & = 1 + 0 + \frac{1}{2} (i t)^2 + o (t^2) \\
      & = 1 - \frac{1}{2} t^2 + o(t^2) 
    \end{align*}
    Now calculate the CF of $U_n$. Since the $Z_i$'s are iid, we can get 
    \begin{equation}
      \varphi_{U_n} (t) = \Big( \varphi_{Z_i} \big( \frac{t}{\sqrt{n}} \big) \Big)^n = \Big( 1 - \frac{t^2}{2n} + o \big( \frac{t^2}{n} \big) \Big)^n
    \end{equation}
    The $o(t^2 / n)$ term vanishes as $n \rightarrow \infty$, and using the limit $e^x = \lim_{n \rightarrow \infty} \big( 1 + \frac{x}{n} \big)^n$, we have 
    \begin{equation}
      \lim_{n \rightarrow \infty} \varphi_{U_n} (t) = \lim_{n \rightarrow \infty} \Big( 1 - \frac{t^2}{2n} + o \big( \frac{t^2}{n} \big) \Big)^n = e^{-t^2 / 2}
    \end{equation}
    which is precisely the CF of a standard Gaussian random variable. Since CFs are unique, our result is proven. Essentially, we have proved convergence in distribution of random variables by showing convergence of their characteristic functions. 
  \end{proof}

  A big misconception is that this normalized sum has PDF that converges to a bell curve. It is the CDF (by definition of convergence in distribution) that converges to that of a Gaussian. That way, we can state this for discrete, continuous, mixtures: doesn't matter. They don't even need to have a density, since if we just took a bunch of Bernoulli's, the PMF of their sum would never be defined for an irrational number like $\pi$. But it would be defined for the CDF, and even though the CDF of a discrete random variable will have jumps, these jumps would get smaller and smaller until it converges pointwise. Even if the $X_i$'s had densities, the CLT does not say that their mean converges to the PDF of a normal. Just because the CDF converges, it doens't mean the PDF will look similiar. 

  It also turns out (?) that we can use CLT to prove the weak law of large numbers, since (roughly speaking) as $n$ increases, the distribution of $\overline{X}_n$ concentrates more and more around $\mu$, and therefore the probability of $|\overline{X}_n - \mu| < \epsilon$ tends to $1$. 


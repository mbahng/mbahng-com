\section{Central Limit Theorems}

  By the law of large numbers, the sample averages converge almost surely (and therefore converge in probability) to the expected value $\mu$ as $n \rightarrow \infty$. The CLT describes the size and the distributional form of the stochastic fluctuations around $\mu$ during this convergence. That is, it states that as $n$ gets larger, the distribution of the difference $\overline{X}_n - \mu$ approximates a $\mathcal{N}(0, \sigma^2 / n)$ distribution, where $\sigma^2$ is the variance of $X$. 


  Roughly speaking, the (weak) law of large numbers says that 
  \begin{equation}
    \frac{S_n - n \mathbb{E}[X]}{n} \xrightarrow{i.p.} 0
  \end{equation}
  That is, if we consider the sequence of functions $\{S_n - n \mathbb{E}[x]\}_{n \in \mathbb{N}}$, this sequence is sublinear (i.e. is $o(n)$). CLT does two things: 
  \begin{enumerate}
    \item It specifically quantifies this fluctuation $\{S_n - n \mathbb{E}[X]$ by saying that it is approximately of order $\sqrt{n}$. 
    \item Furthermore, this fluctuation, when divided by $\sqrt{n}$ converges in distribution to a Gaussian. 
    \begin{equation}
      \frac{S_n - n \mathbb{E}[X]}{\sqrt{n}} \xrightarrow{D} \mathcal{N}(0, \sigma_X^2)
    \end{equation}
  \end{enumerate}

  \begin{theorem}[Central Limit Theorem]
    Let $X_1, X_2, X_3, ...$ be a sequence of iid random variables, with mean $\mu = \mathbb{E}[X]$ and with variance $\Var(X) = \sigma^2 < \infty$. Then, the sequence of random variables $\{\overline{X}_n\}_{n \in \mathbb{N}}$ converges in distribution to a Gaussian $\mathcal{N}(\mu, \sigma^2 / n)$. That is, 
    \begin{equation}
      \frac{\overline{X}_n - \mu}{\sigma \sqrt{n}} \xrightarrow{D} \mathcal{N}(0, 1)
    \end{equation}
  \end{theorem}
  \begin{proof}
    Let $Z_i = \frac{X_i - \mu}{\sigma}$ and let $U_n = \frac{1}{\sqrt{n}} \sum_{i=1}^n Z_i$ (we can normalize the $X_i$'s since they have finite mean and variance). Note that since we have finite second moments 
    \begin{align*}
      & \mathbb{E}[Z_i] = 0 < \infty \\
      & \mathrm{Var}[Z_i] = \mathbb{E}[ (Z_i - \mathbb{E}[Z_i])^2] = \mathbb{E}[Z_i^2] = 1 < \infty 
    \end{align*}
    we can Taylor expand the characteristic function $\varphi_{Z_i} (t)$ up to at least the second order (from moment generating property theorem). So, we have 
    \begin{align*}
      \varphi_{Z_i} (t) & = 1 + \frac{\mathbb{E}[Z_i]}{1!} (i t)^1 + \frac{\mathbb{E}[Z_i^2]}{2!} (i t)^2 + o (t^2) \\
      & = 1 + 0 + \frac{1}{2} (i t)^2 + o (t^2) \\
      & = 1 - \frac{1}{2} t^2 + o(t^2) 
    \end{align*}
    Now calculate the CF of $U_n$. Since the $Z_i$'s are iid, we can get 
    \begin{equation}
      \varphi_{U_n} (t) = \Big( \varphi_{Z_i} \big( \frac{t}{\sqrt{n}} \big) \Big)^n = \Big( 1 - \frac{t^2}{2n} + o \big( \frac{t^2}{n} \big) \Big)^n
    \end{equation}
    The $o(t^2 / n)$ term vanishes as $n \rightarrow \infty$, and using the limit $e^x = \lim_{n \rightarrow \infty} \big( 1 + \frac{x}{n} \big)^n$, we have 
    \begin{equation}
      \lim_{n \rightarrow \infty} \varphi_{U_n} (t) = \lim_{n \rightarrow \infty} \Big( 1 - \frac{t^2}{2n} + o \big( \frac{t^2}{n} \big) \Big)^n = e^{-t^2 / 2}
    \end{equation}
    which is precisely the CF of a standard Gaussian random variable. Since CFs are unique, our result is proven. Essentially, we have proved convergence in distribution of random variables by showing convergence of their characteristic functions. 
  \end{proof}

  A big misconception is that this normalized sum has PDF that converges to a bell curve. It is the CDF (by definition of convergence in distribution) that converges to that of a Gaussian. That way, we can state this for discrete, continuous, mixtures: doesn't matter. They don't even need to have a density, since if we just took a bunch of Bernoulli's, the PMF of their sum would never be defined for an irrational number like $\pi$. But it would be defined for the CDF, and even though the CDF of a discrete random variable will have jumps, these jumps would get smaller and smaller until it converges pointwise. Even if the $X_i$'s had densities, the CLT does not say that their mean converges to the PDF of a normal. Just because the CDF converges, it doens't mean the PDF will look similiar. 

  It also turns out (?) that we can use CLT to prove the weak law of large numbers, since (roughly speaking) as $n$ increases, the distribution of $\overline{X}_n$ concentrates more and more around $\mu$, and therefore the probability of $|\overline{X}_n - \mu| < \epsilon$ tends to $1$. 

\subsection{Exercises} 

  \begin{exercise}[Durrett 3.1.1]
    Generalize the proof of Lemma 3.1.1 to conclude that if $\max_{1 \leq j \leq n} |c_{j,n}| \to 0$, $\sum_{j=1}^n c_{j,n} \to \lambda$, and $\sup_n \sum_{j=1}^n |c_{j,n}| < \infty$ then $\prod_{j=1}^n (1 + c_{j,n}) \to e^\lambda$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.1.2]
    If the $X_i$ have a Poisson distribution with mean 1, then $S_n$ has a Poisson distribution with mean $n$, i.e., $P(S_n = k) = e^{-n}n^k/k!$. Use Stirling's formula to show that if $(k - n)/\sqrt{n} \to x$ then
    \begin{equation*}
      \sqrt{2\pi n}P(S_n = k) \to \exp(-x^2/2)
    \end{equation*}
    As in the case of coin flips it follows that
    \begin{equation*}
      P(a \leq (S_n - n)/\sqrt{n} \leq b) \to \int_a^b (2\pi)^{-1/2} e^{-x^2/2} dx
    \end{equation*}
    but proving the last conclusion is not part of the exercise.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.1.3]
    Suppose $P(X_i = 1) = P(X_i = -1) = 1/2$. Show that if $a \in (0, 1)$
    \begin{equation*}
      \frac{1}{2n} \log P(S_{2n} \geq 2na) \to -\gamma(a)
    \end{equation*}
    where $\gamma(a) = \frac{1}{2}\{(1 + a) \log(1 + a) + (1 - a) \log(1 - a)\}$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.1.4]
    Suppose $P(X_i = k) = e^{-1}/k!$ for $k = 0, 1, \dots$. Show that if $a > 1$
    \begin{equation*}
      \frac{1}{n} \log P(S_n \geq na) \to a - 1 - a \log a
    \end{equation*}
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.2.1]
    Give an example of random variables $X_n$ with densities $f_n$ so that $X_n \Rightarrow$ a uniform distribution on $(0,1)$ but $f_n(x)$ does not converge to 1 for any $x \in [0, 1]$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.2.2]
    \textbf{Convergence of maxima.} Let $X_1, X_2, \dots$ be independent with distribution $F$, and let $M_n = \max_{m \leq n} X_m$. Then $P(M_n \leq x) = F(x)^n$. Prove the following limit laws for $M_n$:
    \begin{enumerate}
      \item If $F(x) = 1 - x^{-\alpha}$ for $x \geq 1$ where $\alpha > 0$ then for $y > 0$
      \begin{equation*}
        P(M_n/n^{1/\alpha} \leq y) \to \exp(-y^{-\alpha})
      \end{equation*}
      \item If $F(x) = 1 - |x|^\beta$ for $-1 \leq x \leq 0$ where $\beta > 0$ then for $y < 0$
      \begin{equation*}
        P(n^{1/\beta} M_n \leq y) \to \exp(-|y|^\beta)
      \end{equation*}
      \item If $F(x) = 1 - e^{-x}$ for $x \geq 0$ then for all $y \in (-\infty, \infty)$
      \begin{equation*}
        P(M_n - \log n \leq y) \to \exp(-e^{-y})
      \end{equation*}
    \end{enumerate}
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.2.3]
    Let $X_1, X_2, \dots$ be i.i.d. and have the standard normal distribution. (i) From Theorem 1.2.6, we know
    \begin{equation*}
      P(X_i > x) \sim \frac{1}{\sqrt{2\pi} x} e^{-x^2/2} \quad \text{as } x \to \infty
    \end{equation*}
    Use this to conclude that for any real number $\theta$
    \begin{equation*}
      P(X_i > x + (\theta/x))/P(X_i > x) \to e^{-\theta}
    \end{equation*}
    (ii) Show that if we define $b_n$ by $P(X_i > b_n) = 1/n$
    \begin{equation*}
      P(b_n(M_n - b_n) \leq x) \to \exp(-e^{-x})
    \end{equation*}
    (iii) Show that $b_n \sim (2 \log n)^{1/2}$ and conclude $M_n/(2 \log n)^{1/2} \to 1$ in probability.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.2.4]
    \textbf{Fatou's lemma.} Let $g \geq 0$ be continuous. If $X_n \Rightarrow X_\infty$ then
    \begin{equation*}
      \liminf_{n \to \infty} E g(X_n) \geq E g(X_\infty)
    \end{equation*}
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.2.5]
    \textbf{Integration to the limit.} Suppose $g, h$ are continuous with $g(x) > 0$, and $|h(x)|/g(x) \to 0$ as $|x| \to \infty$. If $F_n \Rightarrow F$ and $\int g(x) dF_n(x) \leq C < \infty$ then
    \begin{equation*}
      \int h(x) dF_n(x) \to \int h(x) dF(x)
    \end{equation*}
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.2.6]
    \textbf{The Lévy Metric.} Show that
    \begin{equation*}
      \rho(F, G) = \inf\{\epsilon : F(x - \epsilon) - \epsilon \leq G(x) \leq F(x + \epsilon) + \epsilon \text{ for all } x\}
    \end{equation*}
    defines a metric on the space of distributions and $\rho(F_n, F) \to 0$ if and only if $F_n \Rightarrow F$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.2.7]
    The \textbf{Ky Fan metric} on random variables is defined by
    \begin{equation*}
      \alpha(X, Y) = \inf\{\epsilon \geq 0 : P(|X - Y| > \epsilon) \leq \epsilon\}
    \end{equation*}
    Show that if $\alpha(X, Y) = \alpha$ then the corresponding distributions have Lévy distance $\rho(F, G) \leq \alpha$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.2.8]
    Let $\alpha(X, Y)$ be the metric in the previous exercise and let $\beta(X, Y) = E(|X - Y|/(1 + |X - Y|))$ be the metric of Exercise 2.3.6. If $\alpha(X, Y) = a$ then
    \begin{equation*}
      a^2/(1 + a) \leq \beta(X, Y) \leq a + (1 - a)a/(1 + a)
    \end{equation*}
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.2.9]
    If $F_n \Rightarrow F$ and $F$ is continuous then $\sup_x |F_n(x) - F(x)| \to 0$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.2.10]
    If $F$ is any distribution function there is a sequence of distribution functions of the form 
    \begin{equation*}
      \sum_{m=1}^n a_{n,m} 1_{(x_{n,m} \leq x)}
    \end{equation*}
    with $F_n \Rightarrow F$
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.2.11]
    Let $X_n, 1 \leq n \leq \infty$, be integer valued. Show that $X_n \Rightarrow X_\infty$ if and only if $P(X_n = m) \to P(X_\infty = m)$ for all $m$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.2.12]
    Show that if $X_n \to X$ in probability then $X_n \Rightarrow X$ and that, conversely, if $X_n \Rightarrow c$, where $c$ is a constant then $X_n \to c$ in probability.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.2.13]
    \textbf{Converging together lemma.} If $X_n \Rightarrow X$ and $Y_n \Rightarrow c$, where $c$ is a constant then $X_n + Y_n \Rightarrow X + c$. A useful consequence of this result is that if $X_n \Rightarrow X$ and $Z_n - X_n \Rightarrow 0$ then $Z_n \Rightarrow X$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.2.14]
    Suppose $X_n \Rightarrow X, Y_n \geq 0,$ and $Y_n \Rightarrow c$, where $c > 0$ is a constant then $X_n Y_n \Rightarrow cX$. This result is true without the assumptions $Y_n \geq 0$ and $c > 0$. We have imposed these only to make the proof less tedious.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.2.15]
    Show that if $X_n = (X_n^1, \dots, X_n^n)$ is uniformly distributed over the surface of the sphere of radius $\sqrt{n}$ in $\mathbf{R}^n$ then $X_n^1 \Rightarrow$ a standard normal.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.2.16]
    Suppose $Y_n \geq 0$, $E Y_n^\alpha \to 1$ and $E Y_n^\beta \to 1$ for some $0 < \alpha < \beta$. Show that $Y_n \to 1$ in probability.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.2.17]
    For each $K < \infty$ and $y < 1$ there is a $c_{y,K} > 0$ so that $EX^2 = 1$ and $EX^4 \leq K$ implies $P(|X| > y) \geq c_{y,K}$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.3.1]
    Show that if $\varphi$ is a ch.f. then $\text{Re} \varphi$ and $|\varphi|^2$ are also.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.3.2]
    (i) Imitate the proof of Theorem 3.3.11 to show that
    \begin{equation*}
      \mu(\{a\}) = \lim_{T \to \infty} \frac{1}{2T} \int_{-T}^T e^{-ita} \varphi(t) dt
    \end{equation*}
    (ii) If $P(X \in h\mathbf{Z}) = 1$ where $h > 0$ then its ch.f. has $\varphi(2\pi/h + t) = \varphi(t)$ so
    \begin{equation*}
      P(X = x) = \frac{h}{2\pi} \int_{-\pi/h}^{\pi/h} e^{-itx} \varphi(t) dt \quad \text{for } x \in h\mathbf{Z}
    \end{equation*}
    (iii) If $X = Y + b$ then $E \exp(itX) = e^{itb} E \exp(itY)$. So if $P(X \in b + h\mathbf{Z}) = 1$, the inversion formula in (ii) is valid for $x \in b + h\mathbf{Z}$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.3.3]
    Suppose $X$ and $Y$ are independent and have ch.f. $\varphi$ and distribution $\mu$. Apply Exercise 3.3.2 to $X - Y$ and use Exercise 2.1.5 to get
    \begin{equation*}
      \lim_{T \to \infty} \frac{1}{2T} \int_{-T}^T |\varphi(t)|^2 dt = P(X - Y = 0) = \sum_x \mu(\{x\})^2
    \end{equation*}
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.3.4]
    Give an example of a measure $\mu$ with a density but for which $\int |\varphi(t)| dt = \infty$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.3.5]
    Show that if $X_1, \dots, X_n$ are independent and uniformly distributed on $(-1, 1)$, then for $n \geq 2$, $X_1 + \dots + X_n$ has density
    \begin{equation*}
      f(x) = \frac{1}{\pi} \int_0^\infty (\sin t / t)^n \cos tx \, dt
    \end{equation*}
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.3.6]
    Use the result in Example 3.3.16 to conclude that if $X_1, X_2, \dots$ are independent and have the Cauchy distribution, then $(X_1 + \dots + X_n)/n$ has the same distribution as $X_1$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.3.7]
    Suppose that $X_n \Rightarrow X$ and $X_n$ has a normal distribution with mean 0 and variance $\sigma_n^2$. Prove that $\sigma_n^2 \to \sigma^2 \in [0, \infty)$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.3.8]
    Show that if $X_n$ and $Y_n$ are independent for $1 \leq n \leq \infty$, $X_n \Rightarrow X_\infty$, and $Y_n \Rightarrow Y_\infty$, then $X_n + Y_n \Rightarrow X_\infty + Y_\infty$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.3.9]
    Let $X_1, X_2, \dots$ be independent and let $S_n = X_1 + \dots + X_n$. Let $\varphi_j$ be the ch.f. of $X_j$ and suppose that $S_n \to S_\infty$ a.s. Then $S_\infty$ has ch.f. $\prod_{j=1}^\infty \varphi_j(t)$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.3.10]
    Using the identity $\sin t = 2 \sin(t/2) \cos(t/2)$ repeatedly leads to $(\sin t)/t = \prod_{m=1}^\infty \cos(t/2^m)$. Prove the last identity by interpreting each side as a characteristic function.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.3.11]
    Let $X_1, X_2, \dots$ be independent taking values 0 and 1 with probability 1/2 each. $X = 2 \sum_{j \geq 1} X_j/3^j$ has the Cantor distribution. Compute the ch.f. $\varphi$ of $X$ and notice that $\varphi$ has the same value at $t = 3^k\pi$ for $k = 0, 1, 2, \dots$
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.3.12]
    Use Theorem 3.3.18 and the series expansion for $e^{-t^2/2}$ to show that the standard normal distribution has
    \begin{equation*}
      EX^{2n} = (2n)!/2^n n! = (2n - 1)(2n - 3) \cdots 3 \cdot 1 \equiv (2n - 1)!!
    \end{equation*}
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.3.13]
    (i) Suppose that the family of measures $\{\mu_i, i \in I\}$ is tight. Use (d) in Theorem 3.3.1 and (3.3.3) with $n = 0$ to show that their ch.f.'s $\varphi_i$ are equicontinuous. (ii) Suppose $\mu_n \Rightarrow \mu_\infty$. Use Theorem 3.3.17 and equicontinuity to conclude that the ch.f.'s $\varphi_n \to \varphi_\infty$ uniformly on compact sets. (iii) Give an example to show that the convergence need not be uniform on the whole real line.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.3.14]
    Let $X_1, X_2, \dots$ be i.i.d. with characteristic function $\varphi$. (i) If $\varphi'(0) = ia$ and $S_n = X_1 + \dots + X_n$ then $S_n/n \to a$ in probability. (ii) If $S_n/n \to a$ in probability then $\varphi(t/n)^n \to e^{iat}$ as $n \to \infty$. (iii) Use (ii) and uniform continuity established in (d) of Theorem 3.3.1 to show that $(\varphi(h) - 1)/h \to ia$ as $h \to 0$ through the positive reals. Thus the weak law holds if and only if $\varphi'(0)$ exists.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.3.15]
    $2 \int_0^\infty (1 - \text{Re} \varphi(t))/(\pi t^2) dt = \int |y| dF(y)$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.3.16]
    Show that if $\lim_{t \downarrow 0}(\varphi(t) - 1)/t^2 = c > -\infty$ then $EX = 0$ and $E|X|^2 = -2c < \infty$. In particular, if $\varphi(t) = 1 + o(t^2)$ then $\varphi(t) \equiv 1$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.3.17]
    If $Y_n$ are r.v.'s with ch.f.'s $\varphi_n$ then $Y_n \Rightarrow 0$ if and only if there is a $\delta > 0$ so that $\varphi_n(t) \to 1$ for $|t| \leq \delta$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.3.18]
    Let $X_1, X_2, \dots$ be independent. If $S_n = \sum_{m \leq n} X_m$ converges in distribution then it converges in probability (and hence a.s. by Exercise 2.5.10).
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.3.19]
    Show that $\exp(-|t|^\alpha)$ is a characteristic function for $0 < \alpha \leq 1$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.3.20]
    If $X_1, X_2, \dots$ are independent and have characteristic function $\exp(-|t|^\alpha)$ then $(X_1 + \dots + X_n)/n^{1/\alpha}$ has the same distribution as $X_1$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.3.21]
    Let $\varphi_1$ and $\varphi_2$ be ch.f.'s. Show that $A = \{t : \varphi_1(t) = \varphi_2(t)\}$ is closed, contains 0, and is symmetric about 0. Show that if $A$ is a set with these properties and $\varphi_1(t) = e^{-|t|}$ there is a $\varphi_2$ so that $\{t : \varphi_1(t) = \varphi_2(t)\} = A$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.3.22]
    Find independent r.v.'s $X, Y$, and $Z$ so that $Y$ and $Z$ do not have the same distribution but $X + Y$ and $X + Z$ do.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.3.23]
    Show that if $X$ and $Y$ are independent and $X + Y$ and $X$ have the same distribution then $Y = 0$ a.s.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.3.24]
    Let $G(x) = P(|X| < x)$, $\lambda = \sup\{x : G(x) < 1\}$, and $\nu_k = E|X|^k$. Show that $\nu_k^{1/k} \to \lambda$, so the assumption of Theorem 3.3.26 holds if $\lambda < \infty$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.3.25]
    Suppose $|X|$ has density $Cx^\alpha \exp(-x^\lambda)$ on $(0, \infty)$. Changing variables $y = x^\lambda$, $dx = (1/\lambda)x^{1/\lambda-1} \, dy$
    \begin{equation*}
      E|X|^n = \int_0^\infty C\lambda^{-1} y^{(n+\alpha)/\lambda} \exp(-y) y^{1/\lambda-1} \, dy = C\lambda^{-1} \Gamma((n + \alpha + 1)/\lambda)
    \end{equation*}
    Use the identity $\Gamma(x + 1) = x\Gamma(x)$ for $x \geq 0$ to conclude that the assumption of Theorem 3.3.26 is satisfied for $\lambda \geq 1$ but not for $\lambda < 1$. This shows the normal ($\lambda = 2$) and gamma ($\lambda = 1$) distributions are determined by their moments.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.4.1]
    Suppose you roll a die 180 times. Use the normal approximation (with the histogram correction) to estimate the probability you will get fewer than 25 sixes.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.4.2]
    Let $X_1, X_2, \dots$ be i.i.d. with $EX_i = 0$, $0 < \text{var}(X_i) < \infty$, and let $S_n = X_1 + \dots + X_n$. (a) Use the central limit theorem and Kolmogorov's zero-one law to conclude that $\limsup S_n/\sqrt{n} = \infty$ a.s. (b) Use an argument by contradiction to show that $S_n/\sqrt{n}$ does not converge in probability. Hint: Consider $n = m!$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.4.3]
    Let $X_1, X_2, \dots$ be i.i.d. and let $S_n = X_1 + \dots + X_n$. Assume that $S_n/\sqrt{n} \Rightarrow$ a limit and conclude that $EX_i^2 < \infty$. Sketch: Suppose $EX_i^2 = \infty$. Let $X'_1, X'_2, \dots$ be an independent copy of the original sequence. Let $Y_i = X_i - X'_i$, $U_i = Y_i 1_{(|Y_i| \leq A)}$, $V_i = Y_i 1_{(|Y_i| > A)}$, and observe that for any $K$
    \begin{align*}
      P\left(\sum_{m=1}^n Y_m \geq K\sqrt{n}\right) &\geq P\left(\sum_{m=1}^n U_m \geq K\sqrt{n}, \sum_{m=1}^n V_m \geq 0\right) \\
      &\geq \frac{1}{2} P\left(\sum_{m=1}^n U_m \geq K\sqrt{n}\right) \geq \frac{1}{5}
    \end{align*}
    for large $n$ if $A$ is large enough. Since $K$ is arbitrary, this is a contradiction.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.4.4]
    Let $X_1, X_2, \dots$ be i.i.d. with $X_i \geq 0$, $EX_i = 1$, and $\text{var}(X_i) = \sigma^2 \in (0, \infty)$. Show that $2(\sqrt{S_n} - \sqrt{n}) \Rightarrow \sigma\chi$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.4.5]
    \textbf{Self-normalized sums.} Let $X_1, X_2, \dots$ be i.i.d. with $EX_i = 0$ and $EX_i^2 = \sigma^2 \in (0, \infty)$. Then
    \begin{equation*}
      \sum_{m=1}^n X_m / \left( \sum_{m=1}^n X_m^2 \right)^{1/2} \Rightarrow \chi
    \end{equation*}
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.4.6]
    \textbf{Random index central limit theorem.} Let $X_1, X_2, \dots$ be i.i.d. with $EX_i = 0$ and $EX_i^2 = \sigma^2 \in (0, \infty)$, and let $S_n = X_1 + \dots + X_n$. Let $N_n$ be a sequence of nonnegative integer-valued random variables and $a_n$ a sequence of integers with $a_n \to \infty$ and $N_n/a_n \to 1$ in probability. Show that
    \begin{equation*}
      S_{N_n}/\sigma\sqrt{a_n} \Rightarrow \chi
    \end{equation*}
    Hint: Use Kolmogorov's inequality (Theorem 2.5.5) to conclude that if $Y_n = S_{N_n}/\sigma\sqrt{a_n}$ and $Z_n = S_{a_n}/\sigma\sqrt{a_n}$, then $Y_n - Z_n \to 0$ in probability.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.4.7]
    \textbf{A central limit theorem in renewal theory.} Let $Y_1, Y_2, \dots$ be i.i.d. positive random variables with $EY_i = \mu$ and $\text{var}(Y_i) = \sigma^2 \in (0, \infty)$. Let $S_n = Y_1 + \dots + Y_n$ and $N_t = \sup\{m : S_m \leq t\}$. Apply the previous exercise to $X_i = Y_i - \mu$ to prove that as $t \to \infty$
    \begin{equation*}
      (\mu N_t - t)/(\sigma^2 t / \mu)^{1/2} \Rightarrow \chi
    \end{equation*}
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.4.8]
    \textbf{A second proof of the renewal CLT.} Let $Y_1, Y_2, \dots, S_n$, and $N_t$ be as in the last exercise. Let $u = [t/\mu]$, $D_t = S_u - t$. Use Kolmogorov's inequality to show
    \begin{equation*}
      P(|S_{u+m} - (S_u + m\mu)| > t^{2/5} \text{ for some } m \in [-t^{3/5}, t^{3/5}]) \to 0 \quad \text{as } t \to \infty
    \end{equation*}
    Conclude $|N_t - (t - D_t)/\mu| / t^{1/2} \to 0$ in probability and then obtain the result in the previous exercise.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.4.9]
    Suppose $P(X_m = m) = P(X_m = -m) = m^{-2}/2$, and for $m \geq 2$
    \begin{equation*}
      P(X_m = 1) = P(X_m = -1) = (1 - m^{-2})/2
    \end{equation*}
    Show that $\text{var}(S_n)/n \to 2$ but $S_n/\sqrt{n} \Rightarrow \chi$. The trouble here is that $X_{n,m} = X_m/\sqrt{n}$ does not satisfy (ii) of Theorem 3.4.10.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.4.10]
    Show that if $|X_i| \leq M$ and $\sum_n \text{var}(X_n) = \infty$ then
    \begin{equation*}
      (S_n - ES_n)/\sqrt{\text{var}(S_n)} \Rightarrow \chi
    \end{equation*}
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.4.11]
    Suppose $EX_i = 0$, $EX_i^2 = 1$ and $E|X_i|^{2+\delta} \leq C$ for some $0 < \delta, C < \infty$. Show that $S_n/\sqrt{n} \Rightarrow \chi$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.4.12]
    Prove \textbf{Lyapunov's Theorem.} Let $\alpha_n = \{\text{var}(S_n)\}^{1/2}$. If there is a $\delta > 0$ so that
    \begin{equation*}
      \lim_{n \to \infty} \alpha_n^{-(2+\delta)} \sum_{m=1}^n E(|X_m - EX_m|^{2+\delta}) = 0
    \end{equation*}
    then $(S_n - ES_n)/\alpha_n \Rightarrow \chi$. Note that the previous exercise is a special case of this result.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.4.13]
    Suppose $P(X_j = j) = P(X_j = -j) = 1/2j^\beta$ and $P(X_j = 0) = 1 - j^{-\beta}$ where $\beta > 0$. Show that (i) If $\beta > 1$ then $S_n \to S_\infty$ a.s. (ii) if $\beta < 1$ then $S_n/n^{(3-\beta)/2} \Rightarrow c\chi$. (iii) if $\beta = 1$ then $S_n/n \Rightarrow \aleph$ where
    \begin{equation*}
      E \exp(it\aleph) = \exp \left( -\int_0^1 x^{-1}(1 - \cos xt) \, dx \right)
    \end{equation*}
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.6.1]
    Show that $\|\mu - \nu\| \leq 2\delta$ if and only if there are random variables $X$ and $Y$ with distributions $\mu$ and $\nu$ so that $P(X \neq Y) \leq \delta$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.7.4]
    \textbf{M/G/$\infty$ queue.} As one walks around the Duke campus it seems that every student is talking on their smartphone. The argument for arrivals at the ATM implies that the beginnings of calls follow a Poisson process. As for the calls themselves, while many people on the telephone show a lack of memory, there is no reason to suppose that the duration of a call has an exponential distribution, so we use a general distribution function $G$ with $G(0) = 0$ and mean $\mu$. Show that in the long run the number of calls in the system will be Poisson with mean
    \begin{equation*}
      \lambda \int_{r=0}^\infty (1 - G(r)) \, dr = \lambda\mu \tag{3.7.3}
    \end{equation*}
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 3.7.5]
    Suppose that a Poisson number of Duke students with mean 2190 will show up to watch the next women's basketball game. What is the probability that for all of the 365 days there is at least one person in the crowd who has that birthday. (Pretend all birthday have equal probability and February 29th does not exist.)
  \end{exercise}
  \begin{solution}

  \end{solution}

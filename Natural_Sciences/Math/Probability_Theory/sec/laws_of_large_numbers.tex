\section{Laws of Large Numbers} 

  \begin{theorem}[Weak Law of Large Numbers]
    Let $X_1, X_2, ..., X_n$ be a sequence of iid random variables, with finite mean $\mathbb{E}[X]$. Then, the average of the random variables $S_n / n$ converges in probability to $\mathbb{E}[X]$. 
    \begin{equation}
      \frac{S_n}{n} = \frac{1}{n} \sum_{i=1}^n X_i \xrightarrow{i.p} \mathbb{E}[X]
    \end{equation}
    That is, for any $\epsilon > 0$, 
    \begin{equation}
      \lim_{n \rightarrow \infty} \mathbb{P} \bigg( \bigg| \Big( \frac{1}{n} \sum_{k=1}^n X_k \Big) - \mathbb{E}[X] \bigg| > \epsilon \bigg) = 0
    \end{equation}
  \end{theorem}
  \begin{proof}
    We first do the proof assuming additionally that $X$ has finite variance, so $\mathrm{Var}[X] < \infty$. We will show that the random variable $S_n/n$ converges in mean square to $\mathbb{E}[X]$, which will imply convergence in probability. Note that $\mathbb{E}[S_n / n] = \mathbb{E}[X]$, and 
    \begin{align*}
      \lim_{n \rightarrow \infty} \mathbb{E} \bigg[ \bigg| \frac{S_n}{n} - \mathbb{E}[X] \bigg|^2 \bigg] & = \lim_{n \rightarrow \infty} \mathbb{E} \bigg[ \bigg| \frac{S_n}{n} - \mathbb{E}\Big[\frac{S_n}{n}\Big] \bigg|^2 \bigg] \\
      & = \lim_{n \rightarrow \infty} \mathrm{Var}\Big( \frac{S_n}{n} \Big) \\
      & = \lim_{n \rightarrow \infty} \frac{\mathrm{Var}(S_n)}{n^2} \\
      & = \lim_{n \rightarrow \infty} \frac{\mathrm{Var}[X]}{n} = 0
    \end{align*}
  \end{proof}

  \begin{theorem}[Strong Law of Large Numbers]
    Let $X_1, X_2, ..., X_n$ be a sequence of iid random variables, with finite mean $\mathbb{E}(X_k)$ and with finite variance. Then, the average of the random variables $S_n / n$ converges almost surely to $\mathbb{E}[X]$. 
    \begin{equation}
      \frac{S_n}{n} \xrightarrow{a.s.} \mathbb{E}[X]
    \end{equation}
    That is, 
    \begin{equation}
      \mathbb{P} \bigg( \Big\{ \omega \in \Omega \mid \lim_{n \rightarrow \infty} \Big( \frac{1}{n} \sum_{i=1}^n X_i (\omega) \Big) = \mathbb{E}[X] \Big\} \bigg) = 1
    \end{equation}
  \end{theorem}

  Now let's compare these two laws. They both deal with averages of random variables, i.e. we keep sampling from $X$ and compute the averages $\overline{X}_n$. The weak law states that for a specified large $n$, the average $\overline{X}_n$ is likely to be near $\mathbb{E}[X]$. But it leaves open the possibility that $|\overline{X}_n - \mathbb{E}[X]| > \epsilon$ happens an infinite number of times (although less frequently). So no matter how big of an $n$ we choose, there could always be an $\overline{X}_n$ in the future that fails to satisfy $|\overline{X}_n - \mathbb{E}[X]| > \epsilon$. However, the strong law shows that this almost surely will not occur. That is, with probability $1$, we have for any $\epsilon > 0$ the inequality $|\overline{X}_n - \mathbb{E}[X]| < \epsilon$ for all large enough $n$ greater than a certain $N$. Note that the weak law does not guarantee the existence of such an $N$. 

  This result is very useful because it justifies experiments that estimate some value by taking averages. 

  \begin{example}[Estimating Speed of Light]
    Say that we are conducting an experiment to justify the speed of light, which will have true value $\mu$. The laws of large numbers say that in theory, after obtaining enough data, we can get arbitrarily close to the true speed of light. Choose $\epsilon > 0$ arbitrarily small. We can obtain $n$ estimates $X_1, \ldots, X_n$ of the speed of light and compute the average 
    \begin{equation}
      \overline{X}_n = \frac{1}{n} \sum_{i=1}^n X_i
    \end{equation}
    As we obtain more data, we can compute $\overline{X}_n$ for each $n = 1, 2, \ldots$. The weak law says that $\mathbb{P}(|\overline{X}_n - \mu| > \epsilon) \rightarrow 0$ as $n \rightarrow \infty$, i.e. the probability of our estimate being off by more than $\epsilon$ goes to $0$ (though it may happen with nonzero probability if we consider the infinite sequence). The strong law says that the number of times $|\overline{X}_n - \mu|$ is greater than $\epsilon$ is finite (with probability $1$), and after a certain point our estimates will perfectly lie within the error $\epsilon$. This gives us considerable confidence in the value $\overline{X}_n$ because it guarantees the existence of some $N \in \mathbb{N}$ s.t. $|\overline{X}_n - \mu| < \epsilon$ for all $n > N$, i.e. the average \textit{never} fails for $n > N$. 
  \end{example}

\subsection{Concentration Inequalities}

  Concentration inequalities give you probability bounds on random variables taking atypical values. For example, given a random variable with certain mean and variance, the probability of that random variable taking values outside a certain range around the mean is very small. It's called concentration because the probability concentrates around a certain range. 

  The basic question here is that we would like to model a random variable $X$ over a probability space $\Omega$ and have some data $X_1, X_2, \ldots, X_n$ iid according to $X$. Let us have a fixed function $f: \mathbb{R}^n \longrightarrow \mathbb{R}$ that transforms the joint random variable $(X_1, \ldots, X_n)$ to create a new scalar RV 
  \begin{equation}
    f(X_1, \ldots, X_n) = f \circ (X_1, \ldots, X_n) : \Omega \longrightarrow \mathbb{R}
  \end{equation}
  $f(X_1, \ldots, X_n)$ is a random variable so it has a mean, denote it $\mathbb{E}[f]$. Then concentration generally refers to the probability that the value of $f$ is at least some distance further from its mean. 
  \begin{equation}
    \mathbb{P} \big( |f(x) - \mathbb{E}[f] | \geq t \big) \leq \epsilon
  \end{equation}
  for some small positive $\epsilon$. Usually, we would like this $\epsilon$ to be an exponentially decaying function of $t$ so that the bound goes down fast. This is what's so great about the Gaussian, which is why we'll introduce it here. 

  \begin{theorem}[Gaussian Tail Inequality]
    Given $X \sim \mathcal{N}(0, 1)$, the inequality says that the probability of $X$ taking values past a certain $t$ decays exponentially. 
    \begin{equation}
      \mathbb{P} \big( |X| > t \big) \leq \frac{2 e^{-t^2/2}}{t}
    \end{equation}
    If we have $X_1, \ldots, X_n \sim \mathcal{N}(0, 1)$, then 
    \begin{equation}
      \mathbb{P} \big( |\overline{X}| > t \big) \leq \frac{2}{\sqrt{n} t} e^{-n t^2/2}
    \end{equation}
    We can assume that the coefficient is less than $1$ if $n$ is large. The above tells us that this bound exponentially decays with $t$ but also with the number of samples $n$. 
  \end{theorem}
  \begin{proof}
    We can simply check 
    \begin{equation}
      \phi(s) = \frac{1}{\sqrt{2\pi}} e^{-s^2/2} \implies \phi^\prime (s) = s \, \phi(s)
    \end{equation}
    and use this to evaluate
    \begin{align*}
      \mathbb{P}(X > t ) & = \int_t^\infty \phi(s) \,ds \\
      & = \int_t^\infty \frac{s}{s} \phi(s) \,ds \\
      & < \frac{1}{t} \int_t^\infty s \phi(s)\,ds \\
      & = \frac{1}{t} \int_t^\infty \phi^\prime (s)\,ds \\
      & = \frac{\phi(t)}{t}
    \end{align*}
  \end{proof}

  Due to the exponential nature of the probability bound, we are extremely confident in getting the majority of our samples from a small interval. If we had taken some distribution like a Cauchy, with PDF of form 
  \begin{equation}
    f(x) \propto \frac{1}{1 + x^2}
  \end{equation}
  Then we see that even though the shape looks like a Gaussian at first glance, the fat tails go down at the rate of $1/x^2$. It turns out that due to this, when we sample numerically, we occasionally get extreme values. 

  \begin{theorem}[Markov's Inequality]
    If $X$ is a non-negative random variable of finite expectation and $\alpha > 0$, then 
    \begin{equation}
      \mathbb{P}(X > \alpha) \leq \frac{\mathbb{E}[X]}{\alpha}
    \end{equation}
    That is, the probability that $X$ takes a value greater than $\alpha$ is at most the expectation of $X$ divided by $\alpha$. This is meaningful only when $\mathbb{E}[X] < \alpha$, since otherwise the RHS will be greater than $1$.  
  \end{theorem}
  \begin{proof}
    Given any $\alpha > 0$, we can set 
    \begin{equation}
      X = X \cdot 1_{X \leq \alpha} + X \cdot 1_{X > \alpha}
    \end{equation}
    and by linearity, 
    \begin{align*}
      \mathbb{E}[X] & = \mathbb{E}[X \cdot 1_{X \leq \alpha} + X \cdot 1_{X > \alpha}] \\
      & \geq \mathbb{E}[ X \cdot 1_{X > \alpha}] \\
      & \geq \alpha \mathbb{E}[1_{X > \alpha}] \\
      & = \alpha \, \mathbb{P}(X > x) 
    \end{align*}
  \end{proof}

  In other words, the probability that $X > \alpha$ goes down at least as fast as $1/\alpha$. For example, setting $\alpha = 2 \mathbb{E}[X]$, the probability that $X$ takes value that is at least twice its expectation is at most $1/2$. Furthermore, as $X$ gets very large, the probability that it will take a value beyond a large $\alpha$ goes down faster than $1/\alpha$. But this is a very conservative inequality, and usually the probability goes down much faster. 

  Markov's inequality is very conservative but very general, too. If we make further assumptions about the random variable $X$, we can often make stronger bounds. Chebyshev's inequality assumes a (possibly negative) random variable with finite variance and states that the probability will go down as $1/x^2$. 

  \begin{theorem}[Chebyshev Inequality]
    Given (possibly negative) random variable $X$, if $\mathbb{E}[X] = \mu < +\infty$ and $\Var(X) = \sigma^2 < +\infty$, then for all $\alpha > 0$, 
    \begin{equation}
      \mathbb{P} \big( |X - \mu| > k \sigma \big) \leq \frac{1}{k^2} \iff \mathbb{P}(|X - \mu| > \alpha) \leq \frac{\mathrm{Var}[X]}{\alpha^2}
    \end{equation}
    That is, the probability that $X$ takes a value further than $k$ standard deviations away from $\mu$ goes down by $1/k^2$. Therefore, if $\sigma$ is small, then this bound will be small since there is more concentration in the mean. 
  \end{theorem}
  \begin{proof}
    We apply Markov's inequality to the non-negative random variable $|X - \mu|$. 
    \begin{equation}
      \mathbb{P}(|X - \mu| > \alpha) = \mathbb{P}(|X - \mu|^2 > \alpha^2) \leq \frac{\mathbb{E}(|X - \mu|^2)}{\alpha^2} = \frac{\mathrm{Var}[X]}{\alpha^2}
    \end{equation}
    since the numerator on the RHS is the definition of variance. 
  \end{proof}

  Chebyshev inequality is just Markov's inequality applied to $X^2$ (assuming $0$ mean), and often yields a better bound. But even Chebyshev's inequality turns out to be quite loose, and even this $1/k^2$ is not a very nice bound. We could apply Markov's inequality to higher powers of $X$, e.g. given a random variable $X$, we can apply Markov's inequality to the $k$th power of nonnegative random variable $|X - \mathbb{E}[X]|$: 
  \begin{equation}
    \mathbb{P} (|X - \mathbb{E}[X] | > \alpha) = \mathbb{P}\big( |X - \mathbb{E}[X] |^k > \alpha^k \big) \leq \frac{\mathbb{E}( |X - \mathbb{E}[X] |^k )}{\alpha^k}
  \end{equation}
  The natural culmination of all this is to apply Markov's inequality to $e^X$ (or, for a little flexibility, $e^{t X}$, where $t$ is a constant to be optimized). This gives us an exponential bound on $\mathbb{P}(X > \alpha)$. 

  \begin{example}[Gaussian]
    For the normal distribution, recall the 67-95-99.7 rule. It is well known that the probability of a random variable taking values within $2$ standard deviations from the mean is 95\%, so the probability that it takes outside is 5\%, or $1/20$, which is less than the $1/2^2 = 1/4$ bound given by Chebyshev. 
  \end{example}

  \subsubsection{Chernoff Bound and MGFs}

    \begin{theorem}[Chernoff Bound]
      Given a (possibly negative) random variable $X$, assume that its moment generating function $M_X (s) = \mathbb{E}[e^{s X}]$ is finite for every $s \in [-\epsilon, \epsilon]$. Then, since $x \mapsto e^{s x}$ is monotonically increasing, we have the identity 
      \begin{equation}
        \mathbb{P}(X > \alpha) = \mathbb{P}(e^{s X} > e^{s \alpha}) \text{ for } s > 0
      \end{equation}
      But since the new random variable $e^{s X}$ is nonnegative, we can now go back to Markov inequality and write 
      \begin{equation}
        \mathbb{P}(X > \alpha) = \mathbb{P}(e^{s X} > e^{s \alpha}) \geq \frac{\mathbb{E}[e^{s X}]}{e^{s \alpha}} = M_X (s) \, e^{-s \alpha}
      \end{equation}
      for $s > 0$ (for identity above to hold) \textit{and} $s \in D_X$ (and it is in domain of convergence). Now, we have an exponentially decaying bound in terms of $\alpha$. We have the freedom to choose $s$, since our bound is in terms of $\alpha$, so we must choose $s$ that minimizes $M_X (s) \, e^{-s \alpha}$. Ultimately, our best bound is 
      \begin{equation}
        \mathbb{P}(X > \alpha) \leq \inf_{s > 0} M_X (s) \, e^{-s \alpha}
      \end{equation}
      After we optimize over $s$ what remains on the RHS is a function of $\alpha$. 
    \end{theorem}

    Now, we can calculate the MGF of $X$ directly if we knew the distribution of $X$, but we can also get bounds on it given some coarse statistics of $X$. 

    \begin{lemma}
    Let $X$ be a $0$-mean random variable s.t. $a \leq X \leq b$ with probability $1$. Then for all $t > 0$, 
    \begin{equation}
      \mathbb{E}[ e^{t X}] \leq e^{t^2 (b - a)^2 / 8}
    \end{equation}
    \end{lemma}
    \begin{proof}
      We can write $x = \lambda a + (1 - \lambda b)$, $0 \leq \lambda \leq 1$, and convexity of the exponential tells us that 
      \begin{equation}
        e^{tx} \leq \lambda e^{ta} + (1 - \lambda) e^{tb}
      \end{equation}
      Plugging in $\lambda = (b - x) / (b - a)$ then gives 
      \begin{equation}
        e^{tx} \leq \frac{b - x}{b - a} e^{tx} + \frac{x - a}{b - a} e^{tb}
      \end{equation}
      Take expectations of both sides, and using linearity of expectation and the fact that $\mathbb{E}[X] = 0$. 
      \begin{equation}
        \mathbb{E}[e^{tX}] \leq \frac{b - \mathbb{E} X}{b - a} e^{ta} + \frac{\mathbb{E} X - a}{b - a} e^{tb} = \frac{b e^{ta} - a e^{tb}}{b - a} \leq e^{t^2 (b - a)^2 / 8}
      \end{equation}
    \end{proof}

  \subsubsection{Hoeffding's Inequality}

    Hoeffding's inequality is one of the most important inequalities in concentration of measure. The proof of this inequality involves many useful tricks. 

    \begin{theorem}[Hoeffding's Inequality]
      Let $X_1, X_2, \ldots, X_n$ be independent (not necessarily identical) random variables s.t. $a_i \leq X_i \leq b_i$ almost surely. Consider the random variable $\overline{X} = \frac{1}{n} (X_1 + \ldots + X_n)$. Then, for all $t > 0$, we have the two inequalities
      \begin{align*}
        \mathbb{P}\big( \overline{X} - \mathbb{E}[\overline{X}] \geq t \big) & \leq \exp \bigg( -\frac{2 n^2 t^2}{\sum_{i=1}^n (b_i - a_i)^2} \bigg) \\
        \mathbb{P}\big( \overline{X} - \mathbb{E}[\overline{X}] \leq -t \big) & \leq \exp \bigg( -\frac{2 n^2 t^2}{\sum_{i=1}^n (b_i - a_i)^2} \bigg)
      \end{align*}
      which can be combined to produce 
      \begin{equation}
        \mathbb{P}\big( \big| \overline{X} - \mathbb{E}[\overline{X}] \big| \geq t \big) \leq 2 \exp \bigg( -\frac{2 n^2 t^2}{\sum_{i=1}^n (b_i - a_i)^2} \bigg)
      \end{equation}
      We can create an equivalent bound on the sum $S_n = X_1 + \ldots + X_n$: 
      \begin{align*}
        \mathbb{P}\big(| S_n - \mathbb{E}[S_n] | \geq t\big) & = \mathbb{P}\big( n |\overline{X} - \mathbb{E}[\overline{X}] | \geq t \big) \\
          & = \mathbb{P} \big( |\overline{X} - \mathbb{E}[X] | \geq \frac{t}{n} \big) \\
          & \leq 2 \exp \bigg( -\frac{2 t^2}{\sum_{i=1}^n (b_i - a_i)^2} \bigg) 
      \end{align*}
    \end{theorem}
    \begin{proof}
      We will prove just with the case where $X_1, \ldots X_n$ are all bounded by $[a, b]$, which gives 
      \[\mathbb{P} \big( |\overline{X} - \mathbb{E}[\overline{X}] | \geq t \big) \leq 2 \exp \bigg( - \frac{2 n t^2}{(b - a)}\bigg)\] 
      Now, we can write 
      \begin{align*}
        \mathbb{P}(\overline{X}_n > \epsilon ) & = \mathbb{P} \Big( \sum_{i=1}^n X_i \geq n \epsilon \Big) \\
        & = \mathbb{P} \big( e^{t \sum X_i} \geq e^{t n \epsilon} \big) & (\text{Variational Technique}) \\
        & \leq e^{- t n \epsilon} \, \mathbb{E}[e^{t \sum X_i}] & (\text{Markov's Inequality}) \\
        & = e^{-t n \epsilon} \, \big( \mathbb{E}[ e^{t X_i}] \big)^n & (\text{Independence}) \\
        & \leq e^{-t n \epsilon} e^{n \frac{t (b - a)^2}{2}} & (\text{prev. lemma}) 
      \end{align*}
      The step where we introduce an extra parameter $t$ is called a variational technique, used for optimization, and we can adjust $t$ to make it as small as possible. Taking the derivative of the final expression w.r.t. $t$ and solving for $0$ gives us $t = \frac{4 \epsilon}{(b - a)^2}$, and substituting into the expression gives the bound as 
      \begin{equation}
        \mathbb{P}(\overline{X}_n > \epsilon ) \leq \exp \bigg(- \frac{2 n \epsilon^2}{(b - a)^2}
      \end{equation}
    \end{proof}

    By further rearranging, we can write it as 
    \begin{equation}
      \mathbb{P} \bigg( | \overline{X} - \mathbb{E}[\overline{X}] | \geq t \sqrt{\frac{\sum_{i=1}^n (b_i - a_i)^2}{n^2}} \bigg) \leq 2 \exp(-2t^2)
    \end{equation}
    which now looks like our Chebyshev inequality, but without a notion of standard deviation. But note the fact if $a_i \leq X_i \leq b_i$, then $\mathrm{Var}(X_i) \leq (b_i - a_i)^2$ (since $\mathrm{Var}(X_i) = \mathbb{E}[(X_i - \mathbb{E}[X_i])^2] \leq \mathbb{E}[(b_i - a_i)^2]$). So, we have 
    \begin{equation}
      \mathrm{Var}(\overline{X}) \leq \frac{\sum_{i=1}^n (b_i - a_i)^2}{n^2} \implies \mathbb{P}\bigg( |\overline{X} - \mathbb{E}[\overline{X}] | \geq t \sqrt{\frac{\sum_{i=1}^n (b_i - a_i)^2}{n^2}} \geq \mathrm{Var}(\overline{X}) \bigg) \leq 2\exp(-2t^2)
    \end{equation}
    which allows us to interpret Hoeffding's inequality in a more familiar way. It says that the probability that the sample average is more than $t$ standard deviations from its expectation is at most $2 e^{-2t^2}$. 

    \begin{corollary}
      If $X_1, X_2, \ldots, X_n$ are independent with $\mathbb{P}(a_i \leq X_i \leq b_i) = 1$ and common mean $\mu$, then 
      \begin{equation}
        \mathbb{P}\bigg[ \big| \overline{X}_n - \mu \big| \leq \sqrt{ \frac{\sum_{i=1}^n (b_i - a_i)^2}{2n^2} \log \Big(\frac{2}{\delta}\Big)} \bigg] \geq 1 - \delta
      \end{equation}
    \end{corollary}

    \begin{example}[Bernoulli]
      Applying Hoeffding's inequality to a sequence of $n$ $p$-coin tosses $X_1, \ldots, X_n \sim \mathrm{Bernoulli}(p)$ gives 
      \begin{equation}
        \mathbb{P}\big( | \overline{X}_n - p | > \epsilon \big) \leq 2 \exp^{-2 n \epsilon^2}
      \end{equation}
    \end{example}

    \begin{example}[Mean]
      Suppose we have $X_1, X_2, \ldots X_n \sim \mathrm{Bernoulli}(p)$, all iid. Then, by Hoeffding's inequality, the average $\overline{X} = \frac{1}{n} (X_1 + \ldots + X_n)$ is tightly concentrated around $p$. 
      \begin{equation}
        \mathbb{P} \big( | \overline{X} - p | \geq t \big) \leq 2 e^{-2 n t^2}
      \end{equation}
      Note that $b_i - a_i = 1 - 0 = 1$ for all $i$. There is an exponential decay in the probability of the sample mean deviating from its expectation. 
    \end{example}

    \begin{example}[Hypercube]
      Pick $X \in [-1, +1]^d$ uniformly at random, i.e. choose iid $X_1,\ldots, X_d \sim \mathrm{Uniform}[-1, +1]$. The expectation is 
      \begin{equation}
        \mathbb{E} ||X||^2 = \sum_{i=1}^d \mathbb{E} X_i^2 = \sum_{i=1}^d \int_{-1}^1 x^2 f_X (x) \,dx = \sum_{i=1}^d \int_{-1}^1 \frac{1}{2} x^2 \,dx = \frac{d}{3}
      \end{equation}
      Then, it can be shown that $||X|| = $ is tightly concentrated around $\sqrt{d/3}$. We show this again with Hoeffding's inequality by showing the concentration of $||X||^2$ around $d/3$. 
      \begin{equation}
        \mathbb{P} \bigg( \bigg| ||X||^2 - \frac{d}{3} \bigg| \geq t \bigg) \leq 2 \exp \Big( - \frac{ d t^2}{2} \Big)
      \end{equation}
      This tells us that if we choose the uniform random vector $X \in [-1, +1]^d$, the vast majority of our samples will have $||X|| \approx \sqrt{d/3}$. 
    \end{example}


    Hoeffding's inequality does not use any information about the random variables expect for the fact that they are bounded. If the variance of $X_i$ is small, then we can get a sharper inequality from Bernstein's inequality. 

    \begin{theorem}[Bernstein's Inequality]
      If $\mathbb{P}(|X_i| \leq c) = 1$ and $\mathbb{E}[X_i] = 0$, set $\overline{X} = \frac{1}{n} \sum_{i=1}^n X_i$. Then, for any $t > 0$, 
      \begin{equation}
        \mathbb{P} \big( \big| \overline{X} \big| > \epsilon \big) \leq 2 \exp \bigg( - \frac{n \epsilon^2}{2 \sigma^2 + 2 c \epsilon /3} \bigg)
      \end{equation}
      where $\sigma^2 = \frac{1}{n} \sum_{i=1}^n \mathrm{Var}(X_i)$. 
    \end{theorem}

  \subsubsection{Concentration of Lipshitz Functions}

    Observing the Hoeffding bound, one might wonder whether such concentration applies only to averages or sums of random variables. After all, what's so special about averages? It turns out that the relevant feature of the average that yields tight concentration is that it is smooth in the way that if we change the value of one random variable the function does not change dramatically. 

    \begin{theorem}[Bounded Difference Inequality]
      Let has have independent random variables $X_1, X_2, \ldots, X_n$ and a function $f: \mathbb{R}^n \longrightarrow \mathbb{R}$ that satisfies the \textbf{bounded difference property} that 
      \begin{equation}
        \big| f(x_1, \ldots, x_k, \ldots, x_n) - f(x_1, \ldots, x_k^\prime, \ldots, x_n) \big| \leq c_k
      \end{equation}
      for every $x, x^\prime \in \mathbb{R}^n$. That is, the function changes by at most $c_k$ if its $k$th coordinate is changed. Then, for all $t \geq 0$, we have the concentration inequality: 
      \begin{align*}
        \mathbb{P} \big( f(X_1, \ldots, X_n) - \mathbb{E}[ f(X_1, \ldots, X_n)] \geq t \big) & \leq \exp \bigg(- \frac{2t^2}{\sum_{k=1}^n c_k^2} \bigg) \\
        \mathbb{P} \big( f(X_1, \ldots, X_n) - \mathbb{E}[ f(X_1, \ldots, X_n)] \leq -t \big) & \leq \exp \bigg(- \frac{2t^2}{\sum_{k=1}^n c_k^2} \bigg)
      \end{align*}
      Combining the two gives 
      \[\mathbb{P} \big( \big| f(X_1, \ldots, X_n) - \mathbb{E}[ f(X_1, \ldots, X_n)] \geq t \big| \big) \leq \exp \bigg(- \frac{2t^2}{\sum_{k=1}^n c_k^2} \bigg)\]
    \end{theorem}

    In fact, any smooth function of bounded independent random variables is tightly concentrated around its expectation, and the notion of smoothness is Lipshitz continuity. 

    \begin{definition}
      A function $f: \mathbb{R}^n \longrightarrow \mathbb{R}$ is $L$-Lipschitz w.r.t. the $l_p$-metric if for all $\mathbf{x}, \mathbf{y} \in \mathbb{R}^n$, 
      \begin{equation}
        |f(\mathbf{x}) - f(\mathbf{y})| \leq L ||\mathbf{x} - \mathbf{y}||_p
      \end{equation}
    \end{definition}

    \begin{example}
      For $x = (x_1, x_2, \ldots, x_n)$, we define the average $a(x) = \frac{1}{n} (x_1 + \ldots + x_n)$. Then, $a$ is $(1/n)$-Lipschitz w.r.t. the $l_1$ metric, since for any $\mathbf{x}, \mathbf{y}$, 
      \begin{align*}
        |a(\mathbf{x}) - a(\mathbf{y})| & = \bigg| \frac{1}{n} \big[ (x_1 - y_1) + \ldots + (x_n - y_n) \big] \bigg| \\
        & = \frac{1}{n} \big( |x_1 - y_1| + \ldots + |x_n - y_n| \big) \\
        & = \frac{1}{n} ||\mathbf{x} - \mathbf{y} ||_1
      \end{align*}
    \end{example}

    It turns out that Hoeffding's bound holds for all Lipschitz functions w.r.t. the $l_1$ metric. 

    \begin{theorem}
      Suppose $X_1, X_2, \ldots, X_n$ are independent and bounded with $a_i \leq x_i \leq b_i$. Then, for any $f: \mathbb{R}^n \longrightarrow \mathbb{R}$ that is $L$-Lipschitz w.r.t. the $l_1$-metric, we have 
      \begin{align*}
        \mathbb{P} [ f \geq \mathbb{E}(f) + t] & = \mathbb{P} [ f - \mathbb{E}(f) \geq t] \leq \exp\bigg(- \frac{2 t^2}{L^2 \sum_{i=1}^n (b_i - a_i)^2} \bigg) \\
        \mathbb{P} [ f \leq \mathbb{E}(f) - t] & = \mathbb{P} [ f - \mathbb{E}(f) \leq -t] \leq \exp\bigg(- \frac{2 t^2}{L^2 \sum_{i=1}^n (b_i - a_i)^2} \bigg)
      \end{align*}
      and combining these inequalities gives 
      \begin{equation}
        \mathbb{P} [ |f - \mathbb{E}(f)| \geq t] \leq \exp\bigg(- \frac{2 t^2}{L^2 \sum_{i=1}^n (b_i - a_i)^2} \bigg)
      \end{equation}
    \end{theorem}

\subsection{Exercises} 

  \begin{exercise}[Durrett 2.1.1]
    Suppose $(X_1, \dots, X_n)$ has density $f(x_1, x_2, \dots, x_n)$, that is
    \begin{equation*}
      P((X_1, X_2, \dots, X_n) \in A) = \int_A f(x) \, dx \text{ for } A \in \mathcal{R}^n
    \end{equation*}
    If $f(x)$ can be written as $g_1(x_1) \cdots g_n(x_n)$ where the $g_m \geq 0$ are measurable, then $X_1, X_2, \dots, X_n$ are independent. Note that the $g_m$ are not assumed to be probability densities.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.1.2]
    Suppose $X_1, \dots, X_n$ are random variables that take values in countable sets $S_1, \dots, S_n$. Then in order for $X_1, \dots, X_n$ to be independent, it is sufficient that whenever $x_i \in S_i$
    \begin{equation*}
      P(X_1 = x_1, \dots, X_n = x_n) = \prod_{i=1}^n P(X_i = x_i)
    \end{equation*}
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.1.3]
    Let $\rho(x, y)$ be a metric. (i) Suppose $h$ is differentiable with $h(0) = 0$, $h'(x) > 0$ for $x > 0$ and $h'(x)$ decreasing on $[0, \infty)$. Then $h(\rho(x, y))$ is a metric. (ii) $h(x) = x/(x + 1)$ satisfies the hypotheses in (i).
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.1.4]
    Let $\Omega = (0, 1)$, $\mathcal{F} = \text{Borel sets}$, $P = \text{Lebesgue measure}$. $X_n(\omega) = \sin(2\pi n\omega)$, $n = 1, 2, \dots$ are uncorrelated but not independent.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.1.5]
    \begin{enumerate}
      \item Show that if $X$ and $Y$ are independent with distributions $\mu$ and $\nu$ then
      \begin{equation*}
        P(X + Y = 0) = \sum_y \mu(\{-y\})\nu(\{y\})
      \end{equation*}
      \item Conclude that if $X$ has continuous distribution $P(X = Y) = 0$.
    \end{enumerate}
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.1.6]
    Prove directly from the definition that if $X$ and $Y$ are independent and $f$ and $g$ are measurable functions then $f(X)$ and $g(Y)$ are independent.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.1.7]
    Let $K \geq 3$ be a prime and let $X$ and $Y$ be independent random variables that are uniformly distributed on $\{0, 1, \dots, K-1\}$. For $0 \leq n < K$, let $Z_n = X + nY \pmod K$. Show that $Z_0, Z_1, \dots, Z_{K-1}$ are \textbf{pairwise independent}, i.e., each pair is independent. They are not independent because if we know the values of two of the variables then we know the values of all the variables.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.1.8]
    Find four random variables taking values in $\{-1, 1\}$ so that any three are independent but all four are not. Hint: Consider products of independent random variables.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.1.9]
    Let $\Omega = \{1, 2, 3, 4\}$, $\mathcal{F} = \text{all subsets of } \Omega$, and $P(\{i\}) = 1/4$. Give an example of two collections of sets $\mathcal{A}_1$ and $\mathcal{A}_2$ that are independent but whose generated $\sigma$-fields are not.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.1.10]
    Show that if $X$ and $Y$ are independent, integer-valued random variables, then
    \begin{equation*}
      P(X + Y = n) = \sum_m P(X = m)P(Y = n - m)
    \end{equation*}
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.1.11]
    In Example 1.6.13, we introduced the Poisson distribution with parameter $\lambda$, which is given by $P(Z = k) = e^{-\lambda}\lambda^k/k!$ for $k = 0, 1, 2, \dots$. Use the previous exercise to show that if $X = \text{Poisson}(\lambda)$ and $Y = \text{Poisson}(\mu)$ are independent then $X + Y = \text{Poisson}(\lambda + \mu)$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.1.12]
    $X$ is said to have a $\text{Binomial}(n, p)$ distribution if
    \begin{equation*}
      P(X = m) = \binom{n}{m} p^m (1 - p)^{n-m}
    \end{equation*}
    (i) Show that if $X = \text{Binomial}(n, p)$ and $Y = \text{Binomial}(m, p)$ are independent then $X + Y = \text{Binomial}(n + m, p)$. (ii) Look at Example 1.6.12 and use induction to conclude that the sum of $n$ independent $\text{Bernoulli}(p)$ random variables is $\text{Binomial}(n, p)$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.1.13]
    It should not be surprising that the distribution of $X + Y$ can be $F * G$ without the random variables being independent. Suppose $X, Y \in \{0, 1, 2\}$ and take each value with probability $1/3$. (a) Find the distribution of $X + Y$ assuming $X$ and $Y$ are independent. (b) Find all the joint distributions $(X, Y)$ so that the distribution of $X + Y$ is the same as the answer to (a).
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.1.14]
    Let $X, Y \geq 0$ be independent with distribution functions $F$ and $G$. Find the distribution function of $XY$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.1.15]
    If we want an infinite sequence of coin tossings, we do not have to use Kolmogorov's theorem. Let $\Omega$ be the unit interval $(0, 1)$ equipped with the Borel sets $\mathcal{F}$ and Lebesgue measure $P$. Let $Y_n(\omega) = 1$ if $\lfloor 2^n \omega \rfloor$ is odd and $0$ if $\lfloor 2^n \omega \rfloor$ is even. Show that $Y_1, Y_2, \dots$ are independent with $P(Y_k = 0) = P(Y_k = 1) = 1/2$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.2.1]
    Let $X_1, X_2, \dots$ be uncorrelated with $EX_i = \mu_i$ and $\text{var}(X_i)/i \to 0$ as $i \to \infty$. Let $S_n = X_1 + \dots + X_n$ and $\nu_n = ES_n/n$ then as $n \to \infty$, $S_n/n - \nu_n \to 0$ in $L^2$ and in probability.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.2.2]
    The $L^2$ weak law generalizes immediately to certain dependent sequences. Suppose $EX_n = 0$ and $EX_n X_m \leq r(n - m)$ for $m \leq n$ (no absolute value on the left-hand side!) with $r(k) \to 0$ as $k \to \infty$. Show that $(X_1 + \dots + X_n)/n \to 0$ in probability.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.2.3]
    \textbf{Monte Carlo integration.} (i) Let $f$ be a measurable function on $[0, 1]$ with $\int_0^1 |f(x)| \, dx < \infty$. Let $U_1, U_2, \dots$ be independent and uniformly distributed on $[0, 1]$, and let
    \begin{equation*}
      I_n = n^{-1}(f(U_1) + \dots + f(U_n))
    \end{equation*}
    Show that $I_n \to I \equiv \int_0^1 f \, dx$ in probability. (ii) Suppose $\int_0^1 |f(x)|^2 \, dx < \infty$. Use Chebyshev's inequality to estimate $P(|I_n - I| > a/n^{1/2})$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.2.4]
    Let $X_1, X_2, \dots$ be i.i.d. with $P(X_i = (-1)^k k) = C/k^2 \log k$ for $k \geq 2$ where $C$ is chosen to make the sum of the probabilities $= 1$. Show that $E|X_i| = \infty$, but there is a finite constant $\mu$ so that $S_n/n \to \mu$ in probability.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.2.5]
    Let $X_1, X_2, \dots$ be i.i.d. with $P(X_i > x) = e/x \log x$ for $x \geq e$. Show that $E|X_i| = \infty$, but there is a sequence of constants $\mu_n \to \infty$ so that $S_n/n - \mu_n \to 0$ in probability.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.2.6]
    (i) Show that if $X \geq 0$ is integer valued $EX = \sum_{n \geq 1} P(X \geq n)$. (ii) Find a similar expression for $EX^2$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.2.7]
    Generalize Lemma 2.2.13 to conclude that if $H(x) = \int_{(-\infty, x]} h(y) \, dy$ with $h(y) \geq 0$, then
    \begin{equation*}
      E H(X) = \int_{-\infty}^{\infty} h(y) P(X \geq y) \, dy
    \end{equation*}
    An important special case is $H(x) = \exp(\theta x)$ with $\theta > 0$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.2.8]
    \textbf{An unfair ``fair game.''} Let $p_k = 1/2^k k(k + 1)$, $k = 1, 2, \dots$ and $p_0 = 1 - \sum_{k \geq 1} p_k$.
    \begin{equation*}
      \sum_{k=1}^{\infty} 2^k p_k = (1 - \frac{1}{2}) + (\frac{1}{2} - \frac{1}{3}) + \dots = 1
    \end{equation*}
    so if we let $X_1, X_2, \dots$ be i.i.d. with $P(X_n = -1) = p_0$ and $P(X_n = 2^k - 1) = p_k$ for $k \geq 1$ then $EX_n = 0$. Let $S_n = X_1 + \dots + X_n$. Use Theorem 2.2.11 with $b_n = 2^{m(n)}$ where $m(n) = \min\{m : 2^{-m} m^{-3/2} \leq n^{-1}\}$ to conclude that $S_n/(n/ \log_2 n) \to -1$ in probability.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.2.9]
    \textbf{Weak law for positive variables.} Suppose $X_1, X_2, \dots$ are i.i.d., $P(0 \leq X_i < \infty) = 1$ and $P(X_i > x) > 0$ for all $x$. Let $\mu(s) = \int_0^s x \, dF(x)$ and $\nu(s) = \mu(s)/s(1 - F(s))$. It is known that there exist constants $a_n$ so that $S_n/a_n \to 1$ in probability, if and only if $\nu(s) \to \infty$ as $s \to \infty$. Pick $b_n \geq 1$ so that $n\mu(b_n) = b_n$ (this works for large $n$), and use Theorem 2.2.11 to prove that the condition is sufficient.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.3.1]
    Prove that $P(\limsup A_n) \geq \limsup P(A_n)$ and $P(\liminf A_n) \leq \liminf P(A_n)$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.3.2]
    Prove the first result in Theorem 2.3.4 directly from the definition.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.3.4]
    \textbf{Fatou's lemma.} Suppose $X_n \geq 0$ and $X_n \to X$ in probability. Show that $\liminf_{n \to \infty} EX_n \geq EX$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.3.5]
    \textbf{Dominated convergence.} Suppose $X_n \to X$ in probability and (a) $|X_n| \leq Y$ with $EY < \infty$ or (b) there is a continuous function $g$ with $g(x) > 0$ for large $x$ with $|x|/g(x) \to 0$ as $|x| \to \infty$ so that $Eg(X_n) \leq C < \infty$ for all $n$. Show that $EX_n \to EX$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.3.6]
    \textbf{Metric for convergence in probability.} Show (a) that $d(X, Y) = E(|X - Y|/(1 + |X - Y|))$ defines a metric on the set of random variables, i.e., (i) $d(X, Y) = 0$ if and only if $X = Y$ a.s., (ii) $d(X, Y) = d(Y, X)$, (iii) $d(X, Z) \leq d(X, Y) + d(Y, Z)$ and (b) that $d(X_n, X) \to 0$ as $n \to \infty$ if and only if $X_n \to X$ in probability.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.3.7]
    Show that random variables are a complete space under the metric defined in the previous exercise, i.e., if $d(X_m, X_n) \to 0$ whenever $m, n \to \infty$ then there is a r.v. $X_{\infty}$ so that $X_n \to X_{\infty}$ in probability.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.3.8]
    Let $A_n$ be a sequence of independent events with $P(A_n) < 1$ for all $n$. Show that $P(\cup A_n) = 1$ implies $\sum_n P(A_n) = \infty$ and hence $P(A_n \text{ i.o.}) = 1$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.3.9]
    (i) If $P(A_n) \to 0$ and $\sum_{n=1}^{\infty} P(A_n^c \cap A_{n+1}) < \infty$ then $P(A_n \text{ i.o.}) = 0$. (ii) Find an example of a sequence $A_n$ to which the result in (i) can be applied but the Borel-Cantelli lemma cannot.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.3.10]
    \textbf{Kochen-Stone lemma.} Suppose $\sum P(A_k) = \infty$. Use Exercises 1.6.6 and 2.3.1 to show that if
    \begin{equation*}
      \limsup_{n \to \infty} \left( \sum_{k=1}^n P(A_k) \right)^2 / \left( \sum_{1 \leq j, k \leq n} P(A_j \cap A_k) \right) = \alpha > 0
    \end{equation*}
    then $P(A_n \text{ i.o.}) \geq \alpha$. The case $\alpha = 1$ contains Theorem 2.3.7.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.3.11]
    Let $X_1, X_2, \dots$ be independent with $P(X_n = 1) = p_n$ and $P(X_n = 0) = 1 - p_n$. Show that (i) $X_n \to 0$ in probability if and only if $p_n \to 0$, and (ii) $X_n \to 0$ a.s. if and only if $\sum p_n < \infty$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.3.12]
    Let $X_1, X_2, \dots$ be a sequence of r.v.'s on $(\Omega, \mathcal{F}, P)$ where $\Omega$ is a countable set and $\mathcal{F}$ consists of all subsets of $\Omega$. Show that $X_n \to X$ in probability implies $X_n \to X$ a.s.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.3.13]
    If $X_n$ is any sequence of random variables, there are constants $c_n \to \infty$ so that $X_n/c_n \to 0$ a.s.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.3.14]
    Let $X_1, X_2, \dots$ be independent. Show that $\sup X_n < \infty$ a.s. if and only if $\sum_n P(X_n > A) < \infty$ for some $A$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.3.15]
    Let $X_1, X_2, \dots$ be i.i.d. with $P(X_i > x) = e^{-x}$, let $M_n = \max_{1 \leq m \leq n} X_m$. Show that (i) $\limsup_{n \to \infty} X_n/ \log n = 1$ a.s. and (ii) $M_n/ \log n \to 1$ a.s.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.3.16]
    Let $X_1, X_2, \dots$ be i.i.d. with distribution $F$, let $\lambda_n \uparrow \infty$, and let $A_n = \{\max_{1 \leq m \leq n} X_m > \lambda_n \}$. Show that $P(A_n \text{ i.o.}) = 0$ or $1$ according as $\sum_{n \geq 1} (1 - F(\lambda_n)) < \infty$ or $= \infty$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.3.17]
    Let $Y_1, Y_2, \dots$ be i.i.d. Find necessary and sufficient conditions for (i) $Y_n/n \to 0$ almost surely, (ii) $(\max_{m \leq n} Y_m)/n \to 0$ almost surely, (iii) $(\max_{m \leq n} Y_m)/n \to 0$ in probability, and (iv) $Y_n/n \to 0$ in probability.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.3.18]
    Let $0 \leq X_1 \leq X_2 \dots$ be random variables with $EX_n \sim an^{\alpha}$ with $a, \alpha > 0$, and $\text{var}(X_n) \leq Bn^{\beta}$ with $\beta < 2\alpha$. Show that $X_n/n^{\alpha} \to a$ a.s.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.3.19]
    Let $X_n$ be independent Poisson r.v.'s with $EX_n = \lambda_n$, and let $S_n = X_1 + \dots + X_n$. Show that if $\sum \lambda_n = \infty$ then $S_n/ES_n \to 1$ a.s.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.3.20]
    Show that if $X_n$ is the outcome of the $n$th play of the St. Petersburg game (Example 2.2.16) then $\limsup_{n \to \infty} X_n / (n \log_2 n) = \infty$ a.s. and hence the same result holds for $S_n$. This shows that the convergence $S_n / (n \log_2 n) \to 1$ in probability proved in Section 2.2 does not occur a.s.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.4.1]
    \textbf{Lazy janitor.} Suppose the $i$th light bulb burns for an amount of time $X_i$ and then remains burned out for time $Y_i$ before being replaced. Suppose the $X_i, Y_i$ are positive and independent with the $X$'s having distribution $F$ and the $Y$'s having distribution $G$, both of which have finite mean. Let $R_t$ be the amount of time in $[0, t]$ that we have a working light bulb. Show that $R_t/t \to EX_i/(EX_i + EY_i)$ almost surely.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.4.2]
    Let $X_0 = (1, 0)$ and define $X_n \in \mathbf{R}^2$ inductively by declaring that $X_{n+1}$ is chosen at random from the ball of radius $|X_n|$ centered at the origin, i.e., $X_{n+1}/|X_n|$ is uniformly distributed on the ball of radius 1 and independent of $X_1, \dots, X_n$. Prove that $n^{-1} \log |X_n| \to c$ a.s. and compute $c$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.4.3]
    \textbf{Investment problem.} We assume that at the beginning of each year you can buy bonds for $\$1$ that are worth $\$a$ at the end of the year or stocks that are worth a random amount $V \geq 0$. If you always invest a fixed proportion $p$ of your wealth in bonds, then your wealth at the end of year $n + 1$ is $W_{n+1} = (ap + (1 - p)V_n)W_n$. Suppose $V_1, V_2, \dots$ are i.i.d. with $EV_n^2 < \infty$ and $E(V_n^{-2}) < \infty$. (i) Show that $n^{-1} \log W_n \to c(p)$ a.s. (ii) Show that $c(p)$ is concave. (iii) By investigating $c'(0)$ and $c'(1)$, give conditions on $V$ that guarantee that the optimal choice of $p$ is in $(0, 1)$. (iv) Suppose $P(V = 1) = P(V = 4) = 1/2$. Find the optimal $p$ as a function of $a$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.5.1]
    Suppose $X_1, X_2, \dots$ are i.i.d. with $EX_i = 0, \text{var}(X_i) = C < \infty$. Use Theorem 2.5.5 with $n = m^{\alpha}$ where $\alpha(2p - 1) > 1$ to conclude that if $S_n = X_1 + \dots + X_n$ and $p > 1/2$ then $S_n/n^p \to 0$ almost surely.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.5.2]
    The converse of Theorem 2.5.12 is much easier. Let $p > 0$. If $S_n/n^{1/p} \to 0$ a.s. then $E|X_1|^p < \infty$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.5.3]
    Let $X_1, X_2, \dots$ be i.i.d. standard normals. Show that for any $t$
    \begin{equation*}
      \sum_{n=1}^{\infty} X_n \cdot \frac{\sin(n\pi t)}{n} \text{ converges a.s.}
    \end{equation*}
    We will see this series again at the end of Section 8.1 [REF].
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.5.4]
    Let $X_1, X_2, \dots$ be independent with $EX_n = 0, \text{var}(X_n) = \sigma_n^2$. (i) Show that if $\sum_n \sigma_n^2/n^2 < \infty$ then $\sum_n X_n/n$ converges a.s. and hence $n^{-1} \sum_{m=1}^n X_m \to 0$ a.s. (ii) Suppose $\sum \sigma_n^2/n^2 = \infty$ and without loss of generality that $\sigma_n^2 \leq n^2$ for all $n$. Show that there are independent random variables $X_n$ with $EX_n = 0$ and $\text{var}(X_n) \leq \sigma_n^2$ so that $X_n/n$ and hence $n^{-1} \sum_{m \leq n} X_m$ does not converge to 0 a.s.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.5.5]
    Let $X_n \geq 0$ be independent for $n \geq 1$. The following are equivalent: (i) $\sum_{n=1}^{\infty} X_n < \infty$ a.s. (ii) $\sum_{n=1}^{\infty} [P(X_n > 1) + E(X_n 1_{\{X_n \leq 1\}})] < \infty$ (iii) $\sum_{n=1}^{\infty} E(X_n/(1 + X_n)) < \infty$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.5.6]
    Let $\psi(x) = x^2$ when $|x| \leq 1$ and $= |x|$ when $|x| \geq 1$. Show that if $X_1, X_2, \dots$ are independent with $EX_n = 0$ and $\sum_{n=1}^{\infty} E\psi(X_n) < \infty$ then $\sum_{n=1}^{\infty} X_n$ converges a.s.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.5.7]
    Let $X_n$ be independent. Suppose $\sum_{n=1}^{\infty} E|X_n|^{p(n)} < \infty$ where $0 < p(n) \leq 2$ for all $n$ and $EX_n = 0$ when $p(n) > 1$. Show that $\sum_{n=1}^{\infty} X_n$ converges a.s.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.5.8]
    Let $X_1, X_2, \dots$ be i.i.d. and not $\equiv 0$. Then the radius of convergence of the power series $\sum_{n \geq 1} X_n(\omega)z^n$ (i.e., $r(\omega) = \sup\{c : \sum |X_n(\omega)|c^n < \infty \}$) is $1$ a.s. or $0$ a.s., according as $E \log^+ |X_1| < \infty$ or $= \infty$ where $\log^+ x = \max(\log x, 0)$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.5.9]
    Let $X_1, X_2, \dots$ be independent and let $S_{m, n} = X_{m+1} + \dots + X_n$. Then
    \begin{equation*}
      (*) \quad P\left( \max_{m < j \leq n} |S_{m, j}| > 2a \right) \min_{m < k \leq n} P(|S_{k, n}| \leq a) \leq P(|S_{m, n}| > a)
    \end{equation*}
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.5.10]
    Use $(*)$ to prove a theorem of P. LÃ©vy: Let $X_1, X_2, \dots$ be independent and let $S_n = X_1 + \dots + X_n$. If $\lim_{n \to \infty} S_n$ exists in probability then it also exists a.s.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.5.11]
    Let $X_1, X_2, \dots$ be i.i.d. and $S_n = X_1 + \dots + X_n$. Use $(*)$ to conclude that if $S_n/n \to 0$ in probability then $(\max_{1 \leq m \leq n} S_m)/n \to 0$ in probability.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.5.12]
    Let $X_1, X_2, \dots$ be i.i.d. and $S_n = X_1 + \dots + X_n$. Suppose $a_n \uparrow \infty$ and $a(2^n)/a(2^{n-1})$ is bounded. (i) Use $(*)$ to show that if $S_n/a(n) \to 0$ in probability and $S_{2^n}/a(2^n) \to 0$ a.s. then $S_n/a(n) \to 0$ a.s. (ii) Suppose in addition that $EX_1 = 0$ and $EX_1^2 < \infty$. Use the previous exercise and Chebyshev's inequality to conclude that $S_n/n^{1/2}(\log_2 n)^{1/2+\epsilon} \to 0$ a.s.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.6.1]
    Show that $t/E(\xi_i \wedge t) \leq U(t) \leq 2t/E(\xi_i \wedge t)$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.6.2]
    Deduce Theorem 2.6.3 from Theorem 2.6.1 by showing
    \begin{equation*}
      \limsup_{t \to \infty} E(N_t/t)^2 < \infty.
    \end{equation*}
    Hint: Use a comparison like the one in the proof of Theorem 2.6.3.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.6.3]
    Customers arrive at times of a Poisson process with rate 1. If the server is occupied, they leave. (Think of a public telephone or prostitute.) If not, they enter service and require a service time with a distribution $F$ that has mean $\mu$. Show that the times at which customers enter service are a renewal process with mean $\mu+1$, and use Theorem 2.6.1 to conclude that the asymptotic fraction of customers served is $1/(\mu+1)$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.6.4]
    Let $A_t = t - T_{N(t)-1}$ be the ``age'' at time $t$, i.e., the amount of time since the last renewal. If we fix $x > 0$ then $H(t) = P(A_t > x)$ satisfies the renewal equation
    \begin{equation*}
      H(t) = (1 - F(t)) \cdot 1_{(x, \infty)}(t) + \int_0^t H(t - s) \, dF(s)
    \end{equation*}
    so $P(A_t > x) \to \frac{1}{\mu} \int_{(x, \infty)} (1 - F(t)) \, dt$, which is the limit distribution for the residual lifetime $B_t = T_{N(t)} - t$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.6.5]
    Use the renewal equation in the last problem and Theorem 2.6.9 to conclude that if $T$ is a rate $\lambda$ Poisson process $A_t$ has the same distribution as $\xi_i \wedge t$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.6.6]
    Let $A_t = t - T_{N(t)-1}$ and $B_t = T_{N(t)} - t$. Show that
    \begin{equation*}
      P(A_t > x, B_t > y) \to \frac{1}{\mu} \int_{x+y}^{\infty} (1 - F(t)) \, dt
    \end{equation*}
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.6.7]
    \textbf{Alternating renewal process.} Let $\xi_1, \xi_2, \dots > 0$ be i.i.d. with distribution $F_1$ and let $\eta_1, \eta_2, \dots > 0$ be i.i.d. with distribution $F_2$. Let $T_0 = 0$ and for $k \geq 1$ let $S_k = T_{k-1} + \xi_k$ and $T_k = S_k + \eta_k$. In words, we have a machine that works for an amount of time $\xi_k$, breaks down, and then requires $\eta_k$ units of time to be repaired. Let $F = F_1 * F_2$ and let $H(t)$ be the probability the machine is working at time $t$. Show that if $F$ is nonarithmetic then as $t \to \infty$
    \begin{equation*}
      H(t) \to \mu_1/(\mu_1 + \mu_2)
    \end{equation*}
    where $\mu_i$ is the mean of $F_i$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.6.8]
    Write a renewal equation for $H(t) = P(\text{number of renewals in } [0, t] \text{ is odd})$ and use the renewal theorem to show that $H(t) \to 1/2$. Note: This is a special case of the previous exercise.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.6.9]
    \textbf{Renewal densities.} Show that if $F(t)$ has a directly Riemann integrable density function $f(t)$, then the $V = U - 1_{[0, \infty)}$ has a density $v$ that satisfies
    \begin{equation*}
      v(t) = f(t) + \int_0^t v(t - s) \, dF(s)
    \end{equation*}
    Use the renewal theorem to conclude that if $f$ is directly Riemann integrable then $v(t) \to 1/\mu$ as $t \to \infty$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.7.1]
    Consider $\gamma(a)$ defined in (2.7.1). The following are equivalent: (a) $\gamma(a) = -\infty$, (b) $P(X_1 \geq a) = 0$, and (c) $P(S_n \geq na) = 0$ for all $n$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.7.2]
    Use the definition to conclude that if $\lambda \in [0, 1]$ is rational then $\gamma(\lambda a + (1 - \lambda)b) \geq \lambda \gamma(a) + (1 - \lambda) \gamma(b)$. Use monotonicity to conclude that the last relationship holds for all $\lambda \in [0, 1]$ so $\gamma$ is concave and hence Lipschitz continuous on compact subsets of $\gamma(a) > -\infty$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.7.3]
    Let $X_1, X_2, \dots$ be i.i.d. Poisson with mean 1, and let $S_n = X_1 + \dots + X_n$. Find $\lim_{n \to \infty} (1/n) \log P(S_n \geq na)$ for $a > 1$. The answer and another proof can be found in Exercise 3.1.4.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.7.4]
    Show that for coin flips $\varphi(\theta) \leq \exp(\varphi(\theta) - 1) \leq \exp(\beta\theta^2)$ for $\theta \leq 1$ where $\beta = \sum_{n=1}^{\infty} 1/(2n)! \approx 0.586$, and use (2.7.2) to conclude that $P(S_n \geq an) \leq \exp(-na^2/4\beta)$ for all $a \in [0, 1]$. It is customary to simplify this further by using $\beta \leq \sum_{n=1}^{\infty} 2^{-n} = 1$.
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.7.5]
    Suppose $EX_i = 0$ and $E \exp(\theta X_i) = \infty$ for all $\theta > 0$. Then
    \begin{equation*}
      \frac{1}{n} \log P(S_n \geq na) \to 0 \text{ for all } a > 0
    \end{equation*}
  \end{exercise}
  \begin{solution}

  \end{solution}

  \begin{exercise}[Durrett 2.7.6]
    Suppose $EX_i = 0$. Show that if $\epsilon > 0$ then
    \begin{equation*}
      \liminf_{n \to \infty} P(S_n \geq na)/nP(X_1 \geq n(a + \epsilon)) \geq 1
    \end{equation*}
    Hint: Let $F_n = \{X_i \geq n(a + \epsilon) \text{ for exactly one } i \leq n\}$.
  \end{exercise}
  \begin{solution}

  \end{solution}


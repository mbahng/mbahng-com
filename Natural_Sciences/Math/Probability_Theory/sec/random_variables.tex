\section{Random Variables}

\subsection{Probability Spaces} 

  \begin{definition}[Probability Space]
    A \textbf{probability space} is a measure space $(\Omega, \mathcal{F}, \mathbb{P})$ with $\mathbb{P}(\Omega) = 1$. 
    \begin{enumerate}
      \item $\Omega$ is called the \textbf{sample space} and an element $\omega \in \Omega$ is called an \textbf{outcome}. 
      \item $\mathcal{F}$ is called the \textbf{event space} and an element $A \in \mathcal{F}$ is called an \textbf{event}. 
      \item The measure of an event $\mathbb{P}(A)$ is called the \textbf{probability} of that event. 
    \end{enumerate}
  \end{definition}

  \begin{definition}[Discrete Probability Space]
    If $\Omega$ is a countable set, then we can take its $\sigma$-algebra $\mathcal{F}$ to be the power set of $\Omega$ and construct the measurable space $(\Omega, 2^\Omega, \mathbb{P})$. From the axioms, for any event $A \in \mathcal{F}$, we have
    \begin{equation}
      \mathbb{P} (A) = \sum_{\omega \in A} \mathbb{P}(\{\omega\}) \text{ and } \sum_{\omega \in \Omega} \mathbb{P}(\{\omega\}) = 1
    \end{equation}
    The greatest $\sigma$-algebra $F = 2^{\Omega}$ describes the complete information. The cases $\mathbb{P}(\{\omega\}) = 0$ is permitted by the definition, but rarely used since such $\omega$ can safely be excluded from the sample space. Therefore, we can define the probability measure $\mathbb{P}$ by simply defining it for all singleton sets $\{\omega\}$. 
  \end{definition}

  This may be confusing, since for discrete spaces, it looks like we're assigning probabilities to each $\omega \in \Omega$, but we are actually assigning them to singleton \textit{sets}. We should be writing $\mathbb{P}(\{\omega\})$, but sometimes we abuse notation and write $\mathbb{P}(\omega)$. 

  \begin{example}
    Consider the flip of a fair coin with outcomes either hands or tails. Then, $\Omega = \{H, T\}$. The $\sigma$-algebra $F = 2^{\Omega}$ contains $2^2 = 4$ events: 
    \begin{align*}
      \{\} &= \text{Neither heads nor tails} \\
      \{H\} &= \text{Heads} \\
      \{T\} &= \text{Tails} \\
      \{H, T\} &= \text{Either heads or tails}
    \end{align*}
    That is, $\mathcal{F} = \{\{\}, \{H\}, \{T\}, \{H, T\}\}$. Our probability measure $\mathbb{P}$ is defined
    \begin{equation}
      \mathbb{P}(f) = \begin{cases}
      0 & f = \{\} \\
      0.5 & f = \{H\} \\
      0.5 & f = \{T\} \\
      1 & f = \{H, T\}
      \end{cases}
    \end{equation}
  \end{example}

  \begin{example}[3 Coin Tosses]
    A fair coin is tossed 3 times, creating 8 possible outcomes. 
    \begin{equation}
      \Omega = \{HHH, HHT, HTH, HTT, THH, THT, TTH, TTT\}
    \end{equation}
    The complete information is described by the $\sigma$-algebra $\mathcal{F} = 2^{\Omega} = 2^8 = 256$ events, where each of the events is a subset of $\Omega$. 

    Alice knows the outcome of the second toss only. Thus, her incomplete information is described by the partition 
    \begin{equation}
      \Omega = A_1 \sqcup A_2 = \{HHH, HHT, THH, THT\} \sqcup \{HTH, HTT, TTH, TTT\}
    \end{equation}
    and the corresponding $\sigma$-algebra is 
    \begin{equation}
    \mathcal{F}_{Alice} = \{\emptyset, A_1, A_2, \Sigma\}
    \end{equation}
    Bryan knows only the total number of tails, so his partition contains 4 parts: 
    \begin{align*}
      \Omega & = B_0 \sqcup B_1 \sqcup B_2 \sqcup B_3 \\
        & = \{HHH\} \sqcup \{HHT, HTH, TTH\} \sqcup \{TTH, THT, HTT\} \sqcup \{TTT\}
    \end{align*}
    When we calculate Bryan's event space, we have
    \begin{align*}
      \mathcal{F}_{Bryan} & = \big\{\emptyset, \{HHH\}, \{HHT\}, \{HTH\}, \{THH\}, \{HHT, HTH\}, \{HHT, THH\}, \\
      & \;\;\;\;\{TTH, THT\}, \{TTH\}, \{THT\}, \{HTT\}, \{TTH, THT\},\{TTH, HTT\}, \\
      & \;\;\;\;\{THT, HTT\}, \{TTT\}, \Omega \big\} 
    \end{align*}
    Note that the event space of Bryan (and Alice) is not merely just $2^{\Omega}$ since we have some predetermined knowledge of the outcome space $\Omega$. Therefore, we can partition it into 4 cases and construct the event space by putting only the events that are subsets of each partition. For example, it wouldn't make sense to have an event 
    \begin{equation}
      \{HHH, TTT\}
    \end{equation}
    since the events $\{HHH\}$ and $\{TTT\}$ are in completely different outcome spaces (given the number of tails). That is, if we knew that 3 tails were thrown, the event $\{HHH, TTT\}$ wouldn't make any sense. However, the event $\Omega$ or $\emptyset$ is viable since they describe the case of whether the coin was tossed at all or not. 
    Furthermore, $\mathcal{F}_{Alice}$ and $\mathcal{F}_{Bryan}$ are incomparable. That is, $\mathcal{F}_{Alice} \not\subseteq \mathcal{F}_{Bryan}$ and $\mathcal{F}_{Bryan} \not\subseteq \mathcal{F}_{Alice}$, even though both are subalgebras of $2^{\Omega}$. 
  \end{example}

  \begin{example}[Geometric Measure on $\mathbb{N}$]
    Let $\Omega = \mathbb{N}$ and $\mathcal{F} = 2^\mathbb{N}$. We can completely define the probability measure by assigning them to singletons $k \in \mathbb{N}$. One such assignment is 
    \begin{equation}
      \mathbb{P}(\{k\}) = \frac{1}{2^k}
    \end{equation}
    or more generally, 
    \begin{equation}
      \mathbb{P}(\{k\}) = p (1 - p)^{k-1}
    \end{equation}
  \end{example}

  \begin{example}[Poisson Measure on $\mathbb{N}_0$]
    Let $\Omega = \mathbb{N} \cup \{0\}$. Then, $\mathcal{F} = 2^\Omega$ and we can define $\mathbb{P}$ on the singleton sets as 
    \begin{equation}
      \mathbb{P}(\{k\}) = \frac{e^{-\lambda} \lambda^k}{k!}
    \end{equation}
    for any $\lambda > 0$. We can then compute the probability of, say all primes, by taking 
    \begin{equation}
      \mathbb{P}(\text{primes}) = \sum_{k \text{ prime}} \mathbb{P}(\{k\})
    \end{equation}
    which we know to be monotonically increasing and bounded above, so it must converge. Whether this has a closed form solution is another matter. Again, in reality we are assigning probability measures on all $\mathcal{F}$-measurable sets, but just doing it through assignment of measure through singleton sets. 
  \end{example}
  
  \begin{example}[Voters]
    If $100$ voters are to be drawn randomly from among all voters in California and asked whom they will vote for governor, then the set of all sequences of $100$ Californian voters would be the sample space $\Omega$. We assume that sampling without replacement is used: only sequences of $100$ different voters are allowed. For simplicity an ordered sample is considered, that is a sequence $\{Alice, Bryan\}$ is different from $\{Bryan, Alice\}$. We also take for granted that each potential voter knows exactly his/her future choice, that is he/she doesn’t choose randomly.

    Alice knows only whether or not Arnold Schwarzenegger has received at least $60$ votes. Her incomplete information is described by the $\sigma$-algebra $\mathcal{F}_{Alice}$ that contains:
    \begin{enumerate}
      \item the set of all sequences in $\Omega$ where at least $60$ people vote for Schwarzenegger
      \item the set of all sequences where fewer than $60$ vote for Schwarzenegger
      \item the whole sample space $\Omega$
      \item the empty set $\emptyset$
    \end{enumerate}
    Bryan knows the exact number of voters who are going to vote for Schwarzenegger. His incomplete information is described by the corresponding partition $\Omega = B_0 \sqcup B_1 \ldots B_{100}$ and the $\sigma$-algebra $\mathcal{F}_{Bryan}$ consists of $2^{101}$ events. 

    In this case Alice’s $\sigma$-algebra is a subset of Bryan’s: $\mathcal{F}_{Alice} \subset \mathcal{F}_{Bryan}$. Bryan’s $\sigma$-algebra is in turn a subset of the much larger "complete information" $\sigma$-algebra $2^{\Omega}$ consisting of $2^{n(n-1)\ldots (n-99)}$ events, where $n$ is the number of all potential voters in California. 
  \end{example}

  Now if we move to uncountable outcome spaces, then things are not as nice, which is why we need to machinery of measure theory to study them. Let us try to model a probability measure on $\Omega = [0, 1]$. It is uncountable, and it turns out that $2^\Omega$ has cardinality strictly greater than even the continuum. If we try to model a uniform probability measure $\mathbb{P}$, then for some subset $A \in 2^\Omega$, it should be the case that $\mathbb{P}(A) = \mathbb{P}(A \oplus k)$, where $A \oplus k$ is just some translated version of $A$ still contained within $[0, 1]$. This applies to singleton sets, and it turns out that if we try to assign a nonzero probability measure to any singleton $\{k\}$, then the probability measure of $\Omega$ blows up to infinity, which we can't have. So the only thing we can do is have every singleton have zero probability. Remember that a measure by definition has the \textit{countable additivity} property, which says that 
  \begin{equation}
    \mu \bigg( \bigsqcup_{k=1}^\infty A_k \bigg) = \sum_{k=1}^\infty \mu(A_k)
  \end{equation}
  for all \textit{countable} collections $\{A_k\}$. Summation is not defined for uncountable collections, and so having a probability $0$ on every singleton does not imply that the probability of any uncountable set has is $0$. That is, having $\mathbb{P}(\{k\}) = 0$ for all $k \in [0, 1]$ does not tell you what $\mathbb{P}([0, 1])$ is. So now rather than assigning probabilities to singletons, like we did with discrete sets, the approach is to assign probabilities directly to our event space $\mathcal{F}$. We can do this by directly assigning the Lebesgue measure to the Borel algebra of $[0, 1]$, which has the properties 
  \begin{enumerate}
    \item $\mathbb{P}((a, b)) = \mathbb{P}([a, b)) = \mathbb{P}((a, b]) = \mathbb{P}([a, b]) = b - a$
    \item Translation invariance as stated above. 
  \end{enumerate}
  Over uncountable $\Omega$, we cannot afford to work with $2^\Omega$, since there is an impossibility theorem that says that there is no measure defined on $2^{[0, 1]}$ with the two properties above. Therefore, we must work with a smaller $\sigma$-algebra. Since the subsets of interest are usually intervals (or more generally, open sets), people usually take the Borel $\sigma$-algebra of open intervals on $[0, 1]$. The Lebesgue measure on $\mathbb{R}$ is not a probability measure since it $\lambda(\mathbb{R}) = \infty$, but we can construct a uniform probability measure on any bounded set of $\mathbb{R}$. Usually, these continuous probability spaces are $\mathbb{R}^n$, and we define some measure $\mu$ directly on its $\sigma$-algebra. 

  \begin{definition}[Atom]
    Let $(\Omega, \mathcal{F}, \mathbb{P})$ be uncountable. If for some $\omega \in \Omega$, $\mathbb{P}(\{\omega\}) \neq 0$, then $\omega$ is called an \textbf{atom}. 
  \end{definition}

  Now, given a general (discrete or continuous, or a combination of both) distribution, the set of all the atoms are an at most countable (maybe empty) set whose probability is the sum of probabilities of all atoms (by countable additivity). That is, given $\omega_1, \omega_2, \ldots$ atoms, 
  \begin{equation}
    \mathbb{P} \bigg( \bigsqcup_{i=1}^\infty \{\omega_i\} \bigg) = \sum_{i=1}^\infty \mathbb{P}(\{\omega_i\})
  \end{equation}
  \begin{enumerate}
    \item If this sum is equal to $1$ then all other points can be safely excluded from the sample space $\Omega$, returning us to the discrete case. 
    \item If this sum is $0$ then we just have some continuous sample space. This means $\mathbb{P}(\{\omega\}) = 0$ for all $\omega \in \Omega$, and so $\Omega$ must be uncountable (since if it was countable, then we should be able to sum the $\mathbb{P}(\{\omega\})$'s to get $1$, but it's $0$). Remember that summation is only defined for at most countable elements. 
    \item If the sum of probabilities of all atoms is strictly between $0$ and $1$, then the probability space decomposes into a discrete, atomic part and a non-atomic, continuous part. 
  \end{enumerate}

  We can think of the sample space $\Omega$ as the set of all conceivable futures and an event $F \in \mathcal{F}$ as some subset of conceivable futures. The probability $\mathbb{P}(F)$ represents our degree of certainty that our future will be contained in such an event. If some measure space $X$ has a finite total measure, we can construct a probability space from it by normalizing the measure. 

  \begin{theorem}[Inclusion Exclusion Principle]
    One familiar property commonly seen in probability and combinatorics is the inclusion exclusion principle. If $A, B \in \mathcal{A}$, 
    \begin{equation}
      \mu(A \cup B) = \mu(A) + \mu(B) - \mu(A \cap B)
    \end{equation}
    and by induction, if $A_1, \ldots, A_n \in \mathcal{F}$, then 
    \begin{equation}
      \mu\bigg( \bigcup_{i=1}^n A_i \bigg) = \sum_{i=1}^n \mu(A_i) - \sum_{i < j} \mu(A_i \cap A_j) + \sum_{i < j < k} \mu(A_i \cap A_j \cap A_k) + \ldots + (-1)^{n-1} \mu\bigg( \bigcap_{i=1}^n \mu(A_i) \bigg)
    \end{equation}
  \end{theorem}

  Let us focus on the $\sigma$-algebra $\mathcal{F}$. We can see that the $\sigma$-algebra \textit{models our knowledge of the experiment}. That is, given some outcome space $\Omega$, let us have two $\sigma$-algebras $\mathcal{F}$ and $\mathcal{G}$ such that $\mathcal{F} \subset \mathcal{G}$, i.e. $\mathcal{F}$ is a sub-$\sigma$-algebra of $\mathcal{G}$. What does this mean? Remember that the elements of the event space are the events that can be measured. If $\mathcal{G}$ is \textit{finer} than $\mathcal{F}$, then every set $F$ that is $\mathcal{F}$-measurable is also $\mathcal{G}$-measurable, and so someone who has knowledge of $\mu$ over $\mathcal{G}$ knows more than another who has knowledge of $\mu$ over $\mathcal{F}$. 

  For example, let us have a dice roll, with $\Omega = \{1, 2, 3, 4, 5, 6\}$.
  \begin{enumerate}
    \item Abby's knowledge is modeled by $\mathcal{F} = \{\emptyset, \{1, 2, 3\}, \{4, 5, 6\}, \Omega\}$, with 
    \begin{equation}
      \mathbb{P}(F) = \begin{cases} 
        0 & \text{ if } F = \emptyset \\
        1/2 & \text{ if } F = \{1, 2, 3\}, \{4, 5, 6\} \\ 
        1 & \text{ if } F = \Omega \end{cases}
    \end{equation}
    \item Bob's knowledge is modeled by $\mathcal{G} = 2^\Omega$ with the following values 
    \begin{equation}
      \mathbb{P}(\{1\}) = \mathbb{P}(\{2\}) = \mathbb{P}(\{3\}) = \mathbb{P}(\{4\}) = \mathbb{P}(\{5\}) = \mathbb{P}(\{6\}) = \frac{1}{6}
    \end{equation}
  \end{enumerate}
  We can see that $\mathcal{F} \subset \mathcal{G}$ and that Bob has more information than Abby since from the values of $\mathbb{P}$ over his $\sigma$-algebra, he can deduce that $\mathbb{P}(\{1, 2, 3\}) = \mathbb{P}(\{1\}) + \mathbb{P}(\{2\}) = \mathbb{P}(\{3\}) = 1/2$ (and likewise for $4, 5, 6$). All Abby knows is that the probability that the roll is $1, 2, 3$ is $1/2$, but in her view, the individual probabilities may not be uniformly $1/6$ at all (it could be $\mathbb{P}(\{1\}) = \mathbb{P}(\{2\}) = 0$ and $\mathbb{P}(\{3\}) = 1/2$, for example). More specifically, Bob has \textit{complete} information of the experiment since $\mathcal{G} = 2^\Omega$, so he knows the probability of every possible event. But no matter how little information one has about the experiment, \textit{everybody} will always know that the probability that \textit{any} outcome will happen is $1$ (hence $\mathbb{P}(\Omega) = 1$) and the probability that no outcome will happen is $0$ ($\mathbb{P}(\emptyset) = 0$), which is consistent with the definition of $\sigma$-algebras requiring to have $\emptyset$ and $\Omega$. Note that we can have two $\sigma$-algebras s.t. both model incomplete information and aren't strictly finer than one another, i.e. $\mathcal{F} \not\subset \mathcal{G}$ and $\mathcal{G} \not\subset \mathcal{F}$. 

  Note that given the same random experiment, we don't need to always have the same sample space or the same random variable. For example, let's have a coin toss. One could be interested in whether it lands heads or tails, which means $\Omega = \{0, 1\}$, but another could be interested in the number of times the coin flips midair, in which $\Omega = \mathbb{N}_0$. We could even be interested in the set of all trajectories of the coin, which would result in a huge space of all trajectories of the flip, or the velocity at which it lands on the table, which would lead to $\Omega = \mathbb{R}^+$. 

  Here is a definition which will be useful shortly when talking about how $\sigma$-algebras model knowledge. 

  \begin{definition}[Sub-$\sigma$-Algebras]
    Given a $\sigma$-algebra $\mathcal{F}$, a \textbf{sub-$\boldsymbol{\sigma}$-algebra} of $\mathcal{F}$ is a $\sigma$-algebra $\mathcal{G}$ s.t. $\mathcal{G} \subset \mathcal{F}$. 
  \end{definition}

  This will allows us to compare $\sigma$-algebras by taking two $\sigma$-algebras $\mathcal{G}, \mathcal{H} \subset \mathcal{F}$, which $\mu$ is guaranteed to be defined on since it is defined over $\mathcal{F}$. 

  Note that as you get more and more information, your $\sigma$-algebra can "grow" and get closer to something that models complete information. This means that given some $\sigma$-algebra $\mathcal{F}$ that models complete information, we can take a sequence of nondecreasing sub-$\sigma$-algebras of $\mathcal{F}$ 
  \begin{equation}
    \mathcal{F}_1 \subset \mathcal{F}_2 \subset \ldots \subset \mathcal{F}_i \subset \ldots
  \end{equation}
  such that $\mathcal{F}_i \subset \mathcal{F}$, which models our increasing knowledge of the experiment. 

  \begin{definition}[Filtration]
    Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space and $I$ be an index set with total order $\geq$ (usually, $\mathbb{N}, \mathbb{R}$). For every $i \in I$, let $\mathcal{F}_i$ be a sub-$\sigma$-algebra of $\mathcal{F}$ satisfying 
    \begin{equation}
      \mathcal{F}_i \subset \mathcal{F}_j \text{ if } i \geq j
    \end{equation}
    Note that we do not write it as a sequence like before since $I$ may be uncountable. Then, a \textbf{filtration} $\mathbb{F} = \{\mathcal{F}_i\}_{i \in I}$ is a family of $\sigma$-algebras that are ordered nondecreasingly. If $\mathcal{F}$ is a filtration, then $(\Omega, \mathcal{F}, \mathbb{F}, \mathbb{P})$ is called a \textbf{filtered probability space}. 
  \end{definition}

  \begin{example}[Filtration of 3 Coin Tosses]
    Let us describe a concrete example of a 3-coin toss filtration. The probability space is 
    \begin{equation}
      \Omega = \{000, 001, 010, 011, 100, 101, 110, 111\}
    \end{equation}
    which has $8$ outcomes so a complete $\sigma$-algebra would consist of $2^8 = 256$ outcomes. 
    \begin{enumerate}
      \item Before the experiment, we have no information at all, so 
      \begin{equation}
        \mathcal{F}_0 = \{\emptyset, \Omega\}
      \end{equation}
      which has $2^{2^0} = 2$ elements. 
      
      \item After the first coin toss, we would have information on what the first flip landed on (whether it was of form $( 0, \ast, \ast)$ or $(1, \ast, \ast)$), so we have a $\sigma$-algebra generated by these two events 
      \begin{align*}
        \mathcal{F}_1 & = \sigma(\{( 0, \ast, \ast)\}, \{(1, \ast, \ast)\}) \\
        & = \sigma(\{000, 001, 010, 011\}, \{100, 101, 110, 111\}) \\
        & = \{\emptyset, \{000, 001, 010, 011\}, \{100, 101, 110, 111\}, \Omega\}
      \end{align*}
      which has $2^{2^1} = 4$ elements. 
      
      \item After the second coin toss, we would have information on what the first two flips landed on (whether it was of form $( 0, 0, \ast)$, $( 0, 1, \ast)$, $(1, 0, \ast)$, $(1, 1, \ast)$), so we have a $\sigma$-algebra generated by these 4 events 
      \begin{align}
        \mathcal{F}_2 & = \sigma(\{( 0, 0, \ast)\}, \{( 0, 1, \ast)\}, \{(1, 0, \ast)\}, \{(1, 1, \ast)\}) \\
        & = \sigma(\{000, 001\}, \{010, 011\}, \{100, 101\}, \{110, 111\}) \\
        & = \{\emptyset, \{000, 001\}, \{010, 011\}, \{100, 101\}, \{110, 111\}, \\
        & \;\;\;\;\;\; \{000, 001, 010, 011\}, \{000, 001, 100, 101\}, \{000, 001, 110, 111\}, \\
        & \;\;\;\;\;\; \{010, 011, 100, 101\}, \{010, 011, 110, 111\}, \{100, 101, 110, 111\}, \\ 
        & \;\;\;\;\;\; \{000, 001, 010, 011, 100, 101\}, \{000, 001, 010, 011, 110, 111\}, \\
        & \;\;\;\;\;\; \{000, 001, 110, 101, 110, 111\}, \{010, 011, 110, 101, 110, 111\},  \Omega\}
      \end{align}
      which has $2^{2^2} = 16$ elements. 
      
      \item After the third coin toss, we would have information on what the first three flips landed on (all 8 possibilities in $\Omega$), so we have a $\sigma$-algebra generated by these 8 events 
      \begin{align}
        \mathcal{F}_3 & = \sigma(\{000\}, \{001\}, \{010\}, \{011\}, \{100\}, \{101\}, \{110\}, \{111\})
      \end{align}
      This is too big to write down explicitly, but it has $2^{2^3} = 256$ elements. 
    \end{enumerate}
  \end{example}

\subsection{Conditional Probabilities}

  \begin{definition}[Conditional Probability w.r.t. Events]
    Given a measure space $(\Omega, \mathcal{F}, \mathbb{P})$, let $B$ be an event such that $\mathbb{P}(B) > 0$. The \textbf{conditional probability} of $A$ given $B$ is defined 
    \begin{equation}
      \mathbb{P}(A \mid B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)}
    \end{equation}
  \end{definition}

  Note that we can't condition on events that have probability $0$, which is why we need the $\mathbb{P}(B) > 0$ condition. If this is the case, it doesn't even make sense to talk about a conditional probability $\mathbb{P}(A \mid B)$. For example, if we take the probability space $[0, 1]$ with its Borel algebra and the Lebesgue measure, then we cannot condition something on the rationals, e.g. $\mathbb{P}(\{\omega < 0.5\} \mid \omega \in \mathbb{Q})$ does not make sense. In fact, doing so can lead to contradictions, one being the \textbf{Borel-Kolmogorov paradox}. 

  An extremely useful theorem is that the conditional probability taken as a measure gives us a new viable measure on the same probability space $\Omega$. 

  \begin{theorem}
    Given probability space $(\Omega, \mathcal{F}, \mathbb{P})$, let $B \in \mathcal{F}$ with $\mathbb{P}(B) > 0$. Then, $\mathbb{P}( \cdot \mid B): \mathcal{F} \longrightarrow [0, 1]$ is a probability measure on $(\Omega, \mathcal{F})$. 
  \end{theorem}
  \begin{proof}
    We prove the properties of a probability measure. 
    \begin{enumerate}
      \item The empty set has measure $0$. 
      \begin{equation}
        \mathbb{P}(\emptyset \mid B) = \frac{\mathbb{P}( \emptyset \cap B)}{\mathbb{P}(B)} = \frac{\mathbb{P}(\emptyset)}{\mathbb{P}(B)} = \frac{0}{\mathbb{P}(B)} = 0
      \end{equation}
      \item The entire space has measure $1$. 
      \begin{equation}
        \mathbb{P}(\Omega \mid B) =  \frac{\mathbb{P}( \Omega \cap B)}{\mathbb{P}(B)} = \frac{\mathbb{P}(B)}{\mathbb{P}(B)} = 1
      \end{equation}
      \item Countable additivity of disjoint events. Let $A_i \in \mathcal{F}$ for $i = 1, 2, \ldots$ which are disjoint. Then, their union is in $\mathcal{F}$ by definition of $\sigma$-algebra. Now, 
      \begin{align*}
        \mathbb{P}\bigg( \bigcup_{i=1}^\infty A_i \bigg| B \bigg) & = \frac{1}{\mathbb{P}(B)} \mathbb{P} \bigg[ \Big( \bigcup_{i=1}^\infty A_i \Big) \cap B \bigg] \\
        & = \frac{1}{\mathbb{P}(B)} \mathbb{P} \bigg[ \bigcup_{i=1}^\infty (A_i \cap B) \bigg] \\
        & = \frac{1}{\mathbb{P}(B)} \sum_{i=1}^\infty \mathbb{P} (A_i \cap B) \\
        & = \sum_{i=1}^\infty \frac{\mathbb{P} (A_i \cap B)}{\mathbb{P}(B)} = \sum_{i=1}^\infty \mathbb{P}(A_i \mid B) 
      \end{align*}
    \end{enumerate}
  \end{proof}

  \begin{lemma}[Law of Total Probability]
    Suppose $A_1, A_2, ..., A_n$ is a partition of $\Omega$. Then, 
    \begin{equation}
      \{B \cap A_k\}_{k=1}^n
    \end{equation}
    is a partition of $B$, and 
    \begin{equation}
      \mathbb{P}(B) = \sum_{k=1}^n \mathbb{P} (B|A_k)\, \mathbb{P}(A_k)
    \end{equation}
    This is also called the \textit{Partition rule}. 
  \end{lemma}

  \begin{theorem}[Bayes Rule]
    Let $A, B \in \mathcal{F}$. Then, 
    \begin{equation}
      \mathbb{P}(B \mid A) = \frac{\mathbb{P}(A \mid B) \, \mathbb{P}(B)}{\mathbb{P}(A)}
    \end{equation}
  \end{theorem}
  \begin{proof}
    We know that 
    \begin{equation}
      \mathbb{P}(A \mid B) = \frac{\mathbb{P} (A \cap B)}{\mathbb{P}(B)} \text{ and } \mathbb{P}(B \mid A) = \frac{\mathbb{P}(B \cap A)}{\mathbb{P}(B)}
    \end{equation}
    and so we can write 
    \begin{equation}
      \mathbb{P} (A \mid B) \, \mathbb{P}(B) = \mathbb{P}(A \cap B) = \mathbb{P}(B \mid A) \, \mathbb{P}(A)
    \end{equation}
  \end{proof}

\subsection{Random Variables and Vectors} 

  Random variables are motivated by the following. When you have a random experiment, the experimenter may not be interested in the specific elementary outcomes. So if you have sample space $\Omega$, you may not be concerned about what $\omega \in \Omega$ shows up, but more interested in some numerical function of the elementary outcome. For example, if you toss a coin 10 times, you're not interested in what sequence in $\{0, 1\}^{10}$ shows up, but you may want to just know how many heads came up. In other words, your interest defines a numerical function $X: \Omega \rightarrow \mathbb{R}$. This is useful, since in many cases the sample space $\Omega$ can be extremely complicated (e.g. the sample space of all weather conditions) and the elementary outcomes also complicated, so you may want to know some simpler aspect (e.g. the temperature). 

  The name ``random variable'' is very misleading. It's not random nor a variable. It is a deterministic function $X: (\Omega, \mathcal{F}, \mathbb{P}) \longrightarrow \mathbb{R}$ that assigns numbers to outcomes. The only source of randomness itself is which $\omega \in \Omega$ is chosen. But we can't just choose any function on $\Omega$; they must satisfy the nice property of measurability. Now, to talk about random variables, recall that the definition of a measurable function $f: (X, \mathcal{A}) \longrightarrow \mathbb{R}$ is one where the preimage of every Borel set $B \in \mathcal{R}$ is in $\mathcal{A}$. With a potential measure $\mu$, this allows us to define the Lebesgue integral of $f$. Note that this is also equivalent to the more easily provable fact that the preimage of every half-interval $(-\infty, t)$ is in $\mathcal{A}$. That is, $f^{-1}((-\infty, t]) \in \mathcal{A}$ for all $t \in \mathbb{R}$. 

  \begin{definition}[Random Variable][def:random-variable]
    A \textbf{random variable} $X$ on probability space $(\Omega, \mathcal{F}, \mathbb{P})$ is an $\mathcal{F}$-measurable function $X: (\Omega, \mathcal{F}, \mathbb{P}) \longrightarrow \mathbb{R}$. That is, for every subset $B \in \mathcal{R}$, its preimage 
    \begin{equation}
      X^{-1} (B) = \{\omega \in \Omega \mid X(\omega) \in B\} \in \mathcal{F}
    \end{equation}
  \end{definition}

  The reason we want $X$ to be $\mathcal{F}$-measurable is because now we can define probabilities on Borel sets $B$ of $\mathbb{R}$ by computing the probabilities of the preimage of $B$, which must be $\mathcal{F}$-measurable. In a way, a random variable ``pushes forward'' the probability measure $\mathbb{P}$, originally defined on $\mathcal{F}$, to $\mathcal{R}$.  

  \begin{definition}[Probability Law of Random Variable $X$]
    Let $X$ be a random variable on probability space $(\Omega, \mathcal{F}, \mathbb{P})$. The \textbf{probability law of $X$} is a function $\mathbb{P}_X : \mathcal{R} \longrightarrow [0, 1]$ defined, for each Borel set $B$ of $\mathbb{R}$, as 
    \begin{equation}
      \mathbb{P}_X (B) \coloneqq \mathbb{P} \big( X^{-1}(B) \big) = \mathbb{P} \big( \{\omega \in \Omega \mid X(\omega) \in B\} \big)
    \end{equation}
    Note that $\mathbb{P}$ refers to the probability measure on $\mathcal{F}$, and $\mathbb{P}_X$ refers to the probability law on $\mathcal{R}$. In shorthand, we can write $\mathbb{P}_X = \mathbb{P} \circ X^{-1}$. By abuse of notation, it is generally written
    \begin{equation}
      \mathbb{P}(X \in B)
    \end{equation}
    It is important to get used to this notation. Whenever we write $\mathbb{P}(X \ldots)$, we are always working in the probability law of $X$. Furthermore, whatever condition we put within the parentheses describes a measurable set. For example, 
    \begin{enumerate}
      \item $\mathbb{P}(X = x)$ describes the probability law of $X$ evaluated on the set $\{x\}$. 
      \item $\mathbb{P}(X \leq x)$ describes the probability law of $X$ evaluated on the set $(-\infty, x]$. 
      \item $\mathbb{P}(Y \leq y)$ describes the probability law of $Y$ evaluated on the set $(-\infty, y]$.
      \item $\mathbb{P}(a \leq Y < b)$ describes the probability law of $Y$ evaluated on the set $[a, b)$. 
      \item $\mathbb{P}(Z \in \mathbb{Q})$ describes the probability law of $Z$ evaluated on the set $\mathbb{Q}$. 
    \end{enumerate}
  \end{definition}

  \begin{theorem}[$\sigma$-Algebra Generated by Random Variable $X$]
    Given probability space $(\Omega, \mathcal{F}, \mathbb{P})$ and a random variable $X$, $\mathbb{P}_X$ is a probability measure on $(\mathbb{R}, \mathcal{R})$. Now, it turns out that the collection of all preimages of Borel sets under $X$ forms a $\sigma$-algebra on $\Omega$. We call it 
    \begin{equation}
      \sigma(X) \coloneqq \big\{ A \subset \Omega \mid A = X^{-1}(B) \text{ for some } B \in \mathcal{R} \big\}
    \end{equation}
    which is a $\sigma$-algebra of $\Omega$. Since $X$ is a measurable function, every $X^{-1}(B)$ is $\mathcal{F}$-measurable, and so $\sigma(X)$ is a sub-$\sigma$-algebra of $\mathcal{F}$. It is never the case that $\sigma(X) \supset \mathcal{F}$, since that means that $X$ itself is not $\mathcal{F}$-measurable. 
  \end{theorem}

  \begin{theorem}[Pushforward Probability Measure][thm:pushforward]
    Given $(\Omega, \mathcal{F}, \mathbb{P})$ and a random variable $X: \Omega \rightarrow \mathbb{R}$, let us define a probability law $\mathbb{P}_X = \mathbb{P} \circ X^{-1}$. Then, 
    \begin{equation}
      (\mathbb{R}, \mathcal{R}, \mathbb{P}_X)
    \end{equation}
    is a probability space. 
  \end{theorem}

  This theorem is extremely useful, since in practical applications, one does not consider an abstract $\Omega$ and works immediately in $\mathbb{R}$. Once we have determined our numerical values of interest (heads or tails, number of heads, sum of dice rolls) with our random variable $X$, we can just throw away $(\Omega, \mathcal{F}, \mathbb{P})$ and work directly in probability space $(\mathbb{R}, \mathcal{R}, \mathbb{P}_X)$. Therefore, we don't actually control $\Omega$ by explicitly defining it as we said before. 

  We could just leave $\Omega$ to be some arbitrary large set, and construct an appropriate random variable $X$ that will generate an appropriate $\sigma$-algebra $\sigma(X)$ that captures the information of the experiment. This allows us to "simplify" the $\sigma$-algebra $\mathcal{F}$ to the scope of the random variable. That is, let $\Omega$ be the sample space of all trajectories of a coin flip before it comes to rest. If we are just looking at whether it is heads or tails, we can define $X$ to have image $\{0, 1\}$. Then, $\sigma(X)$ will be a sub-$\sigma$-algebra of $\mathcal{F}$ that looks at only the four subsets $\emptyset, \Omega$, the set of all trajectories landing heads, and the set of all trajectories landing tails. This simplifies $\mathcal{F}$ to a scope that we are interested in. 

  Let us review once more on the hierarchy of random variables. We usually classify random variables $X$ by the smallest $\sigma$-algebra that they generate, which is $\sigma(X)$. That is, not only is $X$ $\sigma(X)$-measurable, but for all $\sigma$-algebras $\mathcal{G}$ s.t. $\sigma(X) \subset \mathcal{G} \subset \mathcal{F}$, $X$ is also $\mathcal{G}$-measurable. Remember, since this is the case, the only relevant measure on these random variables is how coarse/small $\sigma(X)$ is. 
  \begin{enumerate}
    \item The finest random variable has $\sigma(X) = \mathcal{F}$. 
    
    \item The coarsest random variable is a constant random variable, which has $\sigma(X)$ to be the trivial $\sigma$-algebra $\mathcal{H} = \{\emptyset, \Omega\}$. Note that a constant random variable is still $\mathcal{F}$-measurable. 
    
    \item Every other random variable $X$ has $\mathcal{H} \subset \sigma(X) \subset \mathcal{F}$. 
  \end{enumerate}

  Now it turns out that the space of $\mathcal{F}$-measurable functions $X: \Omega \rightarrow \mathbb{R}$ forms a function space, which means that the set of all random variables on $\Omega$ forms a vector space. We formally show it here. 

  \begin{lemma}
    The set of all $\mathcal{F}$-measurable functions $X: (\Omega, \mathcal{F}) \rightarrow \mathbb{R}$ forms a vector space, denoted $L_\mathcal{F} (\Omega; \mathbb{R})$, or $L_\mathcal{F} (\Omega)$ for short. 
  \end{lemma}
  \begin{proof}

  \end{proof}

  Naturally, we can put the $L^p$-norm on this space, defined 
  \begin{equation}
    ||X||_p \coloneqq \bigg( \int_\Omega |X|^p \, d\mathbb{P} \bigg)^{1/p}
  \end{equation}
  Moreover, if $p = 2$, then we can put an inner product defined 
  \begin{equation}
    \langle X, Y \rangle = \bigg( \int_\Omega X Y \,d \mathbb{P} \bigg)^{1/2}
  \end{equation}

  \begin{definition}
    The Banach space of $\mathcal{F}$-measurable functions is denoted $L_\mathcal{F}^p (\Omega)$, and the Hilbert space is denoted $L_\mathcal{F}^2 (\Omega)$. 
  \end{definition}

  This means that if we have some probability space $(\Omega, \mathcal{F}, \mathbb{P})$ and  sub-$\sigma$-algebra $\mathcal{G} \subset \mathcal{F}$, then any $\mathcal{G}$-measurable function is also $\mathcal{F}$-measurable, since if the preimage of every $B \in \mathcal{R}$ is in $\mathcal{G}$, then it $B \in \mathcal{F}$. This immediately results in the following. 

  \begin{theorem}
    If $\mathcal{G}$ is a sub-$\sigma$-algebra of $\mathcal{F}$, then $L_\mathcal{G} (\Omega)$ is a subspace of $L_\mathcal{F} (\Omega)$. 
  \end{theorem}

  This means that as we get coarser and coarser random variables, the space in which these random variables live in get smaller and smaller, until we get to the constant random variables, which form a $1$-dimensional line in $L_\mathcal{F} (\Omega)$. The origin is simply the constant $0$ random variable. 

  Now when we consider several random variables, they will all be defined on the same probability space. Given two random variables $X$ and $Y$ on $(\Omega, \mathcal{F}, \mathbb{P})$, they will each induce a probability law $\mathbb{P}_X$ and $\mathbb{P}_Y$ which completely characterizes them. Note that it is the same underlying randomness that is feeding these random variables, and so if I know some information about the value of $X$, then we know something about outcome $\omega$, which can be used to find something about the value of $Y$. To capture this, we can imagine the map $(X, Y) : \Omega \longrightarrow \mathbb{R}^2$ defined $(X, Y)(\omega) \coloneqq (X(\omega), Y(\omega))$. And just like how $X$ induces a measure $P_X$ onto $\mathbb{R}$, we can imagine $(X, Y)$ inducing a measure onto $\mathcal{B}(\mathbb{R}^2)$, which can be generated by all semi-infinite rectangles $(-\infty, x] \times (-\infty, y]$. Ideally, we would want to put a measure $\mathbb{P}_{X, Y}$ on $\mathbb{R}^2$ s.t. 
  \begin{equation}
    \mathbb{P}_{X, Y}(B) \coloneqq \mathbb{P}((X, Y)^{-1}(B))
  \end{equation}
  where $(x, y)^{-1}(b) = \{ \omega \in \omega \mid (x(\omega), y(\omega)) \in b\}$ denotes the preimage of $(x, y)$. but is $(x, y)^{-1}(b)$ $\mathcal{f}$-measurable? it turns out that it is. 

  \begin{theorem}
    Let $f: (X, \mathcal{A}, \mu) \longrightarrow \mathbb{R}^n$ have component functions $f_1, f_2, \ldots, f_n$. Then, $f$ is measurable (i.e. $f^{-1} (B) \in \mathcal{A}$ for all $B \in \mathcal{B}(\mathbb{R}^n)$) if and only if all of its component functions are measurable (i.e. $f_i^{-1} (B) \in \mathcal{A}$ for all $B \in \mathcal{B}(\mathbb{R}^n)$). 
  \end{theorem}

  From the theorem above, I have a probability law $\mathbb{P}_{X, Y}$ on all Borel sets of $\mathbb{R}^2$, making $(\mathbb{R}^2, \mathcal{B}(\mathbb{R}^2), \mathbb{P}_{X, Y})$ a probability space. Now, since $X$ and $Y$ are both random variables dependent on the same $\omega \in \Omega$, we could expect certain "combinations" of $X$ and $Y$ to be more probable than other combinations. 

  \begin{definition}[Joint Probability Law]
    Given two random variables $X, Y$ on $(\Omega, \mathcal{F}, \mathbb{P})$, the \textbf{joint random variable} $(X, Y): \Omega \longrightarrow \mathbb{R}^2$ is a measurable function defined 
    \begin{equation}
      (X, Y) (\omega) \coloneqq (X(\omega), Y(\omega))
    \end{equation}
    which induces a \textbf{joint probability law} $\mathbb{P}_{X, Y}: \mathcal{B}(\mathbb{R}^2) \longrightarrow [0, 1]$ defined 
    \begin{equation}
      \mathbb{P}_{X, Y}(B) \coloneqq \mathbb{P}((X, Y)^{-1}(B)) \; \forall B \in \mathcal{R}
    \end{equation}
    of $X, Y$. This law captures everything there is about the interdependence of $X$ and $Y$. 
  \end{definition}

  Given joint probability law $\mathbb{P}_{X, Y}$, we can get the probability laws of $X$ and $Y$ separately. For example, we can take a specific Borel set of $\mathbb{R}$ representing the outcomes of $X$ and look at every single combination of it with every $Y$. But knowing $\mathbb{P}_X$ and $\mathbb{P}_Y$ is not enough to know the joint $\mathbb{P}_{X, Y}$. 

  \begin{definition}[Marginal Probability Law]
    Given a joint probability law $\mathbb{P}_{X, Y}$ of $X, Y$, we can get the \textbf{marginal probability law} of $X$ by feeding in Borel sets of form $B \times \mathbb{R} \in \mathcal{B}(\mathbb{R}^2)$. 
    \begin{equation}
      \mathbb{P}_X (B) = \mathbb{P}_{X, Y} (B \times \mathbb{R})
    \end{equation}
    and the marginal probability law of $Y$ as 
    \begin{equation}
      \mathbb{P}_Y (B) = \mathbb{P}_{X, Y} (\mathbb{R} \times B)
    \end{equation}
  \end{definition}

  \begin{definition}[Joint Cumulative Distribution Function]
    Since sets of the form $(-\infty, x] \times (-\infty, y]$ are Borel in $\mathbb{R}^2$, the \textbf{joint cumulative distribution function} 
    \begin{align*}
      F_{X, Y} & \coloneqq \mathbb{P}_{X, Y} \big( (-\infty, x] \times (-\infty, y] \big) \\
      & = \mathbb{P} \big( \{\omega \mid X(\omega) \leq x\} \cap \{ \omega \mid Y(\omega) \leq y\} \big)
    \end{align*}
    is well-defined. By abuse of notation, we will write $F_{X, Y} (x, y) = \mathbb{P}(X \leq x, Y \leq y)$. The marginal CDFs are defined 
    \begin{align*}
      F_X (x) & \coloneqq \mathbb{P}_{X, Y} ((-\infty, x) \times \mathbb{R}) \\
      F_Y (y) & \coloneqq \mathbb{P}_{X, Y} (\mathbb{R} \times (-\infty, y))
    \end{align*}
  \end{definition}

  \begin{lemma}[Properties of Joint CDF]
    Some common properties of the joint CDF are as follows: 
    \begin{enumerate}
      \item Limits. 
      \begin{equation}
        \lim_{(x, y) \rightarrow (+\infty, +\infty)} F_{X, Y} (x, y) = 1 \text{ and } \lim_{(x, y) \rightarrow (-\infty, -\infty)} F_{X, Y} (x, y) = 0
      \end{equation}
      \item Monotonicity. 
      \begin{equation}
        x_1 \leq x_2, \; y_1 \leq y_2 \implies F_{X, Y} (x_1, y_1) \leq F_{X, Y}(x_2, y_2)
      \end{equation}
      \item Continuity from above. 
      \begin{equation}
        \lim_{\epsilon \rightarrow 0^+} F_{X, Y} (x + \epsilon, y + \epsilon) = F_{X, Y} (x, y) \text{ for all } x, y \in \mathbb{R}
      \end{equation}
      \item Maringal CDFs. 
      \begin{equation}
        \lim_{y \rightarrow \infty} F_{X, Y} (x, y) = F_X (x), \;\;\;\; \lim_{x \rightarrow \infty} F_{X, Y} (x, y) = F_Y (y)
      \end{equation}
    \end{enumerate}
  \end{lemma}

  \subsubsection{Joint Discrete Random Variables}

    \begin{definition}[Joint PMF]
      Given discrete random variables $X$ and $Y$, let their countable images be denoted $E_X, E_Y \subset \mathbb{R}$. Then, $E_X \times E_Y$ is also countable, and so the joint random variable $(X, Y)$ is also discrete. This means that we can write for some Borel $B$ of $\mathbb{R}^2$, 
      \begin{equation}
        \mathbb{P}_{X, Y} (B) = \sum_{(x, y) \in (E_X \times E_Y) \cap B} \mathbb{P}_{X, Y} (\{(x, y)\})
      \end{equation}
      and we can define the PMF as $p_{X, Y} (x, y) \coloneqq \mathbb{P}_{X, Y} (\{(x, y)\})$. By abuse of notation, we write $p_{X, Y} (x, y) = \mathbb{P} (X = x, Y = y)$ and write 
      \begin{equation}
        \mathbb{P}_{X, Y} (B) = \sum_{(x, y) \in (E_X \times E_Y) \cap B} \mathbb{P} (X = x, Y = y)
      \end{equation}
      If you give me a joint PMF $p_{X, Y}$, by the definition above this determines the entire probability law of $\mathbb{P}_{X, Y}$. 
    \end{definition} 

    \begin{definition}[Conditional PMF]
      Let $X, Y$ be discrete random variables on $(\Omega, \mathcal{F}, \mathbb{P})$. The \textbf{conditional PMF} of $X$ given $Y = y$ is defined 
      \begin{equation}
        p_{X \mid Y} (x \mid y) \coloneqq \frac{p_{X, Y} (x, y)}{p_Y (y)} = \frac{\mathbb{P}_{X, Y} (\{x, y\})}{\mathbb{P}_Y (\{y\})}
      \end{equation}
      and again by abuse of notation, we can simply write 
      \begin{equation}
        \mathbb{P}(X = x \mid Y = y) \coloneqq \frac{\mathbb{P}(X = x, Y = y)}{\mathbb{P}(Y = y)}
      \end{equation}
    \end{definition}

    \begin{theorem}[TFAE]
      Let $X$ and $Y$ be discrete random variables. Then, the following are equivalent: 
      \begin{enumerate}
        \item $X$ and $Y$ are independent. 
        \item For all $x, y \in \mathbb{R}$, the events $\{X = x\}$ (aka $X^{-1} (\{x\})$) and $\{Y = y\}$ (aka $Y^{-1} (\{y\})$) are independent. That is, 
        \begin{equation}
          \mathbb{P} \big[ X^{-1}(\{x\}) \cap Y^{-1}(\{y\}) \big] = \mathbb{P}(X^{-1}(\{x\})) \, \mathbb{P}(Y^{-1}(\{y\}))
        \end{equation}
        \item For all $x, y \in \mathbb{R}$, $p_{X, Y} (x, y) = p_X (x) \cdot p_Y (y)$. 
        \item For all $x, y \in \mathbb{R}$ such that $p_Y (y) > 0$, we have $p_{X \mid Y}(x \mid y) = p_X (x)$. 
      \end{enumerate}
    \end{theorem}

  \subsubsection{Joint Continuous Random Variables}

    \begin{definition}
      $X$ and $Y$ are jointly continuous if the joint law $\mathbb{P}_{X, Y}$ is absolutely continuous w.r.t. the Lebesgue measure on $\mathbb{R}^2$ (i.e. a Borel set of Lebesgue measure $0$ must have $P_{X, Y} = 0$ also). 
    \end{definition}

    However, $X$ and $Y$ continuous does not always imply that $(X, Y)$ are jointly continuous! If we have $X \sim \mathcal{N}(0, 1)$ and $Y = 2 X \sim \mathcal{N}(0, 4)$. Jointly continuous allows us to define a PDF on it. 

    \begin{theorem}[Radon-Nikodym Theorem]
      If $X$ and $Y$ are jointly continuous RVs, then there exists a measurable $f_{X, Y} : \mathbb{R}^2 \longrightarrow [0, \infty)$ s.t. for any $B \in \mathcal{B}(\mathbb{R}^2)$, we have 
      \begin{equation}
        \mathbb{P}_{X, Y} (B) = \int_B f_{X, Y} \, d\lambda
      \end{equation}
    \end{theorem}

    The Radon-Nikodym Theorem guarantees the existence of such $f_{X, Y}$. Taking $B = (-\infty, x] \times (-\infty, y]$, we can define the joint CDF as 
    \begin{equation}
      F_{X, Y} (x, y) = \mathbb{P}(X \leq x, Y \leq y) \coloneqq \mathbb{P}_{X, Y} \big( (-\infty, x] \times (-\infty, y] \bigg) = \int_{-\infty}^x \int_{-\infty}^y f_{X, Y} (s, t) \, dt \,ds
    \end{equation}

\subsection{Cumulative Distribution Functions} 

  Now, remember that the Borel algebra $\mathcal{R}$ is generated by the semi-infinite intervals of form $(-\infty, t]$ (for all $t \in \mathbb{R}$), which are considered "nice" Borel sets. So, $\mathbb{P}_X( (-\infty, t])$ is well defined for all $t \in \mathbb{R}$. In fact, this has a name, and when we talk about the "distribution" of some random variable, we refer to the CDF. 

  \begin{definition}[Cumulative Distribution Function]
    Given $(\Omega, \mathcal{F}, \mathbb{P})$ and a random variable $X: \Omega \rightarrow \mathbb{R}$. Then, the \textbf{cumulative distribution function} of $X$ is defined 
    \begin{equation}
      F_X (x) =\mathbb{P}\big( \{\omega \in \Omega \mid X(\omega) \leq x\} \big)
    \end{equation}
    We can also define this with the probability law $\mathbb{P}_X$ as 
    \begin{equation}
      F_X (x) = \mathbb{P}_X \big( (-\infty, x] \big)
    \end{equation}
    By abuse of notation, we will write the CDF as $P(X \leq x)$. It satisfies the properties: 
    \begin{enumerate}
      \item Limits: 
      \begin{equation}
        \lim_{x \rightarrow -\infty} F_X (x) = 0 \text{ and } \lim_{x \rightarrow \infty} F_X (x) = 1
      \end{equation}
      \item Monotonicity: 
      \begin{equation}
      x \leq y \implies F_X (x) \leq F_X (y)
      \end{equation}
      \item Right-continuity: For all $x \in \mathbb{R}$
      \begin{equation}
        \lim_{\epsilon \rightarrow 0^+} F_X (x + \epsilon) = F_X (x)
      \end{equation}
      So, if there are jumps, the hole can exist as the function approaches a value from the left. 
    \end{enumerate}
    What is remarkable is that any function satisfying these three properties satisfies these 3 properties gives you a viable CDF (and as shown below, completely determines a unique random variable). 
  \end{definition}

  So if you give me the probability law for all Borel sets of $\mathbb{R}$, then I can easily define the CDF since $(-\infty, x]$ are also Borel sets. It turns out that if we know \textit{just} the CDF, then since the semi-infinite intervals form a generating class of $\mathcal{R}$, it turns out that we can completely define $\mathbb{P}_X$. The proof of the theorem below is a bit more involved, using $\pi$-systems, but it is good to know. 

  \begin{theorem}
    The CDF $F_X (\cdot)$ uniquely specifies the probability law $\mathbb{P}_X$ for any random variable $X$. 
  \end{theorem}

  To summarize, given a probability space $(\Omega, \mathcal{F}, \mathbb{P})$, a random variable just pushes a measure onto the measure space $(\mathbb{R}, \mathcal{R})$. If we only care about the values of the random variable, then we can forget about $\Omega$ and only look at $(\mathbb{R}, \mathcal{R}, \mathbb{P}_X)$. The CDF on $(\mathbb{R}, \mathcal{R}, \mathbb{P}_X)$ will be well defined since semi-finite intervals are also Borel. If I am just given a CDF $F_X (\cdot)$, then this is enough for me to specify a unique probability measure $\mathbb{P}_X$ on $(\mathbb{R}, \mathcal{R})$. So although $\mathbb{P}_X$ contains the complete description of the random variable $X$, in practice we will use $F_X$ since it also captures all the information of $X$ and it's easier to work with. 

  \begin{definition}[Convergence in Distribution]
    We say $X_n$ \textbf{converges to $X$ in distribution} if the CDF of $X_n$ converges pointwise to the CDF of $X$, i.e. 
    \begin{equation}
      \lim_{n \rightarrow \infty} F_{X_n} (x) = F_X (x)
    \end{equation}
    for all $x$ where $F_{X}$ is continuous. 
  \end{definition}

\subsection{Independence}

  \begin{definition}[Independence of $2$ Events]
    Given probability space $(\Omega, \mathcal{F}, \mathbb{\mathbb{P}})$, events $A, B \in \mathcal{F}$ are said to be \textbf{independent under $\mathbf{\mathbb{P}}$} if 
    \begin{equation}
      \mathbb{P}(A \cap B) = \mathbb{P}(A) \, \mathbb{P}(B)
    \end{equation}
    This leads to the immediate property that if $\mathbb{P}(B) > 0$, with $A, B$ independent, then 
    \begin{equation}
      \mathbb{P}(A \mid B) = \mathbb{P}(A)
    \end{equation}
  \end{definition}

  Note that $A$ and $B$ may be independent under one measure, but not under another measure. The property that $\mathbb{P}(A \mid B) = \mathbb{P}(A)$ is \textit{not} the definition of independence, since it has the more restricting property that $\mathbb{P}(B) > 0$, so only refer to the definition that $\mathbb{P}(A \cap B) = \mathbb{P}(A) \, \mathbb{P}(B)$. This is the true definition of independent events that we should rely on, not the one that says that $A$ and $B$ are independent if "one does not affect the other." This old definition is misleading and false. For example, take the probability space $[0, 1]$, with Borel $\sigma$-algebra, and Lebesgue measure $\mathbb{P} = \lambda$, and let $A = \mathbb{Q}$ and $B = \mathbb{R} \setminus \mathbb{Q}$. Then, contradictory to our old definition, $A$ and $B$ are independent since $\mathbb{P}(A \cap B) = \mathbb{P}(A) \, \mathbb{P}(B) = 0$! By the definition, an event $A$ is independent of itself if $\mathbb{P}(A) = 0$ or $1$ (e.g. $A$ is rationals, irrationals, cantor set, $\emptyset$, $\Omega$, etc.). 

  \begin{definition}[Independence of $n$ Events]
    Given probability space $(\Omega, \mathcal{F}, \mathbb{P})$, 
    \begin{enumerate}
      \item Let us have a finite collection of events $A_1, A_2, \ldots, A_n \in \mathcal{F}$. They are \textbf{independent} if for all nonempty $I_0 \subset \{1, 2, \ldots n\}$, 
      \begin{equation}
        \mathbb{P} \bigg( \bigcap_{i \in I_0} A_i \bigg) = \prod_{i \in I_0} \mathbb{P}(A_i)
      \end{equation}
      Note that it is not enough to just prove that 
      \begin{equation}
        \mathbb{P}(A_1 \cap \ldots \cap A_n) = \prod_{i=1}^n \mathbb{P}(A_i)
      \end{equation}
      We must verify this for all $2^n$ possible choices (to be precise, we don't need to prove for $I_0 = \emptyset$ and $I_0 = \{A_i\}$), so for $2^n - n - 1$ choices. 
      
      \item Let $\{A_i\}_{i \in I}$ be a collection of events indexed by a possibly uncountable $I$. They are independent if for all nonempty and finite $I_0 \subset I$, we have 
      \begin{equation}
        \mathbb{P} \bigg( \bigcap_{i \in I_0} A_i \bigg) = \prod_{i \in I_0} \mathbb{P}(A_i)
      \end{equation}
    \end{enumerate}
  \end{definition}

  Now when we are trying to compare two $\sigma$-algebras, the measure defined for one may not even be defined on the other. To ensure that a measure is defined on both, it makes sense to take its $\sigma$-algebra and construct two sub-$\sigma$-algebras, which $\mu$ is guaranteed to be defined on. 

  \begin{definition}[Independence of $\sigma$-Algebras]
    Let us have probability space $(\Omega, \mathcal{F}, \mathbb{P})$. 
    \begin{enumerate}
      \item Let $\mathcal{F}_1, \mathcal{F}_2$ be two sub-$\sigma$-algebras of $\mathcal{F}$. $\mathcal{F}_1$ and $\mathcal{F}_2$ are independent if for any $A_1 \in \mathcal{F}_1, A_2 \in \mathcal{F}_2$, $A_1$ and $A_2$ are independent. 
      \item Let $\{ \mathcal{F}_i\}_{i \in I}$ be an arbitrary collection of sub-$\sigma$-algebras of $\mathcal{F}$, indexed by possibly uncountable $I$. Then, they are independent if for any choices of $A_i \in \mathcal{F}_i$ for $i \in I$, $\{A_i\}_{i \in I}$ are independent events. 
    \end{enumerate}
  \end{definition}

  \begin{definition}[Independent Random Variables]
    Two random variables $X, Y$ are \textbf{independent} if $\sigma(X)$ and $\sigma(Y)$ are independent $\sigma$-algebras. That is, for any Borel sets $B_1, B_2 \in \mathcal{R}$, the events $X^{-1}(B_1)$ and $Y^{-1}(B_2)$ are independent: 
    \begin{equation}
      \mathbb{P}\big[ X^{-1}(B_1) \cap Y^{-1}(B_2) \big] = \mathbb{P}(X^{-1}(B_1)) \, \mathbb{P}(Y^{-1}(B_2))
    \end{equation}
    or by abusing notation, 
    \begin{equation}
      \mathbb{P}(X \in B_1, Y \in B_2) = \mathbb{P}(X \in B_1) \, \mathbb{P}(Y \in B_2)
    \end{equation}
  \end{definition}

  If $X, Y$ are independent, then we can say something about the CDFs 
  \begin{equation}
    F_{X, Y} (x, y) = F_X (x) \, F_Y (y)
  \end{equation}
  In fact, we can say something stronger. 

  \begin{theorem}
    $X$ and $Y$ are independent RVs if and only if 
    \begin{equation}
      F_{X, Y} (x, y) = F_X (x) \, F_Y (y)
    \end{equation}
  \end{theorem}

  Moving onto multiple variables, we can define that $X_1, X_2, \ldots, X_n$ are independent RVs if $\sigma(X_1), \ldots, \sigma(X_n)$ are independent $\sigma$-algebras. 

\subsection{Borel-Cantelli Lemmas}

  There are many Borel-Cantelli lemmas, and we will introduce the two most famous ones. To understand what these lemmas say, given a sequence $A_1, A_2, \ldots$ of events in $\sigma$-algebra $\mathcal{F}$, we must first understand what the daunting term  
  \begin{equation}
    \bigcap_{n=1}^\infty \bigcup_{i = n}^\infty A_i
  \end{equation}
  means. Now let's try to explain what the intersection of the unions mean. First, remember that $\sigma$-algebras are stable under both countable unions and countable intersections, this is also in $\mathcal{F}$. We can interpret 
  \begin{equation}
    \bigcap_{n=1}^\infty \bigcup_{i=n}^\infty A_i = \{ A_n \text{ i.o.}\}
  \end{equation}
  as the \textit{event that infinitely many $A_n$'s occur}, where i.o. means "infinitely often." To parse this, let's start from the innermost term and call it 
  \begin{equation}
    B_n = \bigcup_{i=n}^\infty A_i \implies \{A_n \text{ i.o.}\} = \bigcap_{n=1}^\infty B_n
  \end{equation}
  $B_n$ is the event that at least one of the $A_n, A_{n+1}, A_{n+2}, \ldots$ occurs, often referred to as the \textit{$n$th tail event}. Now the intersection of all $B_n$'s is the event that \textit{all} $B_n$'s occur. In other words, this is the event that for no matter how big of an $N \in \mathbb{N}$ I choose, there is always at least an event $A_n$ with $n > N$ that occurs. This is shortly summarized as the event that infinitely many $A_n$'s occur. 

  \begin{lemma}[1st Borel-Cantelli Lemma]
    Given probability space $(\Omega, \mathcal{F}, \mathbb{P})$, if $A_1, A_2, \ldots$ is a sequence of events such that 
    \begin{equation}
      \sum_{n=1}^\infty \mathbb{P}(A_n) < \infty
    \end{equation}
    the almost surely (with probability $1$) only finitely many $A_n$'s will occur. 
    \begin{equation}
      \mathbb{P} \bigg( \bigcap_{n=1}^\infty \bigcup_{i = n}^\infty A_i \bigg) = 0
    \end{equation}
  \end{lemma}
  \begin{proof}
    Setting $B_n$ as above, we have 
    \begin{align*}
      \mathbb{P}\bigg( \bigcap_{n=1}^\infty B_n \bigg) & = \lim_{n \rightarrow \infty} \mathbb{P}(B_n) & (\text{continuity of probability}) \\
      & = \lim_{n \rightarrow \infty} \mathbb{P} \bigg( \bigcup_{i=1}^\infty A_i \bigg) & (\text{substitute } B_i) \\
      & \leq \lim_{n \rightarrow \infty} \sum_{i = n}^\infty \mathbb{P}(A_i) = 0 & (\text{tail sum of convergent series is } 0)
    \end{align*}
  \end{proof}

  The second Borel-Cantelli lemma is like a partial contrapositive to the first lemma, where it starts with the assumption that the sum of the $\mathbb{P}(A_n)$'s are infinite (along with the addition case that they are independent). 

  \begin{lemma}[2nd Borel-Cantelli Lemma]
    If $A_1, A_2, \ldots$ are independent events such that 
    \begin{equation}
      \sum_{n=1}^\infty \mathbb{P}(A_n) = \infty,
    \end{equation}
    then almost surely (with probability $1$) infinitely many $A_n$'s will occur. That is, 
    \begin{equation}
      \mathbb{P} \bigg( \bigcap_{n=1}^\infty \bigcup_{i = n}^\infty A_i \bigg) = 1
    \end{equation}
  \end{lemma}

  The intuition behind this lemma is challenging: We can let $\mathbb{P}(A_n) = P_n$ and interpret the sum as a series of $P_n$'s. Since the series $P_1 + P_2 + \ldots$ is finite, this implies that 
  \begin{equation}
    \lim_{n \rightarrow \infty} P_n = 0
  \end{equation}
  (but not the converse) and going to zero rather fast such that the series is finite. So, you are working with a sequence of events $A_n$ that are becoming more and more unlikely rather fast. The lemma says that beyond a certain point $n_0$, none of the events $A_n$ will occur almost surely. For the second lemma, we can go as far as we like in the sequence of $A_n$'s, up to any $A_{n_0}$, but beyond that there is always an infinite number of $A_n$'s that occur beyond $A_{n_0}$. 

\subsection{Exercises}

  \begin{exercise}[Durrett 1.2.1]
    Suppose $X$ and $Y$ are random variables on $(\Omega, \mathcal{F}, P)$ and let $A \in \mathcal{F}$. Show that if we let $Z(\omega) = X(\omega)$ for $\omega \in A$ and $Z(\omega) = Y(\omega)$ for $\omega \in A^c$, then $Z$ is a random variable.
  \end{exercise}
  \begin{solution}
    
  \end{solution}

  \begin{exercise}[Durrett 1.2.2]
    Let $\chi$ have the standard normal distribution. Use Theorem 1.2.6 to get upper and lower bounds on $P(\chi \geq 4)$.
  \end{exercise}
  \begin{solution}
    
  \end{solution}

  \begin{exercise}[Durrett 1.2.3]
    Show that a distribution function has at most countably many discontinuities.
  \end{exercise}
  \begin{solution}
    
  \end{solution}

  \begin{exercise}[Durrett 1.2.4]
    Show that if $F(x) = P(X \leq x)$ is continuous then $Y = F(X)$ has a uniform distribution on $(0,1)$, that is, if $y \in [0, 1]$, $P(Y \leq y) = y$.
  \end{exercise}
  \begin{solution}
    
  \end{solution}

  \begin{exercise}[Durrett 1.2.5]
    Suppose $X$ has continuous density $f$, $P(\alpha \leq X \leq \beta) = 1$ and $g$ is a function that is strictly increasing and differentiable on $(\alpha, \beta)$. Then $g(X)$ has density $f(g^{-1}(y))/g'(g^{-1}(y))$ for $y \in (g(\alpha), g(\beta))$ and 0 otherwise. When $g(x) = ax + b$ with $a > 0$, $g^{-1}(y) = (y - b)/a$ so the answer is $(1/a)f((y - b)/a)$.
  \end{exercise}
  \begin{solution}
    
  \end{solution}

  \begin{exercise}[Durrett 1.2.6]
    Suppose $X$ has a normal distribution. Use the previous exercise to compute the density of $\exp(X)$. (The answer is called the \textbf{lognormal distribution}.)
  \end{exercise}
  \begin{solution}
    
  \end{solution}

  \begin{exercise}[Durrett 1.2.7]
    \begin{enumerate}
      \item Suppose $X$ has density function $f$. Compute the distribution function of $X^2$ and then differentiate to find its density function.
      \item Work out the answer when $X$ has a standard normal distribution to find the density of the \textbf{chi-square distribution}.
    \end{enumerate}
  \end{exercise}
  \begin{solution}
    
  \end{solution}

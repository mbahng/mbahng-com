\section{Sequences of Functions}

  Let us slightly extend the notion of limits of sequences with functions. 

  \begin{definition}[Pointwise Convergence]
    \label{def:pointwise-convergence}
    Let $E \subset \mathbb{R}$ and $f, f_n : E \to \mathbb{R}$. We say that $f_n \to f$ \textbf{pointwise} if 
    \begin{equation}
      \lim_{n \to \infty} f_n (x) = f(x) \text{ for all } x \in E
    \end{equation}
  \end{definition}

  The next question to ask is whether properties of functions are preserved under the limit operations. For example, if the $f_n$'s are continuous, differentiable, or Riemann integrable, is the same true for the limit function $f$? What are the relations between $f_n^\prime$ and $f^\prime$ or $\int f_n$ and $\int f$? To say that a function $f$ is continuous at a point $x$ means 
  \begin{equation}
    \lim_{t \to x} f(t) = f(x)
  \end{equation}
  For functions, the analogous result is whether 
  \begin{equation}
    \lim_{t \to x} \lim_{n \to \infty} f_n (t) = \lim_{n \to \infty} \lim_{t \to x} f_n (t)
  \end{equation}

  Is this true? Let's look at a few examples. 

  \begin{example}[Double Sequence May be Swappable]
    Consider the double sequence $\big( \frac{1}{m + n} \big)_{m, n \in \mathbb{N}}$. We can compute the limit as both $n, m \to +\infty$ in many ways. 
    \begin{enumerate}
      \item We can first set $m \to +\infty$, then $n \to +\infty$. 
      \item We can first set $n \to +\infty$, then $m \to +\infty$. 
      \item We might want to take $n$ twice as slow as $m$. 
    \end{enumerate}
    All of these converge to the same value of $0$, so there is no problem. 
  \end{example}

  \begin{example}
    Let $f_n (x) = x/n$. Then $f_n \to 0$ pointwise, where $0$ is the $0$ function. This is true since for every fixed $x$, we can set $n$ so large that $x/n < \epsilon$ for any $\epsilon$. 
  \end{example}

  In this case, we are considering a double sequence in $\mathbb{R}$. However, if we fix one value, then it becomes a sequence of functions, and we already have established that classes of functions form a vector space. In general, interchanging limits are not allowed. 

  \begin{example}[Cannot Exchange Limits Under Pointwise Convergence]
    Consider the slightly different sequence $\big( \frac{m}{m + n} \big)_{m, n}$. 
    \begin{enumerate}
      \item If $m >> n$, i.e. take the sequence of values $(10^k, k)$, then this will approach $1$. 
      \item If $n >> m$, i.e take the sequence of values $(k, 10^k)$, then this will approach $0$. 
      \item In intermediate cases, you can in fact get any number between $0$ and $1$. 
    \end{enumerate}
  \end{example}

  Therefore, in general, the limits are not equal. Even worse, the properties of these functions are violated. To get equality, we need to make a stronger assumption than simple existence of limits. 

  \begin{example}[Pointwise Limit of Continuous Functions is not Continuous]
    Let $f_n (x) : [0, 1] \to \mathbb{R}$  defined $f_n (x) = x^n$. Then, 
    \begin{align}
      0 \leq x < 1 & \implies x^n \to 0 \text{ as } n \to \infty \\ 
      x = 1 & \implies x^n \to 1 \text{ as } n \to \infty
    \end{align}
    So, 
    \begin{equation}
      f_n \to f^\ast (x) \coloneqq \begin{cases} 
        0 & \text{ if } x < 1 \\ 
        1 & \text{ if } x = 1
      \end{cases}
    \end{equation}
    Note that all $f_n$ are continuous but $f^\ast$ is discontinuous since 
    \begin{equation}
      \lim_{x \to 1} \lim_{n \to \infty} f_n (x) = 0 \neq 1 = \lim_{n \to \infty} \lim_{x \to 1} f_n (x)
    \end{equation}
  \end{example}

  \begin{example}[Pointwise Series Sum of Continuous Functions is not Continuous]
    Consider 
    \begin{equation}
      f_n (x) = \frac{x^2}{(1 + x^2)^n}
    \end{equation}
    and let 
    \begin{equation}
      f(x) = \sum_{n=0}^\infty f_n (x) = \sum_{n=0}^\infty \frac{x^2}{(1 + x^2)^n}
    \end{equation}
    Since $f_n (0) = 0$, we have $f(0) = 0$. For $x \neq 0$, the series is a convergent geometric series with sum $1 + x^2$. Therefore, 
    \begin{equation}
      f(x) = \begin{cases} 
        0 & \text{ if } x = 0 \\ 
        1 + x^2 & \text{ if } x \neq 0
      \end{cases}
    \end{equation}
    Therefore the sum of a convergent series of continuous functions may not be continuous. 
  \end{example}

  \begin{example}[Cannot Exchange Integrals and Limits Under Pointwise Convergence]
    Consider the function $f_n: [0, 1] \to \mathbb{R}$ defined by $f_n (0) = f_n (1/n) = 0$ and $f_n (1/2n) = 2n$, with everything else linearly interpolated. Then $f_n \to 0$ since it is constantly $0$ at $0$ and for every $x > 0$, there exists $1/N < x$ and so $f_n (x) = 0$ for all $n \geq N$. However, $\int_0^1 f_n (x) \,dx = 1$, so 
    \begin{equation}
      \lim_{n \to \infty} \int_0^1 f_n (x)\,dx \neq \int_0^1 \lim_{n \to \infty} f_n (x)\,dx
    \end{equation}
    So integration is not continuous with respect to the topology induced by pointwise convergence. 
  \end{example}

  The problem is not in the construction of the limits or the integral, but with the pointwise convergence. With pointwise convergence, 
  \begin{enumerate}
    \item we cannot exchange two limits, which implies limit of derivatives may not equal to the derivative of the limit
    \item limits do not preserve continuity 
    \item cannot exchange integrals and limits 
    \item cannot exchange sums
  \end{enumerate}

  \begin{example}
    Review. 
    \begin{equation}
      \bigg( 1 - \frac{1}{n} \bigg)^n \to e^{-1}
    \end{equation}
  \end{example}

  So now the motivation is to find properties of these sequences of functions that do allow these manipulations. 

\subsection{Uniform Convergence}

  The problem seems to be that certain points may converge at a much slower rate than others. Therefore, we want to impose some sort of uniform condition, which says that the values of the function at every point must converge. 

  \begin{definition}[Uniform Convergence]
    \label{def:uniform-convergence}
    Given $f_n : E \to \mathbb{R}$ of bounded functions, $(f_n)$ is said to \textbf{converge uniformly} to a bounded function $f: E \to \mathbb{R}$ if $\forall \epsilon > 0$, there exists $N \in \mathbb{N}$ s.t. 
    \begin{equation}
      n \geq N \implies |f_n (x) - f(x) | < \epsilon \text{ for all } x \in E
    \end{equation}
    the ``for all $x \in E$'' is the uniform part, which is similar to uniform continuity. 
  \end{definition}

  \begin{theorem}[Uniform Convergence iff Supremum of Difference Converges to 0]
    This is an immediate consequence of the definition. 
    \begin{equation}
      f_n \to f \text{ uniformly on } E \iff \lim_{n \to \infty} \sup_{x \in E} |f_n (x) - f(x)| = 0
    \end{equation}
  \end{theorem}
  \begin{proof}
    We can prove this with a bunch of iff statements. Let $f_n \to f$ uniformly, then this is equivalent to $\forall \epsilon > 0$, $\exists N \in \mathbb{N}$ s.t. $n \geq N$ implies
    \begin{align}
      & |f_n (x) - f(n)| < \epsilon \quad \forall x \in E \\ 
      & \iff M_n \sup_{x \in E} |f_n (x) - f(x)| \leq \epsilon \\ 
      & \iff M_n \to 0 \text{ as } n \to 0
    \end{align}
    which implies that $M_n \to 0$ as $n \to 0$. 
  \end{proof}

  Generally, to prove uniform convergence, you will need to find that $|f_n (x) - f(x)|$ is bounded by something that is independent of $x$, and it goes to $0$ as $n \to \infty$. Here is an equivalent condition. 

  \begin{definition}[Uniformly Cauchy]
    A sequence $f_n: E \to \mathbb{R}$ is called \textbf{uniformly Cauchy} if $\forall \epsilon > 0$, there exists $N \in \mathbb{N}$ s.t. $\forall n, m \geq N$, 
    \begin{equation}
      |f_n (x) - f_m (x)| < \epsilon \text{ for all } x \in E
    \end{equation}
  \end{definition}

  \begin{lemma}[Cauchy Criterion of Uniform Convergence]
    $(f_n)$ converges uniformly iff $(f_n)$ is uniformly Cauchy. 
  \end{lemma}
  \begin{proof}
    We prove bidirectionally. 
    \begin{enumerate}
      \item $(\rightarrow)$. Suppose $(f_n)$ converges uniformly on $E$, and let $f$ be the limit function. Then there exists $N \in \mathbb{N}$ s.t. 
      \begin{equation}
        n \geq N \implies |f_n (x) - f(x)| < \frac{\epsilon}{2} \text{ for all } x \in E
      \end{equation} 
      Therefore, 
      \begin{equation}
        n, m \geq N \implies |f_n (x) - f_m (x)| \leq |f_n (x) - f(x)| + |f(x) - f_m (x)| < \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon 
      \end{equation}
      for all $x \in E$. 

      \item $(\leftarrow)$. Suppose the uniform Cauchy criterion holds. For every $x \in E$, the sequence $(f_n (x))_n$ converges as a Cauchy sequence in $\mathbb{R}$. Call this limit $f(x)$. Thus the sequence $(f_n)$ converges pointwise to $f$. 

      Now let $\epsilon > 0$. Then, by uniform Cauchy, $\exists N \in \mathbb{N}$ s.t. 
      \begin{equation}
        n, m \geq N \implies |f_n (x) - f_m (x)| < \epsilon \quad \forall x \in E
      \end{equation}
      Now, if we fix $n$\footnote{This is a classic trick used to convert Cauchy convergence to regular convergence. What we want to do is that since given some $x_n$, the rest of the points $x_m$ must also be close to $x_n$, so the limit of the $x_m$ must also be close to $x_n$, which is basically the definition of convergence.}, we know that 
      \begin{equation}
        m \geq N \implies |f_n (x) - f_m (x)| < \epsilon 
      \end{equation}
      Now we can take the limit as $m \to \infty$, and therefore the limit of the LHS must be bounded by that of the RHS. 
      \begin{equation}
        \lim_{m \to \infty} |f_n (x) - f_m (x)| = |f_n (x) - f(x)| \leq \epsilon
      \end{equation}
      Now this is true for all $n \geq N$, which basically is the definition of convergence.\footnote{If you are dissatisfied with the $\leq$, just set $\epsilon/2$ to get strictly less than. }
    \end{enumerate}
  \end{proof}

  The wrong proof (that uniformly Cauchy implies uniform convergence) that I had in my first attempt was that I tried to directly bound the distances using some variant of the triangle inequality, like 
  \begin{equation}
    |f_n (x) - f_m (x)| \leq |f_n (x) - f(x)| + |f(x) - f_m (x)|
  \end{equation}
  However, this doesn't really help since this is an \textit{upper bound}. Perhaps we can use the reverse triangle inequality. 
  \begin{equation}
    | |f_n (x)| - |f_m (x)| | \leq |f_n (x) - f_m (x)|
  \end{equation}
  but again, this inequality lacks $f(x)$ entirely. 

  \begin{example}
    We have uniform convergence
    \begin{equation}
      \frac{\sin(e^n x)}{n} \to 0 \text{ since  } \bigg| \frac{\sin(e^n x)}{n} \bigg| \leq \frac{1}{n} \to 0
    \end{equation}
  \end{example}

  \begin{example}
    $f_n (x) = x^n$ does not converge uniformly to \textit{any} function in $[0, 1]$. It suffices to find a sequence $(y_n) \subset [0, 1]$ s.t. $|f_n (y_n) - f(y_n)| \not\to 0$ as $n \to \infty$. Take $y_n = 1 - \frac{1}{n}$. Then $f_n (y_n) \to \frac{1}{e} \neq 0$, but $f(y_n) \to 1$. 
  \end{example}

  \begin{example}
    The triangle functions do not converge uniformly since $f_n$ is unbounded, i.e. $f_n (\frac{1}{2n}) = 2n$, while $f_n ( \frac{1}{n}) = 0$. So we can construct two sequences that  
    \begin{equation}
      (1/2n) \to \infty, (1/n) \to 0 \text{ as } n \to \infty
    \end{equation}
  \end{example}

  \begin{lemma}[Uniform Convergence Implies Pointwise Convergence]
    Uniform convergence implies pointwise convergence. 
  \end{lemma}
  \begin{proof}
    Say $f_n \to f$ uniformly. Then for every $\epsilon > 0$, there exists a $N \in \mathbb{N}$ s.t. 
    \begin{equation}
      n \geq N \implies |f_n (x) - f(x)| < \epsilon \text{ for all } x \in E
    \end{equation}
    So just fix a point $x$, take any $\epsilon > 0$, and we have our $\delta$ due to uniform convergence. 
  \end{proof}

  \begin{theorem}[Weierstrass M-Test]
    Suppose $(f_n)$ is a sequence of functions on $E$, and suppose 
    \begin{equation}
      |f_n (x)| \leq M_n \text{ for all } x \in E
    \end{equation}
    for each $n \in \mathbb{N}$. Then if $\sum_n M_n$ converges, $\sum_n f_n$ converges uniformly.  
  \end{theorem}
  \begin{proof}
    If $\sum_n M_n$ converges, then for arbitrary $\epsilon > 0$, we have 
    \begin{equation}
      \bigg| \sum_{i=n}^m f_i (x) \bigg| \leq \sum_{i = n}^m M_n \leq \epsilon
    \end{equation}
    provided $n, m \in \mathbb{N}$ are large enough. Therefore this is uniformly Cauchy, and so uniformly convergent. 
  \end{proof} 

  Here is my wrong first attempt for a proof. Assume that $\sum_{k=1}^\infty M_k$ converges, and we wish to show that $\forall \epsilon > 0$, $\exists N \in \mathbb{N}$ s.t. 
  \begin{equation}
    n \geq N \implies \bigg| \underbrace{\sum_{k=1}^\infty f_k (x) - \sum_{k=1}^n f_k (x)}_{\text{not allowed}} \bigg| = \bigg| \sum_{k=n+1}^\infty f_k (x) \bigg|< \epsilon 
  \end{equation}
  The problem is that in our statement, we are assuming that we can actually subtract a finite term $\sum_{k=1}^n f_k (x)$ from a potentially divergent one: $\sum_{k=1}^\infty f_k(x)$. Therefore, we are assuming that this is convergent in the first place! This is exactly why we want to work with Cauchy sequences, which doesn't carry this kind of assumption, and so the sum from $i=n$ to $m$ is guaranteed to be finite. 

  After this, the rest of the steps are fine, since I use the fact that absolutely convergent series are also convergent, and so 
  \begin{equation}
    \bigg| \sum_{k=n+1}^\infty f_k (x) \bigg| \leq \sum_{k=n+1}^\infty |f_k (x)| \leq \sum_{k=n+1}^\infty M_k
  \end{equation}

  \begin{example}[Converse of Weierstrass M-test Not True]
    The converse is clearly not true, since we can just take any uniformly convergent bounded sequence and set $M_n \to +\infty$ as $n \to \infty$. 
  \end{example}

  The final theorem is a significant one. So far, we uniform convergence to prove that the limit of some sequence satisfies some property. In here, it's sort of the opposite. 

  \begin{theorem}[Dini's Theorem]
    \label{thm:dini}
    Suppose $K$ is compact, and 
    \begin{enumerate}
      \item $(f_n)$ is a sequence of continuous functions on $K$. 
      \item $f_n \to f$ pointwise on $K$. 
      \item $f$ is continuous.\footnote{Note that the pointwise limit of continuous functions may not be continuous!}
      \item $f_n \leq f_{n+1}$ for all $n \in \mathbb{N}$. 
    \end{enumerate}
    Then $f_n \to f$ uniformly on $K$. 
  \end{theorem} 
  \begin{proof}
    For convenience, let's define $g_n \coloneqq f - f_n \geq 0$, which is continuous on $K$. It suffices to prove that $g_n \to 0$ uniformly; that is, $\forall \epsilon > 0$, $\exists N \in \mathbb{N}$ s.t. 
    \begin{equation}
      n \geq N \implies |g_n (x)| < \epsilon
    \end{equation}
    Fix $\epsilon > 0$, and from compactness, we should already be thinking of trying to construct an open cover of $K$. Let 
    \begin{equation}
      E_n \coloneqq \{ x \in K \mid g_n (x) < \epsilon \}
    \end{equation}
    Note three things. 
    \begin{enumerate}
      \item First, $E_n$ is open as the preimage of continuous $g_n$. 
      \item Second $g_n$ is decreasing, so $g_n \geq g_{n+1}$, and so 
      \begin{equation}
        E_n \subset E_{n+1} \quad \forall n \in \mathbb{N}
      \end{equation}
      \item Third, $g_n \to 0$ pointwise, so at some point the $E_n$'s must cover all of $K$.\footnote{Remember that this is for \textit{fixed} $\epsilon$! This final point is not true if $\epsilon$ is not fixed.} 
      \begin{equation}
        \bigcup_{n = 1}^{\infty} E_n = K
      \end{equation}
    \end{enumerate}
    We have constructed an open cover, and now we take the fact that $K$ is compact to get a finite subcover $\{E_{n_k}\}_{k}$. But since the $E_n$'s are nested, we can set $N = \max_k \{n_k\}$, which implies $E_N = K$. Therefore, as long as $n \geq N$, $g_n$ is small enough so that it will map all of $E_N = K$ to a value less than $\epsilon$. We have 
    \begin{equation}
      n \geq N \implies g_n (x) = |g_n (x)| < \epsilon
    \end{equation}
  \end{proof}

  Let's reflect on this proof and how we used the theorem's assumptions. First, monotonicity of $f_n$ was needed to make sure that $E_n$ was increasing. Second, compactness of $K$ was needed to extract a finite subcover that we can exploit. 

\subsection{Limits of Uniformly Convergent Functions} 

  Now we will formalize and prove the manipulations that are unlocked by uniform convergence. 

  \begin{theorem}[Limits are Swappable]
    Suppose $f_n \to f$ uniformly over a set $E$ of a metric space. Let $x \in E^\prime$, and suppose that $\lim_{t \to x} f_n (t) = A_n$ for all $n$. Then, 
    \begin{enumerate}
      \item $(A_n)$ converges, and 
      \item we have 
        \begin{equation}
          \lim_{t \to x} f(t) = \lim_{n \to \infty} A_n \iff \lim_{t \to x} \lim_{n \to \infty} f_n (t) = \lim_{n \to \infty} \lim_{t \to x} f_n (t)
        \end{equation}
    \end{enumerate}
  \end{theorem}
  \begin{proof}
    Let $\epsilon > 0$, then by uniform continuity, there exists a $N \in \mathbb{N}$ s.t. 
    \begin{equation}
      n, m \geq N \implies |f_n (t) - f_m (t)| < \epsilon
    \end{equation}
    Now let $t \to x$, and since the limit exists, we have 
    \begin{equation}
      n, m \geq N \implies |A_n - A_m| < \epsilon
    \end{equation} 
    and so $(A_n)$ is by definition a Cauchy sequence in $\mathbb{R}$. Say that $A_n \to A$. 

    Now we wish to prove that $\lim_{t \to x} f(t) = A$. Take $\epsilon > 0$, and we wish to show that there exists some $\delta > 0$ s.t. 
    \begin{equation}
      |t - x| < \delta \implies |f(t) - A| < \epsilon
    \end{equation}
    Note that by the triangle inequality
    \begin{equation}
      |f(t) - A| \leq |f(t) - f_n (t)| + |f_n (t) - A_n| + |A_n - A|
    \end{equation}
    Therefore, we can take $\frac{\epsilon}{3} > 0$. 
    \begin{enumerate}
      \item By uniform convergence of $f_n \to f$, there exists a $N_1 \in \mathbb{N}$ s.t. 
      \begin{equation}
        n \geq N_1 \implies |f_n (x) - f(x)| < \frac{\epsilon}{3} \text{ for all } x \in E
      \end{equation}

      \item By convergence of $A_n \to A$, there exists a $N_2 \in \mathbb{N}$ s.t. 
      \begin{equation}
        n \geq N_2 \implies |A_n - A| < \frac{\epsilon}{3} 
      \end{equation}

      \item Therefore, choose $N = \max\{N_1, N_2\}$, and for this $N$, by convergence of $f_n (t) \to A_n$ we can choose a $\delta$ s.t. 
        \begin{equation}
          |x - t| < \delta \implies |f_n (t) - A_n| < \frac{\epsilon}{3} 
        \end{equation}
    \end{enumerate}
    This essentially bounds the three values, and so by choosing the $\delta$ in (3), we get 
    \begin{equation}
      |t - x| < \delta \implies |f(t) - A| \leq \frac{\epsilon}{3} + \frac{\epsilon}{3} + \frac{\epsilon}{3} = \epsilon
    \end{equation}
  \end{proof}

  \begin{corollary}[Uniform Limits of Continuous Functions are Continuous]
    If $(f_n)$ is a sequence of continuous functions on $E$, and if $f_n \to f$ uniformly on $E$, then $f$ is continuous on $E$. 
  \end{corollary}
  \begin{proof}
    Using the sequential definition of continuity, we claim that $\lim_{t \to x} f(t) = f(x)$. We know from the previous theorem that 
    \begin{equation}
      \lim_{t \to x} \underbrace{\lim_{n \to \infty} f_n (t)}_{= f(t)} = \lim_{n \to \infty} \underbrace{\lim_{t \to x} f_n (t)}_{= f_n (x)}
    \end{equation}
    where the LHS follows from $(f_n)$ being uniformly convergent, which implies pointwise convergence, and the RHS follows from $f_n$ being continuous.  
  \end{proof} 

  However, the converse is not true. A sequence of continuous functions may converge to a continuous function though not uniformly. Now let's move onto integration. 

  \begin{theorem}[Limits of Integrals are Integrals of Uniform Limits]
    Let $(f_n)$ be a sequence of Riemann integrable functions on $[a, b]$. If $f_n \to f$ uniformly on $[a, b]$, then $f \in \mathcal{R}([a, b])$ and 
    \begin{equation}
      \lim_{n \to \infty} \int_a^b f_n(x) \,dx = \int_a^b f(x) \,dx = \int_a^b \lim_{n \to \infty} f_n (x) \,dx 
    \end{equation}
  \end{theorem} 
  \begin{proof}
    Let $\epsilon_n = \sup | f_n (x) - f(x)|$ with the supremum taken over $a \leq x \leq b$. Then, 
    \begin{equation}
      f_n - \epsilon_n \leq f \leq f_n + \epsilon_n
    \end{equation}
    so the upper and lower integrals of $f$ satisfy 
    \begin{equation}
      \int_a^b (f_n - \epsilon_n) \,dx \leq \int_{\bar{a}}^b f(x) \,dx \leq \int_a^{\bar{b}} f(x) \,dx \leq \int_a^b (f_n + \epsilon_n) \,dx
    \end{equation}
    Hence, we have 
    \begin{equation}
      0 \leq \int_a^{\bar{b}} f(x)\,dx - \int_{\bar{a}}^b f(x) \,dx \leq 2 \epsilon_n (b - a) 
    \end{equation}
    and taking $n \to \infty$ sets $\epsilon_n \to 0$. 
  \end{proof}

  \begin{corollary}[Series Function May be Integrated Term by Term]
    If $f_n \in \mathcal{R}([a, b])$, and $f$ is defined 
    \begin{equation}
      f(x) \coloneqq \sum_{n=1}^\infty f_n (x) 
    \end{equation}
    with the series converging uniformly on $[a, b]$, then 
    \begin{equation}
      \int_a^b f(x) \,dx = \sum_{n=1}^\infty \int_a^b f_n (x)\,dx
    \end{equation}
  \end{corollary}

  Now we move onto differentiation. 

  \begin{theorem}[Limits of Derivatives]
    Suppose $(f_n)$ is a sequence of differentiable functions on $[a, b]$. If 
    \begin{enumerate}
      \item there exists some $x_0 \in [a, b]$ such that $(f_n (x_0))_n$ converges, and 
      \item $(f_n^\prime) \to f^\prime$\footnote{At this point, $f^\prime$ is just notation since $f$ isn't even defined.} on $[a, b]$, 
    \end{enumerate}
    Then $(f_n)$ converges uniformly to a function $f$, with 
    \begin{equation}
      f^\prime (x) = \lim_{n \to \infty} f_n^\prime (x) 
    \end{equation}
  \end{theorem} 
  \begin{proof}
    Choose $N \in \mathbb{N}$ s.t. for $n, m \geq N$, the following hold 
    \begin{equation}
      |f_n (x_0) - f_m (x_0)| < \frac{\epsilon}{2}, \qquad |f_n^\prime (t) - f_m^\prime (t)| < \frac{\epsilon}{2 (b - a)}
    \end{equation}
    which is possible due to convergence of $f_n (x_0)$ and uniform convergence of the derivative. Then applying the mean value theorem to the function $(f_n - f_m)$ gives 
    \begin{equation}
      |f_n (x) - f_m (x) - f_n (t) + f_m (t)| = (f_n - f_m)^\prime (c) |x - t| 
    \end{equation}
    for any $x, t \in [a, b]$ and $c \in (x, t)$. However, $(f_n - f_m)^\prime$ is bounded, so 
    \begin{equation}
      |f_n (x) - f_m (x) - f_n (t) + f_m (t)| \leq \frac{|x - t| \epsilon}{2 (b - a)} \leq \frac{\epsilon}{2}
    \end{equation}
    and therefore we can use the triangle inequality to get 
    \begin{align}
      |f_n (x) - f_m (x)| & \leq |f_n (x) - f_m (x) - f_n (x_0) + f_m (x_0)| + | f_n (x_0) - f_m (x_0)| \\
                          & \leq \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon
    \end{align}
    and so $f_n$ is uniformly Cauchy $\iff$ $(f_n)$ converges uniformly. 

    To show the equality of the limit, we fix a point $x \in [a, b]$ and define 
    \begin{equation}
      \phi_n (t) = \frac{f_n (t) - f_n (x)}{t - x}, \qquad \phi(t) = \frac{f(t) - f(x)}{t - x} 
    \end{equation}
    for $t \in (a, b), t \neq x$. Then, 
    \begin{equation}
      \lim_{t \to x} \phi_n (t) = f_n^\prime (x) 
    \end{equation}
    and we can see from the above inequality that 
    \begin{equation}
      | \phi_n (t) - \phi(t) | \leq \frac{\epsilon}{2 (b - a)}
    \end{equation}
    for $n, m \geq N$. So we can conclude 
    \begin{equation}
      \phi(t) = \lim_{n \to \infty} \phi_n (t)
    \end{equation}
  \end{proof}
  \begin{proof}
    Here is an alternative shorter proof of the equality (but not uniform convergence of $f_n$!) where we assume $f_n \in C^1([a, b])$. 
    Let's call the limit of $f^\prime_n$ to be $g$ to avoid confusion. We know that since $f_n^\prime$ is continuous, it's integrable and by the fundamental theorem of calculus we have 
    \begin{equation}
      f_n (x) - f_n (0) = \int_0^x f_n^\prime (t) \,dx 
    \end{equation}
    Since $f_n^\prime \to g$ uniformly and $f_n^\prime \in C^0$, $g$ is continuous and so we can define the function
    \begin{equation}
      f(x) \coloneqq f(0) + \int_0^x g(t) \,dt
    \end{equation}
    But we can see that by uniform convergence, we can swap integrals  
    \begin{align}
      f(x) - f(0) & = \int_0^x g(t) \,dt \\ 
                  & = \int_0^x \lim_{n \to \infty} f_n^\prime (t) \,dt \\
                  & = \lim_{n \to \infty} \int_0^x f_n^\prime (t) \,dt \\ 
                  & = \lim_{n \to \infty} f_n (x) - f_n (0) 
    \end{align}
    which implies that $f(x) - f(0) = \lim_{n \to \infty} f_n (x) - f_n (0)$ and so $f(x) = \lim_{n \to \infty} f_n (x)$. But since $f^\prime$ is continuous, the function $f$ defined above is differentiable, with derivative $f^\prime (x)$ to be whatever function is in the integral, i.e. $g(x)$. So
    \begin{equation}
      f^\prime (x) = g(x) = \lim_{n \to \infty} f_n^\prime (x)
    \end{equation}
  \end{proof}

\subsection{Equicontinuous Families} 

  Note that uniform convergence may not be met due to some counterexamples. In general, there are 3 ways that uniform convergence can fail to happen. 

  \begin{enumerate}
    \item \textit{Concentration}. Note that $x^n$ as $n \to \infty$ almost converges except at one point. 
    \item \textit{Translation}. Consider $f_n (x) = \sin(x - n)$. Then by increasing $n$ we are shifting it to $+\infty$. 
    \item \textit{Oscillation}. Consider $f_n (x) = \sin(nx)$. As $n$ increases the function oscillates widely. This is sort of like the worst.\footnote{It turns out that this is the same as (2) under the Fourier transform.} 
  \end{enumerate}

  We would like uniform convergence, so we want conditions to avoid lack of uniform convergence. Keep in mind to counterexamples. To avoid translation, work with compact space, or if not compact, have the functions decay uniformly. To avoid oscillation, we can bound the derivative, which is a restriction on each function $|f_n^\prime (x)| \leq M$. The Cauchy criterion is too much. To avoid going to infinity, just bound $f$: $|f_n (x)| \leq M$ for all $n \in \mathbb{N}, x \in X$. 

  The bounding of derivatives can be a bit strong. We aren't always working with differentiable functions, so we introduce a similar concept. 

  \begin{definition}[Equicontinuous Family]
    A family of functions $\mathcal{F}$ on $E$ is said to be \textbf{equicontinuous} if $\forall \epsilon > 0$ there exists a $\delta > 0$ s.t. 
    \begin{equation}
      |x - y| < \delta \implies |f(x) - f(y)| < \epsilon
    \end{equation}
    for all $x, y \in E, f \in \mathcal{F}$. 
  \end{definition}

  So this doesn't even depend on $f$. You can think of this as uniformly continuous for a class of functions that doesn't depend on $f$. The first class of equicontinuous functions you should know are those with bounded derivatives. 

  \begin{lemma}[Functions with Bounded Derivatives Are Equicontinuous]
    Fix $M \geq 0$. Then 
    \begin{equation}
      \mathcal{F}_n \coloneqq \{f : [0, 1] \to \mathbb{R} \mid |f^\prime (x)| \leq M \} 
    \end{equation}
    is an equicontinuous family. 
  \end{lemma}
  \begin{proof}
    For any $f \in \mathcal{F}$, the MVT $|f(x) - f(y)| = |f^\prime (c) (x - y)|$ for some $c \in (x, y)$. But since $f^\prime (c)$ is bounded by $M$, take $\delta = \epsilon/M$. 
  \end{proof}

  \begin{example}
    $\mathcal{F} = \{\sin(nx)\}_{n \in \mathbb{N}}$ is not equicontinuous on $[0, 1]$ since 
    \begin{equation}
      \bigg| \sin\Big( n \frac{\pi}{2n} \Big) - \sin \Big( n \frac{\pi}{n} \Big) \bigg| = 1
    \end{equation}
    for all $n$. So setting $x_n = \frac{\pi}{2n}, y_n = \frac{\pi}{n}$, we have $d(x_n, y_n) \to 0$ while $d(f(x_n), f(y_n)) \geq 1$. So this is not equicontinuous. 
  \end{example} 

  \begin{definition}[Pointwise Bounded]
    Given a sequence of functions $(f_n)$ over $E$, we say the sequence is \textbf{pointwise bounded} if it satisfies each of the equivalent conditions. 
    \begin{enumerate}
      \item There exists some function $\phi(x)$ s.t. $|f_n (x)| < \phi(x)$ for all $x \in E, n \in \mathbb{N}$. 
      \item For every $x \in E$, the sequence $(|f_n (x)|)_n$ is bounded. 
    \end{enumerate}
  \end{definition}

  \begin{definition}[Uniformly Bounded]
    Given a sequence of functions $(f_n)$ over $E$, we say the sequence is \textbf{uniformly bounded} if there exists some $M$ s.t. $|f_n (x)| \leq M$ for all $x \in E, n \in \mathbb{N}$. 
  \end{definition}

  Therefore, uniform boundedness is stronger, since this bound doesn't even depend on $x$. 

  \begin{lemma}[Uniform Boundedness Implies Pointwise Boundedness]
    Uniform boundedness implies pointwise boundedness. 
  \end{lemma}
  \begin{proof}
    Take $\phi(x) = M$. 
  \end{proof}

  We may wonder what the conditions are for the converse. The following theorem gives us those conditions. 

  \begin{theorem}[Conditions for Uniform Boundedness]
    If $K$ is compact, with 
    \begin{enumerate}
      \item $f_n$ is continuous on $K$ for each $n$
      \item $f_n$ pointwise bounded. 
      \item $f_n$ equicontinuous on $K$
    \end{enumerate}
    Then $\{f_n\}$ is uniformly bounded. 
  \end{theorem}
  \begin{proof}
    
  \end{proof}

  If $(f_n)$ is pointwise bounded on $E$ and $E_1$ is a countable subset of $E$, it is always possible to find a subsequence $(f_{n_k})$ that converges for every $x \in E_1$. As an intuitive example, suppose 
  \begin{enumerate}
    \item $(f_{2n} (q_1))$ converges 
    \item $(f_{3n} (q_2))$ converges 
    \item $(f_{5n} (q_3))$ converges 
    \item $(f_{7n} (q_4))$ converges 
  \end{enumerate}
  So combining the first two, we have that $(f_{6n}(q_i))$ converges for $i = 1, 2$. Continuing on, $(f_{30n} (q_i))$ converges for $i = 1, 2, 3$. But you can't do this infinitely. So if you want a single subsequence s.t. all the sequences converges, we can do 
  \begin{equation}
    f_2, f_6, f_{30}, f_{210}, f_{2310}, \ldots
  \end{equation}
  Since 
  \begin{enumerate}
    \item if you take out $f_2$, it is a subsequence of $(f_{3n})$ which converges for $q_2$, and 
    \item if you also take out $f_{6}$, it is a subsequence of $(f_{6n})$ which converges for $q_1, q_2$, and 
    \item if you also take out $f_{30}$, it is a subsequence of $(f_{30n})$ which converges for $q_1, q_2, q_3$
  \end{enumerate}
  so $f_{n_k} (q_i)$ converges for all $i$. Now let's formalize this argument. 

  \begin{lemma} 
    Let $(f_n)$ be a sequence of functions on $[0, 1]$ that's uniformly bounded. Let $\{q_m\}_{m=1}^\infty$ be a countable set of numbers in $[0, 1]$. Then $\exists$ a subsequence $(f_{n_k})$ for which $f_{n_k} (q_m)$ is convergent for all $m \in \mathbb{N}$. 
  \end{lemma}
  \begin{proof}
    Intuitively, if we find a sequence of functions, we want to look at each point---say $1$---and look at $(f_n (1))_n$. $(f_n (1))$ is bounded and so contains a convergent subsequence $(f_{n_k} (1))_k$. Now with this subsequence, we look at $(f_{n_k} (0))_k$ which is bounded and therefore $(f_{n_{k_j}}(0))_j$ converges, and $(f_{n_{k_{j}}}(1))_j$ must converge as a subsequence of convergent $(f_{n_k} (1))_k$. Now do this for all $q$'s, and we get a single subsequence that converges for all of them. 

    For ease of notation, let $f_{ij}$ denote the $j$th term of the $i$th subsequence. Then there exists $(f_{n, 1})_n$ s.t. $(f_{n, 1} (q_1))_n$ converges. Take a subsequence $f_{n, 2}$ of $f_{n, 1}$ s.t. $(f_{n, 2} (q_2))_n$ converges. Given $(f_{n, k})_n$, find a subsequence of it, called $(f_{n, k+1})_n$ for which $(f_{n, k+1} (q_{k+1}))_n$ converges. Now $(f_{n, n})_n$ is a subsequence of the original one ($n$th term of $n$th subsequence) for which $(f_{q, n})_n$ is eventually a subsequence of $(f_{n, j})_n$ for any fixed $j$. 
  \end{proof} 

  Now the Ascolli's theorem gives us conditions to get rid of translation, oscillations, and infinity. To prove the second statement, we will need a lemma, so we state it now, along with providing a neat trick for constructing sequences. 

  \begin{theorem}[Arzela-Ascolli's Theorem]
    We claim the following. 
    \begin{enumerate}
      \item If a sequence of continuous functions $f_n: [0, 1] \to \mathbb{R}$ (or more generally, over a compact set) converges uniformly, then they form an equicontinuous family. 
      \item If a sequence of functions $f_n: [0, 1] \to \mathbb{R}$ is equicontinuous and so uniformly bounded, then it has a uniformly convergent subsequence. 
    \end{enumerate}
  \end{theorem}
  \begin{proof}
    Assume $(f_n)$ is uniformly convergent. Then it is uniformly Cauchy. To prove equicontinuity, given a $\epsilon > 0$ we need to find a $\delta > 0$ for all the functions. Since $f_n$ is uniformly Cauchy, $\exists N$ s.t. if $n \geq N$, then 
    \begin{equation}
      \sup_{x \in [0, 1]} | f_n (x) - f_N (x) | < \epsilon/3
    \end{equation} 
    Consider the first $N$ functions $f_1, \ldots, f_N$. They are all continuous on a compact set and so uniformly continuous. So for each $f_i$, there exists a $\delta_i$ s.t. $|x - y| < \delta_i \implies |f_i (x) - f_i (y)| < \epsilon$. So take $\delta = \frac{1}{3} \min_i \delta_i > 0$. So for all $1 \leq i \leq N$, 
    \begin{equation}
      |x - y| < \delta \implies |f_i (x) - f_i (y)| < \epsilon/3
    \end{equation} 
    and for $n \geq N$, 
    \begin{equation}
      |f_n (x) - f_n (y)| \leq \underbrace{|f_n (x) - f_N (x)}_{< \epsilon/3} + \underbrace{f_N (x) - f_N (y)}_{< \epsilon/3} + \underbrace{f_N (y) - f_n (y)}_{< \epsilon/3} < \epsilon
    \end{equation} 
    For the second part, let $E = \mathbb{Q} \cap [0, 1]$. It is a good thing that $E$ is dense in $[0, 1]$. Let $(f_n)$ be an equicontinuous on $[0, 1]$ and uniformly bounded. Due to the lemma, there exists a $(f_{n_k})_k$ so that the $f_{n_k}$ converges pointwise on $E$ (since $E$ is countable). We will now use equicontinuity of $(f_{n_k})_k$ to prove it's uniformly Cauchy on $[0, 1]$, which will imply that it's convergent. To make notation easier we will call $f_{n_k} = g_k$. Let $\epsilon > 0$. Since $g_k$ is equicontinuous, $\exists \delta > 0$ s.t. 
    \begin{equation}
      |x - y| < \delta \implies |g_k (x) - g_k (y)| < \epsilon
    \end{equation} 
    Since $E = \{q_1, q_2, \ldots \}$ is dense in $[0, 1]$, $\{B_\delta (q_i) \}_{i=1}^\infty$ is an open cover of $[0, 1]$. Since $[0, 1]$ is compact, there exists a finite subcover 
    \begin{equation}
      [0, 1] \subset \bigcup_{j=1}^N B_{\delta} (q_{i_j})
    \end{equation}
    Since $(g_k (q_{i_j}))$ converges for each $1 \leq j \leq N$, there exists $M_j$ s.t. 
    \begin{equation}
      n, m \geq M_j \implies |g_n (q_{i_j} - g_m (q_{i_j}))| < \epsilon
    \end{equation}
    Take $M = \max_j M_j$. Now if $m, n \geq M$, given $x \in [0, 1]$ $\exists q_i$ with $1 \leq i \leq N$ so that $x \in B_\delta (q_i)$, and so 
    \begin{align}
      |g_n (x) - g_m (x)| & \leq \underbrace{| g_n (x) - g_n (q_i)|}_{< \epsilon} + \underbrace{| g_n (q_i) - g_m (q_i)|}_{< \epsilon} + \underbrace{|g_m (q_i) - g_m (x)|}_{< \epsilon} < 3\epsilon
    \end{align}
    where the first and third inequalities come from equicontinuity, and the middle come from convergence on $E$. So by setting $\delta/3$ we are done. 
  \end{proof}

  \begin{example}
    An important application is in the existence of minimizers/maximizers for optimization problems involving functions. To minimize 
    \begin{equation}
      J(f) = \int_0^1 \sqrt{1 + f^\prime (t)} \,dt 
    \end{equation}
    is the length of the curve of $f: [0, 1] \to \mathbb{R}$. To minimize the length of the curve, we must search over a set of functions. So to use EVT, you must know what the compact subsets of functions. 
  \end{example}

  Ascolli's theorem exactly characterizes these compact subsets. These compact subsets of function spaces is the closure of equicontinuous functions. 

  \begin{corollary}
    A set of functions $K \subset C([0, 1])$ is compact iff it is, under the supremum metric $\sup_{x \in [0, 1]}$, 
    \begin{enumerate}
      \item closed 
      \item bounded 
      \item equicontinuous
    \end{enumerate}
    The first two are needed for finite dimensions. The third condition is for function spaces. 
  \end{corollary} 

  \begin{theorem}[Contraction Mapping Theorem]
    Let $(X, d)$ be a metric space with $J: X \to X$ and let there exist $c < 1$ s.t.  
    \begin{equation}
      d(J(x), J(y)) \leq c d(x, y) \text{ for all } x, y \in X
    \end{equation}
    Then there exists a unique $x^\ast \in X$ s.t. $J(x^\ast) = x^\ast$. 
  \end{theorem}

\subsection{The Stone-Weierstrass Theorem} 

  The Stone-Weierstrass theorem is a bit more general, while the Weierstrass approximation theorem is for polynomials. 

  \begin{theorem}[Weierstrass Approximation Theorem]
    If $f \in C([a, b])$, there exists a sequence of polynomials $(p_n)$ that converges uniformly to $f$. 
  \end{theorem}

  \begin{lemma} 
    If $f: [0, 1] \to \mathbb{R}$ is continuously differentiable on $[0, 1]$, then 
    \begin{equation}
      \lim_{n \to \infty} \int_0^1 f(x) \sin(nx) \,dx = 0
    \end{equation} 
    This means that as $n \to \infty$, the integral becomes small. 
  \end{lemma}
  \begin{proof}
    
  \end{proof}

  Now we prove a stronger version. 

  \begin{theorem}
    For every $f \in C([0, 1])$, 
    \begin{equation}
      \lim_{n \to \infty} \int_0^1 f(x)\, \sin(nx) \,dx = 0
    \end{equation}
  \end{theorem}
  \begin{proof}
    Let $\epsilon > 0$. Then by the Weierstrass approximation theorem\footnote{aka, the set of polynomials is dense in the set of continuous functions with the supremum metric. Remember polynomial interpolation, which is for a finite number of points. This is a little different.}, there exists a polynomial $p: [0, 1] \to \mathbb{R}$ for which 
    \begin{equation}
      \sup_{x \in [0, 1]} | p(x) - f(x)| < \epsilon 
    \end{equation}
    Now consider 
    \begin{align}
      \bigg| \int_0^1 f(x) \, \sin(nx) \,dx \bigg| & \leq \bigg| \int_0^1 p(x) \sin(nx) \,dx \bigg| + \bigg| \int_0^1 (f(x) - p(x)) \sin(nx) \,dx \bigg| \\
                                                   & \leq \bigg| \int_0^1 p(x) \sin(nx) \,dx \bigg| + \epsilon \\
                                                   & \leq 2 \epsilon 
    \end{align}
    where the first inequality is the triangle inequality, the second is due to the Weierstrass approximation theorem, and the third is due to $p(x)$ being infinitely differentiable, and so by the lemma above it is $\leq \epsilon$. 

  \end{proof}

  \begin{theorem}
    
  \end{theorem}
  \begin{proof}
    Suppose for the sake of contradiction that $\sin(n_k x) \to g(x)$ uniformly for subsequence $(n_k)_k$. Then $g(x)$ must be continuous on $[0, 1]$. Then 
    \begin{equation} 
      \int_a^b g(x)^2 \,dx = \lim_{n \to \infty} \int_0^1 g(x) \sin(n_k x) \,dx = 0
    \end{equation}
    due to the theorem, which implies that $g = 0$. But since $g(nx) = \pm 1$ for some $x$ for all $n$, we have a contradiction. 
  \end{proof}

\subsection{Approximation of the Identity} 

  \begin{definition}[Approximation of the Identity]
    A family of functions $\{\varphi_\epsilon\}$ parameterized by $\epsilon$ is called an \textbf{approximation of the identity} if 
    \begin{enumerate}
      \item $\int_{-\infty}^{\infty} \varphi_\epsilon (y) \,dy = 1$ 
      \item $\lim_{\epsilon \to 0} \int_{|y| > \delta} \varphi_\epsilon (y) \,dy = 0$ for all $\delta > 0$ 
      \item $\varphi_\epsilon \geq 0$.\footnote{This condition is flexible, but it makes things a bit easier for now.}
    \end{enumerate}
  \end{definition}

  \begin{example}
    Consider the functions $f_\epsilon$ satisfying $f(-\epsilon) = f(\epsilon) = 0$, $f(0) = 1/\epsilon$, and everything in between linearly interpolated. Then this is an approximation of the identity. 
  \end{example}

  \begin{example}
    Take any $\varphi \geq 0$ s.t. $\int_{-\infty}^\infty \varphi = 1$ and define 
    \begin{equation}
      \varphi_\epsilon = \frac{1}{\epsilon} \varphi \bigg( \frac{x}{\epsilon} \bigg)
    \end{equation}
    which we can think of as squeezing the function horizontally to $0$ and making the amplitude very large. Then we see that 
    \begin{equation}
      \int_{-\infty}^{\infty} \varphi_\epsilon (y) \,dy = \int_{-\infty}^{\infty} \frac{1}{\epsilon} \varphi \bigg(\frac{y}{\epsilon} \bigg) = \int_{-\infty}^\infty \varphi(x) \,dx = 1
    \end{equation}
    Fix $\delta > 0$. Then 
    \begin{equation}
      \int_\delta^{\infty} \varphi_\epsilon (x) \,dx = \int_\delta^\infty \frac{1}{\epsilon} \varphi \bigg( \frac{x}{\epsilon} \bigg) \,dx = \int_{\delta/\epsilon}^\infty \varphi(y) \,dy \to 0 \text{ as } \epsilon \to 0
    \end{equation}
    Since $\delta$ is fixed, we have $\delta/\epsilon \to +\infty$ as $\epsilon \to 0$, and so this integral of the tail above converges to $0$.  
  \end{example} 

  The approximation of the identity (AoI) has an amazing property. 

  \begin{theorem}
    Let $\{\varphi_\epsilon\}$ be an AoI. Assume $f: \mathbb{R} \to \mathbb{R}$ is bounded and continuous. Then 
    \begin{equation}
      \lim_{\epsilon \to 0} \int_{-\infty}^\infty \varphi_\epsilon (y) \, f(y) \,dy = f(0)
    \end{equation}

    \begin{figure}[H]
      \centering 
      \caption{The triangle has area $1$. Now if you integrate the product of $\varphi_\epsilon (y)$ and $f(y)$, it's like taking the product and multiplying by $f$. But as $\epsilon \to 0$, the triangle's area is $1$, and at the end you just multiply by $f(0)$. } 
      \label{fig:aoi}
    \end{figure}
  \end{theorem}
  \begin{proof}
    We have 
    \begin{align}
      \bigg| \int_{-\infty}^\infty \varphi_\epsilon (y) \, f(y) \,dy - f(0) \bigg| & = \bigg| \int_{-\infty}^\infty \varphi_\epsilon (y) \, \big( f(y) - f(0) \big) \,dy \bigg| \\ 
                                                                                   & \leq \bigg| \int_{-\delta}^{\delta} \varphi_\epsilon (y) \big( f(y) - f(0) \big) \,dy \bigg| + \bigg| \int_{|y| > \delta} \varphi_\epsilon (y) \big( f(y) - f(0) \big) \,dy \bigg| \\ 
                                                                                   & \leq \sup_{y \in [-\delta, \delta]} |f(y) - f(0)| + 2 M \int_{|y| > \delta} \varphi_\epsilon (y) \,dy 
    \end{align}
    where the final step follows from the left integral is less than $\sup_{[-\delta, \delta]} |f(y) - f(0)|$, and for the right integral, we have $f(y) - f(0) \leq 2 \sup |f| \leq 2 M$. Since you don't know the limit exists, you take the limsup, 
    \begin{equation}
      \limsup_{\epsilon \to 0} \bigg| \int_{-\infty}^\infty \varphi_\epsilon (y) \, f(y) \,dy - f(0) \bigg| \leq \sup_{y \in [-\delta, \delta]} |f(y) - f(0)| \to 0 \text{ as } \delta \to 0
    \end{equation}
  \end{proof}

  \begin{corollary}[Convolution]
    If $\{f_\epsilon\}$ is an AoI with $f$ bounded and continuous and $x \in \mathbb{R}$, we have 
    \begin{equation}
      \lim_{\epsilon \to 0} \int_{-\infty}^\infty \varphi_\epsilon (y) \, f(x - y) \,dy  = f(x)
    \end{equation}
    This is called the \textbf{convolution} of $f$ with $\varphi_\epsilon$. 
  \end{corollary}

  \begin{definition}[Dirac Delta Function]
    $\varphi_\epsilon \to \delta_0$ as $\epsilon \to 0$, the Dirac delta function. This is a limit. 
  \end{definition}

  \begin{theorem}
    Consider the functions 
    \begin{equation}
      \varphi_n (x) = \begin{cases} 
        c_n (1 - x^2)^n & \text{ if } x \in [-1, 1] \\ 
        0 & \text{ else }
      \end{cases}, \qquad c_n = \bigg( \int_{-1}^1 (1 - x^2)^n \,dx \bigg)^{-1}
    \end{equation}
    Then, 
    \begin{equation}
      \int_{-1}^1 \varphi_n (x) = \int_{-\infty}^\infty \varphi_n (x) \,dx = 1
    \end{equation}
    so $\{\varphi_n\}$ is an approximation of the identity. 
  \end{theorem}
  \begin{proof}
    Note that $c_n$ is chosen such that $\int_{-\infty}^\infty \varphi_n (x) \,dx = 1$ and $\varphi_n \geq 0$. We want to show 
    \begin{equation}
      \int_{|x| > \delta} \varphi_n (x) \,dx \to 0 \text{ as } n \to \infty \text{ for all } \delta > 0
    \end{equation}
    We claim that $c_n \leq 10 \sqrt{n}$. since we wish to upper bound the multiplicative inverse of an integral, it suffices to lower bound the inverse---i.e. the integral itself. 
    \begin{align}
      \int_{-1}^1 (1 - x^2)^n \,dx = 2 \int_0^1 (1 - x^2)^n \,dx & \geq 2 \int_0^{1/\sqrt{n}} (1 - x^2)^n \,dx \\
                                                                 & \geq 2 \int_0^{1/\sqrt{n}} (1 - nx^2) \,dx 
    \end{align}
    where the last inequality follows from the binomial inequality $(1 - s)^n \geq 1 - sn$. Therefore, the final integral is now computable, so it equals 
    \begin{equation}
      '' = 2 \bigg(x - \frac{nx^3}{3}\bigg) \bigg|_0^{1/\sqrt{n}} = \frac{4}{3} \frac{1}{\sqrt{n}} \implies c_n \leq \bigg( \frac{4}{3} \frac{1}{\sqrt{n}})^{-1} < \sqrt{n}
    \end{equation}  
    Now we have 
    \begin{align}
      \int_{|x| > \delta} \varphi_n (x) \,dx & = 2 \int_\delta^{+\infty} \varphi_n (x) \,dx \\ 
                                             & = 2 \int_\delta^1 c_n (1 - x^2)^n \,dx \\ 
                                             & \leq 20 \sqrt{n} \int_\delta^1 (1 - x^2)^n \,dx \\ 
                                             & \leq 20 \sqrt{n} (1 - \delta^2)^n \to 0 \text{ as } n \to \infty 
    \end{align} 
    Note that we could have claim that the bound be $c_n \leq (10\sqrt{n})^{1000}$ and this would still be true. 
  \end{proof} 

  \begin{theorem}[Stone-Weierstrass Theorem]
    Let $f \in C([a, b])$. Then $\forall \epsilon > 0$, $\exists$ a polynomial $p$ s.t. 
    \begin{equation}
      d(f, p) \coloneqq \sup_{x \in [a, b]} d\big( f(x), p(x) ) < \epsilon
    \end{equation}
    This is equivalent to saying that $\mathbb{R}[x]$ is dense in $C([a, b])$, or that for any $f \in C([a, b])$, $\exists$ sequence $(p_n)$ of polynomials s.t. $p_n \to p$ uniformly on $[a, b]$. 
  \end{theorem}
  \begin{proof}
    By translation and dilation, it suffices to take $[a, b] = [-1, 1]$. This is because translation/dilations are automorphisms (?). It also suffices to consider only $f$ for which $f(-1) > 0$ and $f(1) = k$ for some number $k$. This is because we can always replace $f$ with $\Tilde{f}$ defined by 
    \begin{equation}
      \Tilde{f}(x) \coloneqq f(x) - \bigg( \frac{(1 + x) f(1) + (1 - x)f(-1)}{2} \bigg)
    \end{equation} 
    Let's extend $f$ by $0$ outside of $[-1, 1]$. $f$ is now a bounded continuous function $\mathbb{R}$ implying that $f$ is uniformly continuous. Then we can take the integral. 
    \begin{equation}
      \varphi_n (x) = \begin{cases} 
        \frac{(1 - x^2)^n}{\int (1 - x^2)^n \,dx} & \text{ if } x \in [-1, 1] \\ 
        0 & \text{ else} 
      \end{cases} 
    \end{equation}
    Since $f$ is bounded, uniformly continuous on $\mathbb{R}$, and since $\{\varphi\}$ is an approximation of the identity, 
    \begin{equation}
      \int_{-\infty}^\infty \varphi_n (t) f(x - t) \,dt \to f(x)
    \end{equation} 
    and so $p_n$ is defined on $[-\frac{1}{2}, \frac{1}{2}]$ with $p_n \to f$ uniformly. 
  \end{proof}

  Now we can use the exact same strategy to prove convergence of Fourier series. 

  \begin{definition}[$L^2$ Inner Product]
    The $L^2$ inner product is defined on $C([a, b])$ as 
    \begin{equation}
      \langle f, g \rangle \coloneqq \frac{1}{b - a} \int_a^b f(x) \overline{g(x)} \,dx 
    \end{equation}
    where $f, g$ are complex valued. 
  \end{definition}

  We know that orthonormal bases behave nicely. We present one particularly important one. 

  \begin{lemma} 
    The functions $\{e^{inx}\}_{n \in \mathbb{Z}}$ are orthonormal in $C([0, 2\pi])$.
  \end{lemma} 
  \begin{proof}
    We have for $n, m \in \mathbb{Z}$
    \begin{equation}
      \langle e^{inx}, e^{imx} \rangle = \frac{1}{2\pi} \int_0^{2\pi} e^{inx} \overline{e^{imx}} \,dx = \frac{1}{2\pi} \int_0^{2\pi} e^{inx} e^{-imx} \,dx = \frac{1}{2\pi} \int_0^{2\pi} e^{i(n-m) x} \,dx
    \end{equation}
    So if $n = m$, then $\langle f, g \rangle = 1$. If not, then 
    \begin{equation}
      \langle f, g \rangle = \frac{1}{2 \pi i (n - m)} e^{i (n - m) x} \bigg|_0^{2\pi} = 0 
    \end{equation}
    and we are done. 
  \end{proof}

  \begin{lemma}
    Define 
    \begin{equation}
      \varphi_N (x) = \sum_{k = -N}^{N} e^{i k x}
    \end{equation}
    Then, the family $\{\varphi_N\}_{N = 0}^\infty$ forms an AoI. This is called a \textit{generalized AoI}. 
  \end{lemma}

  With this, we can prove that any sufficiently smooth (i.e. $C^1$) functions can be approximated with Fourier series. 


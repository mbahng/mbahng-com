\section{Transformers} 

\subsection{Self-Attention Layer}

    While we have solved the bottleneck problem, this entire process is still sequential since every hidden decoder node requires us to know the previous hidden node. There are two sequential processes in the regular seq2seq attention model. 
    \begin{enumerate}
      \item The sequential encoding of the input sentence. 
      \item The sequential decoding of the output sentence. This unfortunately is not possible in transformers to parallelize. 
    \end{enumerate}

    We will focus on parallelizing the first part by temporarily forgetting about encoder-decoder models and just thinking about how to incorporate a good encoder with attention that is parallelizable \cite{vaswani2017attention}. This extension is quite simple. Let $\mathbf{w}_{1:n}$ be a sequence of words in vocabulary $\mathcal{V}$. The key here is that rather than inputs having key-values and outputs having queries, \textit{all} words are associated with a 3-tuple of key, value, query. 
    \begin{equation}
      \mathbf{x}_i \mapsto (\mathbf{q}_i, \mathbf{k}_i, \mathbf{v}_i)
    \end{equation}

    \begin{definition}[Standard Scaled Dot-Product Attention]
      If we keep the score function to be the dot-product, the derivations become quite simple. 
      \begin{enumerate}
        \item For each $\mathbf{w}_i$, let $\mathbf{x}_i = E \mathbf{w}_i$ be the word embedding (with $E \in \mathbb{R}^{d \times |\mathcal{V}|}$ the embedding matrix). 

        \item We transform each word embedding with the (learned) weight matrices $\mathbf{Q}, \mathbf{K}, \mathbf{V}$. 
        \begin{align*} 
          \mathbf{q}_i & = \mathbf{Q} \mathbf{x}_i \implies Q = \mathbf{Q} X \\
          \mathbf{k}_i & = \mathbf{K} \mathbf{x}_i \implies K = \mathbf{K} X \\
          \mathbf{v}_i & = \mathbf{V} \mathbf{x}_i \implies V = \mathbf{V} X 
        \end{align*}
        where $X = [x_1, \ldots, x_n] \in \mathbb{R}^{d \times n}$.  

        \item We compute pairwise similarities between keys and queries and normalize with the softmax to get the attention distribution for each word. 
          \begin{equation}
            \mathbf{e}_{ij} = \mathbf{q}_i^T \mathbf{k}_j , \;\;\; \boldsymbol{\alpha}_{ij} = \frac{\exp(\mathbf{e}_{ij})}{\sum_{j^\prime} \exp(\mathbf{e}_{i j^\prime})}
          \end{equation}

        \item Compute the output for each word as a weighted sum of values. 
          \begin{equation}
            \mathbf{o}_i = \sum_j \boldsymbol{\alpha}_{ij} \mathbf{v}_j 
          \end{equation}
      \end{enumerate}

      Ultimately, this can be parallelized into one matrix operation.\footnote{Note that in order to even do such a thing, we must know $n$ beforehand. This can be solved by simply fixing some maximum length, padding everything to be some null token after the end token, and masking all the null tokens to be $0$. More on masking later. } 

      \begin{equation} 
        \mathrm{Attention}(Q, K, V) = \mathrm{softmax} \bigg( \frac{Q K^T}{\sqrt{E_k}} \bigg) V
      \end{equation}

      where the softmax is done to each row.\footnote{We divide by $\sqrt{E_k}$ to stabilize the gradients since as dimensionality increases, the dot product between random vectors tend to get large, leading to large softmax inputs. You can simply compute the variance of two $d$-dimensional Gaussian vectors and see that their variances scales linearly with $d$. }\footnote{You can see that if we have simple dot-product similarity scores, then $E_k = E_v$, but this need not be true in general. We will explore other similiarity score in the next subsection. } 

      Therefore, when you do a forward pass on an attention layer with input $x$, you first get the query vector $q$, extract the attention-weighted values from the key-value dictionary, and then return the weighted sum of the values. To give explicit parameterizations using the query, key, value encoding matrices, we can write this as 
      \begin{equation} 
        \mathrm{Attention}(x \,;\, \mathrm{Q}, \mathrm{K}, \mathrm{V}) = \mathrm{softmax} \bigg( \frac{(\mathbf{Q}x)(\mathbf{K} x)^T}{\sqrt{E_k}} \bigg) (\mathbf{V} x)
      \end{equation}

      This will give us a vector $\mathbf{o}_{1:n}$ consisting of the encoded vectors for each word in the sentence, and best of all, this is parallelizable! 
    \end{definition}

    There are three problems however. 
    \begin{enumerate}
      \item This self-attention encoding does not account for the position/order of the words. Therefore, some positional embedding is needed. 
      \item Our plan is to stack this layer multiple times on top of each other. However, we are just composing linear maps ultimately, so some nonlinearity is needed. 
      \item To use self-attention in \textit{decoders}, as we will see later on, we don't want to have any attention on later parts of a sentence, so we need some way to \textit{mask} future words. 
    \end{enumerate}

    We will deal with the first two problems and address the third problem in the transformer archictecture. 

\subsection{Tokenization and Positional Embeddings}

  Given an input (or an output) $\mathbf{x}$, it must be tokenized into a sequence of tokens. This is a general preprocessing step that is done for any input, whether it be a sequence of words, a sequence of regions in an image, or a sequence of anything. The raw token data will be denoted $w_i$ for $i = 1, \ldots, n$.We can then embed these tokens into a vector space $x_i \in \mathbb{R}^d$. 

  As we will see later, attention does not have a way to discern the order of the input sequences. Therefore, we must add this positional information to the encoding. The most obvious way would be to simply concatenate the position of the token to the end with an index. 
  \begin{equation} 
    x_i \mapsto [x_i, i]
  \end{equation}
  However, this is not ideal since this tends to corrupt the embedding of the token. Instead, we can think of adding certain vectors representing components to the original embedding. 
  \begin{equation} 
    x_i \mapsto x_i + p_i
  \end{equation}
  Certain ways come to mind, such as simply letting $p_i$ be the vector of all $i$'s. This tends not to work in progress since the values of $i$ get too large and corrupts the embeddings too much.\footnote{A helpful Medium article \href{https://medium.com/@waelrashwan/demystifying-transformer-architecture-the-magic-of-positional-encoding-5fe8154d4a64}{here}} Normalizing the values of $i$ to be in $[0, 1]$ is disadvantageous because now the positional embedding $p_i$ is dependent on the length of the total input sequence. Therefore, we need two properties: 
  \begin{enumerate} 
    \item The positional encoding should be independent of the input sequence length. 
    \item The positional encoding shouldn't be too large that it corrupts the semantic meaning behind the original embedding. 
  \end{enumerate}
  It turns out that the sinusoidal function satisfies these properties. 

  \begin{definition}[Sinusoidal Position Embedding]
    Given the embeddings $x_i \in \mathbb{R}^{d}$, we can add a positional encoding to it by 
    \[x_i \mapsto x_i + p_i\]
    where the positional encoding is given by the vector where each component is defined as 
    \begin{equation} 
      (p_i)_j =  \begin{cases} 
          \sin \big( \frac{i}{10000^{2j/d}} \big) & \text{ if } j \text{ is even} \\ 
          \cos \big( \frac{i}{10000^{2j/d}} \big) & \text{ if } j \text{ is odd}
      \end{cases}
    \end{equation}
    where $i$ iterates through the tokens and $j$ iterates through the dimensions of the embedding. 
  \end{definition}

\subsection{Stacked Attention Layers and Multi-Head Attention}

  The second problem of introducing nonlinearities is quite simple. Once we have the output of the first self-attention layer $\mathbf{o}_{1:n} = [\mathbf{o}_1, \ldots, \mathbf{o}_n]$, we can just input each $\mathbf{o}_i$ through a small MLP to introduce nonlinearity before inputting it into the next self-attention layer. 

  \begin{center}
    \includegraphics[scale=0.4]{img/stacked_self_attention_layers.png}
  \end{center}

  Boom, problem solved. 

  Going back, if we want to look at the attention for token $x_i$, we want to look through all $q_i^T k_j$ for all $j$ and find out where it is high. But perhaps we want to focus on different $j$ for different reasons. The following example may illustrate why. 

  \begin{example}[Semantic and Syntactic Attention]
    Given the sentence \textit{I went to the bank and got some money.}, one type of attention may look at the semantic meaning of the words, such as associating \textit{bank} with \textit{money}. However, we may also want to look at the syntactic meaning of the words, such as associating \textit{went} with \textit{bank}. When we read sentences, we have different types of attention for different reasons, and so having multiple heads of attention may be useful. 
  \end{example}

  \begin{definition}[Multi-Head Attention]
    Therefore, let us construct multiple attention heads by defining multiple triplets of $(Q, K, V)$ matrices. This may be more computationally inefficient, so we simply scale down the size of these matrices from 
    \[Q \in \mathbb{R}^{E_q \times d}, K \in \mathbb{R}^{E_k \times d}, V \in \mathbb{R}^{E_v \times d}\] 
    to 
    \[Q_\ell \in \mathbb{R}^{E_q \times d/h}, K_\ell \in \mathbb{R}^{E_k \times d/h}, V_\ell \in \mathbb{R}^{E_v \times d/h}\] 
    where $h$ is the number of heads. We are essentially decreasing the size of the token embedding dimension in order to get more heads. We can then do attention on each head separately. 
    \begin{equation} 
      \mathrm{Attention}_\ell = \mathrm{Attention}(Q_\ell, K_\ell, V_\ell) = \mathrm{softmax} \bigg( \frac{Q_\ell K_\ell^T}{\sqrt{E_k/h}} \bigg) V_\ell
    \end{equation}
    and we can simply concatenate them together to get the final output. 
    \[\mathrm{MultiHead}(Q, K, V) = \mathrm{Concat}(\mathrm{Attention}_1, \ldots, \mathrm{Attention}_h) W^O\]
    where $W^O \in \mathbb{R}^{d \times E_v}$ is a learnable weight matrix that mixes these heads together with a final linear transformation.\footnote{There is a valid concern that these heads may all end up learning the same thing and may just converge onto the same thing. However, this is not what happens in practice.} This entire process is shown in Figure \ref{fig:multi_head_attention}. 
  \end{definition}

  \begin{figure}[H]
    \centering 
    \includegraphics[scale=1.5]{img/multi_head_attention.png}
    \caption{Diagram of multi head attention. } 
    \label{fig:multi_head_attention}
  \end{figure}

    With self-attention out of the way, the transformer architecture becomes quite simple. An overview of it is shown in Figure \ref{fig:transformer}. 

    \begin{figure}[H]
      \centering 
      \includegraphics[scale=1.3]{img/transformer.png}
      \caption{Transformer architecture. } 
      \label{fig:transformer}
    \end{figure}

    The encoder is quite simple. You take the input embedding and add the positional embeddings to get $\{x_i \in \mathbb{R}^d\}_{i=1}^n$. You then pass it through a multi-head self-attention layer, which has outputs of shape $E_v \times n$, and then pass it through a feed forward network, adding residual connections and normalization layers to help with training. You then repeat this process $N$ times, which gives out the encoded sequence $\mathbf{h}_{1:n} = [\mathbf{h}_1, \ldots, \mathbf{h}_n]$, now ready to be fed into the decoder.\footnote{You can see that to support iterating through $n$ times, $E_v$ should equal $d$.}

    The decoder has two different self-attention layers. First, we run the generated output sequence through a masked self-attention layer, which generates the hidden nodes $\mathbf{z}_{1:n} = [\mathbf{z}_1, \ldots, \mathbf{z}_n]$ representing the state of the currently decoded sentence. Again, we have some maximum output length to ensure that we are working with a fixed size, and manually mask all tokens after the current one to be $0$.

    Then, another \textbf{cross-attention} layer takes both $\mathbf{h}_{1:n}$ and $\mathbf{z}_{1:n}$ and with its trained $(\mathbf{K}, \mathbf{V}, \mathbf{Q})$, computes the key, value, and query matrices as 
    \begin{equation}
      K = \mathbf{K} \mathbf{h}_{1:n}, \;\;\; V = \mathbf{V} \mathbf{h}_{1:n}, \;\;\; Q = \mathbf{Q} \mathbf{z}_{1:n}, 
    \end{equation}
    now ready to be plugged into to the self-attention formulas, integrating both the inputs and the current output to generate the result. This again outputs another list of vectors, which are run through an MLP and then have another set of $(\mathbf{K}, \mathbf{V}, \mathbf{Q})$ matrices waiting for them. This makes sense, since we want to use the output sequence to query the input key-values and attend to the correct set of tokens. 
    
    The output of this is then passed through a feed forward network, with some residual connections and normalization, and finally a linear layer transforms the output dimensions to whatever is needed (e.g. size of the vocabulary, or number of classes). Once this is done, a new word is generated,\footnote{Note that we have not specified how to get the corresponding word given an embedding vector. This is not within the scope of these notes and are covered in my natural language processing notes.} and this word (along with all previous words) is now used as the new input to the decoder in place of the start token. This process is done until the stop token is generated by the decoder. Notice that we encode with a bidirectional model (no masking) and generated the target with a unidirectional model (masking). 

    Note that again, parallelization of the decoder is not possible in the transformer architecture. Additionally, you can see that more normalization layers and residual connections are needed to train efficiently. This is very important in practice. 

    Despite all its advantages, self-attention has quadratic runtime complexity with respect to the sequence length since we need to compute attention for all pairs of words. This is worse than the linear runtime complexity in RNNs.  

  \subsubsection{Masking}

    The final aspect we did not address is the masking. When we are training the transformer on a corpus of data, the decoder first computes self-attention on all the previous outputs first to get the query, and then takes in the output of the encoder self-attention layer as the keys and values. Then it does self-attention once more over these triplets, essentially doing a self-attention layer over the entire input and all tokens up to the current decoded output. 

    When training this model, we have access to the entire decoded output, and we want to make sure that we do not perform self-attention on any future words since it will most likely attend 100\% to the next word to generate the next word! This does not learn anything, so we artificially set the attention distribution for all future output words to be $0$. This is usually done by setting the attention scores to $-\infty$ (or more practically, a very negative number) which will result in $0$ after softmaxing.\footnote{Here is a nice explanation \href{https://stackoverflow.com/questions/58127059/how-to-understand-masked-multi-head-attention-in-transformer}{here}.}

\subsection{Practical Implementation}

  In here we go over the nitty gritty details that comes into implementing a transformer in \texttt{pytorch==2.3.0}. 

  \subsubsection{Key, Value, Query Matrices and In Projections}

    The first thing is that these key, value, query does not have to necessarily equal to the dimension embedding, which we will denote as $E$. One flexibility is that we don't necessarily need to set the dimensions of the keys, values, and queries the same. We can see in the constructor of the \texttt{torch.nn.MultiheadAttention} module that you can input your own dimensions for the keys and values, but queries must be the same as $E$. 
    \begin{lstlisting}
      def __init__(self, embed_dim, num_heads, dropout=0., bias=True, add_bias_kv=False, add_zero_attn=False,
                   kdim=None, vdim=None, batch_first=False, device=None, dtype=None) -> None:
         ... 
        self.embed_dim = embed_dim
        self.kdim = kdim if kdim is not None else embed_dim
        self.vdim = vdim if vdim is not None else embed_dim
        self._qkv_same_embed_dim = self.kdim == embed_dim and self.vdim == embed_dim
    \end{lstlisting}

    In fact, $K \in \mathbb{R}^{d_k \times E}$, $V \in \mathbb{R}^{d_v \times E}$, then $QK^T \in \mathbb{R}^{E \times d_k}$. Since this obviously leads to dimension mismatch problem when we multiply it with the matrix $V$, what we do is have an \textbf{in projection} layer that maps everything to dimension $E$. We can check this for the following. 
    \begin{equation}
      K_{proj} \in \mathbb{R}^{E \times d_k}, V_{proj} \in \mathbb{R}^{E \times d_v}, Q_{proj} \in \mathbb{R}^{E \times E}
    \end{equation}
    There are two ways to store these projection matrices, as shown in the constructor. 
    \begin{lstlisting}
      # in the constructor 
      ...
      self._qkv_same_embed_dim = self.kdim == embed_dim and self.vdim == embed_dim

      if not self._qkv_same_embed_dim:
        self.q_proj_weight = Parameter(torch.empty((embed_dim, embed_dim), **factory_kwargs))
        self.k_proj_weight = Parameter(torch.empty((embed_dim, self.kdim), **factory_kwargs))
        self.v_proj_weight = Parameter(torch.empty((embed_dim, self.vdim), **factory_kwargs))
        self.register_parameter('in_proj_weight', None)
      else:
        self.in_proj_weight = Parameter(torch.empty((3 * embed_dim, embed_dim), **factory_kwargs))
        self.register_parameter('q_proj_weight', None)
        self.register_parameter('k_proj_weight', None)
        self.register_parameter('v_proj_weight', None)
    \end{lstlisting}
    \begin{enumerate}
      \item If these shapes are different, then we store them in separate matrices as above. 
      \begin{lstlisting}
        att = nn.MultiheadAttention(embed_dim=50, num_heads=1, bias=False, kdim=30, vdim=40) 
        att.q_proj_weight.shape # torch.Size([50, 50])
        att.k_proj_weight.shape # torch.Size([50, 30])
        att.v_proj_weight.shape # torch.Size([50, 40])
      \end{lstlisting}

      \item If these shapes are the same, then we just store them in a $3E \times E$ matrix by concatenation them. 
      \begin{lstlisting}
        att = nn.MultiheadAttention(embed_dim=50, num_heads=1, bias=False) 
        att.in_proj_weight.shape # torch.Size([150, 50])
      \end{lstlisting}
    \end{enumerate}
    These conditions are asserted throughout the forward pass as well.  

  \subsubsection{Masking} 

    We multiply by a masking matrix. 

  \subsubsection{Computing Attention}

    First we reshape them so that they are batch first. 

    If \texttt{needs\_weights = True}, we also output the attention weights in addition to the output, but it is said that this degrades performance. It is by default true but should be set to false for small tasks. 

  \subsubsection{Forward Pass of MultiheadAttention}

    First, we should look at the main function that computes self-attention. We omit a large part of the code to focus on the relevant details. 

    \begin{lstlisting}
      # torch.nn.functional 
      def multi_head_attention_forward(
        query: Tensor,
        key: Tensor,
        value: Tensor,
        embed_dim_to_check: int,
        num_heads: int,
        in_proj_weight: Optional[Tensor],
        in_proj_bias: Optional[Tensor],
        bias_k: Optional[Tensor],
        bias_v: Optional[Tensor],
        add_zero_attn: bool,
        dropout_p: float,
        out_proj_weight: Tensor,
        out_proj_bias: Optional[Tensor],
        training: bool = True,
        ...
      ): 
      # first unsqueezes the input if it is not batched. 

      # look at the input dimensions and check that multiheads divide it evenly 
      #  
      assert embed_dim == embed_dim_to_check, \
          f"was expecting embedding dimension of {embed_dim_to_check}, but got {embed_dim}"
      if isinstance(embed_dim, torch.Tensor):
          # embed_dim can be a tensor when JIT tracing
          head_dim = embed_dim.div(num_heads, rounding_mode='trunc')
      else:
          head_dim = embed_dim // num_heads
      assert head_dim * num_heads == embed_dim, f"embed_dim {embed_dim} not divisible by num_heads {num_heads}"
      if use_separate_proj_weight:
          # allow MHA to have different embedding dimensions when separate projection weights are used
          assert key.shape[:2] == value.shape[:2], \
              f"key's sequence and batch dims {key.shape[:2]} do not match value's {value.shape[:2]}"
      else:
          assert key.shape == value.shape, f"key shape {key.shape} does not match value shape {value.shape}"

      # Computes in-projection, which is an affine map before doing attention. 
      # in_proj_weight = [W_q, W_k, W_v], in_proj_bias = [b_q, b_k, b_v] 
      # computes q = q * W_q + b_q, k = k * W_k + b_k, v = v * W_v + b_v
      if not use_separate_proj_weight:
        q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
      else:
        if in_proj_bias is None:
            b_q = b_k = b_v = None
        else:
            b_q, b_k, b_v = in_proj_bias.chunk(3)
        q, k, v = _in_projection(query, key, value, q_proj_weight, k_proj_weight, v_proj_weight, b_q, b_k, b_v)

      # prepare attention mask 
      # add bias along batch dimension  
      # more preparation with mask
      ... 
      # Now calculate attention
      if need_weights:
        # scale q_scale for the sqrt(E) division factor 
        B, Nt, E = q.shape
        q_scaled = q * math.sqrt(1.0 / float(E))

      if attn_mask is not None:
        # torch.baddbmm is a pybinded C function implementing matrix multiplication 
        # of form attn_mask + q_scaled @ k^T
        attn_output_weights = torch.baddbmm(attn_mask, q_scaled, k.transpose(-2, -1))
      else:
        # torch.bmm is also a pybinded C function q_scaled + k^T
        attn_output_weights = torch.bmm(q_scaled, k.transpose(-2, -1))
      ...
      # softmax it and then multiply it by V. 
      attn_output_weights = softmax(attn_output_weights, dim=-1)
      attn_output = torch.bmm(attn_output_weights, v)

      # final linear layer for more weightings. 
      attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len * bsz, embed_dim)
      attn_output = linear(attn_output, out_proj_weight, out_proj_bias)
      attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))

      # optionally average attention weights over heads
      attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)
      if average_attn_weights:
          attn_output_weights = attn_output_weights.mean(dim=1)

      if not is_batched:
          # squeeze the output if input was unbatched
          attn_output = attn_output.squeeze(1)
          attn_output_weights = attn_output_weights.squeeze(0)
      return attn_output, attn_output_weights
    \end{lstlisting}

    This is precisely the function that is called in the forward method of the \texttt{MultiheadAttention} module. 

  \subsubsection{Transformer}

    In the transformer, we can see that if we peek at the state dictionary, it composes of an encoder and a decoder, each with a certain number of attention layers. There are 6 attention layers each by default. 

    \begin{lstlisting}
      transformer = nn.Transformer()
      transformer.state_dict
      # output 
      <bound method Module.state_dict of Transformer(
        (encoder): TransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_feat
      ures=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048,
       bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512,
       bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_aff
      ine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_aff
      ine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=T
      rue)
        )
        (decoder): TransformerDecoder(
          (layers): ModuleList(
            (0-5): 6 x TransformerDecoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_feat
      ures=512, out_features=512, bias=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_feat
      ures=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048,
       bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512,
       bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_aff
      ine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_aff
      ine=True)
              (norm3): LayerNorm((512,), eps=1e-05, elementwise_aff
      ine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
              (dropout3): Dropout(p=0.1, inplace=False)
            )
          )
          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=T
      rue)
        )
      )>
    \end{lstlisting}


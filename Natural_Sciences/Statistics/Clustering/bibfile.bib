@article{1967macqueen,
  title={Some methods for classification and analysis of multivariate observations},
  journal={Berkeley Symp. on Math. Statist. and Prob},
  author={J. MacQueen},
  year={1967},
  url={https://api.semanticscholar.org/CorpusID:6278891}
}

@article{1938young,
	abstract = {Necessary and sufficient conditions are given for a set of numbers to be the mutual distances of a set of real points in Euclidean space, and matrices are found whose ranks determine the dimension of the smallest Euclidean space containing such points. Methods are indicated for determining the configuration of these points, and for approximating to them by points in a space of lower dimensionality.},
	author = {Young, Gale and Householder, A. S.},
	date = {1938/03/01},
	date-added = {2025-07-26 13:22:45 -0400},
	date-modified = {2025-07-26 13:22:45 -0400},
	doi = {10.1007/BF02287916},
	id = {Young1938},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {1},
	pages = {19--22},
	title = {Discussion of a set of points in terms of their mutual distances},
	url = {https://doi.org/10.1007/BF02287916},
	volume = {3},
	year = {1938},
	bdsk-url-1 = {https://doi.org/10.1007/BF02287916}
}

@article{1952torgerson,
	abstract = {Multidimensional scaling can be considered as involving three basic steps. In the first step, a scale of comparative distances between all pairs of stimuli is obtained. This scale is analogous to the scale of stimuli obtained in the traditional paired comparisons methods. In this scale, however, instead of locating each stimulus-object on a given continuum, the distances between each pair of stimuli are located on a distance continuum. As in paired comparisons, the procedures for obtaining a scale of comparative distances leave the true zero point undetermined. Hence, a comparative distance is not a distance in the usual sense of the term, but is a distance minus an unknown constant. The second step involves estimating this unknown constant. When the unknown constant is obtained, the comparative distances can be converted into absolute distances. In the third step, the dimensionality of the psychological space necessary to account for these absolute distances is determined, and the projections of stimuli on axes of this space are obtained. A set of analytical procedures was developed for each of the three steps given above, including a least-squares solution for obtaining comparative distances by the complete method of triads, two practical methods for estimating the additive constant, and an extension of Young and Householder's Euclidean model to include procedures for obtaining the projections of stimuli on axes from fallible absolute distances.},
	author = {Torgerson, Warren S. },
	date = {1952/12/01},
	date-added = {2025-07-26 13:25:02 -0400},
	date-modified = {2025-07-26 13:25:02 -0400},
	doi = {10.1007/BF02288916},
	id = {Torgerson1952},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {4},
	pages = {401--419},
	title = {Multidimensional scaling: I. Theory and method},
	url = {https://doi.org/10.1007/BF02288916},
	volume = {17},
	year = {1952},
	bdsk-url-1 = {https://doi.org/10.1007/BF02288916}}


@article{2000tenenbaum,
  author = {Joshua B. Tenenbaum  and Vin de Silva  and John C. Langford },
  title = {A Global Geometric Framework for Nonlinear Dimensionality Reduction},
  journal = {Science},
  volume = {290},
  number = {5500},
  pages = {2319-2323},
  year = {2000},
  doi = {10.1126/science.290.5500.2319},
  URL = {https://www.science.org/doi/abs/10.1126/science.290.5500.2319},
  eprint = {https://www.science.org/doi/pdf/10.1126/science.290.5500.2319},
  abstract = {Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations. The human brain confronts the same problem in everyday perception, extracting from its high-dimensional sensory inputs—30,000 auditory nerve fibers or 106 optic nerve fibers—a manageably small number of perceptually relevant features. Here we describe an approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set. Unlike classical techniques such as principal component analysis (PCA) and multidimensional scaling (MDS), our approach is capable of discovering the nonlinear degrees of freedom that underlie complex natural observations, such as human handwriting or images of a face under different viewing conditions. In contrast to previous algorithms for nonlinear dimensionality reduction, ours efficiently computes a globally optimal solution, and, for an important class of data manifolds, is guaranteed to converge asymptotically to the true structure.}
}

@article{2000roweis,
  author = {Sam T. Roweis  and Lawrence K. Saul },
  title = {Nonlinear Dimensionality Reduction by Locally Linear Embedding},
  journal = {Science},
  volume = {290},
  number = {5500},
  pages = {2323-2326},
  year = {2000},
  doi = {10.1126/science.290.5500.2323},
  URL = {https://www.science.org/doi/abs/10.1126/science.290.5500.2323},
  eprint = {https://www.science.org/doi/pdf/10.1126/science.290.5500.2323},
  abstract = {Many areas of science depend on exploratory data analysis and visualization. The need to analyze large amounts of multivariate data raises the fundamental problem of dimensionality reduction: how to discover compact representations of high-dimensional data. Here, we introduce locally linear embedding (LLE), an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs. Unlike clustering methods for local dimensionality reduction, LLE maps its inputs into a single global coordinate system of lower dimensionality, and its optimizations do not involve local minima. By exploiting the local symmetries of linear reconstructions, LLE is able to learn the global structure of nonlinear manifolds, such as those generated by images of faces or documents of text.}
}

@inproceedings{2003belkin,
  author = {Belkin, Mikhail and Niyogi, Partha},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {T. Dietterich and S. Becker and Z. Ghahramani},
  pages = {},
  publisher = {MIT Press},
  title = {Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering},
  url = {https://proceedings.neurips.cc/paper_files/paper/2001/file/f106b7f99d2cb30c3db1c3cc0fde9ccb-Paper.pdf},
  volume = {14},
  year = {2001}
}

@article{2006coifman,
  title = {Diffusion maps},
  journal = {Applied and Computational Harmonic Analysis},
  volume = {21},
  number = {1},
  pages = {5-30},
  year = {2006},
  note = {Special Issue: Diffusion Maps and Wavelets},
  issn = {1063-5203},
  doi = {https://doi.org/10.1016/j.acha.2006.04.006},
  url = {https://www.sciencedirect.com/science/article/pii/S1063520306000546},
  author = {Ronald R. Coifman and Stéphane Lafon},
  keywords = {Diffusion processes, Diffusion metric, Manifold learning, Dimensionality reduction, Eigenmaps, Graph Laplacian},
  abstract = {In this paper, we provide a framework based upon diffusion processes for finding meaningful geometric descriptions of data sets. We show that eigenfunctions of Markov matrices can be used to construct coordinates called diffusion maps that generate efficient representations of complex geometric structures. The associated family of diffusion distances, obtained by iterating the Markov matrix, defines multiscale geometries that prove to be useful in the context of data parametrization and dimensionality reduction. The proposed framework relates the spectral properties of Markov processes to their geometric counterparts and it unifies ideas arising in a variety of contexts such as machine learning, spectral graph theory and eigenmap methods.}
}

@article{2008maaten,
  author  = {Laurens van der Maaten and Geoffrey Hinton},
  title   = {Visualizing Data using t-SNE},
  journal = {Journal of Machine Learning Research},
  year    = {2008},
  volume  = {9},
  number  = {86},
  pages   = {2579--2605},
  url     = {http://jmlr.org/papers/v9/vandermaaten08a.html}
}

@misc{2018mcinnes,
  title={UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction}, 
  author={Leland McInnes and John Healy and James Melville},
  year={2020},
  eprint={1802.03426},
  archivePrefix={arXiv},
  primaryClass={stat.ML},
  url={https://arxiv.org/abs/1802.03426}, 
}

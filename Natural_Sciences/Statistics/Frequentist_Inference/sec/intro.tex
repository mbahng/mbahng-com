  In statistics, we are given some data $\mathcal{D} = \{x_i\}_{i=1}^n$. The simplest thing we can do is summarize this data by extracting some nice characteristics---for example, the mean. This is known as \textbf{descriptive statistics}. In \textbf{inferential statistics}, we have much stronger assumptions. We assume that that data are realizations of random variables following a joint probability distribution. Sometimes, we may assume that these \textbf{samples} are iid coming from $\mathbb{P}^\ast$, known as the \textit{true data generating distribution} (and sometimes known as the \textit{population} in survey statistics or causal inference). As the name suggests, we must infer from $\mathcal{D}$ what $\mathbb{P}$ is. This immediately raises some questions: How should we interpret the population? What are we inferring? And how does this process work? Let's establish this confusion with an example. 

  \begin{example}[Measurement Problem]
    Say we have a dataset consisting of real-valued measurements $x_1, \ldots, x_n$ to estimate some quantity $\theta$. We may try to summarize the mean of this data by computing 
    \begin{equation}
      \bar{x} = \frac{x_1 + x_2 + \ldots + x_n}{n} 
    \end{equation} 
    This seems so common and intuitive that we might forget why this specific formula works. Two nice properties are: 
    \begin{enumerate}
      \item It minimizes the sum of least squares 
      \begin{equation}
        \bar{x} = \argmin_{a} \sum_{i=1}^n (x_i - a)^2 
      \end{equation}

      \item The value $\bar{x}$ makes the sum of the residuals to be $0$. 
    \end{enumerate} 
    These two properties land on the level of descriptive statistics. They describe the mean as a reasonable descriptive measure of the center of the observations, but they cannot justify $\bar{x}$ as an estimate of the true value $\theta$ since no explicit assumption has been made connecting the observations $x_i$ with $\theta$. 

    To do inference, we can furthermore assume that the $x_i$ are observed values of $n$ independent random variables which have a common distribution depending on $\theta$. Which assumptions we make will determine which estimators are reasonable. Here are two cases in which means are not a reasonable estimate. 
    \begin{enumerate}
      \item We assume that $x_i = \theta + \epsilon_i$ where $\epsilon_i$ satisfies $\mathbb{P}(\epsilon_i < 0) = \mathbb{P}(\epsilon_i > 0)$. 

      \item \textit{Larger samples may not improve estimate}. If the $x_i$ turns out to have finite variance the variance of the mean is $\sigma^2 /n$. However, if the $x_i$'s have a Cauchy distribution, then the distribution of $\bar{x}$ is the same as $x$, so nothing is gained by taking more measurements. 
    \end{enumerate}
  \end{example}

  To answer the first question, the population is usually introduced as some finite true distribution of some quantity, but more often it is treated as an abstract data generating distribution. For example, say that we have a large barrel of grains, and we take a random sample of 100 grains and measure their weight. Though we can spend much more effort and time weighing every single grain in the barrel, for practical reasons we want to work with the sample. On the other hand, think of the distribution of facial features of humanity. We may assume that every time a human is born, we can think of it being sampled from some abstract distribution (specified by ``God''), and so even taking all humans in the world is still a sample of this population. 

\section{Statistical Decision Theory} 

\subsection{Statistical Models} 

  In probability, we \hyperref[prob-thm:pushforward]{implicitly define a probability space $(\Omega, \mathcal{F}, \mathbf{P})$} and work explicitly with the random variable $X: \Omega \to \mathcal{X}$, which represents the ``data before it is observed.'' Therefore, when introducing the definition of the statistical model, the random variable $X$ is always implicitly defined.  
  
  \begin{definition}[Statistical Model]
    Let $(\mathcal{X}, \mathscr{X})$ be a measurable space. 
    \begin{enumerate}
      \item A family of distributions $\mathcal{P}$ over $\mathcal{X}$ is called a \textbf{statistical model}.\footnote{Note that this is \textit{not} a probability space! For each $\mathbf{P} \in \mathcal{P}$, the triplet $(\mathcal{X}, \mathscr{X}, \mathbf{P})$ is a probability space.} It is sometimes written $(\mathcal{X}, \mathscr{X}, \mathcal{P})$ to make the sample space more explicit. 
      \item Usually, we accompany $\mathcal{P}$ with a \textbf{parameter space} $\Theta$, along with a surjective map $\theta \to \mathbf{P}_{\theta}$. 
    \end{enumerate}
  \end{definition} 

  \begin{example}
    Suppose we want to model the net worth of American adults. We can model it as $\mathcal{X} = \mathbb{R}$. Conventionally, we can just take the Borel $\sigma$-algebra $\mathcal{B}(\mathcal{X})$. As for the family of distributions, we have a few options. 
    \begin{enumerate}
      \item \textit{All}. We set $\mathcal{P}$ to be \textit{all} probability measures on $\mathbb{R}$. This is a huge family and has no obvious parameter space. 
      \item \textit{Gaussian with Fixed Variance}. We set $\mathcal{P}$ to be all Gaussian distributions with variance $1$. This is written 
        \begin{equation}
          X \sim N(\theta, 1) \text{ for } \theta \in \mathbb{R} = \Theta
        \end{equation}
      \item \textit{Gaussian}. We set $\mathcal{P}$ to be all Gaussian distributions. Then, we can write 
        \begin{equation}
          X \sim N(\mu, \sigma^2) \text{ for } (\mu, \sigma^2) \in \mathbb{R} \times [0, +\infty) = \Theta
        \end{equation}
    \end{enumerate}
  \end{example}

  The reason we want a parameter space is that we want to introduce some extra structure on $\mathcal{P}$, such as notions of orderings, metrics, or operations. Common choices of $\Theta$ are subsets of vector spaces $\mathbb{R}^n$ or function spaces such as $L^p$. Ideally, we want $\Theta$ to be a set that \textit{indexes} the set $\mathcal{P}$. Such models are called \textit{identifiable}. 

  \begin{definition}[Identifiability]
    A statistical model is \textbf{identifiable by $\Theta$} if there exists a bijection between $\mathcal{P}$ and $\Theta$. In this case, we call it a \textbf{statistical experiment} and write as shorthand $\mathcal{P} = \{ \mathbf{P}_\theta \colon \theta \in \Theta\}$.\footnote{Note that you can always create a trivial identification by setting $\Theta = \mathcal{P}$. However, this doesn't give us anything, and we would like to ideally exploit the structure of $\Theta$.}
  \end{definition} 

  Let's take a look at some models. 

  \begin{example}
    We write some common notations for shorthand. 
    \begin{enumerate}
      \item \textit{Random Variable}. Say $X$ is real-valued. Then, $X \sim N(\theta, 1) \text{ for } \theta \in \Theta$ is shorthand for 
        \begin{equation}
          \big( \mathcal{X} = \mathbb{R}, \mathscr{X} = \mathcal{B}(\mathbb{R}), \mathcal{P} = \{N(\theta, 1) \colon \theta \in \Theta\}  \big)
        \end{equation}

      \item \textit{Joint Independent Random Variables}. $X_1 \ldots X_n \overset{\text{iid}}{\sim} N(\theta, 1)$, $\theta \in \Theta$ is shorthand for 
        \begin{equation}
          \big( \mathcal{X} = \mathbb{R}^n, \mathscr{X} = \mathcal{B}(\mathbb{R}^n), \mathcal{P} = \{N(\theta, 1)^{\otimes n} \colon \theta \in \Theta\} \big)
        \end{equation}

      \item \textit{Regression}. Let $X \in [-1, 1]$ and $Y \in \mathbb{R}$. $Y = f(X) + \epsilon, \; \epsilon \sim N(0, 1)$ for $f \in \mathcal{F}$, where $\mathcal{F}$ is some class of functions. This is shorthand for 
        \begin{equation}
          \big( \mathcal{X} = [-1, 1] \times \mathbb{R}, \mathcal{B}([-1, 1] \times \mathbb{R}), \mathcal{P} = \{\mathbf{P}_{XY} \colon \mathbf{P}_X = U[-1, 1], P_{Y \mid X, f} = N(f(X), 1), f \in \mathcal{F} \}\big)
        \end{equation}
        In other words, it is the set of probability measures such that the $X$-marginal is uniform and the conditional distribution of $Y$ given $x$ is $N(f(x), 1)$. 

      \item \textit{Regression with Fixed Design}. Say that we have data $\{(x_i, y_i)\}_{i=1}^n$, and we are interested in the \textit{conditional distribution} $Y \mid X = x$. We can treat $x_i$'s as fixed and model the $Y_i$'s as being generated by the following: 
        \begin{equation}
          Y_i = f(x_i) + \epsilon_i, \qquad i = 1, \ldots, n
        \end{equation} 
        where $\epsilon_i \overset{\text{iid}}{\sim} N(0, 1)$ and $f \in \mathcal{F}$ for some function class $\mathcal{F}$. Therefore, to model the joint distribution, we have $\mathcal{X} = \mathbb{R}^n$ and $\mathscr{X} = \mathcal{B}(\mathbb{R}^n)$. Our statistical model is 
        \begin{equation}
          \mathcal{P} \coloneqq \bigg\{ \mathbf{P}_f = \bigotimes_{i=1}^n N \big( f(x_i), 1 \big) \colon f \in \mathcal{F} \bigg\}
        \end{equation}
    \end{enumerate}
  \end{example}

  Naturally, we want to work with densities for each measure in the model, but this requires the measure to be absolutely continuous w.r.t. another measure in order for us to take the Radon-Nikodym derivative. Therefore, the following definition is natural and sets up things nicely. 

  \begin{definition}[Dominated Families of Measures]
    When all distributions $\mathbf{P} \in \mathcal{P}$ are absolutely continuous w.r.t. measure $\mu$, then we say that the family $\mathcal{P}$ is \textbf{dominated (by $\mu$)}. 
  \end{definition} 

  In general, there are two types of models that we consider. 
  \begin{enumerate}
    \item Consider the model $(\mathcal{X}, \mathscr{X}, \mathcal{P})$. In a discrete statistical model, we consider at most countable $X$ and $\mathcal{P}$ dominated by the counting measure $c$. 
    \item Consider the model $(\mathcal{X}, \mathscr{X}, \mathcal{P})$ for some subset $X \subset \mathbb{R}^n$. In a \textit{continuous statistical model}, $\mathcal{P}$ is dominated by the Lebesgue measure over $(\mathcal{X}, \mathscr{X})$. 
  \end{enumerate}

  If a model is dominated by $\sigma$-finite measure $\mu$, then by the \hyperref[meas-def:rn-der]{Radon-Nikodym theorem}, we have a nonnegative measurable function $p = \frac{d \mathbf{P}}{d \mu}$ s.t. for all $A \in \mathscr{X}$, 
  \begin{equation}
    \mathbf{P}(A) = \int_A p \, d\mu
  \end{equation}
  Since we will often work with parameterized families $\mathbf{P}_\theta$, their density will be denoted $p(\cdot \mid \theta)$ or $p_\theta (\cdot)$.  

  \begin{example}[Discrete Models]
    Consider when $X = \{1, \ldots, 6\}$, $\mathscr{X} = 2^X$, and let's look at the probability measure, which is completely defined by 
    \begin{equation}
      \mathbf{P}(\{1\}) = \mathbf{P}(\{2\}) = \mathbf{P}(\{3\}) = \frac{1}{3}
    \end{equation}
    Then, the density is 
    \begin{equation}
      p_\theta (x) = \begin{cases} 
        \frac{1}{3} & \text{ if } x = 1, 2, 3 \\ 
        0 & \text{ if }x = 4, 5, 6
      \end{cases} 
    \end{equation}
    and we can verify for example that 
    \begin{align}
      \frac{2}{3} = \mathbf{P}(\{1, 2\}) & = \int_{\{1, 2\}} p_\theta (x) \, d c(x) \\ 
                                         & = p_\theta (1) c(\{1\}) + p_\theta (2) c(\{2\}) \\ 
                                         & = \frac{1}{3} \cdot 1 + \frac{1}{3} \cdot 1 = \frac{2}{3}
    \end{align}
  \end{example}


  \begin{example}[Absolutely Continuous Models] 

  \end{example}

  Note that we can also change the dominating measure $\mu$. 

  \begin{example}
    Consider the previous example but now consider the dominating measure $c^\prime = 2 c$. Then, the Radon-Nikodym derivative is 
    \begin{equation}
      \frac{d \mathbf{P}}{d c^\prime} (x) = \begin{cases} 
        \frac{1}{6} & \text{ if } x = 1, 2, 3 \\ 
        0 & \text{ if } x = 4, 5, 6
      \end{cases} 
    \end{equation}
    And there is nothing wrong with this since 
    \begin{align}
      \frac{2}{3} = \mathbf{P}(\{1, 2\}) & = \int_{\{1, 2\}} p_\theta (x) \, d c(x) \\ 
                                         & = p_\theta (1) c(\{1\}) + p_\theta (2) c(\{2\}) \\ 
                                         & = \frac{1}{6} \cdot 2 + \frac{1}{6} \cdot 2 = \frac{2}{3}
    \end{align}
  \end{example}

  So the choice of the dominating measure matters and actually influences our density function. However, there is clearly a canonical one: the counting measure and the Lebesgue measure. Furthermore, we can talk about half-discrete and half-continuous measure, where it isn't as obvious which dominating measure to choose. But note that since the sum of two $\sigma$-finite measures is $\sigma$-finite, such a construction is not that difficult. These measures do come up in practice, but you can worry about them when you actually do encounter them. 


  \begin{example}[Non-Identifiable Models]
    Most of the time, we will work with identifiable models, and it may seem like defining this surjective map $\theta$ is overcomplicating things. However, in machine learning, it is often the case that we work with non-identifiable models. Sometimes, we may have $\theta \neq \theta^\prime$ yet $\mathbf{P}_{\theta} = \mathbf{P}_{\theta^\prime}$. 
  \end{example}

  \begin{example}[Linear Redundancy]
    Consider the regression statistical model again, but now we consider the class of linear functions $\mathcal{F} = \{ f(x) = (\theta_1 + \theta_2)x \colon \theta_1, \theta_2 \in \mathbb{R} \}$, which leads to our model 
    \begin{equation}
      Y_i = (\theta_1 + \theta_2) x_i + \epsilon_i, \quad \epsilon_i \overset{\text{iid}}{\sim} N(0, 1) \qquad i = 1, \ldots, n
    \end{equation} 
    Our statistical model is not identifiable since $(\theta_1, \theta_2) = (1, 3)$ and $(\theta_1, \theta_2) = (2, 2)$ both give $(\theta_1 + \theta_2) x_i = 4 x_i$, and hence gives the same product measure 
    \begin{equation}
      (\theta_1, \theta_2) \mapsto \mathbf{P}_f = \bigotimes_{i=1}^n N \big( 4 x_i, 1 \big)
    \end{equation}
  \end{example}

  \begin{example}[Neural Networks]
    Let's look at something that is a bit less obvious. 
  \end{example}

\subsection{Statistics and Sufficiency}

  \begin{definition}[Statistic]
    A \textbf{statistic} is a measurable function $T: (\mathcal{X}, \mathscr{X}) \to (\mathcal{T}, \mathscr{T})$. 
  \end{definition}

  In particular, estimators are statistics.

  Any statistic generates a new model under the pushforward measure. But we may have broken identifiability. The idea of sufficiency is when does $T$ preserve all information on data? The key idea is that a statistic $T$ is sufficient if---once we know $T(X)$---the remaining randomness in $X$ tells us nothing further about which $\mathbf{P}_\theta$ generated the data. 

  \begin{definition}[Sufficient Statistic]
    Let $(\mathcal{X}, \mathscr{X}, \mathcal{P}, \Theta)$ be an identifiable model. A statistic $T$ is \textbf{sufficient for $\mathcal{P}$} if for all $A \in \mathscr{X}$, $\mathbf{P}_\theta (A \mid T)$ admits a \hyperref[prob-def:cond-exp]{version} that does not depend on $\theta$.\footnote{Some technical measure theory stuff: If $\mathcal{T}$ is nice (for example, Borel), then there exists a function $h_{A, \theta}: T \to \mathbb{R}$ s.t. $\mathbf{P}_{\theta} (A) = \mathbf{P}_{\theta} (A \cap T^{-1} (B)) = \int_B h_{A, \theta}(t) \,d\mathbf{P}_{\theta}^T (t), \quad t \mapsto \mathbf{P}_\theta (A \mid T = t)$. }
  \end{definition}

  Sufficiency is a property of how the parameter enters the distribution of the data. The same statistic may be sufficient for one model but another, and crucially depends on how the model is parameterized. 

  \begin{theorem}
    If $T$ is invertible, with $T^{-1}$ measurable, then $T$ is sufficient. 
  \end{theorem}
  \begin{proof}
    $\sigma(T) \subset X$. Take $A \in \mathscr{X}$. Then, $A = T^{-1} (T(A)) \in \sigma(T)$, which implies that $X \subset \sigma(T)$. Therefore, $X = \sigma(T)$. Therefore, 
    \begin{equation}
      \mathbb{E}_{\theta} [\mathbbm{1}_A \mid \sigma(T)] = \mathbbm{1}_A
    \end{equation}
  \end{proof}

  Therefore, sufficient statistics always exist, e.g. the identity map. This isn't interesting, but what is more interesting is whether $T$ destroys some information yet still is sufficient. The following theorem relates this in terms of the likelihood function. 

  \begin{definition}[Likelihood Function]
    Given a $\mu$-dominated identifiable model $(\mathcal{X}, \mathscr{X}, \mathcal{P}, \Theta)$, the function (for fixed $x \in X$)
    \begin{equation}
      L: \Theta \to [0, +\infty), \quad L(\theta) = p(x \mid \theta) 
    \end{equation}
    is called the \textbf{likelihood function}. 
  \end{definition}

  The theorem states that a statistic is sufficient if and only if the likelihood function can be factorized.  

  \begin{theorem}[Fisher-Neyman Factorization] 
    Consider a identifiable model $(\mathcal{X}, \mathscr{X}, \mathcal{P}, \Theta)$ dominated by a $\sigma$-finite measure $\mu$, with density $p(x \mid \theta) = \frac{d \mathbf{P}_\theta}{d \mu}$. Then, $T$ is sufficient iff 
    \begin{equation}
      p(x \mid \theta) = g(T(x), \theta) \, h(x)
    \end{equation}
    for measurable functions $g: (T, \mathcal{T}) \times \Theta \to [0, +\infty)$ and $h: X \to [0, +\infty)$. 
  \end{theorem}
  \begin{proof}
    In text. 
  \end{proof}

  \begin{example}[Sums of Two Dice] 
    Consider two dice rolls where $(\mathcal{X} = \{1, \ldots, 6\}^2, \mathscr{X} = 2^\mathcal{X})$ and statistic $T: X \to \mathbb{N}$ defined $T(x, y) = x + y$. We can think of $T$ as ``extracting'' information from the two dice rolls to their sum. We will consider two models. 
    \begin{enumerate}
      \item Let $\Theta = \{ \theta = (\theta_1, \ldots, \theta_6) \in \mathbb{R}^6 \colon \theta_1 + \ldots + \theta_6 = 1 \}$, and let $\mathbf{P}_{\theta}$ be the multinomial measure assigning $\mathbf{P}_{\theta} (\{k\}) = \theta_k$. Then, our model is the family $\mathcal{P} = \{ \mathbf{P}_{\theta} \otimes \mathbf{P}_{\theta} \colon \theta \in \Theta \}$, and $T$ is not a sufficient statistic. Consider the conditional probability 
        \begin{equation}
          \mathbf{P}_\theta^{\otimes 2}(\{(1, 6)\} \mid S = 7) = \frac{\mathbf{P}_{\theta} (\{1\}) \mathbf{P}_{\theta} (\{6\})}{\sum_{k=1}^6 \mathbf{P}_{\theta} (\{k\}) \mathbf{P}_{\theta} (\{7 - k\})} = \frac{\theta_1 \theta_6}{\sum_{k=1}^6 \theta_k \theta_{7 - k}}
        \end{equation}
        This is clearly dependent on $\theta$. 

      \item Let $\Theta = \mathbb{R}$ and $\mathbf{P}_{\theta}$ be defined by the density 
        \begin{equation}
          p_\theta (x) = \frac{e^{\theta x}}{\sum_{k=1}^6 e^{\theta k}} 
        \end{equation}
        Note that $\theta = 0$ gives a fair dice, $\theta > 0$ biases towards higher faces, and $\theta < 0$ biases towards lower faces. Then, our model is the family $\mathcal{P} = \{ \mathbf{P}_{\theta} \otimes \mathbf{P}_{\theta} \colon \theta \in \Theta \}$, and $T$ is a sufficient statistic. The joint density is 
        \begin{equation}
          p(x, y \mid \theta) = p(x \mid \theta) p(y \mid \theta) = \frac{e^{\theta x}}{\sum_{k=1}^6 e^{\theta k}} \frac{e^{\theta y}}{\sum_{k=1}^6 e^{\theta k}} = \underbrace{\frac{e^{\theta (x + y)}}{\big( \sum_{k=1}^6 e^{\theta k} \big)^2 }}_{g(x + y, \theta)} \cdot \underbrace{1}_{h (x, y)}
        \end{equation}
        and so by the Fisher-Neyman Factorization, $T$ is sufficient. 
    \end{enumerate}

    Both models are identifiable, but $T$ behaves differently. Note that the first model is a vastly larger model in which no reduction beyond the full data is possible, and so $T$ is not able to capture this well enough. On the other hand, the second model is indexed by a scalar parameter. 
  \end{example}

  \begin{question}
    What does it mean for Model 1 to be nonparameteric? 
  \end{question}

  \begin{definition}[Minimal Sufficient Statistic]
    A sufficient statistic $T$ is \textbf{minimal sufficient} for an experiment $(\mathcal{X}, \mathscr{X}, \mathcal{P}, \Theta)$ if for any other sufficient $S$, it satisfies $\sigma(T) \subset \sigma(S)$ modulo $\mathbf{P}_\theta$-null sets. 
    \begin{equation}
      \sigma(T) \subset \sigma(\sigma(S) \cup N), \quad N = \{A \in \mathscr{X} \colon \mathbf{P}_{\theta}(A) = 0 \; \forall \theta \in \Theta\} 
    \end{equation}
  \end{definition}

  That is, minimal sufficient statistics partition the sample space into the coarsest equivalence classes that preserve all information about $\theta$. Another way to think about it is that a sufficient statistic $T$ is minimal sufficient if it is a function of every other sufficient statistic. 

  \begin{theorem}
    Let $(\mathcal{X}, \mathscr{X}, \mathcal{P}, \Theta)$ be a dominated model. A statistic $T$ is minimal sufficient iff 
    \begin{equation}
      T(x) = T(x^\prime) \iff \theta \mapsto \frac{p(x \mid \theta)}{p(x^\prime \mid \theta)} \text{ is constant}
    \end{equation}
   $\mathbf{P}_{\theta}$-a.s. for all $\theta \in \Theta$. 
  \end{theorem}
  \begin{proof}
    
  \end{proof}

  \begin{example}[Minimal Sufficiency for Uniform]
    Let $X_1, \ldots, X_n \sim U(0, \theta)$ for $\theta > 0$. Then, $X_{(n)} = \max\{X_1, \ldots, X_n\}$ is minimal since 
    \begin{equation}
      x \mapsto \frac{p(x \mid \theta)}{y \mid \theta} = \frac{\mathbbm{1}\{x_{(n)} \leq \theta\}}{ \mathbbm{1} \{y_{(n)} \leq \theta\} }
    \end{equation}
    is constant a.e. iff $y_{(n)} = x_{(n)}$. 
  \end{example}

  \begin{question}
    Page 4 of Casella. How does estimator, estimand relate to statistic? What does observable actually mean? How are hypothesis testing, point estimation, density estimation, etc realized under this framework? 
  \end{question} 

  \begin{definition}[Complete Statistic]
    A \textbf{statistic} $T: (\mathcal{T}, \mathscr{T})$ is \textbf{complete} for statistical model $\mathcal{P}$ if for all $\sigma(T)$-measurable random variables $U$, it holds that 
    \begin{equation}
      \forall \mathbf{P} \in \mathcal{P}, \; \mathbb{E}_{\mathbf{P}}[U] = 0 \implies \forall \mathbf{P} \in \mathcal{P}, \; U = 0 \; \mathbf{P}-\text{a.s.}
    \end{equation}
    This says that $T$ is allowed to vary with the data, but while it varies, it doesn't carry additional information about what generated the data. 
  \end{definition}

  \begin{theorem}[Bahadur]
    If $T: (\mathcal{X}, \mathscr{X}) \to (\mathcal{T}, \mathscr{T})$ is complete and sufficient for $\mathbf{P}$, then $T$ is minimal sufficient. 
  \end{theorem}
  \begin{proof}
    Let $T$ be such a statistic and let $S$ be another sufficient statistic. We wish to show that $\sigma(T) \subset \sigma(S)$ mod $\mathbf{P}_{\theta}$-null sets for all $\theta \in \Theta$. Fix $B in \sigma(T)$ and define 
    \begin{equation}
      H_B \coloneqq \mathbb{E}_{\theta}[ \mathbbm{1} \mid \sigma(S)] 
    \end{equation} 
    which clearly satisfies $0 \leq H_B \leq 1$. Let $U = \mathbb{E}_{\theta} [H_B \mid \sigma(T)] - \mathbbm{1}_B$. 
  \end{proof}

  So in a sense, completeness is stronger than minimality. 

  \begin{definition}[Ancillary]
    A statistic $V: (\mathcal{X}, \mathscr{X})  \to (\mathbf)$
  \end{definition}

  This connects to completeness in a very nice way, but first let's give an example. 

  \begin{example}
    Consider uniform. 
  \end{example}

  \begin{theorem}[Basu]
    Consider a statistical model $(\mathcal{X}, \mathscr{X}, \mathcal{P}, \Theta)$. If $T: (\mathcal{X}, \mathscr{X}) \to (\mathcal{T}, \mathscr{T})$ is complete and sufficient and $V$ is ancillary, then $V \perp T$ under $\mathbf{P}_{\theta}$ for all $\theta \in \Theta$. 
  \end{theorem}
  \begin{proof}
    
  \end{proof}

  \begin{example}
    Is a geometric intuition of being an orthogonal decomposition in $L^2$ accurate? 
  \end{example}

\subsection{Model Equivalence} 

  While a sufficient statistic allows us to reduce our model to a simpler one, we may want to look for maximal compression. 

  \begin{definition}[Observationally Equivalent]
    If we have two models $M_1 = (\mathcal{X}, \mathscr{X}, \mathcal{P}, \Theta), M_2 = (Y, \mathscr{Y}, \mathcal{Q}, \Theta)$, they are \textbf{observationally equivalent} if there exists sufficient statistics $T: (\mathcal{X}, \mathscr{X}) \to (T, \mathcal{T}), S: (\mathcal{Y}, \mathscr{Y}) \to (T, \mathcal{T})$ such that 
    \begin{equation}
      \mathbf{P}_{\theta, T} (A) = \mathbf{Q}_{\theta, S} (A) \quad \text{ for all } A \in \mathcal{T}, \mathbf{P}_{\theta} \in \mathcal{P}, \mathbf{Q}_{\theta} \in \mathcal{Q}
    \end{equation}
  \end{definition} 

  \begin{example}
    Let $X = (\mathcal{X}_1, \ldots, X_n) \sim N(\mu, 1)$ iid. Let $Y \sim N(\mu, \frac{1}{n})$. Then, 
    \begin{equation}
      T(\mathcal{X}) = \frac{1}{n} \sum_{i=1}^n X_i
    \end{equation}
  \end{example}

  But what if we wanted to go from $Y$ back to $X$? This leads us to the idea of simulation equivalence, which is informally observationally equivalent under sufficient statistics ``with additional randomness.'' e.g. if we set $\Tilde{X} = Y - Z_i$ for $Z_i \sim N(0, 1)$ iid. 

\subsection{Exponential Families} 

  A nice property is that basically a family of models where completeness is easy to verify. 

  \begin{definition}[Exponential Family]
    A collection of probability measures $\mathcal{P} = \{\mathbf{P}_\theta \colon \theta \in \Theta\}$ is an \textbf{exponential family} if it is $\mu$-dominated for some $\sigma$-finite $\mu$, and the density is of the following form 
    \begin{equation}
      p(x \mid \theta) \coloneqq \frac{d \mathbf{P}_\theta}{d \mu} (x) = \exp \big( \eta^T (\theta) T(x) - B(\theta) \big) \, h(x)
    \end{equation}
    where $T: \mathcal{X} \to \mathbb{R}^k$ is measurable, $h: \mathcal{X} \to [0, +\infty)$ is measurable, $\eta: \Theta \to \mathbb{R}^k$ is any function (not necessarily measurable), and $B : \Theta \to \mathbb{R}$ is any function. 
  \end{definition}

  Note that this can change if $\mu$ changes. So really, this should be an exponential family \textit{with respect to a dominating measure}. Then, every statement and example we talk about is always with respect to this measure $\mu$. Usually, we will talk about with respect to counting measure or Lebesgue measure.  

  Exponential families have a sufficient statistic that is ``natural'' in the sense that $T$ is always a sufficient statistic. Second, if we have iid observations with model $\mathcal{P}^n = \{\mathbf{P}^{\otimes n} \colon \theta \in \Theta\}$, then $\sum_{i=1}^n T(x_i)$ is sufficient. This makes sense intuitively since the product of these densities is still exponential (since the product becomes a sum in the exponent). Then use Fisher-Neyman factorization. 

  \begin{definition}
    An exponential family is \textbf{canonical} if if $\Theta \subset \mathbb{R}^k$ and $\eta (\theta) = \theta$. We consider 
    \begin{equation}
      \mathcal{H} \coloneqq \{\eta \in \mathbb{R}^k \colon \int e^{\eta^T T(x)} h(x) \, d\mu(x) < \infty \}
    \end{equation}
    the natural, or canonical, parameter space. 
  \end{definition}

  \begin{definition}
    We call a canonical exponential family \textbf{full rank} if $\mathcal{H}$ contains an open subset. 
  \end{definition}

  Note that being canonical really limits what you can consider as a dominating measure. So the $\mathcal{H}$ really does a lot. 

  \begin{theorem}
    Let $\mathcal{P} = \{\mathbf{P}_{\eta} \colon \eta \in \mathcal{H} \}$ by a canonical exponential family of full rank. Then, $T$ is complete and sufficient. 
  \end{theorem}
  \begin{proof}
    This is mysterious, but it's a very elegant proof. A few notes about this proof. Suppose that I can write (which is not always correct) 
    \begin{equation}
      p(t \mid \eta) = e^{\eta^T t - B(\eta)} 
    \end{equation}
    Then, $\mathbb{E}_{\eta} g(T) = 0$ for all $\eta$ implies that $g(T) = 0$ a.s. Then, we can look at the Laplace transform 
    \begin{equation}
      \int g(t) e^{\eta^T t - B(\eta)} \, dv(t) = 0 \text{ for all } \eta 
    \end{equation}
    But since $B$ is independent of the integral, it should mean that 
    \begin{equation}
      \int e^{\eta^T t} \, \underbrace{g(t) d\nu(t)}_{d \nu^\prime (t)} = 0
    \end{equation}
    Then $g(t) = 0$ for all $\nu$ almost all $t$. 
  \end{proof}

  Note that completeness and sufficiency are independent. Only when they come together, we get minimal statistic. 

  What if we have $\{\mathbf{P}_{\theta} \colon \theta \in \Theta\}$ not in canonical form? If $\eta$ is injective, then there is hope. Define $\Xi = \eta(\Theta)$ and define the reparameterized family 
  \begin{equation}
    \{\mathbf{Q}_\xi \colon \xi \in \Xi \}, \quad \mathbf{Q}_{\xi} \coloneqq \mathbf{P}_{\eta^{-1}(\xi)} 
  \end{equation} 

  \begin{example}
    Make sure to verify the examples of exponential families: Poisson, Gaussian, etc. 
  \end{example}

\subsection{Decision Problems}

  \begin{definition}
    A \textbf{decision space} is a measurable space $(\mathcal{D}, \mathscr{D})$. 
  \end{definition}

  \begin{example}
    A \textbf{decision rule} is a statistic taking values in a decision space, i.e. a measurable function $\delta : X \to (D, \mathcal{D})$ 
  \end{example}

  \begin{definition}[Loss Function]
    Given identifiable model, the \textbf{loss function} is a function $L: \Theta \times D \to \mathbb{R}$ where the map $d \mapsto L(\theta, d)$ is measurable. 
    $\theta: \mathbf{P} \to \Theta$ satisfies the property that $\theta(\mathcal{P}) = \theta(\mathcal{Q}) \implies \mathbf{P} = \mathbf{Q}$. 
  \end{definition}

  \begin{definition}
    Given identifiable model $M$, decision space $(\mathcal{D}, \mathscr{D})$, and a loss function $L: \theta \times \mathcal{D} \to \mathbb{R}$, the \textbf{risk} of the 
    \begin{equation}
      R(P, \delta) = \int L(\theta(\mathbf{P}), \delta(x)) \, d\mathbf{P}(x), \quad R(\theta, \delta) = \int L(\theta , \delta(x)) \,d\mathbf{P}_{\theta(x)} \text{ for } \theta \in \Theta
    \end{equation}
  \end{definition}

  The two main questions are: 
  \begin{enumerate}
    \item Which decision functions $\delta$ are good? 
    \item Which models are better? When do we prefer $(\mathcal{Y}, \mathscr{Y}, \mathbf{Q}, \Theta)$ over $(\mathcal{X}, \mathscr{X}, \mathbf{P}, \Theta)$.
  \end{enumerate}

  \begin{example}[Point Estimation]
    
  \end{example}

  \begin{example}[Hypothesis Testing]
    
  \end{example}

  \begin{example}[Confidence Sets]
    
  \end{example}

  \begin{example}[Density Estimation]
    
  \end{example}


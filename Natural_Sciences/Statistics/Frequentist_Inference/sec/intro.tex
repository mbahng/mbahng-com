  In statistics, we are given some data $\mathcal{D} = \{x_i\}_{i=1}^n$. The simplest thing we can do is summarize this data by extracting some nice characteristics---for example, the mean. This is known as \textbf{descriptive statistics}. In \textbf{inferential statistics}, we have much stronger assumptions. We assume that that data are realizations of random variables following a joint probability distribution. Sometimes, we may assume that these \textbf{samples} are iid coming from $\mathbb{P}^\ast$, known as the \textit{true data generating distribution} (and sometimes known as the \textit{population} in survey statistics or causal inference). As the name suggests, we must infer from $\mathcal{D}$ what $\mathbb{P}$ is. This immediately raises some questions: How should we interpret the population? What are we inferring? And how does this process work? Let's establish this confusion with an example. 

  \begin{example}[Measurement Problem]
    Say we have a dataset consisting of real-valued measurements $x_1, \ldots, x_n$ to estimate some quantity $\theta$. We may try to summarize the mean of this data by computing 
    \begin{equation}
      \bar{x} = \frac{x_1 + x_2 + \ldots + x_n}{n} 
    \end{equation} 
    This seems so common and intuitive that we might forget why this specific formula works. Two nice properties are: 
    \begin{enumerate}
      \item It minimizes the sum of least squares 
      \begin{equation}
        \bar{x} = \argmin_{a} \sum_{i=1}^n (x_i - a)^2 
      \end{equation}

      \item The value $\bar{x}$ makes the sum of the residuals to be $0$. 
    \end{enumerate} 
    These two properties land on the level of descriptive statistics. They describe the mean as a reasonable descriptive measure of the center of the observations, but they cannot justify $\bar{x}$ as an estimate of the true value $\theta$ since no explicit assumption has been made connecting the observations $x_i$ with $\theta$. 

    To do inference, we can furthermore assume that the $x_i$ are observed values of $n$ independent random variables which have a common distribution depending on $\theta$. Which assumptions we make will determine which estimators are reasonable. Here are two cases in which means are not a reasonable estimate. 
    \begin{enumerate}
      \item We assume that $x_i = \theta + \epsilon_i$ where $\epsilon_i$ satisfies $\mathbb{P}(\epsilon_i < 0) = \mathbb{P}(\epsilon_i > 0)$. 

      \item \textit{Larger samples may not improve estimate}. If the $x_i$ turns out to have finite variance the variance of the mean is $\sigma^2 /n$. However, if the $x_i$'s have a Cauchy distribution, then the distribution of $\bar{x}$ is the same as $x$, so nothing is gained by taking more measurements. 
    \end{enumerate}
  \end{example}

  To answer the first question, the population is usually introduced as some finite true distribution of some quantity, but more often it is treated as an abstract data generating distribution. For example, say that we have a large barrel of grains, and we take a random sample of 100 grains and measure their weight. Though we can spend much more effort and time weighing every single grain in the barrel, for practical reasons we want to work with the sample. On the other hand, think of the distribution of facial features of humanity. We may assume that every time a human is born, we can think of it being sampled from some abstract distribution (specified by ``God''), and so even taking all humans in the world is still a sample of this population. 

\section{Statistical Decision Theory} 

\subsection{Statistical Models} 

  In probability, we \hyperref[prob-thm:pushforward]{implicitly define a probability space $(\Omega, \mathcal{F}, \mathbf{P})$} and work explicitly with the random variable $X: \Omega \to \mathcal{X}$, which represents the ``data before it is observed.'' Therefore, when introducing the definition of the statistical model, the random variable $X$ is always implicitly defined.  
  
  \begin{definition}[Statistical Model]
    Given a measurable space $(\mathcal{X}, \mathscr{X})$, a \textbf{statistical model} is a collection of probability measures $\mathcal{P}$. The triple $(\mathcal{X}, \mathscr{X}, \mathcal{P})$ is called a \textbf{statistical experiment}.\footnote{Note that this is \textit{not} a probability space! For each $\mathbf{P} \in \mathcal{P}$, the triplet $(\mathcal{X}, \mathscr{X}, \mathbf{P})$ is a probability space.} 
  \end{definition} 

  Therefore, we abuse notation and talk about the following in shorthand: 
  \begin{enumerate}
    \item $X \sim N(\theta, 1)$ for $\theta \in [-1, 1]$, means the statistical model $(\mathbb{R}, \mathcal{B}(\mathbb{R}), \{N(\theta, 1) \colon \theta \in [-1, 1]\})$. 
    \item $X_1, \ldots, X_n \overset{\text{iid}}{\sim} N(\theta, 1)$ with $\theta \in \Theta$ is shorthand for the statistical model $(\mathbb{R}^n, \mathcal{B}(\mathbb{R}^n), \{N(\theta, 1)^{\otimes n} \colon \theta \in \Theta\})$. 
  \end{enumerate}

  Naturally, we want to work with densities for each measure in the model, but this requires the measure to be absolutely continuous w.r.t. another measure in order for us to take the Radon-Nikodym derivative. Therefore, the following definition is natural and sets up things nicely. 

  \begin{definition}[Dominated Families of Measures]
    When all distributions $\mathbf{P} \in \mathcal{P}$ are absolutely continuous w.r.t. measure $\mu$, then we say that the family $\mathcal{P}$ is \textbf{dominated (by $\mu$)}. 
  \end{definition} 

  In general, there are two types of models that we consider. 
  \begin{enumerate}
    \item Consider the model $(\mathcal{X}, \mathscr{X}, \mathcal{P})$. In a discrete statistical model, we consider at most countable $X$ and $\mathcal{P}$ dominated by the counting measure $c$. 
    \item Consider the model $(\mathcal{X}, \mathscr{X}, \mathcal{P})$ for some subset $X \subset \mathbb{R}^n$. In a \textit{continuous statistical model}, $\mathcal{P}$ is dominated by the Lebesgue measure over $(\mathcal{X}, \mathscr{X})$. 
  \end{enumerate}

  If a model is dominated by $\mu$, then by the \hyperref[meas-def:rn-der]{Radon-Nikodym theorem}, we have a nonnegative measurable function $p = \frac{d \mathbf{P}}{d \mu}$ s.t. for all $F \in \mathscr{X}$, 
  \begin{equation}
    \mathbf{P}(A) = \int_A p \, d\mu
  \end{equation}
  Since we will often work with parameterized families $\mathbf{P}_\theta$, their density will be denoted $p(\cdot \mid \theta)$ or $p_\theta (\cdot)$.  

  \begin{example}[Discrete Models]
    Consider when $X = \{1, \ldots, 6\}$, $\mathscr{X} = 2^X$, and let's look at the probability measure, which is completely defined by 
    \begin{equation}
      \mathbf{P}(\{1\}) = \mathbf{P}(\{2\}) = \mathbf{P}(\{3\}) = \frac{1}{3}
    \end{equation}
    Then, the density is 
    \begin{equation}
      p_\theta (x) = \begin{cases} 
        \frac{1}{3} & \text{ if } x = 1, 2, 3 \\ 
        0 & \text{ if }x = 4, 5, 6
      \end{cases} 
    \end{equation}
    and we can verify for example that 
    \begin{align}
      \frac{2}{3} = \mathbf{P}(\{1, 2\}) & = \int_{\{1, 2\}} p_\theta (x) \, d c(x) \\ 
                                         & = p_\theta (1) c(\{1\}) + p_\theta (2) c(\{2\}) \\ 
                                         & = \frac{1}{3} \cdot 1 + \frac{1}{3} \cdot 1 = \frac{2}{3}
    \end{align}
  \end{example}

  Note that we can also change the dominating measure $\mu$. 

  \begin{example}
    Consider the previous example but now consider the dominating measure $c^\prime = 2 c$. Then, the Radon-Nikodym derivative is 
    \begin{equation}
      \frac{d \mathbf{P}}{d c^\prime} (x) = \begin{cases} 
        \frac{1}{6} & \text{ if } x = 1, 2, 3 \\ 
        0 & \text{ if } x = 4, 5, 6
      \end{cases} 
    \end{equation}
    And there is nothing wrong with this since 
    \begin{align}
      \frac{2}{3} = \mathbf{P}(\{1, 2\}) & = \int_{\{1, 2\}} p_\theta (x) \, d c(x) \\ 
                                         & = p_\theta (1) c(\{1\}) + p_\theta (2) c(\{2\}) \\ 
                                         & = \frac{1}{6} \cdot 2 + \frac{1}{6} \cdot 2 = \frac{2}{3}
    \end{align}
  \end{example}

  \begin{question}
    What happens if we have a half-discrete and half-continuous measure? What is the dominating measure? This isn't as obvious, and then we're really in trouble if there is no canonical form. 
  \end{question}

  So the choice of the dominating measure matters and actually influences our density function. However, there is clearly a canonical one: the counting measure and the Lebesgue measure. 

  \begin{example}[Absolutely Continuous Models]
  \end{example}

  Often, the family $\mathcal{P}$ may be too abstract for us to work with, and we want to parameterize it by some functionals $\theta: \mathcal{P} \to \Theta$. This new space $\Theta$ has some extra structure that makes it easier to work with.  

  \begin{definition}[Parameter Space]
    A \textbf{parameter space} for the model $\mathcal{P}$ is a set $\Theta$, together with a map $\theta: \mathcal{P} \to \Theta$. 
  \end{definition}

  \begin{example}
    
  \end{example}

  \begin{definition}[Identifiability]
    A statistical model is \textbf{identifiable} by parameter space $\Theta$ if for all $\mathbf{P}, \mathbf{P}^\prime \in \mathcal{P}$, 
    \begin{equation}
      \theta(\mathbf{P}) = \theta(\mathbf{P}^\prime) \implies \mathbf{P} = \mathbf{P}^\prime
    \end{equation}
    If a model is identifiable, then we can index the distributions by the parameter value $\mathcal{P} = \{\mathbf{P}_\theta \colon \theta \in \Theta\}$, and write our experiment as the tuple $(\mathcal{X}, \mathscr{X}, \mathcal{P}, \Theta)$. 
  \end{definition}

  \begin{question}
    When do we ever work with statistical models that are not identifiable? 
  \end{question} 

\subsection{Statistics and Sufficiency}

  \begin{definition}[Statistic]
    A \textbf{statistic} is a measurable function $T: (\mathcal{X}, \mathscr{X}) \to (T, \mathcal{T})$. 
  \end{definition}

  In particular, estimators are statistics.

  Any statistic generates a new model under the pushforward measure. But we may have broken identifiability. The idea of sufficiency is when does $T$ preserve all information on data? The key idea is that a statistic $T$ is sufficient if---once we know $T(X)$---the remaining randomness in $X$ tells us nothing further about which $\mathbf{P}_\theta$ generated the data. 

  \begin{definition}[Sufficient Statistic]
    Let $(\mathcal{X}, \mathscr{X}, \mathcal{P}, \Theta)$ be an identifiable model. A statistic $T$ is \textbf{sufficient for $\mathcal{P}$} if for all $A \in \mathscr{X}$, $\mathbf{P}_\theta (A \mid T)$ admits a \hyperref[prob-def:cond-exp]{version} that does not depend on $\theta$.\footnote{Some technical measure theory stuff: If $\mathcal{T}$ is nice (for example, Borel), then there exists a function $h_{A, \theta}: T \to \mathbb{R}$ s.t. $\mathbf{P}_{\theta} (A) = \mathbf{P}_{\theta} (A \cap T^{-1} (B)) = \int_B h_{A, \theta}(t) \,d\mathbf{P}_{\theta}^T (t), \quad t \mapsto \mathbf{P}_\theta (A \mid T = t)$. }
  \end{definition}

  Sufficiency is a property of how the parameter enters the distribution of the data. The same statistic may be sufficient for one model but another, and crucially depends on how the model is parameterized. 

  \begin{theorem}
    If $T$ is invertible, with $T^{-1}$ measurable, then $T$ is sufficient. 
  \end{theorem}
  \begin{proof}
    $\sigma(T) \subset X$. Take $A \in \mathscr{X}$. Then, $A = T^{-1} (T(A)) \in \sigma(T)$, which implies that $X \subset \sigma(T)$. Therefore, $X = \sigma(T)$. Therefore, 
    \begin{equation}
      \mathbb{E}_{\theta} [\mathbbm{1}_A \mid \sigma(T)] = \mathbbm{1}_A
    \end{equation}
  \end{proof}

  Therefore, sufficient statistics always exist, e.g. the identity map. This isn't interesting, but what is more interesting is whether $T$ destroys some information yet still is sufficient. The following theorem relates this in terms of the likelihood function. 

  \begin{definition}[Likelihood Function]
    Given a $\mu$-dominated identifiable model $(\mathcal{X}, \mathscr{X}, \mathcal{P}, \Theta)$, the function (for fixed $x \in X$)
    \begin{equation}
      L: \Theta \to [0, +\infty), \quad L(\theta) = p(x \mid \theta) 
    \end{equation}
    is called the \textbf{likelihood function}. 
  \end{definition}

  The theorem states that a statistic is sufficient if and only if the likelihood function can be factorized.  

  \begin{theorem}[Fisher-Neyman Factorization] 
    Consider a identifiable model $(\mathcal{X}, \mathscr{X}, \mathcal{P}, \Theta)$ dominated by a $\sigma$-finite measure $\mu$, with density $p(x \mid \theta) = \frac{d \mathbf{P}_\theta}{d \mu}$. Then, $T$ is sufficient iff 
    \begin{equation}
      p(x \mid \theta) = g(T(x), \theta) \, h(x)
    \end{equation}
    for measurable functions $g: (T, \mathcal{T}) \times \Theta \to [0, +\infty)$ and $h: X \to [0, +\infty)$. 
  \end{theorem}
  \begin{proof}
    In text. 
  \end{proof}

  \begin{example}[Sums of Two Dice] 
    Consider two dice rolls where $(\mathcal{X} = \{1, \ldots, 6\}^2, \mathscr{X} = 2^\mathcal{X})$ and statistic $T: X \to \mathbb{N}$ defined $T(x, y) = x + y$. We can think of $T$ as ``extracting'' information from the two dice rolls to their sum. We will consider two models. 
    \begin{enumerate}
      \item Let $\Theta = \{ \theta = (\theta_1, \ldots, \theta_6) \in \mathbb{R}^6 \colon \theta_1 + \ldots + \theta_6 = 1 \}$, and let $\mathbf{P}_{\theta}$ be the multinomial measure assigning $\mathbf{P}_{\theta} (\{k\}) = \theta_k$. Then, our model is the family $\mathcal{P} = \{ \mathbf{P}_{\theta} \otimes \mathbf{P}_{\theta} \colon \theta \in \Theta \}$, and $T$ is not a sufficient statistic. Consider the conditional probability 
        \begin{equation}
          \mathbf{P}_\theta^{\otimes 2}(\{(1, 6)\} \mid S = 7) = \frac{\mathbf{P}_{\theta} (\{1\}) \mathbf{P}_{\theta} (\{6\})}{\sum_{k=1}^6 \mathbf{P}_{\theta} (\{k\}) \mathbf{P}_{\theta} (\{7 - k\})} = \frac{\theta_1 \theta_6}{\sum_{k=1}^6 \theta_k \theta_{7 - k}}
        \end{equation}
        This is clearly dependent on $\theta$. 

      \item Let $\Theta = \mathbb{R}$ and $\mathbf{P}_{\theta}$ be defined by the density 
        \begin{equation}
          p_\theta (x) = \frac{e^{\theta x}}{\sum_{k=1}^6 e^{\theta k}} 
        \end{equation}
        Note that $\theta = 0$ gives a fair dice, $\theta > 0$ biases towards higher faces, and $\theta < 0$ biases towards lower faces. Then, our model is the family $\mathcal{P} = \{ \mathbf{P}_{\theta} \otimes \mathbf{P}_{\theta} \colon \theta \in \Theta \}$, and $T$ is a sufficient statistic. The joint density is 
        \begin{equation}
          p(x, y \mid \theta) = p(x \mid \theta) p(y \mid \theta) = \frac{e^{\theta x}}{\sum_{k=1}^6 e^{\theta k}} \frac{e^{\theta y}}{\sum_{k=1}^6 e^{\theta k}} = \underbrace{\frac{e^{\theta (x + y)}}{\big( \sum_{k=1}^6 e^{\theta k} \big)^2 }}_{g(x + y, \theta)} \cdot \underbrace{1}_{h (x, y)}
        \end{equation}
        and so by the Fisher-Neyman Factorization, $T$ is sufficient. 
    \end{enumerate}

    Both models are identifiable, but $T$ behaves differently. Note that the first model is a vastly larger model in which no reduction beyond the full data is possible, and so $T$ is not able to capture this well enough. On the other hand, the second model is indexed by a scalar parameter. 
  \end{example}

  \begin{question}
    What does it mean for Model 1 to be nonparameteric? 
  \end{question}

  \begin{question}
    Organization wise, why did you put the experiment and simulation equivalence? I feel like the flow from sufficient stats to minimal sufficient, complete and ancillary is more natural. 
  \end{question}

  \begin{definition}[Minimal Sufficient Statistic]
    A sufficient statistic $T$ is \textbf{minimal sufficient} for an experiment $(\mathcal{X}, \mathscr{X}, \mathcal{P}, \Theta)$ if for any other sufficient $S$, it satisfies $\sigma(T) \subset \sigma(S)$ modulo $\mathbf{P}_\theta$-null sets. 
    \begin{equation}
      \sigma(T) \subset \sigma(\sigma(S) \cup N), \quad N = \{A \in \mathscr{X} \colon \mathbf{P}_{\theta}(A) = 0 \; \forall \theta \in \Theta\} 
    \end{equation}
  \end{definition}

  That is, minimal sufficient statistics partition the sample space into the coarsest equivalence classes that preserve all information about $\theta$. Another way to think about it is that a sufficient statistic $T$ is minimal sufficient if it is a function of every other sufficient statistic. 

  \begin{theorem}
    Let $(\mathcal{X}, \mathscr{X}, \mathcal{P}, \Theta)$ be a dominated model. A statistic $T$ is minimal sufficient iff 
    \begin{equation}
      T(x) = T(x^\prime) \iff \theta \mapsto \frac{p(x \mid \theta)}{p(x^\prime \mid \theta)} \text{ is constant}
    \end{equation}
   $\mathbf{P}_{\theta}$-a.s. for all $\theta \in \Theta$. 
  \end{theorem}
  \begin{proof}
    
  \end{proof}

  \begin{question}
    Can we verify this theorem statement with Prop 1.18 in the notes? 
  \end{question}

  \begin{example}[Minimal Sufficiency for Uniform]
    Let $X_1, \ldots, X_n \sim U(0, \theta)$ for $\theta > 0$. Then, $X_{(n)} = \max\{X_1, \ldots, X_n\}$ is minimal since 
    \begin{equation}
      x \mapsto \frac{p(x \mid \theta)}{y \mid \theta} = \frac{\mathbbm{1}\{x_{(n)} \leq \theta\}}{ \mathbbm{1} \{y_{(n)} \leq \theta\} }
    \end{equation}
    is constant a.e. iff $y_{(n)} = x_{(n)}$. 
  \end{example}

  \begin{question}
    Is the ``independent'' term in Basu's Theorem mean independent RVs? 
  \end{question}

  \begin{question}
    Page 4 of Casella. How does estimator, estimand relate to statistic? What does observable actually mean? How are hypothesis testing, point estimation, density estimation, etc realized under this framework? 
  \end{question}

\subsection{Model Equivalence} 

  While a sufficient statistic allows us to reduce our model to a simpler one, we may want to look for maximal compression. 

  \begin{definition}[Observationally Equivalent]
    If we have two models $M_1 = (\mathcal{X}, \mathscr{X}, \mathcal{P}, \Theta), M_2 = (Y, \mathscr{Y}, \mathcal{Q}, \Theta)$, they are \textbf{observationally equivalent} if there exists sufficient statistics $T: (\mathcal{X}, \mathscr{X}) \to (T, \mathcal{T}), S: (\mathcal{Y}, \mathscr{Y}) \to (T, \mathcal{T})$ such that 
    \begin{equation}
      \mathbf{P}_{\theta, T} (A) = \mathbf{Q}_{\theta, S} (A) \quad \text{ for all } A \in \mathcal{T}, \mathbf{P}_{\theta} \in \mathcal{P}, \mathbf{Q}_{\theta} \in \mathcal{Q}
    \end{equation}
  \end{definition} 

  \begin{example}
    Let $X = (\mathcal{X}_1, \ldots, X_n) \sim N(\mu, 1)$ iid. Let $Y \sim N(\mu, \frac{1}{n})$. Then, 
    \begin{equation}
      T(\mathcal{X}) = \frac{1}{n} \sum_{i=1}^n X_i
    \end{equation}
  \end{example}

  But what if we wanted to go from $Y$ back to $X$? This leads us to the idea of simulation equivalence, which is informally observationally equivalent under sufficient statistics ``with additional randomness.'' e.g. if we set $\Tilde{X} = Y - Z_i$ for $Z_i \sim N(0, 1)$ iid. 

\subsection{Decision Problems}

  \begin{definition}[Statistical Experiment]
    \begin{enumerate}
      \item The \textbf{decision rule} is a measurable function $\delta : X \to (D, \mathcal{D})$ 
      \item The \textbf{loss function} is $L: \Theta \times D \to \mathbb{R}$. $\theta: \mathbf{P} \to \Theta$ satisfies the property that $\theta(\mathcal{P}) = \theta(\mathcal{Q}) \implies \mathbf{P} = \mathbf{Q}$. 
    \end{enumerate}
    The \textbf{risk} is defined 
    \begin{equation}
      R(P, \delta) = \int L(\theta(\mathbf{P}), \delta(x)) \, d\mathbf{P}(x), \quad R(\theta, \delta) = \int L(\theta , \delta(x)) \,d\mathbf{P}_{\theta(x)} \text{ for } \theta \in \Theta
    \end{equation}
  \end{definition}

  The two main questions are: 
  \begin{enumerate}
    \item Which decision functions $\delta$ are good? 
    \item Which models are better? When do we prefer $(\mathcal{Y}, \mathscr{Y}, \mathbf{Q}, \Theta)$ over $(\mathcal{X}, \mathscr{X}, \mathbf{P}, \Theta)$.
  \end{enumerate}

  \begin{example}[Point Estimation]
    
  \end{example}

  \begin{example}[Hypothesis Testing]
    
  \end{example}

  \begin{example}[Confidence Sets]
    
  \end{example}

  \begin{example}[Density Estimation]
    
  \end{example}


\documentclass{article}

  % packages
    % basic stuff for rendering math
    \usepackage[letterpaper, top=1in, bottom=1in, left=1in, right=1in]{geometry}
    \usepackage[utf8]{inputenc}
    \usepackage[english]{babel}
    \usepackage{amsmath} 
    \usepackage{amssymb}
    \usepackage{natbib}

    % extra math symbols and utilities
    \usepackage{mathtools}        % for extra stuff like \coloneqq
    \usepackage{mathrsfs}         % for extra stuff like \mathsrc{}
    \usepackage{centernot}        % for the centernot arrow 
    \usepackage{bm}               % for better boldsymbol/mathbf 
    \usepackage{bbm}              % for indicator functions
    \usepackage{enumitem}         % better control over enumerate, itemize
    \usepackage{hyperref}         % for hypertext linking
    \usepackage{xr-hyper}
    \usepackage{fancyvrb}         % for better verbatim environments
    \usepackage{newverbs}         % for texttt{}
    \usepackage{xcolor}           % for colored text 
    \usepackage{listings}         % to include code
    \usepackage{lstautogobble}    % helper package for code
    \usepackage{parcolumns}       % for side by side columns for two column code
    \usepackage{algorithm}
    \usepackage{algpseudocode}

    % page layout
    \usepackage{fancyhdr}         % for headers and footers 
    \usepackage{uniquecounter} 
    \usepackage{lastpage}         % to include last page number in footer 
    \usepackage{parskip}          % for no indentation and space between paragraphs    
    \usepackage[T1]{fontenc}      % to include \textbackslash
    \usepackage{footnote}
    \usepackage{etoolbox}

    % for custom environments
    \usepackage{tcolorbox}        % for better colored boxes in custom environments
    \tcbuselibrary{breakable}     % to allow tcolorboxes to break across pages

    % figures
    \usepackage{pgfplots}
    \pgfplotsset{compat=1.18}
    \usepackage{float}            % for [H] figure placement
    \usepackage{tikz}
    \usepackage{tikz-cd}
    \usepackage{circuitikz}
    \usetikzlibrary{positioning, shapes, arrows, fit, calc}
    \usepackage{graphicx}
    \usepackage{caption} 
    \usepackage{subcaption}
    \captionsetup{font=small}

    % for tabular stuff 
    \usepackage{dcolumn}

    \usepackage[nottoc]{tocbibind}
    \pdfsuppresswarningpagegroup=1
    \hfuzz=5.002pt                % ignore overfull hbox badness warnings below this limit

  % New and replaced operators
    \DeclareMathOperator{\Tr}{Tr}
    \DeclareMathOperator{\Sym}{Sym}
    \DeclareMathOperator{\Span}{span}
    \DeclareMathOperator{\elbo}{ELBO}
    \DeclareMathOperator{\std}{std}
    \DeclareMathOperator{\Cov}{Cov}
    \DeclareMathOperator{\Var}{Var}
    \DeclareMathOperator{\proj}{proj}
    \DeclareMathOperator{\Corr}{Corr}
    \DeclareMathOperator{\pos}{pos}
    \DeclareMathOperator*{\argmin}{\arg\!\min}
    \DeclareMathOperator*{\argmax}{\arg\!\max}
    \newcommand{\ket}[1]{\ensuremath{\left|#1\right\rangle}}
    \newcommand{\bra}[1]{\ensuremath{\left\langle#1\right|}}
    \newcommand{\braket}[2]{\langle #1 | #2 \rangle}
    \newcommand{\qed}{\hfill$\blacksquare$}     % I like QED squares to be black 

  % Custom Environments
    \newtcolorbox[auto counter, number within=section]{question}[1][]
    {
      colframe = orange!25,
      colback  = orange!10,
      coltitle = orange!20!black,  
      breakable, 
      title = \textbf{Question \thetcbcounter ~(#1)}
    }

    \newtcolorbox[auto counter, number within=section]{exercise}[1][]
    {
      colframe = teal!25,
      colback  = teal!10,
      coltitle = teal!20!black,  
      breakable, 
      title = \textbf{Exercise \thetcbcounter ~(#1)}
    }
    \newtcolorbox[auto counter, number within=section]{solution}[1][]
    {
      colframe = violet!25,
      colback  = violet!10,
      coltitle = violet!20!black,  
      breakable, 
      title = \textbf{Solution \thetcbcounter}
    }
    \newtcolorbox[auto counter, number within=section]{lemma}[1][]
    {
      colframe = red!25,
      colback  = red!10,
      coltitle = red!20!black,  
      breakable, 
      title = \textbf{Lemma \thetcbcounter ~(#1)}
    }
    \newtcolorbox[auto counter, number within=section]{theorem}[1][]
    {
      colframe = red!25,
      colback  = red!10,
      coltitle = red!20!black,  
      breakable, 
      title = \textbf{Theorem \thetcbcounter ~(#1)}
    } 
    \newtcolorbox[auto counter, number within=section]{proposition}[1][]
    {
      colframe = red!25,
      colback  = red!10,
      coltitle = red!20!black,  
      breakable, 
      title = \textbf{Proposition \thetcbcounter ~(#1)}
    } 
    \newtcolorbox[auto counter, number within=section]{corollary}[1][]
    {
      colframe = red!25,
      colback  = red!10,
      coltitle = red!20!black,  
      breakable, 
      title = \textbf{Corollary \thetcbcounter ~(#1)}
    } 
    \newtcolorbox[auto counter, number within=section]{proof}[1][]
    {
      colframe = orange!25,
      colback  = orange!10,
      coltitle = orange!20!black,  
      breakable, 
      title = \textbf{Proof. }
    } 
    \newtcolorbox[auto counter, number within=section]{definition}[1][]
    {
      colframe = yellow!25,
      colback  = yellow!10,
      coltitle = yellow!20!black,  
      breakable, 
      title = \textbf{Definition \thetcbcounter ~(#1)}
    } 
    \newtcolorbox[auto counter, number within=section]{example}[1][]
    {
      colframe = blue!25,
      colback  = blue!10,
      coltitle = blue!20!black,  
      breakable, 
      title = \textbf{Example \thetcbcounter ~(#1)}
    } 
    \newtcolorbox[auto counter, number within=section]{code}[1][]
    {
      colframe = green!25,
      colback  = green!10,
      coltitle = green!20!black,  
      breakable, 
      title = \textbf{Code \thetcbcounter ~(#1)}
    } 
    \newtcolorbox[auto counter, number within=section]{algo}[1][]
    {
      colframe = green!25,
      colback  = green!10,
      coltitle = green!20!black,  
      breakable, 
      title = \textbf{Algorithm \thetcbcounter ~(#1)}
    } 
    
    \definecolor{dkgreen}{rgb}{0,0.6,0}
    \definecolor{gray}{rgb}{0.5,0.5,0.5}
    \definecolor{mauve}{rgb}{0.58,0,0.82}
    \definecolor{darkblue}{rgb}{0,0,139}
    \definecolor{lightgray}{gray}{0.93}
    \renewcommand{\algorithmiccomment}[1]{\hfill$\triangleright$\textcolor{blue}{#1}}

    % default options for listings (for code)
    \lstset{
      autogobble,
      frame=ltbr,
      language=Python,                           % the language of the code
      aboveskip=3mm,
      belowskip=3mm,
      showstringspaces=false,
      columns=fullflexible,
      keepspaces=true,
      basicstyle={\small\ttfamily},
      numbers=left,
      firstnumber=1,                        % start line number at 1
      numberstyle=\tiny\color{gray},
      keywordstyle=\color{blue},
      commentstyle=\color{dkgreen},
      stringstyle=\color{mauve},
      backgroundcolor=\color{lightgray}, 
      breaklines=true,                      % break lines
      breakatwhitespace=true,
      tabsize=3, 
      xleftmargin=2em, 
      framexleftmargin=1.5em, 
      stepnumber=1
    }

  % Page style
    \pagestyle{fancy}
    \fancyhead[L]{Graphical Models}
    \fancyhead[C]{Muchang Bahng}
    \fancyhead[R]{Spring 2024} 
    \fancyfoot[C]{\thepage / \pageref{LastPage}}
    \renewcommand{\footrulewidth}{0.4pt}          % the footer line should be 0.4pt wide
    \renewcommand{\thispagestyle}[1]{}  % needed to include headers in title page

\begin{document}

  \tikzset{
    node_style/.style={
        circle,
        draw=black,
        thick,
        minimum size=20pt,
        inner sep=2pt
    },
    edge_style/.style={
        ->,
        >=latex,
        thick
    },
    runner_node/.style={
        circle,
        draw=black,
        thick,
        minimum size=25pt,
        inner sep=2pt,
        fill=gray!20
    },
    factor_node/.style={
        rectangle,
        draw=black,
        thick,
        minimum size=20pt,
        inner sep=2pt,
        fill=blue!10
    },
    outcome_node/.style={
        circle,
        draw=black,
        thick,
        minimum size=25pt,
        inner sep=2pt,
        fill=green!10
    },
    edge_style/.style={
        ->,
        >=latex,
        thick
    }
  }

\title{Graphical Models}
\author{Muchang Bahng}
\date{Spring 2025}

\maketitle
\tableofcontents
\pagebreak

  The concept of using latent variables to model some process will be used over and over again. We have seen simple examples of latent linear models, but what about nonlinear ones? It turns out that these can be seen as a specific instance of \textit{graphical models}. 

  When computing high-dimensional distributions, the parameters needed to encode this density scales badly. We can see that a general Gaussian mixture model in $\mathbb{R}^n$ with $k$ clusters requires $O(n^2 k)$ parameters. If we wanted to sample from a distribution of portraits, then the dimension $n$ would be the resolution of the image. For a $1024 \times 1024$ image, this requires $n = 3 \cdot 2^{20}$ dimensions, and modeling it with a GMM is hopeless. Fortunately, for complex distributions there is usually some dependencies (e.g. between neighboring pixels) that we can take advantage of. This is exactly what graphical models do. They factor complex distributions so that the scaling is much better. While there are graphical models that do not use latent variables, most interesting applications of graphical models require latent variables, and so we will focus on that. Additionally, we will introduce the EM algorithm, which will be used repeatedly and is particularly important in optimizing \textit{variational autoencoders} in deep learning. 

\section{Bayesian Networks (Directed Graphical Models)} 

  Note that the whole purpose of directed graphical models is to model some sort of \textit{causal} relationship between two random variables. Note that while this is successful in practice, there is really no way to know for sure about any causality. 

  \begin{definition}[Bayesian Network]  
    A \textbf{Bayesian network}, also known as a \textbf{directed probability model}, is a directed acyclic graph of $M$ nodes representing a joint probability distribution of $M$ scalar random variables. An edge pointing $A \rightarrow B$ means that the $B$ is conditionally dependent on $A$, and that there is a very clear casual relationship coming from $A$ to $B$. The \textbf{parents} of a node $x_i$ is denoted $\mathrm{pa}_i$, and the entire joint distribution can be broken up as such: 
    \begin{equation}
      p(\mathbf{x}) = \prod_{m=1}^M p(x_m \mid x_{\mathrm{pa}_m})
    \end{equation}
    which is unique due to it being a DAG. Not only is a Bayesian network easy to parameterize. We can also sample from the joint distribution by sequentially sampling starting from the parents to the final children, and discarding the ones (marginalizing) that we don't wish to sample. This is known as \textbf{ancestral sampling}. 

    \begin{figure}[H]
      \centering 
      \begin{tikzpicture}[node distance=2cm]
        \node [node_style] (x1) at (0,0) {$x_1$};
        \node [node_style] (x2) at (-1,-1.5) {$x_2$};
        \node [node_style] (x3) at (1,-1.5) {$x_3$};
        \node [node_style] (x4) at (0,-3) {$x_4$};
        
        \draw [edge_style] (x1) -- (x2);
        \draw [edge_style] (x1) -- (x3);
        \draw [edge_style] (x2) -- (x4);
        \draw [edge_style] (x3) -- (x4);
        
        \node [above=0.2cm of x1] {Root Node};
        \node [below=0.2cm of x4] {Child Node};
      \end{tikzpicture}
      \caption{} 
      \label{fig:bayesian_network}
    \end{figure}
  \end{definition}

  This following example cleared up any confusion when I learned Bayesian networks for the first time. 

  \begin{example}[Relay Race]
    Consider a $4\times100$m relay race where the final race time depends on multiple factors. We can model this as a Bayesian network where the total race time $T$ depends on:
    \begin{itemize}
      \item Individual runner capabilities ($R_1$, $R_2$, $R_3$, $R_4$)
      \item Handoff success between runners ($H_1$, $H_2$, $H_3$)
      \item Individual leg performances ($P_1$, $P_2$, $P_3$, $P_4$)
    \end{itemize}

    The joint probability distribution factorizes as:
    \begin{align*}
      & p(T, R_1, R_2, R_3, R_4, H_1, H_2, H_3, P_1, P_2, P_3, P_4) = \\
      & p(T|P_1,P_2,P_3,P_4) \prod_{i=1}^4 p(R_i) \prod_{i=1}^3 p(H_i|R_i,R_{i+1}) \prod_{i=1}^4 p(P_i|R_i,H_{i-1})
    \end{align*}

    \noindent where $H_0$ is undefined for $P_1$, and each runner's performance depends on their capability and the success of the previous handoff (except for the first runner). This network captures both the individual contributions and the critical dependencies between runners during baton exchanges.

    \begin{figure}[H]
      \centering 
      \begin{tikzpicture}[node distance=2cm]
        % Runner nodes
        \node[runner_node] (r1) at (0,0) {$R_1$};
        \node[runner_node] (r2) at (2,0) {$R_2$};
        \node[runner_node] (r3) at (4,0) {$R_3$};
        \node[runner_node] (r4) at (6,0) {$R_4$};
        
        % Handoff nodes
        \node[factor_node] (h1) at (1,-1.5) {$H_1$};
        \node[factor_node] (h2) at (3,-1.5) {$H_2$};
        \node[factor_node] (h3) at (5,-1.5) {$H_3$};
        
        % Performance nodes
        \node[factor_node] (p1) at (0,-3) {$P_1$};
        \node[factor_node] (p2) at (2,-3) {$P_2$};
        \node[factor_node] (p3) at (4,-3) {$P_3$};
        \node[factor_node] (p4) at (6,-3) {$P_4$};
        
        % Final outcome
        \node[outcome_node] (result) at (3,-4.5) {$T$};
        
        % Draw edges
        % Runner to handoff connections
        \draw[edge_style] (r1) -- (h1);
        \draw[edge_style] (r2) -- (h1);
        \draw[edge_style] (r2) -- (h2);
        \draw[edge_style] (r3) -- (h2);
        \draw[edge_style] (r3) -- (h3);
        \draw[edge_style] (r4) -- (h3);
        
        % Handoff to performance connections
        \draw[edge_style] (h1) -- (p2);
        \draw[edge_style] (h2) -- (p3);
        \draw[edge_style] (h3) -- (p4);
        
        % Runner to their performance connections
        \draw[edge_style] (r1) -- (p1);
        \draw[edge_style] (r2) -- (p2);
        \draw[edge_style] (r3) -- (p3);
        \draw[edge_style] (r4) -- (p4);
        
        % Performance to final outcome connections
        \draw[edge_style] (p1) -- (result);
        \draw[edge_style] (p2) -- (result);
        \draw[edge_style] (p3) -- (result);
        \draw[edge_style] (p4) -- (result);
      \end{tikzpicture}
      \caption{Bayesian Network for a 4x100m Relay Race. The graphical representation is much more compact and intuitive than simply writing out all the products. }
      \label{fig:relay_race}
    \end{figure}
  \end{example} 

  Bayesian modelling with hierarchical priors. 

  \begin{example}[Multinomial]
    We first provide some motivation from a computational complexity perspective. Given a joint distribution of 2 random variables $\mathbf{x}_1, \mathbf{x}_2$, say which are multinomial with $K$ classes, their joint distribution $p(\mathbf{x}_1, \mathbf{x}_2)$ is captured by $K^2 - 1$ parameters. For a general $M$ random variables, then we have to keep a total of $K^M - 1$ parameters, and this increases exponentially. By building a directed graph with say $r$ maximum number of variables appearing on either side of the conditioning bar in a single probability distribution, then the computational complexity scales as $O(K^r)$, which may save a lot of time if $r << M$. 
  \end{example}

  Extending upon this example, we can see that we want to balance two things: 
  \begin{enumerate} 
    \item Fully conncted graphs have completely general distributions and have $O(K^M -1)$ number of parameters (too complex). 
    \item If there are no links, the joint distribution fully factorizes into the product of its marginals and has $M(K-1)$ parameters (too simple) . 
  \end{enumerate}
  Graphs that have an intermediate level of connectivity allow for more general distributions compared to the fully factorized one, while requiring fewer parameters than the general joint distribution. One model that balances this out is the hidden markov model. 

  \begin{example}[Chain Graph]
    Consider an $M$-node Markov chain. The marginal distribution $p(\mathbf{x}_1)$ requires $K-1$ parameters, and the remaining conditional distributions $p(\mathbf{x}_i \mid \mathbf{x}_{i-1})$ requires $K(K-1)$ parameters. Therefore, the total number of parameters is 
    \begin{equation}
      K-1 + (M-1) (K-1) K \in O(M K^2)
    \end{equation}
    which scales relatively well, and we have 
    \begin{equation}
      p(\{\mathbf{x}_m\}) = p (\mathbf{x}_1) \prod_{m=2}^M p(\mathbf{x}_m \mid \mathbf{x}_{m-1})
    \end{equation}
    \begin{center}
      TBD
    \end{center}
    We can turn this same graph into a Bayesian model by introducing priors for the paramters. Therefore, each node requires an additional parent representing the distribution over parameters (e.g. prior can be Dirichlet)  
    \begin{equation}
      p(\{\mathbf{x}_m , \mu_m\}) = p(\mathbf{x}_1 \mid \mu_1) p(\mu_1) \prod_{m=2}^M p(\mathbf{x}_m \mid \mathbf{x}_{m-1}, \mu_m) p(\mu_m)
    \end{equation}
    with $p(\mu_m) = \mathrm{Dir}(\mu_m \mid \alpha_m)$ for some predetermined fixed hyperparameter $\alpha_m$. 

    \begin{figure}[H]
      \centering 
      \begin{tikzpicture}
        % First pair
        \node[circle,draw=black,line width=0.8pt,minimum size=0.6cm] (mu1) at (0,1.5) {$\mu_1$};
        \node[circle,draw=black,line width=0.8pt,minimum size=0.6cm] (x1) at (0,0) {$x_1$};
        
        % Second pair
        \node[circle,draw=black,line width=0.8pt,minimum size=0.6cm] (mu2) at (2,1.5) {$\mu_2$};
        \node[circle,draw=black,line width=0.8pt,minimum size=0.6cm] (x2) at (2,0) {$x_2$};
        
        % Last pair
        \node[circle,draw=black,line width=0.8pt,minimum size=0.6cm] (muM) at (7,1.5) {$\mu_M$};
        \node[circle,draw=black,line width=0.8pt,minimum size=0.6cm] (xM) at (7,0) {$x_M$};
        
        % Vertical connections
        \draw[->,black,line width=0.8pt] (mu1) -- (x1);
        \draw[->,black,line width=0.8pt] (mu2) -- (x2);
        \draw[->,black,line width=0.8pt] (muM) -- (xM);
        
        % Horizontal connections with spaced ellipsis
        \draw[->,black,line width=0.8pt] (x1) -- (x2);
        \draw[->,black,line width=0.8pt] (x2) -- (3,0);
        \draw[dotted, black,line width=0.8pt] (4,0) -- (5,0);
        \draw[->,black,line width=0.8pt] (6,0) -- (xM);
      \end{tikzpicture}
      \caption{} 
      \label{fig:dir_prior}
    \end{figure}

    We could also choose to share a common prior over the parameters, trading flexibility for computational feasibility. 

    \begin{figure}[H]
      \centering 
      \begin{tikzpicture}
        % First mu node with x1
        \node[circle,draw=black,line width=0.8pt,minimum size=0.6cm] (mu1) at (0,1.5) {$\mu_1$};
        \node[circle,draw=black,line width=0.8pt,minimum size=0.6cm] (x1) at (0,0) {$x_1$};
        
        % Second x node
        \node[circle,draw=black,line width=0.8pt,minimum size=0.6cm] (x2) at (2,0) {$x_2$};
        
        % Last x node
        \node[circle,draw=black,line width=0.8pt,minimum size=0.6cm] (xM) at (7,0) {$x_M$};
        
        % Shared mu node
        \node[circle,draw=black,line width=0.8pt,minimum size=0.6cm] (mu) at (4.5,1.5) {$\mu$};
        
        % Label for shared prior
        \node[right] at (5,1.5) {Shared prior};
        
        % Vertical connections
        \draw[->,black,line width=0.8pt] (mu1) -- (x1);
        \draw[->,black,line width=0.8pt] (mu) -- (x2);
        \draw[->,black,line width=0.8pt] (mu) -- (xM);
        
        % Horizontal connections with spaced ellipsis
        \draw[->,black,line width=0.8pt] (x1) -- (x2);
        \draw[->,black,line width=0.8pt] (x2) -- (3,0);
        \draw[dotted, black,line width=0.8pt] (4,0) -- (5,0);
        \draw[->,black,line width=0.8pt] (6,0) -- (xM);
      \end{tikzpicture}
      \caption{} 
      \label{fig:shared_dir_prior}
    \end{figure}
  \end{example}

  Another way to make more compact representations is through parameterized models. For example, if we have to compute $p(y = 1 \mid \mathbf{x}_1, \ldots, \mathbf{x}_M)$, this in general has $O(K^M)$ parameters. However, we can obtain a more parsimonious form by using a logistic function acting on a linear combination of the parent variables 
  \begin{equation}
    p(y = 1 \mid \mathbf{x}_1, \ldots, \mathbf{x}_m) = \sigma \bigg( w_0 + \sum_{i=1}^M w_i x_i \bigg) = \sigma(\mathbf{w}^T \mathbf{x})
  \end{equation}
  We can look at an example how this is applied to sampling from high-dimensional Gaussian with \textbf{linear Gaussian models}.  


  \begin{example}[Multivariate Gaussian]
    Consider an arbitrary acyclic graph over $D$ random variables, in which eachnode represents a single continuous Gaussian distribution with its mean given by a linear function of its parents. 
    \[p(x_i \mid \mathbf{pa}_i) = N \bigg( x_i \bigg| w_{ij} x_j + b_j, v_i \bigg) \]
    Given a multivariate Gaussian, let us try to decompose it into a directed graph. The log of the joint distribution takes form 
    \[\ln p(\mathbf{x}) = \sum_{i=1}^D \ln p(x_i \mid \mathrm{pa}_i) = - \sum_{i=1}^D \frac{1}{2 v_i} \bigg( x_i - \sum_{j \in \mathrm{pa}_i} w_{ij} x_j - b_i \bigg)^2 + \mathrm{const}\]
    To compute the mean, we can see that by construction, every $x_i$ is dependent on its ancestors, so 
    \[x_i = \sum_{j \in \mathrm{pa}_i} w_{ij} x_j + b_i + \sqrt{v_i} \epsilon_i, \;\; \epsilon_i \sim N(0, 1)\]
    so by linearity of expectation, we have 
    \[\mathbb{E}[x_i] = \sum_{j \in \mathrm{pa}_i} w_{ij} \mathbb{E}[x_j] + b_i\]
    So again, we can start at the top of the graph and compute the expectation. To compute covariance, we can obtain the $i, j$th element of $\boldsymbol{\Sigma}$ with a recurrence relation: 
    \begin{align*} 
      \Sigma_{ij} & = \mathbb{E}[ (x_i - \mathbb{E}[x_i]) (x_j - \mathbb{E}[x_j])] \\
                  & = \mathbb{E} \bigg[ (x_i - \mathbb{E}[x_i]) \bigg( \sum_{k \in \mathrm{pa}_j} w_{j k} (x_k - \mathbb{E}[x_k])  + \sqrt{v_i} \epsilon_j\bigg) \bigg] \\
                  & = \sum_{k \in \mathrm{pa}_j} w_{j k} \Sigma_{ik} + I_{ij} v_j
    \end{align*}
    If there were no links in the graphs, then the $w_{ij}$'s are $0$, and so $\mathbb{E}[\mathbf{x}] = [b_1, \ldots, b_D]$, making the covariance diagonal.If the graph is fully connected, then the total number of parameters is $D + D(D-1)/2$, which corresponds to a general symmetric covariance matrix.  
  \end{example}

  \begin{example}[Bilinear Gaussian Model]
    Consider the following model
    \begin{align*}
      u & \sim N(0, 1) \\
      v & \sim N(0, 1) \\
      r & \sim N(u v, 1)
    \end{align*}
    where the mean of $r$ is a product of $2$ Gaussians. This is also a parameterized model. 

    \begin{figure}[H]
      \centering 
      \begin{tikzpicture}
        % Left network
        \node[circle,draw=black,line width=0.8pt,minimum size=0.6cm] (u) at (-2,1) {$u$};
        \node[circle,draw=black,line width=0.8pt,minimum size=0.6cm] (v) at (0,1) {$v$};
        \node[circle,draw=black,line width=0.8pt,fill=gray!30,minimum size=0.6cm] (r) at (-1,0) {$r$};

        % Draw directed edges for left network
        \draw[->,black,line width=0.8pt] (u) -- (r);
        \draw[->,black,line width=0.8pt] (v) -- (r);

        % Right network
        \node[circle,draw=black,line width=0.8pt,minimum size=0.6cm] (u1) at (2,1.5) {$u_1$};
        \node[circle,draw=black,line width=0.8pt,minimum size=0.6cm] (u2) at (3,1.5) {$u_2$};
        \node[circle,draw=black,line width=0.8pt,minimum size=0.6cm] (v1) at (4,1.5) {$v_1$};
        \node[circle,draw=black,line width=0.8pt,minimum size=0.6cm] (v2) at (5,1.5) {$v_2$};
        
        \node[circle,draw=black,line width=0.8pt,fill=gray!30,minimum size=0.6cm] (r12) at (2,0) {$r_{12}$};
        \node[circle,draw=black,line width=0.8pt,fill=gray!30,minimum size=0.6cm] (r11) at (3,0) {$r_{11}$};
        \node[circle,draw=black,line width=0.8pt,fill=gray!30,minimum size=0.6cm] (r22) at (4,0) {$r_{22}$};
        \node[circle,draw=black,line width=0.8pt,minimum size=0.6cm] (r21) at (5,0) {$r_{21}$};

        % Draw directed edges for right network
        \draw[->,black,line width=0.8pt] (u1) -- (r12);
        \draw[->,black,line width=0.8pt] (u1) -- (r11);
        \draw[->,black,line width=0.8pt] (u2) -- (r12);
        \draw[->,black,line width=0.8pt] (u2) -- (r22);
        \draw[->,black,line width=0.8pt] (v1) -- (r11);
        \draw[->,black,line width=0.8pt] (v1) -- (r21);
        \draw[->,black,line width=0.8pt] (v2) -- (r22);
        \draw[->,black,line width=0.8pt] (v2) -- (r21);
      \end{tikzpicture}
      \caption{} 
      \label{fig:bilinear_gaussian}
    \end{figure}
  \end{example}

  \begin{definition}[Conditional Independence in Directed Graphs]
    We say that $a$ is independent of $b$ given $c$ if 
    \[p(a \mid b, c) = p(a \mid c)\]
    or equivalently, 
    \[p(a, b \mid c) = p(a \mid b, c)\, p(b \mid c) = p(a \mid c) \, p(b \mid c)\]
    Conveniently, we can directly read conditional independence properties of the joint distribution from the graph without any analytical measurements. 
  \end{definition}

  \begin{example}[Conditional Independence on Dataset]
    We can demonstrate conditional independence with iid data. Consider the problem of density estimation of some dataset $\mathcal{D} = \{x_i\}$ with some parameterized distribution of $\mu$. Originally, the observations are not independent since they depend on $\mu$. 
    \begin{equation}
      p(\mathcal{D}) = \int_{\mu} p(\mathcal{D} \mid \mu) \, p(\mu)\, d\mu 
    \end{equation}

    \begin{figure}[H]
      \centering 
      \begin{tikzpicture}
        \node[circle,draw=black,line width=0.8pt,minimum size=0.8cm] (mu2) at (0,-1) {$\mu$};
        
        \node[circle,draw=black,line width=0.8pt,minimum size=0.6cm] (x12) at (-1.5,-2) {$x_1$};
        \node[circle,draw=black,line width=0.8pt,minimum size=0.6cm] (xn2) at (1.5,-2) {$x_N$};
        
        \draw[->,black,line width=0.8pt] (mu2) -- (x12);
        \draw[->,black,line width=0.8pt] (mu2) -- (xn2);
        \draw[dotted,black,line width=0.8pt] (x12) -- (xn2);
      \end{tikzpicture}
      \caption{}
      \label{fig:conditional_indep_iid_1}
    \end{figure}

    If we condition on $\mu$ and considered the joint over the observed variables, the variables are independent. 
    \begin{equation}
      p(\mathcal{D} \mid \mu) = \prod_{n=1}^N p(x_n \mid \mu)
    \end{equation}

    \begin{figure}[H]
      \centering 
      \begin{tikzpicture}
        \node[circle,draw=black,fill=blue!40,line width=0.8pt,minimum size=0.8cm] (mu1) at (0,2) {$\mu$};
        
        \node[circle,draw=black,line width=0.8pt,minimum size=0.6cm] (x11) at (-1.5,1) {$x_1$};
        \node[circle,draw=black,line width=0.8pt,minimum size=0.6cm] (xn1) at (1.5,1) {$x_N$};
        
        \draw[->,black,line width=0.8pt] (mu1) -- (x11);
        \draw[->,black,line width=0.8pt] (mu1) -- (xn1);
        \draw[dotted,black,line width=0.8pt] (x11) -- (xn1);
      \end{tikzpicture}
      \caption{}
      \label{fig:conditional_indep_iid_2}
    \end{figure}    
  \end{example}

  The example above identifies a node (the parent $\mu$) where, if observed, causes the rest of the nodes to become independent. We can extend on this idea by taking an arbitrary $x_i$ and finding a set of nodes such that if they are observed, then $x_i$ is indepedent from every other node. 

  \begin{definition}[Markov Blanket in Directed Graphs]
    The \textbf{Markov blanket} of a node is the minimal set of nodes that must be observed to make this node independent of all other nodes. It turns out that the parents, children, and coparents are all in the Markov blanket. 

    \begin{figure}[H]
      \centering 
      \begin{tikzpicture}
        % Central node
        \node[circle,draw=black,line width=0.8pt,minimum size=0.6cm] (c) at (0,0) {$x_i$};
        
        % Parent nodes (top)
        \node[circle,draw=black,fill=blue!40,line width=0.8pt,minimum size=0.6cm] (p1) at (-1,1) {};
        \node[circle,draw=black,fill=blue!40,line width=0.8pt,minimum size=0.6cm] (p2) at (1,1) {};
        
        % Middle level nodes
        \node[circle,draw=black,fill=blue!40,line width=0.8pt,minimum size=0.6cm] (m1) at (-2,0) {};
        \node[circle,draw=black,fill=blue!40,line width=0.8pt,minimum size=0.6cm] (m2) at (2,0) {};
        
        % Children nodes (bottom)
        \node[circle,draw=black,fill=blue!40,line width=0.8pt,minimum size=0.6cm] (c1) at (-1,-1) {};
        \node[circle,draw=black,fill=blue!40,line width=0.8pt,minimum size=0.6cm] (c2) at (1,-1) {};
        
        \draw[->,black,line width=0.8pt] (p1) -- (c);
        \draw[->,black,line width=0.8pt] (p2) -- (c);
        \draw[->,black,line width=0.8pt] (c) -- (c1);
        \draw[->,black,line width=0.8pt] (c) -- (c2);
        \draw[->,black,line width=0.8pt] (m1) -- (c1);
        \draw[->,black,line width=0.8pt] (m2) -- (c2);
      \end{tikzpicture}
      \caption{} 
      \label{fig:markov_blanket_directed}
    \end{figure} 

    Note that 
    \begin{equation}
      p(x_i \mid x_{j \neq i}) = \frac{p(x_1, \ldots, x_M)}{\int p(x_1, \ldots, x_M) \,dx} = \frac{\prod_k p(x_k \mid \mathrm{pa}_k)}{\int \prod_k p(x_k \mid \mathrm{pa}_k) \,dx_i}
    \end{equation}
  \end{definition} 

  One final interpretation is that we can view directed graphs as \textbf{distribution filters}. We take the joint probability distribution, will starts off as fully connected, and the directed graphs ``filters" away the edges that are not needed. Therefore, the joint probability distribution $p(\mathbf{x})$ is only allows through the filter if and only if it satisfies the factorization property. 

\section{Markov Random Field (Undirected Graphical Models)}

  As the name implies, undirected models use undirected graphs, which are used to model relationships that go both ways rather than just one. Unlike directed graphs, which are useful for expressing casual relationships between random variables, undirected graphs are useful for expressing soft constraints between random variables.  

  \begin{figure}[H]
    \centering 
    \begin{tikzpicture}[scale=0.8]
      \foreach \i in {1,...,5} {
        \foreach \j in {1,...,5} {
          \node[circle, draw, minimum size=0.6cm] (node\i\j) at (\i, \j) {};
        }
      }

      \foreach \i in {1,...,5} {
        \foreach \j in {1,...,5} {
          \pgfmathtruncatemacro{\right}{\i+1}
          \pgfmathtruncatemacro{\down}{\j-1}
          \pgfmathtruncatemacro{\up}{\j+1}
          \pgfmathtruncatemacro{\left}{\i-1}
          
          \ifnum \i < 5
            \draw (node\i\j) -- (node\right\j);
          \fi
          
          \ifnum \j > 1
            \draw (node\i\j) -- (node\i\down);
          \fi
        }
      }
    \end{tikzpicture}
    \caption{An MRF can be represented with this graph.} 
    \label{fig:mrf_graph}
  \end{figure}

  \begin{definition}[Conditional Independence in Undirected Graphs]
    Fortunately, conditional independence is easier compared to directed models. We can say $A$ is conditionally independent to $B$ given $C$ if $C$ blocks all paths between any node in $A$ and any node in $B$. 

    \begin{figure}[H]
      \centering 
      \begin{tikzpicture}
        \node[circle,draw=black,line width=0.8pt,minimum size=0.4cm] (a1) at (-2,1) {};
        \node[circle,draw=black,line width=0.8pt,minimum size=0.4cm] (a2) at (-2,0) {};
        \node[circle,draw=black,line width=0.8pt,minimum size=0.4cm] (a3) at (-2,-1) {};

        \node[circle,draw=black,line width=0.8pt,fill=blue!40,minimum size=0.4cm] (c1) at (0,0.5) {};
        \node[circle,draw=black,line width=0.8pt,fill=blue!40,minimum size=0.4cm] (c2) at (0,-0.5) {};

        % Right cluster (B)
        \node[circle,draw=black,line width=0.8pt,minimum size=0.4cm] (b1) at (2,1) {};
        \node[circle,draw=black,line width=0.8pt,minimum size=0.4cm] (b2) at (2,0) {};
        \node[circle,draw=black,line width=0.8pt,minimum size=0.4cm] (b3) at (2,-1) {};

        % Draw edges
        \draw[black,line width=0.8pt] (a1) -- (a2);
        \draw[black,line width=0.8pt] (a2) -- (a3);
        \draw[black,line width=0.8pt] (a1) -- (c1);
        \draw[black,line width=0.8pt] (a2) -- (c1);
        \draw[black,line width=0.8pt] (c1) -- (c2);
        \draw[black,line width=0.8pt] (c1) -- (b1);
        \draw[black,line width=0.8pt] (c2) -- (b2);
        \draw[black,line width=0.8pt] (b1) -- (b2);
        \draw[black,line width=0.8pt] (b2) -- (b3);

        % Draw dashed ellipses around clusters
        \draw[color={black},dashed,line width=0.8pt] (-2,0.0) ellipse (0.8cm and 1.8cm) node[below=2cm] {$A$};
        \draw[color={black},dashed,line width=0.8pt] (0,0) ellipse (0.8cm and 1.2cm) node[below=1.5cm] {$C$};
        \draw[color={black},dashed,line width=0.8pt] (2,0.0) ellipse (0.8cm and 1.8cm) node[below=2cm] {$B$};
      \end{tikzpicture}
      \caption{$A$ is conditionally independent given $C$, denoted $A \perp\!\!\!\perp B|C$. } 
      \label{fig:undirected_conditional_independence}
    \end{figure}
  \end{definition}

  \begin{definition}[Markov Blanket in Undirected Graphs]
    The Markov blanket of a node, which is the minimal set of nodes that must be observered to make this node independent of the rest of the nodes, is simply the nodes that are directly connected to that node. 

    \begin{figure}[H]
      \centering 
      \begin{tikzpicture}
        % Central node
        \node[circle,draw=black,line width=0.8pt,minimum size=0.6cm] (c) at (0,0) {};
        
        % Filled outer nodes
        \node[circle,draw=black,fill=blue!40,line width=0.8pt,minimum size=0.6cm] (n1) at (0,1.2) {};
        \node[circle,draw=black,fill=blue!40,line width=0.8pt,minimum size=0.6cm] (n2) at (1.2,0) {};
        \node[circle,draw=black,fill=blue!40,line width=0.8pt,minimum size=0.6cm] (n3) at (0,-1.2) {};
        \node[circle,draw=black,fill=blue!40,line width=0.8pt,minimum size=0.6cm] (n4) at (-1.2,0) {};
        
        % Draw edges
        \draw[black,line width=0.8pt] (c) -- (n1);
        \draw[black,line width=0.8pt] (c) -- (n2);
        \draw[black,line width=0.8pt] (c) -- (n3);
        \draw[black,line width=0.8pt] (c) -- (n4);
      \end{tikzpicture}
      \caption{Once the neighbors of a node are realized, the node is independent of the rest of the nodes. } 
      \label{fig:markov_blanket_undirected}
    \end{figure}

    Therefore, the conditional distribution of $x_i$ conditioned on all the variables in the graph is dependent only on the variables in the Markov blanket. 
  \end{definition}

  Now, let us talk about how we can actually define a probability distribution with this graph. 

  \begin{definition}[Clique] 
    In an undirected graph, a \textbf{clique} is a set of nodes such that there exists a link between all pairs of nodes in that subset. A \textbf{maximal clique} is a clique such that it is not possible to include any other nodes in the set without it ceasing it to be a clique. 
  \end{definition}

  Given a joint random variable $\mathbf{x}$  represented by an undirected graph, the joint distribution is given by the product of non-negative potential functions over the maximal cliques 
  \begin{equation}
    p(\mathbf{x}) = \frac{1}{Z} \prod_C \phi_C (x_C)
  \end{equation}
  where 
  \begin{equation}
    Z = \int p(\mathbf{x}) \,d\mathbf{x}
  \end{equation}
  is the normalizing constant, called the \textbf{partition function}. That is, each $x_C$ is a maximal clique and $\phi_C$ is the nonnegative potential function of that clique. 

  This assignment looks pretty arbitrary. How do we know that any arbitrary joint distribution of $\mathbf{x}$, which has a undirected graphical representation, can be represented as the product of a bunch of functions over the maximum cliques? Fortunately, there is a mathematical result that proves this. 

  \begin{theorem}[Hammersley-Clifford] 
    \label{hammersley}
    The joint probability distribution of any undirected graph can be written as the product of potential functions on the maximal cliques of the graph. Furthermore, for any factorization of these potential functions, there exists an undirected graph for which is the joint.  
  \end{theorem} 


  \begin{example}
    For example, the joint distribution of the graph below
    \begin{figure}[H]
      \centering 
      \begin{tikzpicture}
        % Define nodes
        \node[circle,draw=black,line width=0.8pt,minimum size=0.8cm] (A) at (-1,0) {$A$};
        \node[circle,draw=black,line width=0.8pt,minimum size=0.8cm] (B) at (1,0) {$B$};
        \node[circle,draw=black,line width=0.8pt,minimum size=0.8cm] (C) at (0,1) {$C$};
        \node[circle,draw=black,line width=0.8pt,minimum size=0.8cm] (D) at (0,-1) {$D$};

        % Draw thick edges
        \draw[black,line width=0.8pt] (A) -- (C);
        \draw[black,line width=0.8pt] (A) -- (D);
        \draw[black,line width=0.8pt] (B) -- (C);
        \draw[black,line width=0.8pt] (B) -- (D);
      \end{tikzpicture}
      \caption{} 
      \label{fig:hammersley_clifford_ex}
    \end{figure}
    factorizes into 
    \begin{equation}
      p(A, B, C, D) = \frac{1}{Z} \phi(A, C) \, \phi(C, B) \, \phi(B, D) \, \phi(A, D)
    \end{equation}
  \end{example}

  Note that each potential function $\phi$ is a mapping from the joint configuration of random variables in a clique to non-negative real numbers. The choice of potential functions is not restricted to having specific probabilistic interpretations, but since they must be nonnegative, we can just represent them as an exponential. The negative sign is not needed, but is a remnant of physics notation. 
  \begin{equation}
    p (\mathbf{x}) = \frac{1}{Z} \prod_C \phi_C (x_C) = \frac{1}{Z} \exp \bigg\{ - \sum_C E(x_C) \bigg\} = \frac{1}{Z} \underbrace{\exp \big\{ - E(\mathbf{x}) \big\}}_{\substack{\text{Boltzmann}\\ \text{distribution}}}
  \end{equation}

  Any distribution that can be represented as the form above is called a \textbf{Boltzmann distribution}. So far, all we stated is that the joint probability distribution can be expressed as the product of a bunch of potential functions, but besides the fact that it is nonnegative, there is no probabilistic interpretation of these potentials (or equivalently, the energy functions). While this does give us greater flexibility in choosing potential functions, we must be careful in choosing them (e.g. choosing something like $x^2$ may cause the integral to diverge, making the joint not well-defined).

  Clearly, these potential functions over the cliques should express which configuration of the local variables are preferred to others. It should assign higher values to configurations that are deemed (either by assumption or through training data) to be more probable. That is, each potential is like an ``expert" that provides some opinion (the value) on a configuration, and the product of the values of all the potential represents the total opinion of all the experts. Therefore, global configurations with relatively high probabilities are those that find a good balance in satisfying the (possibly conflicting) influences of the clique potentials. 

  \begin{example}[Transmission of Colds] 
    Say that you want to model a distribution over three binary variables: whether you or not you, your coworker, and your roommate is sick ($0$ represents sick and $1$ represents healthy). Then, you can make simplifying assumptions that your roommate and your coworker do not know each other, so it is very unlikely that one of thme will give the other an infection such as a cold directly. Therefore, we can model the indirect transmission of a cold from your coworker to your roommate by modeling the transmission of the cold from your coworker to you and then you to your roommate. Therefore, we have a model of form

    \begin{center}
      \begin{tikzpicture}
        \node[circle,draw] (hr) at (0,0) {$h_r$};
        \node[circle,draw] (hy) at (2,0) {$h_y$};
        \node[circle,draw] (hc) at (4,0) {$h_c$};

        \draw (hr) -- (hy);
        \draw (hy) -- (hc);
      \end{tikzpicture}
    \end{center}
    One max clique contains $h_y$ and $h_c$. The factor for this clique can be defined by a table and might have values resembling these. 

    \begin{table}[H]
      \centering
      \begin{tabular}{c|c|c|}
      \cline{2-3}
      & \( h_y = 0 \) & \( h_y = 1 \) \\ \hline
      \multicolumn{1}{|c|}{\( h_c = 0 \)} & 2 & 1 \\ \hline
      \multicolumn{1}{|c|}{\( h_c = 1 \)} & 1 & 10 \\ \hline
      \end{tabular}
      \caption{States and Values of \( h_y \) and \( h_c \)}
    \end{table}

    This table completely describes the potential function of this clique. Both of you are usually healthy, so the state $(1, 1)$ gets the maximum value of $1$. If one of you are sick, then it is likely that the other is sick as well, so we have a value of $2$ for $(0, 0)$. Finally, it is most unlikely that one of you is sick and the other healthy, which has a value of $1$. 
  \end{example}

\section{Hidden Markov Models}


\section{Nonlinear Latent Variable Models}

  Now we will consider ourselves with nonlinear latent variables models, which still defines a simple latent random variable $Z$ with prior $p(z)$, but now a family of nonlinear functions $\{f_\theta (z)\}$ that defines the generative component $f_\theta (x \mid z)$. In factor models, we have taken linear transformations of random variables and therefore the likelihood had been easy to calculate, differentiate, and therefore optimize. 

  In the general nonlinear case, we usually deal with $f_\theta$ not as a transformation of $Z$ to $X$, but really $f_\theta (z)$ becomes the \textit{parameters} of $X \mid Z = z$. This allows to define the \textit{implicitly parameterized} family of distributions $\{p_\theta\}$. Given that the true distribution of the data is $p^\ast (x)$, we would like to find a distribution $p_\theta (x)$ that is a good approximation. 
  \begin{equation}
    p^\ast (x) \approx p_\theta (x)
  \end{equation}
  To calculate the likelihood $p_\theta (x)$, we must compute the marginal 
  \begin{equation}
    p_\theta (x) = \int p_\theta (x, z) \,dz = \int p_\theta (x \mid z) \, p(z) \,dz
  \end{equation}
  which is known to be computationally intractable due to the integral. At first, it seems like all hope is lost, but statisticians have a few tricks up their sleeves. 
  \begin{enumerate} 
    \item The first trick is to notice that by Bayes rule, we can compute the likelihood not as an integral, but as 
    \begin{equation}
      p_\theta (x) = \frac{p_\theta (x \mid z) \, p(z)}{p_\theta (z \mid x)} 
    \end{equation}
    So it suffices to find a good approximation of $p_\theta (z \mid x)$, which is a probabilistic discriminative model for the latent variable (i.e. we are trying to compute the distribution of $z$ given $x$ as if we were predicting it). We can do MCMC since $p_\theta (z \mid x) \propto p_\theta (x \mid z) \, p(z)$, but often this can be slow to fit. 

    \item The next trick is called the variational lower bound, which is a lower bound on the log likelihood, and therefore by optimizing it we can hope to optimize the log-likelihood as well. This works well in practice. 

    \item The next trick is by optimizing the Fisher score, which is the gradient of the log likelihood \textit{with respect to the covariates} (not the parameters!). 
  \end{enumerate}

\subsection{Variational Lower Bounds} 

  We focus on this problem and define a family of distributions $\{q_\phi (z \mid x)\}_\phi$ and use it to approximate $p_\theta (z \mid x)$. Therefore, searching for a good $\phi$ and therefore a good $q_\phi$ is basically the problem of \textbf{variational Bayesian inference}. Essentially we are trying to construct an encoder and a decoder.

  \begin{figure}[H]
    \centering 
    \includegraphics[scale=0.4]{img/latent_variable_model.png}
    \caption{If $q_{\phi} = p_{\theta}$, then the diagram commutes, i.e. $p(z) p_{\theta}(x \mid z) = p(x) p_{\theta}(z \mid x) = p_{\theta} (x, z)$.} 
    \label{fig:latent_variable_model}
  \end{figure}

  As we have stated before (and in pretty much all density estimation problems), our job is to maximize the log likelihood of the training set: 
  \begin{equation}
    \sum_{i} \log p(x^{(i)})
  \end{equation}

  In order to do this for this problem, we need a little fact from information theory. 

  \begin{theorem}[Log Likelihood vs Conditional Entropy]
    The KL divergence can be decomposed to 
    \begin{equation}
      KL \big( q_\phi (z \mid x) \mid\mid p_{\theta} (z \mid x) \big) = \mathbb{E}_{q_\phi(z \mid x)} [ \log q_{\phi} (z \mid x)] + \log p_{\theta} (x) - \mathbb{E}_{q_{\phi} (z \mid x)} [\log p_{\theta} (x, z)]
    \end{equation}
    and hence 
  \end{theorem}
  \begin{proof}
    Starting with the definition of KL divergence:
    \begin{align}
      KL(q_\phi(z \mid x) \mid\mid p_\theta(z \mid x)) &= \mathbb{E}_{q_\phi(z \mid x)}\left[\log \frac{q_\phi(z \mid x)}{p_\theta(z \mid x)}\right] \\
      &= \mathbb{E}_{q_\phi(z \mid x)}[\log q_\phi(z \mid x)] - \mathbb{E}_{q_\phi(z \mid x)}[\log p_\theta(z \mid x)]
    \end{align}

    By Bayes' rule, we know that
    \begin{equation}
      p_\theta(z \mid x) = \frac{p_\theta(x, z)}{p_\theta(x)}
    \end{equation}

    Substituting this into our equation gives 
    \begin{align}
      KL(q_\phi(z \mid x) \mid\mid p_\theta(z \mid x)) &= \mathbb{E}_{q_\phi(z \mid x)}[\log q_\phi(z \mid x)] - \mathbb{E}_{q_\phi(z \mid x)}\left[\log \frac{p_\theta(x, z)}{p_\theta(x)}\right] \\
      &= \mathbb{E}_{q_\phi(z \mid x)}[\log q_\phi(z \mid x)] - \mathbb{E}_{q_\phi(z \mid x)}[\log p_\theta(x, z)] + \mathbb{E}_{q_\phi(z \mid x)}[\log p_\theta(x)]
    \end{align}

    Since $\log p_\theta(x)$ is constant with respect to $z$, we can take it out of the expectation. 
    \begin{equation}
      \mathbb{E}_{q_\phi(z \mid x)}[\log q_\phi(z \mid x)] - \mathbb{E}_{q_\phi(z \mid x)}[\log p_\theta(x, z)] + \log p_\theta(x)
    \end{equation}
  \end{proof}

  Therefore maximizing the log-likelihood is equivalent to minimizing the KL-divergence. 
  \begin{equation}
    \log p_\theta (x) = KL \big( q_\phi (z \mid x) \mid\mid p_{\theta} (z \mid x) \big) + \mathbb{E}_{q_{\phi} (z \mid x)} [\log p_{\theta} (x, z)] - \mathbb{E}_{q_\phi(z \mid x)} [ \log q_{\phi} (z \mid x)]
  \end{equation}
  But again the KL divergence part is intractable due to $p_\theta (z \mid x)$ being intractable. Using the fact that the KL divergence is always greater than or equal to $0$, we can drop the term and set a lower bound on the log likelihoods. This lower bound is called the \textit{variational lower bound}.  
  \begin{equation}
    \sum_{i=1}^N \log p_{\theta} (x^{(i)}) \geq \sum_{i=1}^N \mathbb{E}_{q_\phi (z \mid x^{(i)})} [ \log p_{\theta} (x^{(i)}, z)] - \sum_{i=1}^N \mathbb{E}_{q_{\phi} (z \mid x^{(i)})} [ \log q_{\phi} (z \mid x^{(i)}) ]
  \end{equation} 

  \begin{definition}[Variational Lower Bound]
    The \textbf{variational lower bound} of the dataset $\mathcal{D}$ is defined 
    \begin{equation}
      \elbo(\mathcal{D}, \phi, \theta) = \sum_{i=1}^N \mathbb{E}_{q_\phi (z \mid x^{(i)})} [ \log p_{\theta} (x^{(i)}, z)] - \sum_{i=1}^N \mathbb{E}_{q_{\phi} (z \mid x^{(i)})} [ \log q_{\phi} (z \mid x^{(i)})] 
    \end{equation}
    which can be decomposed into the sums of the variational lower bounds of the individual data points. 
    \begin{equation}
      \elbo(\mathcal{D}, \phi, \theta) = \sum_i \elbo(x^{(i)}, \phi, \theta) 
    \end{equation}
    where 
    \begin{equation}
      \elbo(x^{(i)}, \phi, \theta) = \mathbb{E}_{q_\phi (z \mid x^{(i)})} [ \log p_{\theta} (x^{(i)}, z)] - \mathbb{E}_{q_{\phi} (z \mid x^{(i)})} [ \log q_{\phi} (z \mid x^{(i)})] 
    \end{equation}
  \end{definition}

  Note that we can alternatively define ELBO using Jensen's inequality. 

  \begin{definition}[Evidence Lower Bound]
    To lower bound it, we can use Jensen's inequality\footnote{Given convex function $f: \mathbb{R} \rightarrow \mathbb{R}$, and random variable $X$, $\mathbb{E}[f(x)] \geq f(\mathbb{E}[X])$.} with the concave function $f(x) = \log(x)$ over domain $\mathbb{R}^+$ and the following holds true for all $\theta$ and more importantly, for \textit{any arbitrary density function} $q(z)$. Therefore, we have 
    \begin{align}
      \ell(\theta) & = \log p_\theta (x) \\
                   & = \log \int p_\theta (x, z) \,dz \\
                   & = \log \int q_\phi (z) \frac{p_\theta (x, z)}{q_\phi (z)} \,dz \\ 
                   & \geq \int q_\phi (z \mid x) \log \bigg( \frac{p_\theta (x, z)}{q_\phi (z)} \bigg) \,dz \\  
                   & = \elbo(x, q_\phi)
    \end{align}
    The lower bound is called the \textbf{evidence lower bound (ELBO)}, and the ELBO of the whole dataset is 
    \begin{equation}
      \elbo(\mathcal{D}, \phi, \theta) = \sum_{i=1}^N \elbo(x^{(i)}, \phi, \theta)
    \end{equation}
  \end{definition} 

  Note that this lower bound is with respect to \textit{any} distribution $q_\phi$, and it is because of this flexibility that we choose $q_\phi$ in the first place. Therefore, we can vary $\phi$ in hopes that the lower bound is maximized, and optimize with respect to this, hence the name \textit{variational}. For more interpretability, look at the corollary. 

  \begin{corollary}[Decomposition of ELBO]
    The following decomposition of ELBO shows that maximizing the ELBO simultaneously attempts to keep $q_\phi$ close to $p$ and concentrate $q_\phi(z \mid x)$ on those $z$ that maximizes $\ln p_\theta (x \mid z)$. That is, the approximate posterior $q_\phi$ balances between staying close to the prior $p(z)$ and moving towards the maximum likelihood $\argmax_z \ln p_\theta (x \mid z)$. 
    \begin{equation}
      \elbo(x^{(i)}, \phi, \theta) = \underbrace{\mathbb{E}_{q_\phi (z \mid x^{(i)})} [\log p_{\theta} (x^{(i)} \mid z)]}_{\substack{\text{likelihood term} \\ \text{(reconstruction part)}}}- \underbrace{KL(q_{\phi} (z \mid x^{(i)}) \mid\mid p(z))}_{\substack{\text{closeness of encoding to } p(z) \\ \text{(typically Gaussian)}}}
    \end{equation}
    Note the first expression is the likelihood term, which measures the reconstruction quality of the decoder $p_\theta(x^{(i)} \mid z)$ averaged over encodings sampled from $q_\phi(z \mid x^{(i)})$. The second term is the KL divergence between the encoder distribution $q_\phi(z \mid x^{(i)})$ and the prior $p(z)$, which acts as a regularizer by ensuring the encoded distributions remain close to the chosen prior, typically a standard normal distribution.
  \end{corollary}
  \begin{proof}
    Starting with the ELBO for a single data point:
    \begin{align*}
      \elbo(x^{(i)}, \phi, \theta) &= \mathbb{E}_{q_\phi(z \mid x^{(i)})}[\log p_\theta(x^{(i)}, z)] - \mathbb{E}_{q_\phi(z \mid x^{(i)})}[\log q_\phi(z \mid x^{(i)})]
    \end{align*}

    Using the chain rule of probability for the joint distribution:
    \begin{align*}
      p_\theta(x^{(i)}, z) = p_\theta(x^{(i)} \mid z)p(z)
    \end{align*}

    Substituting this into our ELBO:
    \begin{align*}
      \elbo(x^{(i)}, \phi, \theta) &= \mathbb{E}_{q_\phi(z \mid x^{(i)})}[\log p_\theta(x^{(i)} \mid z) + \log p(z)] - \mathbb{E}_{q_\phi(z \mid x^{(i)})}[\log q_\phi(z \mid x^{(i)})] \\
      &= \mathbb{E}_{q_\phi(z \mid x^{(i)})}[\log p_\theta(x^{(i)} \mid z)] + \mathbb{E}_{q_\phi(z \mid x^{(i)})}[\log p(z)] - \mathbb{E}_{q_\phi(z \mid x^{(i)})}[\log q_\phi(z \mid x^{(i)})] \\
      &= \mathbb{E}_{q_\phi(z \mid x^{(i)})}[\log p_\theta(x^{(i)} \mid z)] - \left(\mathbb{E}_{q_\phi(z \mid x^{(i)})}[\log q_\phi(z \mid x^{(i)})] - \mathbb{E}_{q_\phi(z \mid x^{(i)})}[\log p(z)]\right) \\
      &= \underbrace{\mathbb{E}_{q_\phi(z \mid x^{(i)})}[\log p_\theta(x^{(i)} \mid z)]}_{\text{reconstruction term}} - \underbrace{KL(q_\phi(z \mid x^{(i)}) \mid\mid p(z))}_{\text{KL divergence term}}
    \end{align*}
  \end{proof} 

  Therefore, maximizing the ELBO will simultaneously allow us to obtain an accurate generative model $p_\theta (x \mid z) \approx p^\ast (x \mid z)$ and an accurate discriminative model $q_\phi (z \mid x) \approx p_\theta (z \mid x)$. The next step is to actually maximize the ELBO with respect to both $\theta$ and $\phi$. To do this we need to compute the derivatives of $\elbo$ w.r.t. to $\phi$ and $\theta$. 

  \begin{equation}
    \max_{\phi, \theta} \elbo(\mathcal{D}, \phi, \theta)
  \end{equation}
  It turns out that this itself is a nonconvex optimization problem, and to make it doable we iterate between updating $\phi$ and $\theta$. Remember that the ELBO is really an expectation, i.e. an integral, and to get a good estimate of its derivative we must try to change it from the derivative of an expectation to the expectation of a derivative. The gradient with respect to $\theta$ is very easy since from measure theory, we are deriving and integrating over different variables. 

  \begin{lemma}[Gradient of ELBO w.r.t. $\theta$]
    For $\theta$, its unbiased gradient is 
    \begin{equation}
      \nabla_\theta \elbo(x, \theta, \phi) = \mathbb{E}_{q_\phi (z \mid x)} \big[ \nabla_\theta \log p_\theta (x \mid z) \big]
    \end{equation}
    and therefore we can approximate the gradient by sampling $L$ points $p^{(1)}, \ldots, p^{(L)}$ from $p(z)$ and computing the gradient of the log (since we know the closed form of the conditional distribution given $z$), and finally averaging them. 
    \begin{equation}
      \nabla_\theta \elbo(x, \theta, \phi) \approx \frac{1}{L} \sum_{l=1}^L \nabla_\theta \log p_\theta (x \mid z^{(l)})
    \end{equation}
    which is guaranteed to converge by the law of large numbers, and furthermore, we can do this for any batch size $L$.  
  \end{lemma}
  \begin{proof}
    Note that the KL divergence does not depend on $\theta$ and neither does the prior, so they can be removed 
    \begin{align} 
      \nabla_\theta \elbo(x, \theta, \phi) & = \nabla_\theta \big\{ \mathbb{E}_{q_\phi (z \mid x)} [ \log p_\theta (x, z)] - \mathbb{E}_{q_\phi (z \mid x)} [\log q_{\phi} (z \mid x)] \} \\ 
                                           & = \nabla_\theta \big\{ \mathbb{E}_{q_\phi (z \mid x)} [ \log p_\theta (x, z)] \} \\ 
                                           & = \mathbb{E}_{q_\phi (z \mid x)} \big[ \nabla_\theta \{ \log p_\theta (x, z) \big] \\
                                           & = \mathbb{E}_{q_\phi (z \mid x)} \big[ \nabla_\theta \{ \log p_\theta (x \mid z) - \log p_(z) \} \big] \\
                                           & = \mathbb{E}_{q_\phi (z \mid x)} \big[ \nabla_\theta \log p_\theta (x \mid z) \big] 
    \end{align}
  \end{proof}

  However, taking the gradient w.r.t. $\phi$ is more complicated since we cannot put the gradient in the expectation, i.e. swap the derivative and integral (since we are deriving and integrating w.r.t. $\phi$). Fortunately, we have a well-known mathematical identity often used in policy gradient algorithms in reinforcement learning. \cite{W92} 

  \begin{lemma}[Log-Derivative Trick]
    The following identity holds. 
    \begin{equation}
      \nabla_\phi \mathbb{E}_{q_\phi(z)}[f(z)] = \mathbb{E}_{q_\phi(z)}[f(z)\nabla_\phi \log q_\phi(z)]
    \end{equation}
  \end{lemma}
  \begin{proof}
    First, let's write out the left-hand side using the definition of expectation:
    \begin{align*}
    \nabla_\phi \mathbb{E}_{q_\phi(z)}[f(z)] &= \nabla_\phi \int f(z)q_\phi(z)dz
    \end{align*}

    Under suitable regularity conditions, we can exchange the gradient and integral operators:
    \begin{align*}
    &= \int f(z)\nabla_\phi q_\phi(z)dz
    \end{align*}

    Now, we multiply and divide by $q_\phi(z)$ inside the integral:
    \begin{align*}
    &= \int f(z) q_\phi(z) \frac{\nabla_\phi q_\phi(z)}{q_\phi(z)} dz
    \end{align*}

    Recognize that $\nabla_\phi \log q_\phi(z) = \frac{\nabla_\phi q_\phi(z)}{q_\phi(z)}$ by the chain rule:
    \begin{align*}
    &= \int f(z) q_\phi(z) \nabla_\phi \log q_\phi(z) dz
    \end{align*}

    Finally, we can rewrite this back as an expectation:
    \begin{align*}
    &= \mathbb{E}_{q_\phi(z)}[f(z)\nabla_\phi \log q_\phi(z)]
    \end{align*}
  \end{proof}

  \begin{example}[Gradient of Expection of $f(x) = x^2$ w.r.t. Gaussian]
    Assume we have a normal distribution $q$ that is parameterized by $\phi$, specifically
    $q_\phi(x) = N(\phi, 1)$. We want to solve the below problem
    \begin{equation}
      \min_\phi \mathbb{E}_q[x^2]
    \end{equation}

    This is of course a rather silly problem and the optimal $\phi = 0$ is obvious. One way to calculate $\nabla_\phi \mathbb{E}[x^2]$ is using the log-derivative trick as follows
    \begin{align}
      \nabla_\phi \mathbb{E}_q[x^2] &= \nabla_\phi\int q_\phi(x)x^2dx \\
      &= \int x^2\nabla_\phi q_\phi(x)\frac{q_\phi(x)}{q_\phi(x)}dx \\
      &= \int q_\phi(x)\nabla_\phi\log q_\phi(x)x^2dx \\
      &= \mathbb{E}_q[x^2\nabla_\phi\log q_\phi(x)]
    \end{align}

    For our example where $q_\phi(x) = N(\phi, 1)$, this method gives
    \begin{equation}
      \nabla_\phi \mathbb{E}[x^2] = \mathbb{E}_q[x^2(x-\phi)]
    \end{equation}
  \end{example}

  Using this on the gradient of ELBO w.r.t. $\phi$ gives the following form as the expectation of the gradient. 

  \begin{lemma}
    We can use the score function estimator. 
    \begin{align}
      \nabla_\phi \elbo(x, \theta, \phi) & = \nabla_\phi \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x,z) - \log q_\phi(z|x)] \\
                                         & = \mathbb{E}_{q_\phi(z|x)}\big[ \nabla_\phi \big\{ \log q_\phi(z|x)(\log p_\theta(x,z) - \log q_\phi(z|x)) \big\} \big]
    \end{align}
  \end{lemma}
  \begin{proof}
  \end{proof}

  However, REINFORCE is known to have high variance, and so we need large batch sizes $L$ for good convergence. Many methods such as \cite{reduce, reduce2} were developed to reduce this. Later it was shown in \cite{vae1} that the \textit{reparamaterization trick} beat everything else, allowing us to efficiently train neural-net-based nonlinear latent variable models, e.g. the variational autoencoder. We will focus on the reparameterization trick in my deep learning notes and omit it here.  Now that we have approximate closed form solutions for the gradients, we can optimize the two using coordinate ascent. Note that we have shown this for a single sample $x$, and ideally we would do this for a minibatch of samples $x^{(i)}$. 

  \begin{algo}[Coordinate Ascent Variational Inference]
    A common approach to maximize the ELBO is coordinate ascent, where we alternatively optimize with respect to $\phi$ and $\theta$:

    \begin{algorithm}[H]
      \caption{Coordinate Ascent Variational Inference (CAVI) with Reparameterization}
      \begin{algorithmic}[1]
        \Require Initial parameters $\theta^{[0]}$, $\phi^{[0]}$, batch size B, number of samples L
        \While{not converged}
          \State // E-step: optimize variational parameters
          \State Sample minibatch $\{x^{(1)}, \ldots, x^{(B)}\}$ from dataset $\mathcal{D}$
          \State Sample noise $\{\epsilon^{(1)}, \ldots, \epsilon^{(L)}\} \sim p(\epsilon)$ for reparameterization
          \State Transform noise to latent variables: $z^{(l)} = g_{\phi^{[t]}}(\epsilon^{(l)}, x)$ for $l=1,\ldots,L$
          \State // Approximate gradient using Monte Carlo samples
          \State $\hat{g}_\phi \gets \frac{1}{BL} \sum_{i=1}^B \sum_{l=1}^L [\nabla_\phi \log p_{\theta^{[t]}}(x^{(i)} \mid z^{(l)}) - \nabla_\phi \log q_{\phi^{[t]}}(z^{(l)} \mid x^{(i)}) + \nabla_\phi \log p(z^{(l)})]$
          \State $\phi^{[t+1]} \gets \phi^{[t]} + \eta_\phi \hat{g}_\phi$ \Comment{Update with learning rate $\eta_\phi$}
          \State // M-step: optimize model parameters
          \State $\hat{g}_\theta \gets \frac{1}{BL} \sum_{i=1}^B \sum_{l=1}^L \nabla_\theta \log p_{\theta^{[t]}}(x^{(i)} \mid z^{(l)})$
          \State $\theta^{[t+1]} \gets \theta^{[t]} + \eta_\theta \hat{g}_\theta$ \Comment{Update with learning rate $\eta_\theta$}
        \EndWhile
      \end{algorithmic}
    \end{algorithm}
  \end{algo} 

  Once we are done, we have our optimized encoder and decoders $p_\theta$ and $q_\phi$. 

\subsection{EM Algorithm} 

  Let's consider a slightly simpler sub-problem where we have covariates $x^{(i)} \sim X$ coming from distribution $p(x)$. We can again add latent random variables $Z$ but rather than being fixed, the prior $p_\theta (z)$ is also parameterized by $\theta$. Therefore, we would like to find 
  \begin{equation}
    \argmax_\theta p_\theta (x) = \argmax_\theta \int p_\theta (x \mid z) \, p_\theta (z) \,dz
  \end{equation} 
  Even though this integral is not tractable, we will assume that $p_\theta (z \mid x)$ can be computed for a given $\theta$. Let's try to redo our algorithm again with computable posterior assumptions. We have a training set $\mathcal{D} = \{x^{(i)}\}_{i=1}^n \in \mathbb{R}^d$, which we assume are generated by some latent distributions $p_\theta (z)$ followed by the generative component $p_\theta (x \mid z)$. Then, we bound the likelihood of each sample $x^{(i)}$ by an ELBO that varies for all distributions $q^{(i)}$ (we write $q$ rather than $q_\phi$ since the $\phi$ will be irrelevant here). 
  \begin{equation}
    \log p_{\theta} (x^{(i)}) \geq \elbo(x^{(i)}, q^{(i)}, \theta) = \mathbb{E}_{q^{(i)} (z \mid x^{(i)})} [ \log p_{\theta} (x^{(i)}, z)] - \mathbb{E}_{q^{(i)} (z \mid x^{(i)})} [ \log q^{(i)} (z \mid x^{(i)}) ]
  \end{equation} 
  Summing this all up gives the ELBO of our dataset, which is a lower bound for \textit{all} collections of distributions $q^{(1)}, \ldots, q^{(n)}$. 
  \begin{align}
    \sum_{i=1}^N \log p_{\theta} (x^{(i)}) & \geq \elbo(\mathcal{D}, q^{(1)}, \ldots, q^{(n)}, \theta) \\ 
                                           & = \sum_{i=1}^N \mathbb{E}_{q^{(i)} (z \mid x^{(i)})} [ \log p_{\theta} (x^{(i)}, z)] - \sum_{i=1}^N \mathbb{E}_{q^{(i)} (z \mid x^{(i)})} [ \log q^{(i)} (z \mid x^{(i)}) ]
  \end{align} 
  We maximized the ELBO w.r.t. $q$ and $\theta$ by using CAVI, but by invoking our assumption that the posterior $p_\theta (z \mid x)$ can be computed, we can immediately find a maximum. 

  \begin{theorem}[Posterior Maximizes ELBO] 
    When we set $q^(i) (z \mid x) = p(z \mid x^{(i)})$, equality is achieved. 
    \begin{align}
      \sum_{i=1}^N \log p_{\theta} (x^{(i)}) & = \elbo(\mathcal{D}, q^{(1)}, \ldots, q^{(n)}, \theta) \\ 
                                             & = \sum_{i=1}^N \mathbb{E}_{q^{(i)} (z \mid x^{(i)})} [ \log p_{\theta} (x^{(i)}, z)] - \sum_{i=1}^N \mathbb{E}_{q^{(i)} (z \mid x^{(i)})} [ \log q^{(i)} (z \mid x^{(i)}) ]
    \end{align}
  \end{theorem}
  \begin{proof}
    Let's start by examining the gap between $\log p_\theta(x^{(i)})$ and the ELBO. From our previous derivations, this gap is the KL divergence:
    \begin{align}
      \log p_\theta(x^{(i)}) - \elbo(x^{(i)}, q^{(i)}, \theta) &= KL(q^{(i)}(z|x^{(i)}) \| p_\theta(z|x^{(i)})) \\
      &= \mathbb{E}_{q^{(i)}}[\log q^{(i)}(z|x^{(i)}) - \log p_\theta(z|x^{(i)})]
    \end{align}

    When we set $q^{(i)}(z|x^{(i)}) = p_\theta(z|x^{(i)})$:
    \begin{align}
      KL(p_\theta(z|x^{(i)}) \| p_\theta(z|x^{(i)})) &= \mathbb{E}_{p_\theta}[\log p_\theta(z|x^{(i)}) - \log p_\theta(z|x^{(i)})] \\
      &= \mathbb{E}_{p_\theta}[0] = 0
    \end{align}

    Therefore, when summing over all samples:
    \begin{align}
      \sum_{i=1}^N \log p_\theta(x^{(i)}) - \elbo(\mathcal{D}, q^{(1)}, \ldots, q^{(n)}, \theta) &= \sum_{i=1}^N KL(q^{(i)}(z|x^{(i)}) \| p_\theta(z|x^{(i)})) = 0
    \end{align}
  \end{proof} 

  Therefore, our CAVI algorithm has been decomposed into the following. 
  \begin{enumerate}
    \item E-step. Maximizing ELBO over the variational parameters $q_\phi$ is really just setting all the $q^{(i)}$ to the posteriors. Note that this is with respect to a fixed $\theta$ only. 
    \item M-step. Maximizing ELBO over the model parameters $\theta$ with fixed $q$ is the same by taking the gradient w.r.t. $\theta$ which is easy. 
  \end{enumerate}
  This results in the following algorithm. 

  \begin{algo}[EM Algorithm]
    The EM algorithm is described as such: 
    \begin{enumerate}
      \item Initialize $\theta$.
      \item \textit{E-Step}. Since $\log p_\theta(x)$ is bounded below for all $q^{(1)}, \ldots, q^{(n)}$ as 
      \begin{equation}
        \sum_{i=1}^N \log p_\theta(x^{(i)}) \geq \sum_{i=1}^N \elbo(x^{(i)}, q^{(i)}, \theta)
      \end{equation}
      setting $q^{(i)}(z|x^{(i)}) = p_\theta(z|x^{(i)})$ for all $i = 1, \ldots, N$ achieves equality. Note that this equality only holds for the current fixed value of $\theta$.

      \item \textit{M-Step}. We maximize with respect to $\theta$ whilst fixing $q^{(i)}$.\footnote{For specific models like GMM as we will see later, this maximization has closed-form solutions, e.g. $\phi = $ average of responsibilities $\mu_k = $: weighted average of points, $\Sigma_k = $ weighted covariance. For other distributions, this maximum must be found analytically or numerically.}
      \begin{align}  
        \theta & = \argmax_\theta \sum_{i=1}^N \elbo(x^{(i)}, q^{(i)}, \theta) \\
        & = \argmax_\theta \sum_{i=1}^N \mathbb{E}_{q^{(i)}(z|x^{(i)})}[\log p_\theta(x^{(i)}, z)] - \sum_{i=1}^N \mathbb{E}_{q^{(i)}(z|x^{(i)})}[\log q^{(i)}(z|x^{(i)})]
      \end{align}

      \item Repeat steps 2 and 3 until convergence. Step 2 brings improvements because changing $\theta$ creates a new sum of ELBO functions as a new lower bound.
    \end{enumerate}
  \end{algo}

  The EM algorithm is a specific instance of ELBO optimization! The additional assumption that EM has is that we can calculate the posterior densities. 

  \begin{corollary}[Connection to ELBO]
    The EM algorithm can be viewed as coordinate ascent on the ELBO where:
    \begin{itemize}
      \item E-step: Sets $q(z) = p_{\theta^{[t]}}(z|x)$, maximizing ELBO over q
      \item M-step: Maximizes ELBO over $\theta$ with fixed q
    \end{itemize}
  \end{corollary}

  Note that there is a duality between the true parameters $\theta$ and the latent variables $z$. If $\theta$ is known, then the values of $z$ can be found by maximizing the log-likelihood over all possible values of $z$. Conversely, if we know the value of the latent variables $z$, then we can find an estimate of the parameters by grouping the data points into each value of $z$ and optimizing $p_\theta (x \mid z)$, e.g. by averaging the values. This suggests an iterative algorithm in the case where both $\theta$ and $z$ are unknown. We assume that we know $\theta$ and optimize $z$, then optimize $\theta$, and so on, similar to $k$-means clustering. 

  We can formulate the algorithm alternatively yet equivalently.  

  \begin{algo}[EM Algorithm]
    The \textbf{Expectation-Maximization algorithm} optimizes the likelihood above with the following steps. 
    \begin{enumerate}
      \item First initialize $\theta = \theta^{[0]}$ in some way.\footnote{Note that within this $\theta$ are the parameterizations of the initial multinomial density $p_Z$, which is our initial ``guess'' of the distribution of $Z$.}

      \item \textit{E-Step}. Define 
      \begin{equation}
        Q(\theta \mid \theta^{[t]}) = \mathbb{E}_{p_\theta (z \mid x)}[ \log p_\theta(x, z) ] = \int p_{\theta^{[t]}} (z \mid x) \log{p_\theta (x, z)} \,dz
      \end{equation}
      as the expected value of the log-likelihood with respect to the current conditional distribution of $z$, given $x$ and $\theta^{[t]}$. 

      \item \textit{M-Step}. Find the parameters that maximize this quantity. 
      \begin{equation}
        \theta^{[t+1]} = \argmax_\theta Q(\theta \mid \theta^{[t]}]
      \end{equation}
    \end{enumerate}
  \end{algo}

  \begin{theorem}[EM Monotonicity]
    The EM algorithm monotonically increases the observed data log-likelihood:
    \begin{equation}
      \log p_{\theta^{[t+1]}}(x] \geq \log p_{\theta^{[t]}}(x)
    \end{equation}
    Therefore, though there is no guarantee that this will hit the global maximum, it will hit a local maximum. 
  \end{theorem}
  \begin{proof}
    Let's consider the difference in log-likelihoods between iterations:
    \begin{align}
      \log p_{\theta^{[t+1]}}(x) - \log p_{\theta^{[t]}}(x) &= \left[Q(\theta^{[t+1]}|\theta^{[t]}) - H(\theta^{[t+1]}|\theta^{[t]})\right] \\
      &\quad - \left[Q(\theta^{[t]}|\theta^{[t]}) - H(\theta^{[t]}|\theta^{[t]})\right]
    \end{align}
    where $H(\theta|\theta^{[t]}) = \mathbb{E}_{z|x,\theta^{[t]}}[\log p_{\theta}(z|x)]$. By the M-step, we know $Q(\theta^{[t+1]}|\theta^{[t]}) \geq Q(\theta^{[t]}|\theta^{[t]})$. Also, by Jensen's inequality:
    \begin{equation}
      H(\theta^{[t+1]}|\theta^{[t]}) \leq H(\theta^{[t]}|\theta^{[t]})
    \end{equation}
    Therefore, the difference is non-negative.
  \end{proof} 

  For some intuition, we can visualize $l$ as a function of $\theta$. For the sake of visuals, we will assume that $\theta \in \mathbb{R}$ and $l: \mathbb{R} \longrightarrow \mathbb{R}$. On the contrary to what a visual is supposed to do, we want to point out that we cannot just visualize $l$ as a curve in $\mathbb{R} \times \mathbb{R}$. This can be misleading since then it implies that the optimal $\theta$ value is easy to find, as shown in the left. Rather, we have no clue what the whole curve of $l$ looks like, but we can get little snippets (right). 

  \begin{figure}[H]
    \centering 
    \includegraphics[width=0.6\textwidth]{img/visual_of_l.jpg}
    \caption{} 
    \label{fig:visual_of_l}
  \end{figure}

  Rather, all we can do is hope to take whatever easier-to-visualize, lower-bound functions and maximize them as much as we can in hopes of converging onto $l$. Let us walk through the first two iterations of the EM algorithm. We first initialize $\theta$ to, say $\theta_0$. This immediately induces the lower-bound ELBO-sum function $\sum_{i} \text{ELBO} (x^{(i)};\, p_Z^{*i}, \theta)$, which takes in multinomial density functions $p_Z^{*i} = p_1, p_2, \ldots$ and outputs different functions of $\theta$ that are valid lower bounds. Two of these possible lower-bound functions are shown (in green) for when we input some arbitrary density $p_1, p_2$. However, there exists a density $p_Z^{(i)}$ that produces not only the maximum possible lower-bound (called max ELBO, shown in red) but is equal to $l(\theta)$ for that density input $p_Z^{(i)}$. We maximize this function with respect to $\theta$ to get $\theta_1$ as our next assignment of $\theta$. 

  \begin{figure}[H]
    \centering 
    \includegraphics[width=0.7\textwidth]{img/EM_first_iteration.jpg}
    \caption{} 
    \label{fig:EM_first_iteration}
  \end{figure}

  The next step is identical. Now that we have a new value of $\theta = \theta_1$, this induces the lower-bound ELBO-sum function $\sum_{i} \text{ELBO} (x^{(i)};\, p_Z^{*i}, \theta)$ that also takes in multinomial densities $p_Z^{*i}$ and outputs different functions of $\theta$ that are valid lower-bounds. Two possible lower bounds are shown (in green), but the maximum lower-bound (in blue) is produced when we input density $p_Z^{(i)}$. Since this max ELBO function is equal to $\theta$ for this fixed density input $p_Z^{(i)}$, we maximize this function with respect to $\theta$ to get $\theta_2$ as our next assignment of $\theta$. 

  \begin{figure}[H]
    \centering 
    \includegraphics[width=0.7\textwidth]{img/EM_second_iteration.jpg}
    \caption{} 
    \label{fig:EM_second_iteration}
  \end{figure}

\subsection{Gaussian Mixture Models}

  Given a training set ${x^{(i)}}_{i=1}^n$ (without the $y$-labels and so in the unsupervised setting), there are some cases where it may seem like we can fit multiple Gaussian distributions in the input space $\mathcal{X}$. For example, the points below seem like they can be fitted well with 3 Gaussians.

  \begin{figure}[H]
    \centering
    \begin{tikzpicture}
      % Draw axes
      \draw[black, ->, line width=0.8pt] (-0.5,0) -- (10,0);
      \draw[black, ->, line width=0.8pt] (0,-0.5) -- (0,6);
      
      % Equations at top
      \node[blue] at (4,5.5) {$X|Z=1 \sim \mathcal{N}(\mu_1, \Sigma_1)$};
      \node[green!50!black] at (4,4.8) {$X|Z=2 \sim \mathcal{N}(\mu_2, \Sigma_2)$};
      \node[red] at (4,4.1) {$X|Z=3 \sim \mathcal{N}(\mu_3, \Sigma_3)$};
      
      % First Gaussian (blue)
      \draw[blue, rotate around={45:(2,2)}] (2,2) ellipse (2 and 1);
      \draw[blue, rotate around={45:(2,2)}] (2,2) ellipse (1.4 and 0.7);
      \draw[blue, rotate around={45:(2,2)}] (2,2) ellipse (0.8 and 0.4);
      
      % Blue cluster points - more spread out
      \fill[black] (1.7,1.7) circle (1.5pt);
      \fill[black] (1.8,2.1) circle (1.5pt);
      \fill[black] (2.1,2.4) circle (1.5pt);
      \fill[black] (2.2,1.8) circle (1.5pt);
      \fill[black] (1.9,2.3) circle (1.5pt);
      \fill[black] (1.4,1.7) circle (1.5pt);
      \fill[black] (2.5,2.2) circle (1.5pt);
      \fill[black] (2.7,2.4) circle (1.5pt);
      \fill[black] (1.2,1.5) circle (1.5pt);
      \fill[black] (2.9,2.6) circle (1.5pt);
      \fill[black] (3.0,2.7) circle (1.5pt);
      \fill[black] (1.0,1.3) circle (1.5pt);
      
      % Second Gaussian (green)
      \draw[green!50!black, rotate around={-30:(5,3)}] (5,3) ellipse (1.5 and 0.6);
      \draw[green!50!black, rotate around={-30:(5,3)}] (5,3) ellipse (1 and 0.4);
      \draw[green!50!black, rotate around={-30:(5,3)}] (5,3) ellipse (0.5 and 0.2);
      
      % Green cluster points - more oval shaped
      \fill[black] (4.8,2.7) circle (1.5pt);
      \fill[black] (4.9,3.2) circle (1.5pt);
      \fill[black] (5.0,2.9) circle (1.5pt);
      \fill[black] (5.1,3.3) circle (1.5pt);
      \fill[black] (5.2,2.8) circle (1.5pt);
      \fill[black] (4.5,2.9) circle (1.5pt);
      \fill[black] (5.5,3.0) circle (1.5pt);
      \fill[black] (4.3,2.8) circle (1.5pt);
      \fill[black] (5.7,3.1) circle (1.5pt);
      
      % Third Gaussian (red)
      \draw[red, rotate around={60:(8,2)}] (8,2) ellipse (1.8 and 0.7);
      \draw[red, rotate around={60:(8,2)}] (8,2) ellipse (1.2 and 0.5);
      \draw[red, rotate around={60:(8,2)}] (8,2) ellipse (0.6 and 0.3);
      
      % Red cluster points - more spread out
      \fill[black] (7.8,1.7) circle (1.5pt);
      \fill[black] (7.9,2.2) circle (1.5pt);
      \fill[black] (8.0,1.9) circle (1.5pt);
      \fill[black] (8.1,2.4) circle (1.5pt);
      \fill[black] (8.2,2.0) circle (1.5pt);
      \fill[black] (7.5,1.9) circle (1.5pt);
      \fill[black] (8.5,2.3) circle (1.5pt);
      \fill[black] (7.3,1.7) circle (1.5pt);
      \fill[black] (8.7,2.5) circle (1.5pt);
      \fill[black] (8.9,2.3) circle (1.5pt);
      \fill[black] (7.1,1.6) circle (1.5pt);
    \end{tikzpicture}
    \caption{Example of data that can be fitted with 3 Gaussians}
  \end{figure}

  Therefore, we can construct a best-fit model as a composition of a multinomial distribution (to decide which one of the Gaussians $x$ should follow) followed by a Gaussian. 

  \begin{definition}[Gaussian Mixture Model] 
    The \textbf{Gaussian mixture model (GMM)} assumes that the covariates $x \sim X \in \mathbb{R}^d$ are generated by the following.\footnote{Therefore, our model says that each $x^{(i)}$ was generated by randomly choosing $z^{(i)}$ from ${1, \ldots, k}$ according to some multinomial, and then the $x^{(i)}$ was drawn from one of the $k$ Gaussians depending on $z^{(i)}$.} The parameters are $\theta = \{\lambda, \mu_1, \ldots, \mu_k, \Sigma_1, \ldots, \Sigma_k\}$.\footnote{Note that $\lambda$ really has $k-1$ free parameters and $\Sigma_i$'s should be symmetric and positive-definite.}

    \begin{enumerate}
      \item A latent variable $z \sim \mathrm{Multinomial}(\lambda)$, where $\lambda = (\lambda_1, \ldots, \lambda_k)$ with PMF defined 
      \begin{equation}
        p_\theta (z) = \lambda_z
      \end{equation}

      \item The generative random variable $X \mid Z = z \sim \mathcal{N}(\mu_i, \Sigma_i)$ where $\mu_z \in \mathbb{R}^d, \Sigma_z \in \mathbb{R}^{d \times d}$ and PDF defined 
      \begin{equation}
        p_\theta (x \mid z) = \frac{1}{(2\pi)^{d/2}|\Sigma_z|^{1/2}} \exp\left(-\frac{1}{2}(x-\mu_z)^\top\Sigma_z^{-1}(x-\mu_z)\right)
      \end{equation}
    \end{enumerate}
  \end{definition}

  We can write down the log-likelihood of the given data $x^{(i)}$'s as a function of all the parameters above as
  \begin{equation}
    \sum_{i=1}^n \log p_\theta (x^{(i)}) = \sum_{i=1}^n \log \bigg( \sum_{z=1}^k  p_\theta ( x^{(i)} \mid z^{(i)} ) , p_\theta ( z^{(i)} ) \bigg)
  \end{equation}

  \begin{example}[Dual Nature of Latents and Parameters]
    Note that since we only know that the \textit{final} value of the $i$th sample is $x^{(i)}$ and not anything at all about which value $z^{(i)}$ the $i$th sample had, there is an extra unknown in this model. If we did know the values of the hidden variables $z^{(i)}$ (i.e. if we knew which of the $k$ Gaussians each $x^{(i)}$ was generated from), then our log likelihood function would be much more simple since now, our givens will be both $x^{(i)}$ \textit{and} $z^{(i)}$. Therefore, we don't have to condition on the $z^{(i)}$ and can directly calculate the log of the probability of us having sample values $(z^{(1)}, x^{(1)}), (z^{(2)}, x^{(2)}), \ldots, (z^{(n)}, x^{(n)})$.

    \begin{equation}
      \sum_{i=1}^n \log p(x^{(i)}) = \sum_{i=1}^n \log p( x^{(i)}, z^{(i)}) = \sum_{i=1}^n \log p( x^{(i)} \mid z^{(i)}) \, p(z^{(i)})
    \end{equation}

    This model, with known $z^{(i)}$'s, is basically the GDA model, which is easy to calculate. That is, the maximum values of $\phi, \mu, \Sigma$ are

    \begin{align*}
      \phi_j & = \frac{1}{n} \sum_{i=1}^n \mathbbm{1}_{z^{(i)} = j} \\
      \mu_j & = \frac{\sum_{i=1}^n \mathbbm{1}_{z^{(i)} = j} x^{(i)}}{\sum_{i=1}^n \mathbbm{1}_{z^{(i)} = j}} \\
      \Sigma_j & = \frac{1}{\sum_{i=1}^n \mathbbm{1}_{z^{(i)} = j}} \sum_{i=1}^n \mathbbm{1}_{z^{(i)}} \big( x^{(i)} - \mu_j \big),\big(x^{(i)} - \mu_j \big)^T
    \end{align*}
  \end{example}

  But since we do \textit{not} know the values of $z^{(i)}$, we first try to ``guess'' the values of the $z^{(i)}$'s and then update the parameters of our model assuming our guesses are correct. 

  \begin{algo}[EM Algorithm on GMMs]
    The EM Algorithm applied to GMMs has the following steps:

    \begin{enumerate}
      \item Randomly initialize $\theta^{[0]} = \{\lambda, \mu_1, \ldots, \mu_k, \Sigma_1, \ldots, \Sigma_k\}$.\footnote{This might converge faster using K-means initialization.} 

      \item \textbf{(E Step)} Calculate the posterior density $p(z \mid x)$ by applying Bayes rule to each sample keeping the parameter $\theta^{[t]}$ fixed. 
      \begin{equation}
        p_{\theta^{[t]}} (z \mid x^{(i)}) = \frac{p_{\theta^{[t]}} (x^{(i)} \mid z) \, p_{\theta^{[t]}}(z)}{p(x)} = \frac{p_{\theta^{[t]}} (x^{(i)} \mid z) \, p_{\theta^{[t]}}(z)}{\sum_z p_{\theta^{[t]}} (x^{(i)} \mid z) \, p_{\theta^{[t]}}(z)}
      \end{equation}
      We should have $n$ different multinomial distribution parameters, each representing our best guess of what multinomial density $p(z \mid x^{(i)})$ each $x^{(i)}$ had followed in order to be at the given points. Let's label the updated parameters of the multinomial distribution of the $i$th sample to be $\lambda^{[t](i)}$ at the $t$th iteration. 

      \item \textbf{(M Step)} We update $\theta$ as such. 
      \begin{align}
        \lambda^{[t+1]}  & = \frac{1}{n} \sum_{i=1}^n \lambda^{[t](i]} \\
        \mu_j^{[t+1]} & = \frac{\sum_{i=1}^n \lambda^{[t](i]}_j x^{(i)}}{\sum_{i=1}^n \lambda^{[t](i)}} \\ 
        \Sigma_j^{[t+1]} & = \frac{1}{\sum_{i=1}^n \lambda^{[t](i]}} \sum_{i=1}^n \lambda^{[t](i)}_j \,\big(x^{(i)} - \mu_j^{[t+1]} \big]\big(x^{(i)} - \mu_j^{[t+1]} \big]^T
      \end{align}

      \item Repeat steps 2 and 3 until convergence. 
    \end{enumerate}
  \end{algo}

  Let us elaborate further on the intuition of this step. In the normal GDA with given values of $z^{(i)}$, we have $\lambda= \frac{1}{n} \sum_{i=1}^n 1\{z^{(i)} = j\} = \frac{1}{n}\big(\text{Number of Samples in }j\text{th Gaussian}\big)$, which is a sum of "hard" guesses, meaning that each $x^{(i)}$ is undoubtedly in cluster $j$ or not, and so to find out our best guess for the true vector $\lambda$, all we have to do is find out the proportion of all examples in each of the $k$ groups and we're done (without needing to iterate). However, in our EM model, we do not know the $z^{(i)}$'s, and so the best we can do is give the \textit{probability} $\lambda^{(i)}_j$ that $x^{(i)}$ is in cluster $j$. So for each point $x^{(i)}$, the model has changed from it being undoubtedly in group $z^{(i)} = j$ to it having a probability of being in $\lambda^{(i)}_j$ for $j = 1, \ldots, k$.

  \begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
      \centering
      \begin{tikzpicture}[scale=1.]
        \draw[->] (-0.5,0) -- (5,0);
        \draw[->] (0,-0.5) -- (0,3.5);
        
        % Points and labels
        \fill[red] (1,1) circle (3pt);
        \fill[red] (1.5,0.8) circle (3pt);
        \fill[red] (2,1.5) circle (3pt);
        \node[red] at (3,1.5) {$\lambda=\frac{3}{6}$};
        
        \fill[blue] (3,2.5) circle (3pt);
        \fill[blue] (3.2,2) circle (3pt);
        \node[blue] at (4,2.5) {$\lambda=\frac{2}{6}$};
        
        \fill[green!50!black] (4,1) circle (3pt);
        \node[green!50!black] at (4.5,1) {$\lambda=\frac{1}{6}$};
      \end{tikzpicture}
      \caption{Hard label assignments.}
      \label{fig:hard-guesses}
    \end{subfigure}
    \hfill 
    \begin{subfigure}[b]{0.48\textwidth}
      \centering
      \begin{tikzpicture}[scale=1]
        \draw[->] (-0.5,0) -- (5,0);
        \draw[->] (0,-0.5) -- (0,3.5);
        
        % Points and colored vectors
        \fill (0.5,0.5) circle (2pt);
        \node[right] at (0.2,0.2) {$\lambda^{(1)} = ($\textcolor{red}{.8}, \textcolor{blue}{.03}, \textcolor{green!50!black}{.17}$)$};
        
        \fill (1.5,2) circle (2pt);
        \node[above] at (1.5,2.4) {$\lambda^{(2)} = ($\textcolor{red}{.9}, \textcolor{blue}{.05}, \textcolor{green!50!black}{.05}$)$};
        
        \fill (1,1.5) circle (2pt);
        \node[right] at (0.2,1.2) {$\lambda^{(3)} = ($\textcolor{red}{.7}, \textcolor{blue}{.1}, \textcolor{green!50!black}{.2}$)$};
        
        \fill (3.5,2.7) circle (2pt);
        \node[above] at (4,2.7) {$\lambda^{(4)} = ($\textcolor{red}{.15}, \textcolor{blue}{.8}, \textcolor{green!50!black}{.05}$)$};
        
        \fill (3.2,2) circle (2pt);
        \node[right] at (3.2,1.7) {$\lambda^{(5)} = ($\textcolor{red}{.2}, \textcolor{blue}{.6}, \textcolor{green!50!black}{.2}$)$};
        
        \fill (4,0.9) circle (2pt);
        \node[right] at (3,0.6) {$\lambda^{(6)} = ($\textcolor{red}{.1}, \textcolor{blue}{.05}, \textcolor{green!50!black}{.85}$)$};
      \end{tikzpicture}
      \caption{Soft probability assignments.}
      \label{fig:soft-guesses}
    \end{subfigure}
    \caption{The superscript $[t]$ is omitted for clarity.}
    \label{fig:guesses-comparison}
  \end{figure}

  When we update the $\lambda$ in the M-step, we can interpret the vectors $\lambda^{(i)}$ as tuples where $\lambda^{(i)}_j$ describes the expected "portion" of each sample $x^{(i)}$ to be in group $j$. So, we are adding up all the "portions" of the points that are expected to be in cluster $j$ to get $\lambda= \sum_{i=1}^n \lambda^{(i)}$. 

  \begin{figure}[H]
    \centering 
    \includegraphics[scale=0.2]{img/weighted_means.jpg}
    \caption{}
    \label{fig:weighted_means}
  \end{figure}

  Now, given the $j$th Gaussian cluster, we would like to compute its mean $\mu_j$. Since each $x^{(i)}$ has probability $\lambda^{(i)}_j$ of being in cluster $j$, we can weigh each of the $n$ points by $\lambda^{(i)}_j$ (which determines how "relevant" $x^{(i)}$ is to cluster $j$) and average these (already weighted) points to get our "best-guess" of the mean $\mu_j$. Given the MLE of the means, we can straightforwardly compute the MLE of the covariance matrices. 

  In summary, this entire algorithm results from modifying the ``hard'' data of each point $x^{(i)}$ being undoubtedly in one cluster to a model containing points $x^{(i)}$ that have been "smeared" around different clusters, with a probability $\lambda^{(i)}$ being in cluster $j$. 

\subsection{Nonlinear ICA} 

\bibliography{./bibfile}
\bibliographystyle{alpha}
\end{document}

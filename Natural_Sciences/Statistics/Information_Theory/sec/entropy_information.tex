\section{Entropy and Information}

  Let's motivate entropy. First, we want to quantitatively measure the ``surprise'' of an event $E$ happening in a probability space by assigning it a value $H(E)$. We want it to satisfy the following: 
  \begin{enumerate}
    \item \textit{Positive Surprisal}. $H(E) \geq 0$. 
    \item \textit{No Surprisal from Sure Events}. $H(E) = 0$ iff $\mathbf{P}(E) = 1$.
    \item \textit{Additivity of Independent Events}. If $E_1$ and $E_2$ are independent events, then $H(E_1 \cap E_2) = H(E_1) + H(E_2)$. 
    \item \textit{Continuity}. $H$ should be continuous, i.e. slight changes in probability correspond to slight changes in surprisal. 
  \end{enumerate}
  
  The third condition reminds us of the log function, and the fourth condition reminds us to do take advantage of continuity of measure, leading us to define the surprisal of an event as $- \log \mathbf{P}(E)$. Now we can define entropy as the expected surprisal of a random variable, which seems now more motivated. Intuitively, this represents the element of surprise of a certain data point, and distributions that have relatively sharp peaks will have lower entropy (since we expect most of the samples to come from the peaks) while uniform distributions have higher entropy. 

\subsection{Discrete Random Variables}

  Entropy was originally created by Shannon as part of his theory of communication. This is how the vast majority of students learn about entropy. Even though it is not as general, this definition is good to know.  

  \begin{definition}[Entropy in Discrete Setting] 
    Let $(\Omega, \mathcal{F}, \mathbf{P})$ be a probability space.  

    \begin{enumerate}
      \item \textit{Paritition}. Let $\mathcal{Q} = \{Q_k\}_{k=1}^n$ be a finite partition of measurable subsets of $\Omega$. Then, the \textbf{entropy of partition $\mathcal{Q}$} is defined 
        \begin{equation}
          H_{\mathbf{P}} (\mathcal{Q}) = - \sum_{k=1}^n \mathbf{P}(Q_k) \, \log{\mathbf{P}(Q_k)}
        \end{equation}

      \item \textit{Random Variable}. Let $X$ be a random variable over $\Omega$ taking values in finite measure space $(A, 2^A)$. The \textbf{entropy of random variable $X$} is defined 
        \begin{align} 
          H_{\mathbf{P}} (X) & \coloneqq -\sum_{a \in A} \mathbf{P}_X (\{a\}) \log{\mathbf{P}_X (\{a\})} \\ 
                             & = -\sum_{a \in A} \mathbf{P}(X^{-1}(a)) \log{\mathbf{P}(X^{-1}(a))} = H_{\mathbf{P}}(\{X^{-1}(a)\}_{a \in A})
        \end{align}
    \end{enumerate}
    where we define $0 \log{0} = 0$.\footnote{This is consistent with the limit. } 
  \end{definition} 

  The base of the logarithm differs: in mathematical theory, it is usually base $e$, but in signal processing, it is base $2$ for bits. 

  \begin{definition}[Relative Entropy in Discrete Setting]
    Let $\mathbf{P}, \mathbf{Q}$ be two probability measures on $(\Omega, \mathcal{F})$. Given random variable $X$ taking values in finite alphabet $(A, 2^A)$, the \textbf{relative entropy of $X$ with measure $\mathbf{P}$ w.r.t. reference measure $\mathbf{Q}$} is 
    \begin{equation}
      H_{\mathbf{P} || \mathbf{Q}} (X) \coloneqq \begin{cases} 
        \sum_{a \in A} \mathbf{P}_X (\{a\}) \, \log \frac{\mathbf{P}_X (\{a\})}{\mathbf{Q}_X (\{a\})} & \text{ if } \mathbf{P}_{X} \ll \mathbf{Q}_{X} \\ 
        +\infty & \text{ if else }
      \end{cases}
    \end{equation}
    where $\mathbf{P}_{X}, \mathbf{Q}_{X}$ are the induced measures on $\mathcal{A}$. 
  \end{definition} 

  \begin{definition}[Divergence]
    Given a finite measure space $(\Omega, 2^\Omega)$, with two probability measures $\mathbf{P}, \mathbf{Q}$. 
    \begin{equation}
      D(\mathbf{P} \mid \mid \mathbf{Q}) = \sum_{w \in \Omega} \mathbf{P}(\{\omega\}) \log \frac{\mathbf{P}(\{\omega\})}{\mathbf{Q}(\{\omega\})}
    \end{equation}
  \end{definition}

  Note that $H_{\mathbb{P} || \mathbb{Q}} (X) = D(\mathbf{P}_X || \mathbf{Q}_X)$. 

  \begin{lemma}[Divergence is Nonnegative]
    Given any probability measures $\mathbf{P}, \mathbf{Q}$ on a finite measure space $(\Omega, 2^\Omega)$, we have 
    \begin{equation}
      D(\mathbf{P} || \mathbf{Q}) \geq 0
    \end{equation}
    with equality iff $\mathbf{P} = \mathbf{Q}$. 
  \end{lemma}
  \begin{proof}
    
  \end{proof}

\subsection{Relative Entropy} 

  \begin{definition}[Relative Entropy][def:relative-entropy]
    The \textbf{relative entropy}, also called the \textbf{(Kullback-Leibler) divergence}, of two probability measures $\mathbf{P}, \mathbf{Q}$ is defined in the following ways: 
    \begin{enumerate}
      \item \textit{Normal Definition}. Given that $\frac{d\mathbf{P}}{d\mathbf{Q}}$ is the \hyperref[meas-def:rn-der]{Radon-Nikodym derivative}. 
        \begin{equation}
          D(\mathbf{P} || \mathbf{Q}) \coloneqq \begin{cases} 
            \mathbb{E}_{\mathbf{P}} \bigg[ \log \frac{d \mathbf{P}}{d \mathbf{Q}} \bigg] = \int \log \frac{d \mathbf{P}}{d\mathbf{Q}} \, d\mathbf{P} & \text{ if } \mathbf{P} \ll \mathbf{Q} \\  
            +\infty & \text{ if else} 
          \end{cases}
        \end{equation}

      \item \textit{Variational}. Given that $D$ is the divergence of discrete probability measures,  
        \begin{equation}
          D (\mathbf{P} || \mathbf{Q}) \coloneqq \sup_{X \text{ simple}} D(\mathbf{P}_X || \mathbf{Q}_X)
        \end{equation}
    \end{enumerate}
  \end{definition} 

  It is a measure of how one probability distribution is different from a second, reference probability distribution. A relative entropy of $0$ indicates that $p$ and $q$ are identical. It is useful to interpret this measure as a "distance" between two distributions, but it is not a formal metric because it is not symmetric and does not satisfy the triangle inequality. 

  \begin{lemma}[]
    The divergence is always nonnegative. 
  \end{lemma}
  \begin{proof}
    By Jensen's inequality. 
  \end{proof}

  \begin{example}
    If $\mu, \nu$ are both continuous distributions, we have
    \begin{align}
      \mathrm{KL}(p || q) & \coloneqq - \int p(x) \, \ln{q(x)} \,dx - \bigg( - \int p(x) \, \ln{p(x)} \,dx \bigg) \\
      & = - \int p(x) \, \ln \bigg( \frac{q(x)}{p(x)} \bigg) \,dx 
    \end{align}
  \end{example}

\subsection{Mutual Information} 

  \begin{definition}[Mutual Information]
    Consider two random variables $X, Y$ with respective probability measures $\mu_X, \mu_Y$, let $\mu_{X \times Y}$ be their joint distribution and $\mu_X \times \mu_Y$ be their product measure. The \textbf{mutual information} of $X$ and $Y$ is defined 
    \begin{equation}
      I(X; Y) \coloneqq D(\mu_{X \times Y} || \mu_X \times \mu_Y) 
    \end{equation}
  \end{definition}

  \begin{theorem}[Mutual Information and Entropy]
    The mutual information $I(X;Y)$ is the reduction in the uncertainty of $X$ due to the knowledge of $Y$. 
    \begin{equation}
      I(X;Y) = H(X) - H(X|Y)
    \end{equation}
    It follows that
    \begin{align*}
      I(X;Y) & = H(X) - H(X|Y) \\
      & = H(Y) - H(Y|X) \\
      & = H(X) + H(Y) - H(X, Y) \\
      I(X;Y) & = I(Y;X) \\
      I(X;X) & = H(X)
    \end{align*}
  \end{theorem}
  \begin{proof}
    We can write
    \begin{align*}
      I(X;Y) & = \sum_{x, y} p(x, y) \, \log \frac{p(x, y)}{p(x)p(y)} \\
      & = \sum_{x, y} p(x, y) \log \frac{p(x|y)}{p(x)} \\
      & = - \sum_{x, y} p(x, y) \log p(x) + \sum_{x, y} p(x, y) \log p(x|y) \\
      & = - \sum_x p(x) \log p(x) - \Bigg( - \sum_{x, y} p(x, y) \log p(x|y) \Bigg) \\
      & = H(X) - H(X|Y)
    \end{align*}
    By symmetry, we have
    \[I(X;Y) = H(Y) - H(Y|X)\]
    Thus, $X$ says as much about $Y$ as $Y$ says about $X$. Since $H(X, Y) = H(X) + H(Y|X)$, we have
    \[I(X;Y) = H(X) + H(Y) - H(X, Y) \implies I(X;X) = H(X) - H(X|X) = H(X)\]
    That is, the mutual information of a random variable with itself is the entropy of the random variable. This is the reason that entropy is sometimes referred to as \textit{self-information}. 
  \end{proof}

  \begin{example}
    For the joint distribution in the previous example, the mutual information is 
    \[I(X;Y) = H(Y) - H(Y|X) = 2 - \frac{13}{8} = \frac{3}{8} \text{ bits}\]
  \end{example}

  \begin{definition}
    The \textbf{conditional mutual information} of random variables $X, Y, Z$ is the reduction in the uncertainty of $X$ due to knowledge of $Y$ when $Z$ is given. 
    \begin{align*}
      I(X;Y|Z) & = H(X|Z) - H(X|Y, Z) \\
      & = E_{p(x, y, z)} \bigg( \log \frac{p(X, Y|Z)}{p(X|Z) p(Y|Z)}\bigg)
    \end{align*}
  \end{definition}

  Mutual information also satisfies a chain rule. 

  \begin{theorem}[Chain rule for information]
    \[I(X_1, X_2, ..., X_n; Y) = \sum_{i=1}^n I(X_i;Y|X_{i-1}, ..., X_1)\]
  \end{theorem}
  \begin{proof}
    \begin{align*}
      I(X_1, ..., X_n;Y) & = H(X_1, ..., X_n) - H(X_1, ..., X_n |Y) \\
      & = \sum_{i=1}^n H(X_i|X_{i-1}, ..., X_1) - \sum_{i=1}^n H(X_i | X_{i-1}, ..., X_1;Y) \\
      & = \sum_{i=1}^n I(X_i ; Y|X_1, X_2, ..., X_{i-1})
    \end{align*}
  \end{proof}

\subsection{Entropy}

  \begin{definition}[Entropy]
    The \textbf{entropy} of a random variable $X$ is defined 
    \begin{equation}
      H(X) \coloneqq I(X; X)
    \end{equation}
  \end{definition}


\subsection{Differential Entropy} 

  \begin{definition}[Differential Entropy]
    For a continuous random vector, the \textbf{differential entropy} is defined 
    \begin{equation}
      H[X] = - \int p(x) \ln{p(x)} \,dx
    \end{equation}
  \end{definition}

\subsection{Exercises}

  \begin{exercise}[63 Puzzle]
    I am thinking of an integer $0 \leq x \leq 63$. You must identify this $x$ by asking if it is at least a number $y$. How do you get it in the minimum number of questions? 
  \end{exercise}
  \begin{proof}
    The answer is clearly binary search, which gets it done in $\log_2 64 = 6$ questions. More specifically, we can come up with the following predetermined questions which each gives the $i$th binary digit of the solution. 
    \begin{enumerate}
      \item $C_1: x \; \mathrm{mod} \, 64 \geq 32$? 
      \item $C_2: x \; \mathrm{mod} \, 32 \geq 16$? 
      \item $C_3: x \; \mathrm{mod} \, 16 \geq 8$? 
      \item $C_4: x \; \mathrm{mod} \, 8 \geq 4$? 
      \item $C_5: x \; \mathrm{mod} \, 4 \geq 2$? 
      \item $C_6: x \; \mathrm{mod} \, 2 \geq 1$? 
    \end{enumerate}
    Note that if we are assuming a uniform distribution, this is the strategy that maximize stepwise entropy, since the outcome of each question has an equal probability of being $0$ or $1$, leading to 1 bit of information (and not less since all these random variables $C_i$ are independent) This is indeed exactly how much information about the binary expansion of $x$ we get. After 6 questions, the total information content was 6 bits. 
  \end{proof}

  From this, we can claim something. 

  \begin{lemma}
    All outcomes of a random variable $X$ from a set of size $S$ can be communicated in $\lceil \log_2 |S| \rceil$ bits. 
  \end{lemma}

  Let's think more about what information means with another game. 

  \begin{example}[Submarines]
    You are playing battleship on a $8 \times 8$ grid, but there is one $1 \times 1$ submarine that you are trying to hit. Say you choose some square and hit it. 
    \begin{enumerate}
      \item You don't hit it, which happens with probability $63/64$. You then get $\log_2 (64/63) \approx 0.0227$ bits of information. 

      \item You fire again and don't hit, which happens with probability $62/63$. You get $\log_2 (63/62) \approx 0.0230$ bits of information. The total information gained is $0.04560$. 

      \item You keep firing off at squares. You obviously don't want to fire at an already hit square since the probability that you don't hit is $1$, so no information is gained. 

      \item If you keep firing and don't hit after 32 tries, then you have gained a total information of 
      \begin{equation}
        \log_2 \frac{64}{63} + \ldots + \log_2 \frac{33}{32} = 1
      \end{equation}
      bit. This is similar to getting $1$ bit from the first step of binary search, which is consistent with our intuition. 

      \item Let's keep firing and say on the 35th hit, we actually hit the submarine. Then our total information content is 
      \begin{equation}
        \log_2 \frac{64}{63} + \ldots + \log_2 \frac{33}{32} + \ldots + \log_2 \frac{21}{20} + \log_2 \frac{20}{1} = 6
      \end{equation}
      We have then acquired 6 bits of information (around $4.3$ bits for the hit) and gotten all possible information we can get from the grid. 
    \end{enumerate}
  \end{example}

  \begin{lemma}[Approximation of Binomial Distribution]
    We can approximate 
    \begin{equation}
      \binom{n}{k} = \log \frac{n!}{k! (n-k)!} \approx n H_2 \bigg( \frac{p}{n} \bigg)
    \end{equation}
  \end{lemma}

  \begin{exercise}[Bent Coin Lottery]
    A coin with $p = 0.1$ is tossed $1000$ times to get a random vector $\mathbf{x} \in \{0, 1\}^{1000}$. You can buy any of the $2^N$ possible tickets for \$1 each, before the coin tossing. If you own the correct ticket, you get a lot of money. 
    \begin{enumerate}
      \item If you are forced to buy one ticket, which ticket would you buy? 
      \item To have a 99\% change of winning at a lowest possible cost, which tickets should you buy? 
      \item And how many tickets is that? Express it in the form of $2^n$. 
    \end{enumerate}
  \end{exercise}
  \begin{solution}
    Let's go through them.\footnote{Thanks to Thomas Kidane for pointing out my initial mistakes in the solution.}
    \begin{enumerate}
      \item Even though the expected number of $1$'s is 100, the all $0$ ticket would be the most likely outcome. 
      \item From the previous problem, we can intuit that we should buy all the tickets with zero $1$s, then one $1$s, then two $1$s, and so on until some threshold $r$ where the probability is 99\%. By CLT, we can approximate this to be normal with mean $100$ and standard deviation $\sqrt{1000 \cdot 0.1 \cdot 0.9} \approx 9.5$, and therefore a z-score of about $2.3$ will give us $100 + 9.5 * 2.3 = 121.85$ tosses with a 99\% chance of winning. 
      \item To find out how many tickets this is, we compute 
        \begin{equation}
          1 + \binom{1000}{1} + \binom{1000}{2} + \ldots + \binom{1000}{123}
        \end{equation}
        the rightmost is the dominant term, and we use the approximation to get it approximately $2^{530}$ tickets.  
    \end{enumerate}
  \end{solution}

  Therefore, we have essentially ``compressed'' the set of all $2^{1000}$ tickets up to 99\% probability of hitting, into a set of approximately $2^{530}$ tickets, called the \textbf{typical set}, which can be encoded in $530$ bits. Therefore, 
  \begin{enumerate}
    \item the compressor takes the typical set of tickets and creates a bijection into a second set of $530$ bit long strings. 
    \item The decompressor just undos this bijection from the typical set back into the $1000$ bit long strings. 
  \end{enumerate}

  In a general case lottery with $n$-length strings and a probability of $p$, we can compute that you will need approximately 
  \begin{equation}
    \binom{n}{f n + 2.3 \sqrt{N f (1 - f)}} \approx 2^{N H_2 (p) + \epsilon} 
  \end{equation}
  where $\epsilon$ is a small term that scales with $\sqrt{n}$. We see a certain pattern that coincides with the source coding theorem on how well we can compress a certain set that scales with some probability. Note that this depends on precisely defining the typical set. 

  \begin{definition}[Typical Set]
    When a source $X$ produces $N$ independent outcomes 
    \begin{equation}
      \mathbf{x} = x_1, x_2, \ldots, x_n
    \end{equation}
    This string is very likely to be in a \textbf{typical set} consisting of $\sim 2^{n H(X)}$ outcomes all of which have a probability of $\sim 2^{-n H(X)}$. 
  \end{definition}

  \begin{theorem}[Source Coding Theorem]
    $N$ outcomes from a source $X$ can be compressed into roughly $N H(X)$ bits. 
  \end{theorem}


  We have hinted at the fact through Shannon's noisy encoding theorem that there is an optimal way to add redundancies to compress some input. Given a string of random variables $X_1, \ldots, X_n$ generated iid from a $\mathrm{Bernoulli}(p)$ distribution, we want to start to formalize this by introducing a metric to measure the information content of this stochastic process. We motivate the necessity of such a measure using general probability measures and then focus on the discrete case. 

  \subsection{Discrete Random Variables}

    In Shannon's famous paper \cite{shannon}, he talks first on discrete channels, focusing on examples of transmitting languages through n-gram models as ``higher order approximations'' of language.\footnote{In fact, this is where n-gram models were first referenced.} This is an ergodic Markov chain with some stationary distribution. 

    \begin{theorem}[Bounds on Entropy]
      $H$ is bounded by $0$ and $1$, attaining its minimum if and only if all the $p_i$ but one are $0$. It attains its maximum if $p$ is uniform. 
    \end{theorem}

    \begin{theorem}[Entropy of Independent Events]
      If $E_1$ and $E_2$ are independent events, then $H(E_1 \cap E_2) = H(E_1) + H(E_2)$. The information from two independent events is the sum of their informations since information gain from one does not increase information from another independent variable.  
    \end{theorem}

    \begin{example}[Bits]
      Given $X \sim \mathrm{Bernoulli}(p)$, if we observe a value of $1$, then we have received $\log_2 \big( \frac{1}{p} \big)$ bits of information. 
    \end{example}

    Now Shannon's claim is that this information content is the optimal encoding length that we should aim for. For example, given $p = 0.9$, then a $0$ has 3.32 bits of information content and a $1$ has 0.15 bits. This means that $0$'s, which occur infrequently, should be encoded with longer strings and $1$ with shorter strings.   

    \begin{exercise}[Weighing Problem]
      You are given 12 balls, all equal in weight except for one that is either heavier or lighter. Design a strategy to determine which is the odd ball \textit{and} whether it is heavier or lighter in as few uses of the balance as possible. 
    \end{exercise}

    \begin{proof}
      We can tackle this by looking at the first action. We can choose to weigh $n$ vs $n$ balls for $n = 1, \ldots, 6$. Shannon would advise you to choose such that we maximize our entropy, or expected information gain. Let's go through them one at a time. Our three outcomes for all scenarios are $A$ (left is lighter), $B$ (both equal), and $C$ (right is lighter). 
      \begin{enumerate}
        \item 6 v 6. The probability distribution is $(A, B, C) = (1/2, 0, 1/2)$ and so the entropy is $H = 1$ bit. 
        \item 5 v 5. The distribution is $(5/12, 1/6, 5/12)$ giving us $H = 1.48$ bits. 
        \item 4 v 4. The distribution is $(1/3, 1/3, 1/3)$ giving us $H = 1.58$ bits. 
        \item We go on. 
      \end{enumerate}
      We already know that entropy must be maximized in the uniform distribution, so it is best to choose 4 v 4. This is indeed the correct first step. As for the next step. Let's think about what to do in each of the events. 
      \begin{enumerate}
        \item $A$. This means that there are four $H$'s (possibly heavies), four $L$'s, and four $G$'s (possibly good). We are left with 8 balls and we want to maximize the entropy. It turns out that if we measures HHL vs HHL, then the events turn out to have distribution $(3/8, 2/8/ 3/8)$, which is quite uniform.  
        \item $B$. We have eight $G$'s, so to maximize the entropy we can weigh one against another, which has a distribution of $(1/4, 1/2, 1/4)$. 
        \item $C$. By symmetry, we use the same method as $A$. 
      \end{enumerate}
      We just continue this process which is a stepwise optimization of entropy. It turns out that we just need 3 steps. 
    \end{proof}

  \subsection{Joint and Conditional Entropy}

    The naturalness of the definition of joint entropy and conditional entropy is exhibited by the fact that the entropy of a pair of random variables is the entropy of one plus the conditional entropy of the other. 

    \begin{definition}[Joint, Conditional Entropy]
      We can define the joint entropy and conditional entropy between two discrete random variables $X, Y$ as 
      \begin{align*}
        H(X, Y) & = \mathbb{E}_{X \times Y} [-\log p(x, y)] = \sum_{x, y \in \mathcal{X}, \mathcal{Y}} p(x, y) \cdot - \log p(x, y) \\
        H(X \mid Y) & = \mathbb{E}_{X \times Y} [- \log p(x \mid y)]  = \sum_{x, y \in \mathcal{X}, \mathcal{Y}} p(x, y) \cdot - \log p(x \mid y )
      \end{align*}
    \end{definition}

    \begin{example}[Dice Rolls]
      Let $(X, Y)$ have the following joint distribution: 
      \begingroup
      \renewcommand{\arraystretch}{1.4}
      \begin{center}
      \begin{tabular}{c|c|c|c|c}
         &1&2&3&4\\
         \hline
        1&$\frac{1}{8}$&$\frac{1}{16}$&$\frac{1}{32}$&$\frac{1}{32}$\\
        \hline
        2&$\frac{1}{16}$&$\frac{1}{8}$&$\frac{1}{32}$&$\frac{1}{32}$\\
        \hline
        3&$\frac{1}{16}$&$\frac{1}{16}$&$\frac{1}{16}$&$\frac{1}{16}$\\
        \hline
        4&$\frac{1}{4}$&$0$&$0$&$0$
      \end{tabular}
      \end{center}
      \endgroup
      The marginal distribution of $X$ is $\big(\frac{1}{2}, \frac{1}{4}, \frac{1}{8}, \frac{1}{8}\big)$ and the marginal distribution of $Y$ is $\big(\frac{1}{4}, \frac{1}{4}, \frac{1}{4}, \frac{1}{4}\big)$, meaning that
      \[H(X) = \frac{7}{4} \text{ bits,  } H(Y) = 2 \text{ bits}\]
      Also, 
      \begin{align*}
        H(X|Y) & = \sum_{i=1}^4 p(Y=i)\, H(X|Y = i) \\
        & = \frac{1}{4} H\bigg(\frac{1}{2}, \frac{1}{4}, \frac{1}{8}, \frac{1}{8}\bigg) + \frac{1}{4} H\bigg(\frac{1}{4}, \frac{1}{2}, \frac{1}{8}, \frac{1}{8}\bigg) \\
        & \;\; + \frac{1}{4} H\bigg(\frac{1}{4}, \frac{1}{4}, \frac{1}{4}, \frac{1}{4}\bigg) + \frac{1}{4}H(1, 0, 0, 0) \\
        & = \frac{1}{4} \cdot \frac{7}{4} + \frac{1}{4} \cdot \frac{7}{4} + \frac{1}{4} \cdot 2 + \frac{1}{4} \cdot 0 = \frac{11}{8} \text{ bits}
      \end{align*}
      Similarly, $H(Y|X) = \frac{13}{8}$ bits and $H(X, Y) = \frac{27}{8}$ bits. 
    \end{example}

    \begin{theorem}[Joint Entropy]
      The uncertainty of a joint event is less than or equal to the sum of the individual uncertainties, with equality achieved only if the events are independent. 
      \begin{equation}
        H(X, Y) \leq H(X) + H(Y)
      \end{equation}
    \end{theorem}

    Another property is that any change towards ``equalization'' of the probabilities $p_i$ increases $H$. Since we don't have a method of measuring how close to the uniform distribution, we will return back to this after defining the KL divergence.  

    \begin{theorem}[Chain Rule of Entropy]
      The joint entropy is the entropy of $X$ plus the conditional entropy of $Y$ given $X$. 
      \begin{equation}
        H(X, Y) = H(X) + H(Y \mid X) = H(Y) + H(X \mid Y)
      \end{equation}
    \end{theorem}
    \begin{proof}
      By linearity of expectation, 
      \begin{align*}
        H(X) + H(Y|X) & = - \mathbb{E} \Big( \log \frac{1}{p(X)}\Big) - \mathbb{E} \big( \log p(Y|X)\big) \\
        & = - \mathbb{E} \big( \log p(X) + \log p (Y|X) \big) \\
        & = - \mathbb{E} \big( \log p(X) p(Y|X)\big) \\
        & = - \mathbb{E} \big( \log p(X, Y) \big) = H(X, Y)
      \end{align*}
    \end{proof}

    \begin{corollary}[Chain Rule for Entropy]
      Let $X_1, X_2, ..., X_n$ be drawn according to $p(x_1, x_2, ..., x_n)$. Then 
      \begin{equation}
        H(X_1, X_2, ..., X_n) = \sum_{i = 1}^n H(X_i | X_{i-1}, ..., X_1)
      \end{equation}
    \end{corollary}

    \begin{theorem}[Conditioning Never Decreases Uncertainty]
      Since 
      \begin{equation}
        H(X) + H(Y) \geq H(X, Y) = H(X) + H(Y \mid X) 
      \end{equation}
      we have $H(Y) \geq H(Y \mid X)$. That is, the uncertainty of $Y$ is never increased by the knowledge of $X$, which is expressed as 
      \begin{equation}
        H(X \mid Y) \leq H(X)
      \end{equation}
      with equality is $X, Y$ are independent. Indeed, the "uncertainty" of $X$ would decrease if we had any knowledge about a (potentially nonindependent) distribution $Y$. 
    \end{theorem}

    In fact, the amount of uncertainty that decreases when conditioning has a well known name. 

    \begin{definition}[Mutual Information]
      The \textbf{mutual information} between random variables $X, Y$ is the decrease in entropy when we condition $X$ by $Y$. 
      \begin{equation}
        I(X ; Y) = H(X) - H(X \mid Y) = H(Y) - H(Y \mid X)
      \end{equation}
      This can be conditioned on another random variable $Z$. 
      \begin{equation}
        I(X ; Y \mid Z) = H(X \mid Z) - H(X \mid Y, Z) = H(Y \mid Z) - H(Y \mid X, Z)
      \end{equation}
    \end{definition}
    
    Therefore, we can interpret $I(X; Y)$ as the partial information you learn about $X$ from knowing $Y$. The entropy also demonstrates the average length (if base is $2$) number of bits required to transmit the state of a random variable. 

    \begin{theorem}
      From simple substitution, we can derive 
      \begin{equation}
        H(X, Y) = H(X \mid Y) + H(Y \mid X) + I(X; Y)
      \end{equation}
    \end{theorem}

    Unlike entropy, conditioning the mutual information on a third variable can have either a hiding or revealing effect. It can \textit{both} be the case that\footnote{Read the Wikipedia article on \textit{Interaction Information}.}
    \begin{enumerate}
      \item $I(X; Y \mid Z) > I(X; Y)$ happens when $X$ and $Y$ both are causes of some common effect $Z$, i.e. if you know $Z$ has happened, then $X$ and $Y$ are more dependent than before. For example, a car's engine fails to start (event $Z$), it may be because of either blocked fuel pump ($X$) or that the battery is dead ($Y$). Normally, $X$ and $Y$ are independent, so $I(X; Y) = 0$, but if the engine doesn't start, they suddenly become very dependent, since now you can look at the battery ($Y$) and from that conclude the status of the pump ($X$) with much more confidence, making $I(X; Y \mid Z) > 0$. \footnote{Another example is given independent Bernoullis $X, Y$, with $Z = X \oplus Y$ (mod 2), we can clearly see that $I(X; Y) = H(X) - H(X \mid Y) = 0$ since they are independent and so $Y$ does not give any information about $X$. However, if we condition on $Z$ further, this gives complete information on $X$. This is quite intuitive since you would know more about $X$ from knowing both $Y$ and $Z$ rather than just knowing $Y$.}

      \item $I(X; Y) > I(X; Y \mid Z)$ happens when $Z$ is the cause of both $X$ and $Y$. For example, if clouds ($Z$) always cause rain ($X$) and blocks the sun, ($Y$), then we know that $I(X; Y \mid Z) = 0$ since $Z$ already tells us everything about $X$ and $Y$, so $Y$ does not tell us anything more about $X$. But if we only observe whether the sun is blocked, this only tells us partially about whether it is rainy (may or may not be due to clouds or some other factor), making $I(X; Y) > 0$ due to some correlation revealed.   
    \end{enumerate}


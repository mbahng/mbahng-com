\section{Regularized Softmax Regression} 

\subsection{Ridge} 

  In the high dimensional case, we would like to impose some regularization again to control variance. 

  \begin{definition}[Loss]
    The loss function of a ridge logistic regression is 
    \begin{align}
      L(\beta, x, y) & = - y \log(\sigma(\beta^T x)) - (1 - y) \log(1 - \sigma(\beta^T x)) \\ 
                     & = -y \log(\hat{y}) - (1 - y) \log (1 - \hat{y})
    \end{align}
    The loss for a ridge softmax regression is 
    \begin{align}
      L(\theta, x, y) & = - \sum_{k=1}^K y_k \log ()
    \end{align}
  \end{definition}

\subsection{Lasso} 


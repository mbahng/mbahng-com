\section{Softmax Regression}

  We would like to extend this to the multiclass case. In order to do this, we must start with \textit{multivariate} linear regression and produce another link function $o$ that maps it to the parameter space of a multinomial distribution. It should also be a generalization of the sigmoid. 

  \begin{definition}[Softmax]
    The \textbf{softmax function} is defined 
    \begin{equation}
      o(\mathbf{x}) = \frac{e^{\mathbf{x}}}{\|e^{\mathbf{x}}\|} = \frac{1}{\sum_j e^{x_j}}\begin{pmatrix} e^{x_1} \\ \vdots \\ e^{x_D} \end{pmatrix}
    \end{equation}
  \end{definition}

  This is in fact a generalization of the sigmoid. That is, given softmax for 2 classes, we have 
  \begin{equation}
    o\begin{pmatrix} x_1 \\ x_2 \end{pmatrix} = \frac{1}{e^{x_1} + e^{x_2}} \begin{pmatrix} e^{x_1} \\ e^{x_2} \end{pmatrix}
  \end{equation}
  So, the probability of being in class $1$ is 
  \begin{equation}
    \frac{e^{x_1}}{e^{x_1} + e^{x_2}} = \frac{1}{1 + e^{x_2 - x_1}}
  \end{equation}
  and the logistic sigmoid is just a special case of the softmax function that avoids using redundant parameters. We actually end up overparameterizing the softmax because the probabilities must add up to one. Another reason to choose the softmax is that the total derivative turns out to simplify our loss, which also parallels to the sigmoid. 

  \begin{lemma}[Derivative of Softmax]
    The derivative of the softmax is 
    \begin{equation}
      D o (\mathbf{x}) = \mathrm{diag}(o (\mathbf{x})) - o (\mathbf{x}) \otimes o (\mathbf{x})
    \end{equation}
    where $\otimes$ is the outer product. That is, let $y_i$ be the output of the softmax. Then, for the $4 \times 4$ softmax function, we have 
    \begin{equation}
      D o(\mathbf{x}) = \begin{pmatrix} y_1 (1 - y_1) & - y_1 y_2 & -y_1 y_3 & - y_1 y_4 \\ -y_2 y_1 & y_2 (1 - y_2) & - y_2 y_3 & - y_2 y_4 \\ -y_3 y_1 & y_3 y_3 & y_3 (1 - y_3) & -y_3 y_4 \\ -y_4 y_1 & -y_4 y_2 & -y_4 y_3 & y_4 (1 - y_4) \end{pmatrix}
    \end{equation}
  \end{lemma}
  \begin{proof}
    We will provide a way that allows us not to use quotient rule. Given that we are taking the partial derivative of $y_i$ with respect to $x_j$, we can use the log of it to get 
    \[\frac{\partial}{\partial x_j} \log (y_i) = \frac{1}{y_i} \frac{\partial y_i}{\partial x_j} \implies \frac{\partial y_i}{\partial x_j} = y_i \, \frac{\partial}{\partial x_j} \log(y_i)\]
    Now the partial of the log term is 
    \begin{align}
      \log{y_i} & = \log \bigg( \frac{e^{x_i}}{\sum_l e^{x_l}} = x_i - \log \bigg( \sum_l e^{x_l}\bigg) \\
      \frac{\partial}{\partial x_j} \log(y_i) & = \frac{\partial x_i}{\partial x_j} - \frac{\partial}{\partial x_j} \log \bigg( \sum_l e^{x_l}\bigg) \\
      & = 1_{i = j} - \frac{1}{\sum_l e^{x_l}} e^{x_j}
    \end{align}
    and plugging this back in gives 
    \begin{equation}
      \frac{\partial y_i}{\partial x_j} = y_i (1_{i = j} - y_j)
    \end{equation}
  \end{proof} 

  A way to encode multiple classes is with one-hot encoding. 

  \begin{definition}[One-Hot Encoding]
    Given $K$ classes $\{1, \ldots, K\}$, the \textbf{one-hot encoding} of each class is 
    \begin{equation}
      k \mapsto e_k \in \mathbb{R}^K
    \end{equation}
    where $e_k$ is the $k$th basis vector. 
  \end{definition} 

  We choose such an encoding since $\|e_k - e_{k^\prime}\|$ is constant for all $k \neq k^\prime$. Therefore, all classes are just as distinct from one another. 

  \begin{definition}[Softmax Regression Model]
    A \textbf{softmax regression} model of $K$ classes is a probabilistic classification model 
    \begin{equation}
      Y \mid X = x \sim \mathrm{Multinomial}(f(x)), f(x) = o(Wx + b)
    \end{equation} 
    where $o$ is the softmax function. It has the parameters $\theta = \{W \in \mathbb{R}^{k \times d}, b \in \mathbb{R}^k\}$. 
  \end{definition}

  Again, we have a linear map followed by some link function (the softmax) which allows us to nonlinearly map our unbounded linear outputs to some domain that can be easily parameterized by a probability distribution. 

\subsection{Maximum Likelihood Estimation} 

  We do the same steps as that of logistic regression. 

  \begin{lemma}[Likelihood for Multinomial]
    The surrogate likelihood function for a multinomial is 
    \begin{equation}
      \mathbb{P}(X = j) = \mathbb{P}(X = e_j) = \prod_{k=1}^K p_k^{(e_j)_k} 
    \end{equation}
    where we have one-hot encoded the $k$th class. 
  \end{lemma} 
  \begin{proof}
    All terms in the product will be $1$ if $k \neq j$, and so the only term remaining will be $p_j^1 = p_j$. 
  \end{proof} 

  By taking the negative log-likelihood, we can get our loss function. 

  \begin{definition}[Multiclass Cross Entropy Loss as Surrogate Loss]
    The surrogate loss for softmax regression is the \textbf{multiclass cross entropy loss}, which is defined as 
    \begin{equation}
      L(\theta, x, y) = - \sum_{k=1}^K y_k \log \big( f(x) \big)_k
    \end{equation}
  \end{definition}

  Now we define our risk. 

  \begin{theorem}[Risk]
    The expected risk of softmax regression on the cross entropy loss is 
    \begin{equation}
      R(f) = \mathbb{E}_{x, y} \left[ - \sum_{k=1}^K y_k \log \big( f(x) \big)_k \right]
    \end{equation}
    and the empirical risk on a dataset $\mathcal{D} = \{(x^{(i)}, y^{(i)})\}_{i=1}^n$ is 
    \begin{equation}
      \hat{R}(f) = - \frac{1}{n} \sum_{i=1}^n \sum_{k=1}^K y_k \log \big( f(x) \big)_k
    \end{equation}
  \end{theorem}

  Since a closed form solution is not available for logistic regression, it is clearly not available for softmax. Note that since we are taking the derivative of a vector w.r.t. a matrix, we will have to work with higher-order tensors. Fortunately, this reduces down to a cute form that we can still compute with matrices. 

  \begin{theorem}[Gradient of Softmax Loss]
    The gradient of the empirical risk is 
    \begin{align}
      \nabla_W \hat{R}(f) & = \sum_{i=1}^n (f(x^{(i)}) - y^{(i)}) (x^{(i)})^T \\ 
      \nabla_b \hat{R}(f) & = \sum_{i=1}^n (f(x^{(i)}) - y^{(i)})
    \end{align}
  \end{theorem}
  \begin{proof}
    Let's write the cross entropy loss function 
    \begin{equation}
      \ell (\theta) = - \sum_{i=1}^N \sum_{k=1}^K y^{(i)}_k \log \big( h_{\theta} (x^{(i)})\big)_k = - \sum_{i=1}^N y^{(i)} \cdot \log \big( h_{\theta} (x^{(i)} \big)
    \end{equation}
   
    where $\cdot$ is the dot product. The gradient of this function may seem daunting, but it turns out to be very cute. Let us take a single sample $(x^{(i)}, y^{(i)})$, drop the index $i$, and write
    \begin{align}
      z & = W x + b = z \\
      \hat{y} & = a = o (z) \\
      L & = - y \cdot \log (a) = - \sum_{k=1}^K y_k \log(a_k)
    \end{align}
   
    We must compute 
    \begin{equation}
      \frac{\partial L}{\partial W} = \frac{\partial L}{\partial a} \frac{\partial a}{\partial z} \frac{\partial z}{\partial \theta}
    \end{equation}
   
    We can compute $\partial L /\partial z$ as such, using our derivations for the softmax derivative above. We compute element wise. 
    \begin{align}
      \frac{\partial L}{\partial z_j} & = - \sum_{k=1}^K y_k \, \frac{\partial}{\partial z_j} \log (a_k) \\
      & = - \sum_{k=1}^K y_k \frac{1}{a_k} \frac{\partial a_k}{\partial z_j} \\
      & = - \sum_{k=1}^K \frac{y_k}{a_k} \, a_k (1_{\{k = j\}} - a_j) \\
      & = - \sum_{k=1}^K y_k (1_{\{k = j\}} - a_j) \\
      & = \bigg( \sum_{k=1}^K y_k a_j \bigg) - y_j \\
      & = a_j \bigg( \sum_{k=1}^K y_k \bigg) - y_j \\
      & = a_j - y_j
    \end{align}
   
    and combining these gives 
    \begin{equation}
      \frac{\partial L}{\partial z} = (a - y)^T
    \end{equation}
   
    Now, computing $\partial z / \partial W$ gives us a $3$-tensor, which is not ideal to work with. However, let us just compute this with respect to the elements again. We have 
    \begin{equation}
      z_k = \sum_{d=1}^D W_{kd} x_d + b_k, \qquad \frac{\partial z_k}{\partial W_{ij}} = \sum_{d=1}^D x_d \frac{\partial}{\partial W_{ij}} W_{kd}
    \end{equation}
   
    where 
    \begin{equation}
      \frac{\partial}{\partial W_{ij}} W_{kd} = \begin{cases} 
        1 & \text{ if } i = k, j = d \\ 
        0 & \text{ else} 
      \end{cases}
    \end{equation}
   
    Therefore, since $d$ is iterating through all elements, the sum will only be nonzero if $k = i$. That is, $\frac{\partial z_k}{\partial W_{ij}} = x_j$ if $k = i$ and $0$ if else. 
    \begin{equation}
      \frac{\partial z}{\partial W_{ij}} = \begin{bmatrix}
      0 \\
      \vdots \\
      0 \\
      x_j \\
      0 \\
      \vdots \\
      0
      \end{bmatrix} \leftarrow i\text{th element} 
    \end{equation}
   
    Now computing 
    \begin{equation}
      \frac{\partial L}{\partial W_{ij}} = \frac{\partial L}{\partial z} \frac{\partial z}{\partial W_{ij}} = (a - y) \frac{\partial z}{\partial W_{ij}} = \sum_{k=1}^K (a_k - y_k) \frac{\partial z_k}{\partial W_{ij}} = (a_i - y_i) x_j
    \end{equation}
    To get $\partial L / \partial W_{ij}$ we want a matrix whose entry $(i, j)$ is $(a_i - y_i) x_j$. This is simply the outer product as shown below. For the bias term, $\partial z / \partial b$ is simply the identity matrix. 
    \begin{equation}
      \frac{\partial L}{\partial W} = (a - y) x^T, \;\;\;\; \frac{\partial L}{\partial b} = a - y
    \end{equation}
   
    Therefore, summing the gradient over some minibatch $M \subset [N]$ gives 
    \begin{equation}
      \nabla_{W} \ell_M = \sum_{i \in M} (h_{\theta}(x^{(i)}) - y^{(i)}) (x^{(i)})^T, \;\;\;\;\; \nabla_{b} \ell_M = \sum_{i \in M} (h_{\theta}(x^{(i)}) - y^{(i)})
    \end{equation}
    and our stochastic gradient descent algorithm is 
    \begin{align*}
      \theta = \begin{pmatrix} W \\ b \end{pmatrix} & = \begin{pmatrix} W \\ b \end{pmatrix} - \eta \begin{pmatrix} \nabla_{W} \ell_M \\ \nabla_{b} \ell_M \end{pmatrix} \\
      & = \begin{pmatrix} W \\ b \end{pmatrix} - \eta \begin{pmatrix} \sum_{i \in M} (h_{\theta}(x^{(i)}) - y^{(i)}) (x^{(i)})^T \\ \sum_{i \in M} (h_{\theta}(x^{(i)}) - y^{(i)}) \end{pmatrix} 
    \end{align*}
  \end{proof}

  \begin{example}[SGD from Scratch]
    \begin{lstlisting}
      >>> import numpy as np
      >>> n, d, K = 500, 5, 3  # n samples, d features, K classes
      >>> W_true = np.array([[1.0, 2.0, -1.5, 0.5, 3.0],
      ...                   [-2.0, 1.0, 2.5, -1.0, 0.5],
      ...                   [0.5, -1.5, 1.0, 2.0, -2.5]])
      >>> 
      >>> 
      >>> b_true = np.array([0.5, -1.0, 1.5])
      >>> X = np.random.randn(n, d)
      >>> def softmax(z):
      ...   exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # numerical stability
      ...   return exp_z / np.sum(exp_z, axis=1, keepdims=True)
      ... 
      >>> 
      >>> logits = X @ W_true.T + b_true
      >>> probs = softmax(logits)
      >>> Y_labels = np.array([np.random.choice(K, p=prob) for prob in probs])
      >>> Y = np.zeros((n, K)) # Convert to one-hot encoding
      >>> Y[np.arange(n), Y_labels] = 1
      >>> W_hat, b_hat  = np.random.randn(K, d) * 0.1, np.random.randn(K) * 0.1
      >>> lr = 0.05
      >>> for epoch in range(10000):
      ...   logits = X @ W_hat.T + b_hat
      ...   predictions = softmax(logits)
      ...   error = predictions - Y  # shape: (n, K)
      ...   grad_W = error.T @ X / n  # shape: (K, d)
      ...   grad_b = np.mean(error, axis=0)  # shape: (K,)
      ...   W_hat -= lr * grad_W
      ...   b_hat -= lr * grad_b
      ... 
      >>> final_predictions = softmax(X @ W_hat.T + b_hat)
      >>> final_accuracy = np.mean(np.argmax(final_predictions, axis=1)==Y_labels)
      >>> print(W_hat)
      [[ 1.33704007  1.96924581 -2.67632699 -0.09619256  3.33028542]
       [-2.12103603  0.49754612  2.08725926 -1.6633942  -0.24173753]
       [ 0.54933076 -2.50510753  0.39990491  1.88095809 -3.14859809]]
      >>> print(b_hat)
      [-0.01252576 -1.10961749  1.37124823]
      >>> print("Accuracy:", final_accuracy)
      Accuracy: 0.914
    \end{lstlisting}
  \end{example}

\subsection{Significance Tests and Confidence Sets}

\subsection{Concentration Bounds}


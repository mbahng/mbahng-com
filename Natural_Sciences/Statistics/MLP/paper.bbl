\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{SHK{\etalchar{+}}14}

\bibitem[CLHN93]{symmetry}
An~Mei Chen, Haw-minn Lu, and Robert Hecht-Nielsen.
\newblock On the geometry of feedforward neural network error surfaces.
\newblock {\em Neural Computation}, 5(6):910--927, 11 1993.

\bibitem[DDS{\etalchar{+}}09]{ImageNet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em 2009 IEEE Conference on Computer Vision and Pattern Recognition}, pages 248--255, 2009.

\bibitem[FC19]{lottery}
Jonathan Frankle and Michael Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural networks, 2019.

\bibitem[Mis20]{mish}
Diganta Misra.
\newblock Mish: A self regularized non-monotonic activation function, 2020.

\bibitem[SHK{\etalchar{+}}14]{dropout}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock {\em Journal of Machine Learning Research}, 15(56):1929--1958, 2014.

\bibitem[TD20]{fiedler}
Edric Tam and David Dunson.
\newblock Fiedler regularization: Learning neural networks with graph sparsity, 2020.

\end{thebibliography}

If we try sticking to linear algebra, we hope to model time series of the form 
\begin{equation}
  X_t = f(t) + w_t
\end{equation}
so that we can decompose to a deterministic process followed by some white noise. There are several ways to approach this, including kernel smoothing, moving average smoothing, or cubic spline smoothing. However, this falls short when you look the residuals. They will follow some pattern that must be removed due to autocorrelation. 

In linear regression, one of the fundamental assumptions was  independence of errors. Ideally, we would also like independence of features, but this is usually not true (in fact, in extreme cases, multicollinearity can screw us up). The relaxation of these assumptions helps us transition from linear regression to time series analysis. Let's go over some basic things with new terms. 

\begin{definition}[Time Series]
  A stochastic process 
  \begin{equation}
    \{X_1, \ldots, X_t, \ldots \}
  \end{equation}
  of random variables indexed by time $t$ is a \textbf{time series}. The stochastic behavior of $\{X_t\}$ is determined by specifying the PDF/PMF 
  \begin{equation}
    p(x_{t_1}, \ldots, x_{t_m}) 
  \end{equation}
  for all finite collections of time indices 
  \begin{equation}
    \{(t_1, \ldots, t_m), m < \infty \}
  \end{equation}
  i.e. all finite-dimensional distributions of $X_t$. 
\end{definition}

\begin{definition}[White Noise]
  \textbf{White noise} $w_t$ is a random variable indexed by time $t$ satisfying 
  \begin{enumerate}
    \item $\mathbb{E}[w_t] = 0$
    \item $\Var[w_t] = \sigma^2$
    \item $\Cov[w_t, w_s] = 0$ for $s \neq t$. That is, they are uncorrelated but not necessarily independent. 
  \end{enumerate}
  Note that this third condition can be strengthened to independence or uncorrelated Gaussians, which automatically imply independence. 
\end{definition}


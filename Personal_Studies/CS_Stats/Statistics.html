<html lang="en">
<head>
<title>Muchang Bahng | Duke Math</title>
<link rel="apple-touch-icon" sizes="180x180" href="/CSS/favicon/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/CSS/favicon/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="/CSS/favicon/favicon.ico">
<link rel="manifest" href="/site.webmanifest">
<link rel="stylesheet" href="/CSS/Header_Footer.css">
<link rel="stylesheet" href="/CSS/html_notes.css">
<script type="text/javascript" src="/Mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({tex2jax: { inlineMath: [ ["$", "$"], ["\\(", "\\)"] ], displayMath: [ ["$$","$$"], ["\\[", "\\]"] ]} });
</script>
</head>
<style>
  

table, th, td {
    border:1px solid black;
}
</style>
<body>
  <div class="header">
    <div id="menu_button" onclick="myFunction(this)">
      <div id="bar1"></div>
      <div id="bar2"></div>
      <div id="bar3"></div>
    </div>
    <a id="HeaderName" href="https://muchangbahng.netlify.app/" style="text-decoration: none;">Muchang Bahng</a>
  </div>
  
  <div class="space" style="height:100px;"></div>
  <div class="information">
  
  <p class="title">Statistics</p>
  <hr>
  
  <div class="toc">
    <div class="toctitle">Contents</div>
    <ol class="toclist">
      <li><a href="#Section1">Probability Densities</a></li>
      <li><a href="#Section2">Inference: Parameter Estimation</a></li>
      <li><a href="#Section3">Inference: Linear Regression</a></li>
      <li><a href="#Section4">Bias Variance Decomposition</a></li>
      <li><a href="#Section5">Markov Chain Monte Carlo</a></li>
    </ol>
  </div> 

<div class="subsection_title">Classical vs Bayesian Statistics</div>
While traditional epistemology attempts to explain the basis of knowledge as justified true beliefs, <b>Bayesian epistemology</b> backs the idea that beliefs can be interpreted as subjective probabilities. As such, they are subject to the laws of probability theory, which act as the norms of rationality. 
<br>

<p id="Section1" class="section_title">Probability Densities</p>
<a id="show_hide_1" class="show_hide" onclick="show_hide_1()">[Hide]</a>
<hr>
<div id="section_content_1"> 

<div class="subsection_title">Notation: Probability Densities & Sampling</div>
<div>
We will introduce some basic probability concepts. A $d$-dimensional random variable $X$ is any stochastic $d$-vector that can be "generated" or "realized" from an <b>outcome space</b> $\Omega$. That is, the random variable $X$ would randomly pick an element in $\Omega$. The uncertainty of these possible values generated by the random variables is specified by some distribution $\text{Dist}$ with some parameter $\theta$. 
  \[X \sim \text{Dist}(\theta)\]
In this case, $\text{Dist}$ is called a $d$-dimensional distribution. The probability density function of the random variable $X$ can be written in many forms and may define different things, depending on context. Generally, if we do not include the parameter $\theta$ in the density expression, then we assume that it is fixed. 
<ol>
  <li>$\text{Dist}(x\,|\,\theta)$ or $\text{Dist}(x;\, \theta)$ tells us the probability of the random variable (following distribution $\text{Dist}$) will generate value $x$, given some fixed $\theta$. Note that this notation allows us to write densities without having to explicitly name a random variable, e.g. by saying "let $X \sim \text{Dist}$ be a random variable."</li>
  <li>If we have defined the distribution of the random variable $X$, we generally treat $p_X (x) = p_X (x;\, \theta) = \text{Dist}(x\,|\,\theta)$. </li>
  <li>Sometimes, we replace the $p$ with an $f$, and call  $f_X (x) = f(x;\,\theta) = \text{Dist}(x\,|\, \theta)$. </li>
</ol>
From a distribution $X$, we can take $n$ samples, which we will denote 
  \[\mathbf{x} = \{x^{(i)}\}_{i=1}^n = \{x^{(1)}, x^{(2)}, \ldots, x^{(n)}\}\]
with each $x^{(i)} \in \Omega$. This set is often called an <b>observation</b>, or <b>data</b>. Note that the space $\Omega$ can be discrete or continuous, and the density expression accounts for cases. 
<ol>
  <li>If $\Omega$ is discrete, then we can assume that $X$ generates discrete $x^{(i)} \in \Omega \subset \mathbb{N}^d$. In the discrete case, the <i>sum</i> of all probabilities equals $1$. 
    \[\sum_{x \in \Omega} x \, p_X (x) = 1\]
  </li>
  <li>If $\Omega$ is continuous, then we assume that $X$ generates real-valued $x^{(i)} \in \Omega \subset \mathbb{R}^d$. In the continuous case, the <i>integral</i> of all probabilities equals $1$. 
    \[\int_{x \in \Omega} x \, p_X (x) = 1\]</li>
</ol> 
</div>

<div class="subsection_title">Bayes' Rule</div>
<div>
We have seen that Bayesian statistics depends on having some initial belief about an event. Upon some observation, we can gain some sort of information about the event, allowing us to <i>modify</i> our prior distribution to a new one, called the posterior distribution. This simple property is the reason why Bayesian statistics is so useful for machine learning. The way we do this is through <b>Bayes' Rule</b>, which states 
  \[p(H\,|\,D) = \frac{p(D\,|\,H) \; p(H)}{p(D)}\]
Note that
<ol>
  <li>$H$ is the <b>hypothesis</b> whose probability may be affected by <b>data</b> $D$, also called <b>evidence</b>. </li>
  <li>$p(H)$ is the <b>prior distribution</b>, our initial hypothesis of what the distribution would have been. </li>
  <li>$p(H\,|\,D)$ is the <b>posterior distribution</b>, which was determined upon observing the event $B$. </li>
  <li>$p(D\,|\,H)$ is the <b>likelihood</b>. If you were to assume that $A$ is true, then the likelihood tells you the probability of getting result $B$. </li>
  <li>$p(D)$ is the <b>marginal likelihood</b>, which is calculated by conditioning on $A$
    \[p(D) = \sum_H p(D\,|\,H)\; p(H) \text{ or } p(D) \int_H p(D\,|\,H)\; p(H) \, dH\]</li>
</ol>
When computing our prior, the outcomes $H$ are the <b>hypotheses</b>. We can assume that hypotheses are mutually exclusive and exhaustive (if one of these is true, it can't be some undefined third option). These assumptions are reasonable since it is almost always possible to redefine an arbitrary set of hypotheses into a set of hypotheses that <i>are</i> mutually exclusive and exhaustive. 
<br>
There are multiple ways to write Bayes rule. When attempting to calculate the posterior, we can see that $p(D)$ is really just a normalization constant and therefore does not affect the type of distribution the posterior is. So, we can in effect write the above as 
  \[p(H\,|\,D) \propto p(D\,|\,H)\; p(H)\]
or 
  \[\text{Posterior } \propto \text{ Prior } \times \text{ Likelihood}\]
where the $\propto$ symbol means "proportional to." We use this notation more often when calculating posteriors since the normalizing constant isn't as important as finding the shape of the posterior density. 
</div>

<div class="subsection_title">Bernoulli, Binomial Distributions</div>
<div>
The <b>Bernoulli distribution</b> $\text{Bernoulli}(p)$, parameterized by a single number $p$ (probability of success) and defined over 1-dimensional $\Omega = \{0, 1\}$, has density 
  \[\text{Bernoulli}(x\,|\,p) = p^x\,(1 - p)^{1 - x}, \;\;\;\;\;\; x \in \Omega = \{0, 1\}\]
The <b>binomial distribution</b> $\text{Binomial}(N, p)$, parameterized by $N$ (number of trials) and $p$ (probability of success), defined over 1-dimensional $\Omega = \{0, 1, \ldots, N\}$, has density 
  \[\text{Binomial}(x\,|\,N, p) = \binom{N}{x} p^x (1 - p)^{N - x} \]
It is a common fact that given iid $X_i \sim \text{Bernoulli}(p), i = 1, \ldots, N$, the sum of them 
  \[Y = \sum_{i=1}^N X_i\]
is a $\text{Binomial}(N, p)$ random variable. 
</div> 

<div class="subsection_title">Uniform, Beta Distribution</div>
<div>
The <b>uniform distribution</b> $\text{Uniform}(a, b)$, parameterized by the two endpoints of the interval $(a, b) \subset \Omega = \mathbb{R}$, has density 
  \[\text{Uniform}(x\,|\,a, b) = \frac{1}{b-a} \text{ for } a \leq x \leq b\]
In practice, we also write it in proportions if we do not care about the normalization constant $c$. So, equivalently, $\text{Uniform}(x\,|\,a, b) \propto 1$. 
<br>
The <b>Beta distribtion</b> $\text{Beta}(a, b)$ is a generalization of the uniform and Bernoulli distributions, with parameters $a, b$, defined over $\Omega = [0, 1]$, and with density 
  \[\text{Beta}(x\,|\,a, b) = \frac{\Gamma(a + b)}{\Gamma(a) \Gamma(b)} x^{a-1} (1-x)^{b-1} = \frac{x^{a -1} (1 - x)^{b-1}}{\int_0^1 x^{a-1} (1-x)^{b-1}}\,dx\]
or in simpler form depending only on proportionality, 
  \[\text{Beta}(x\,|\,a, b) \propto x^{a - 1} (1 - x)^{b-1}\]
</div>

<div class="subsection_title">Poisson, Gamma Distributions</div>
<div>
The <b>Poisson distribution</b> $\text{Poisson}(\lambda)$, parameterized by $\lambda$ (rate, e.g. of a random time process) and outcome space $\Omega = \mathbb{N}_0$, has the density 
  \[\text{Poisson}(x\,|\,\lambda) = \frac{\lambda^x e^{-\lambda}}{x!}\]
The <b>Gamma distribution</b> $\text{Gamma}(a, b)$, parameterized by $a$ (shape) and $b$ (rate) and outcome space $\Omega = (0, \infty)$, has the density 
  \[\text{Gamma}(x\,|\,a, b) = \frac{b^a}{\Gamma(a)} x^{a-1} e^{-bx}\]
</div>

<div class="subsection_title">Exponential, Geometric Distributions</div>
<div>
The <b>exponential distribtion</b> $\text{Exp}(\lambda)$, parameterized by $\lambda$ (rate, or inverse scale) and outcome space $\Omega = [0, \infty)$, has density 
  \[\text{Exp}(x\,|\,\lambda) = \lambda e^{-\lambda x}\]
The <b>geometric distribution</b> $\text{Geometric}(p)$, parameterized by $p$ (probability of success) and outcome space $\Omega = \{1, 2, \ldots\}$, has density 
  \[\text{Geometric}(x\,|\,p) = (1 - p)^{k-1} p\]
</div>

<div class="subsection_title">Multinomial & Dirichlet Distributions</div> 
<div>
The <b>multinomial distribution</b> $\text{Multinomial}(p_1, p_2, \ldots, p_K)$ has $K-1$ parameters (since the $p_i$'s must sum to $1$) is defined over $\{1, 2, \ldots, K\}$, but another conventional outcome space consists of the $K$ unit vectors in $\mathbb{R}^K$. We will call the collection of $p_i$ parameters $\theta$. That is, 
  \[\Omega = \Bigg\{ \begin{pmatrix} 1 \\ 0 \\ \vdots \\ 0 \end{pmatrix}, \begin{pmatrix} 0 \\ 1 \\ \vdots \\ 0 \end{pmatrix}, \ldots \begin{pmatrix} 0 \\ 0 \\ \vdots \\ 1 \end{pmatrix}\Bigg\}\]
With this outcome space, we can conveniently define the density as 
  \[\text{Multinomial}(x\,|\,p_1, \ldots, p_K) = \prod_{k=1}^K p_k^{x_k}\]
which will give $\text{Multinomial}(k\,|\,p_1, \ldots, p_K) = p_k$ for all $k \in \Omega$. Now considering a data set $\mathbf{x}$ of $N$ independent observations $x^{(1)}, \ldots, x^{(N)}$ (with each $x^{(n)} \in \Omega$), the corresponding likelihood function takes the form 
  \[p(\mathbf{x}\,|\,\theta) = \prod_{n=1}^N p(x^{(n)}\,|\,\theta) = \prod_{n=1}^N \prod_{k=1}^K p_k^{x^{(n)}_k} = \prod_{k=1}^K p_k^{\sum_n x^{(n)}_k} = \prod_{k=1}^K p_k^{m_k}\]
Therefore, the likelihood function depends on the $N$ data points only through the $K$ quantities 
  \[m_k = \sum_n x^{(n)}_k\]
which each represent the number of success observations for $k$. These are called the <b>sufficient statistics</b> for this distribution. 
</div>

<div class="subsection_title">Laplace</div>
<div>
The <b>Laplace distribution</b> $\text{Laplace}(\mu, \beta)$, with parameters $\mu$ (center) and $\beta$ (scale) and outcome space $\Omega = \mathbb{R}$, has density of form 
  \[\text{Laplace}(x\,|\, \mu, \beta) = \frac{1}{2b} \exp\bigg( - \frac{|x - \mu|}{b} \bigg)\]
It is also called the <b>double exponential distribution</b> since it can be thought of as two exponential distributions spliced together back to back. 
</div>

<div class="subsection_title">Gaussian Distribution</div>
<div>
The <b>univariate Gaussian distribution</b> $\mathcal{N}(\mu, \sigma^2)$, with parameters $\mu$ (average) and $\sigma^2$ (variance) and outcome space $\Omega = \mathbb{R}$, has density of form 
  \[\mathcal{N}(x\,|\,\mu, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \bigg( -\frac{1}{2 \sigma^2} (x - \mu)^2 \bigg)\]
The <b>$D$-dimensional Gaussian distribution</b> $\mathcal{N}_d(\mu, \Sigma)$ with parameters $\mu$ (average $D$-vector) and $\Sigma$ ($D \times D$ symmetric positive-definite covariance matrix) and outcome space $\Sigma = \mathbb{R}^D$, takes the form 
  \[\mathcal{N}_D (x\,|\,\mu, \Sigma) = \frac{1}{(2\pi)^{D/2}} \frac{1}{|\Sigma|^{1/2}} \exp \bigg(-\frac{1}{2} (x - \mu)^T \Sigma^{-1} (x - \mu)\bigg)\]
The Gaussian is so important because the central limit theorem (CLT) states that, subject to mild conditions, the sum of a set of random variables has a distribution that converges to the Gaussian. The functional dependence of the Gaussian on $x$ is through the quadratic form 
  \[\Delta^2 = (x - \mu)^T \Sigma^{-1} (x - \mu) \]
which appears in the exponent. This quantity $\Delta$ is called the <b>Mahalanobis distance</b> from $\mu$ to $x$ and reduces to the Euclidean distance when $\Sigma = I$. Remember that since $\Sigma$ is symmetric, it can be decomposed into $n$ orthonormal eigenspaces of dimension $1$ with corresponding eigenvalues $\lambda_i$. The eigendecomposition of $\Sigma$ is 
  \[\Sigma = \sum_{i=1}^D \lambda_i u_i u_i^T\]
and its inverse covariance matrix, called the <b>precision matrix</b>, is 
  \[\Sigma^{-1} = \sum_{i=1}^D \frac{1}{\lambda_i} u_i u_i^T\]
To visualize this, we can imagine a $D$-dimensional ellipsoid in $\mathbb{R}^D$ representing a surface of constant probability for a $D$-dimensional Gaussian. The major axes of the ellipsoid are defined by the eigenvectors $u_i$, each stretched by a factor of  its corresponding eigenvalues $\lambda_i$. 
  <img src="Bayesian_Pictures/Gaussian_equiponential.jpg" width="1000px;" style="max-width: 90%;">
We can interpret the equipotential lines as such: Given that the density value at the peak $\mu$ is $p(\mu)$, 
<ul>
  <li>The densities of $x$ on the equipotential ellipse with semimajor axes lengths $\lambda_1^{1/2}, \lambda_2^{1/2}, \ldots, \lambda_D^{1/2}$ centered at $\mu$ is $e^{-1/2} p(\mu)$. (Left)</li>
  <li>The densities of $x$ on the equipotential ellipse with semimajor axes lengths $\lambda_1, \lambda_2, \ldots, \lambda_D$ centered at $\mu$ is $e^{-1} p(\mu)$. (Center)</li>
  <li>The densities of $x$ on the equipotential ellipse with semimajor axes lengths $\lambda_1^2, \lambda_2^2, \ldots, \lambda_D^2$ centered at $\mu$ is $e^{-2} p(\mu)$. (Right)</li>
</ul>
We can perform a convenient change of basis of the $x_1, x_2, \ldots, x_D$, which are the coefficients of a linear combination of the basis vectors $e_i$, to coordinates to the $y_1, \ldots, y_D$ of the basis vectors $u_1, \ldots, u_D$. We can simply change a vector $x$ in the $x_i$ coordinates to $y_i$ by taking its relative distance from center $\mu$ to get $x - \mu$ and projecting it onto each unit vector $u_i$. This is given by the formula 
  \[\text{proj}_{u_i} (x - \mu) = \frac{u_i^T (x - \mu)}{||u_i||^2} u_i = \big( u_i^T (x - \mu) \big) u_i\]
for all $i = 1, \ldots, D$. The scalar coefficient $u_i^T (x - \mu)$, which we will denote as $y_i$, represents the scalar coefficients of the vector such that 
  \[x = \sum_{i=1}^D x_i e_i = \sum_{j=1}^D y_j u_j\]
In matrix form, we  have 
  \[y = U (x - \mu) \iff \begin{pmatrix} y_1 \\ \vdots \\ y_D \end{pmatrix} \begin{pmatrix} - & u_1 & - \\ \vdots & \ddots & \vdots \\ - & u_D & - \end{pmatrix} \begin{pmatrix} x_1 \\ \vdots \\ x_D \end{pmatrix}\]
and therefore, the change of basis matrix is simply $U$. 
  <img src="Bayesian_Pictures/Gaussian_change_of_basis.jpg" width="800px;" style="max-width: 90%;">
Furthermore, the symmetricity of $\Sigma$ allows us to write its determinant as the product of its eigenvalues, so 
  \[|\Sigma|^{1/2} = \prod_{j=1}^D \lambda_j^{1/2}\]
With all this, in the $y_j$-coordinate system, the Gaussian distribution is greatly simplified to 
  \[p(y) = p(x)|J| = \prod_{j=1}^D \frac{1}{(2 \pi \lambda_j)^{1/2}} \exp \bigg(- \frac{y_j^2}{2 \lambda_j} \bigg)\]
which is the product of $D$ independent univariate Gaussian distributions of mean $0$ (in the $y_j$-coordinates) and variance $\lambda_j$. 
</div>

<div class="subsection_title">Joint & Marginal Gaussian Distributions</div>
<div>

</div>

<div class="subsection_title">Cauchy & Student's t-Distribution</div>
<div>
The <b>student's t-distribution</b>, parameterized by $\mu$ (mean), $\lambda$ (precision), and $\nu$ (degrees of freedom) and outcome space $\Omega = \mathbb{R}$, has the density 
  \[\text{St}(x\,|\,\mu, \lambda, \nu) = \frac{\Gamma(\frac{\nu + 1}{2})}{\Gamma(\frac{\nu}{2})} \; \bigg(\frac{\lambda}{\pi \nu} \bigg)^{1/2} \bigg( 1 + \frac{\lambda (x - \mu)^2}{\nu} \bigg)^{-\frac{\nu + 1}{2}}\]
We can interpret it as being roughly similar to a Gaussian, but with heavier tails. Furthermore, as $\nu \rightarrow \infty$, the t-distribution $\text{St}(x\,|\,\mu, \lambda, \nu)$ becomes a Gaussian $\mathcal{N}(x\,|\,\mu, \lambda^{-1})$ with mean $\mu$ and precision $\lambda$. 
<br>
For the particular case of $\nu = 1$, the t-distribution reduces to a <b>Cauchy distribution</b>. 
</div>

</div>

<p id="Section2" class="section_title">Inference: Parameter Estimation</p>
<a id="show_hide_2" class="show_hide" onclick="show_hide_2()">[Hide]</a>
<hr>
<div id="section_content_2">
Descriptive statistics is a summary statistic that quantitatively describes or summarizes features from a collection of samples $\{x^{(i)}\}$. It is extremely useful, but quite boring. However, inferential statistics is a different story. Given a set of samples $\{x^{(i)}\}_{i=1}^n$, we may have to try to predict/infer either which distribution $X$ these samples came from, or if we know the distribution, what its parameters $\theta$ are. This is called an <i>inference problem</i>, and we approach it by constructing and refining a <b>statistical model</b> that we assume the data has been generated from. Assuming that we know what distribution (but not the parameter $\theta$) the $x^{(i)}$'s come from, we can do 2 things: 
<ul>
  <li><b>Frequentist inference</b> tells us to find the likelihood function 
    \[L(\theta) = p(\mathbf{x}\,|\,\theta)\]
  which is a function of $\theta$. The function $L$ tells us that given that we know $\theta$, what the probability of sampling $\mathbf{x}$ is. Clearly the value of $\theta$ that maximizes $L$ represents the best statistical model. </li>
  <li><b>Bayesian inference</b> tells us to find the desired posterior distribution $p(\theta\,|\,\mathbf{x})$ by assuming a reasonable prior, determining the likelihood, and multiplying them together using Bayes rule. 
    \[p(\theta\,|\,\mathbf{x}) \propto p(\theta)\; p(\mathbf{x}\,|\,\theta) = f(\theta)\]
  Finding the maximum of this function $f(\theta)$ that is proportional to $p(\theta\,|\,\mathbf{x})$ with respect to $\theta$ is the best statistical model. But unlike the frequentist approach, we have an entire distribution to work with. It tells us that given this data $\mathbf{x}$, what is the probability that the parameter value of the statistical model is $\theta$, for all $\theta$. </li>
</ul> 
Throughout this section, we will show how parameter estimation problems are approached, often comparing both the frequentist and Bayesian approach. 

<div class="subsection_title">Computing Posteriors with Beta Prior and Binomial Likelihood</div>
<div>
The motivation behind the Beta distribution is that it satisfies <b>conjugacy</b> with a binomial likelihood. That is, assume that we have some data $\mathbf{x}$ of $N$ observations containing $m$ successes and $N-m$ failures (note that this observation $\mathbf{x}$ was in a way "reduced" to the information of only the number of successes $m$). We assume that there is some true success rate $\theta$ (between $0$ and $1$, of course) coming from these samples, and our job is to try and guess the true rate to the best of our abilities. Before we even observe the data $\mathbf{x}$, our initial guess of $\theta$ might be modeled by the prior distribution $\theta \sim \text{Beta}(a, b)$. Furthermore, the likelihood is clearly a binomial (since it represents the probability of getting $m$ successes out of $N$ samples with fixed rate of success $\theta$), so $m\,|\,\theta \sim \text{Binomial}(N, \theta)$. With these conditions, we claim that the posterior is also a beta, since 
\begin{align*} 
  p(\theta\,|\, m) & \propto p_\theta (\theta) \, m\,|\,\theta) \\
  & \propto \theta^{a - 1} (1 - \theta)^{b - 1} \cdot \theta^m (1 - \theta)^{N - m} \\
  & = \theta^{a + m - 1} (1 - \theta)^{b + N - m - 1}
\end{align*}
which gives a density of the random variable $\theta\,|\,m \sim \text{Beta}(a + m, \beta + N - m)$. Let us introduce an example. Say that you moved into a new city and are waiting at the bus stop. You want to find out the probability $\theta$ that any bus successfully goes to your workplace, and after taking 5 random buses, you see that 2 out of the 5 does arrive at the destination. In this case, we see that
<ul>
  <li>$\theta$ is the parameter that we wish to estimate. </li>
  <li>$m$ is the data/evidence that $m=2$ out of the $N=5$ buses arrives at the destination. </li>
</ul>
Remember, the general process is that we first have some prior density of what $\theta$ could be, which would change to a more accurate posterior distribution $\theta\,|\,m$ upon evidence $m$. Depending on how much information we have or what assumptions we're making, we can choose different priors: 
<ol>
  <li>If we have no clue what the actual proportion of buses arriving actually go to the workplace, we can assume the prior to be 
    \[\theta \sim \text{Uniform}(0, 1) \implies p(\theta) = \begin{cases} 1 & 0 \leq \theta \leq 1 \\ 0 & \text{else} \end{cases} \]</li>
  <li>If we think that there should be more weight to the extreme solutions for when $\theta$ is close to $0$ or $1$, then we can use the prior 
    \[p(\theta) \propto \theta^{-\frac{1}{2}} (1 - \theta)^{-\frac{1}{2}} \text{ for } 0 \leq \theta \leq 1\]
  </li>
  <li>If we already had a lot of information that suggested that $\theta$ was probably close to $0.5$, then we would use the prior 
    \[p(\theta) \propto \theta^{100} (1 - \theta)^{100} \text{ for } 0 \leq \theta \leq 1\]
  </li>
</ol>
Now given that we know $\theta$, the probability of getting $m$ out of $N$ is clearly a binomial 
  \[m\,|\,\theta \sim \text{Binomial}(N, \theta) \implies p(m\,|\,\theta) = \binom{N}{m} \theta^x (1 - \theta)^{N-m} \]
Therefore, using Bayes rule $p(\theta\,|\,m) \propto p(\theta)\; p(m\,|\,\theta)$, we get (by substituing $m=2, N=5$ from our data)
<ol>
  <li>for the uniform prior, 
  \begin{align*} 
  p(\theta\,|\,m) & \propto 1 \cdot \binom{N}{m} \theta^m (1 - \theta)^{N - m} \\
  & \propto \theta^m (1 - \theta)^{N - m} \\
  p(\theta\,|\,m) & \propto \theta^2 (1 - \theta)^{3}
  \end{align*}
  </li>
  <li>for the extreme value-weighed prior, 
  \begin{align*} 
    p(\theta\,|\,m) & \propto \theta^{-\frac{1}{2}} (1 - \theta)^{-\frac{1}{2}} \cdot \binom{N}{m} \theta^m (1 - \theta)^{N-m} \\
    & \propto \theta^{m - \frac{1}{2}} (1 - \theta)^{N - m - \frac{1}{2}} \\
    p(\theta\,|\,m) & \propto \theta^{\frac{3}{2}} (1 - \theta)^{\frac{5}{2}}
  \end{align*}
  </li>
  <li>for the average weighed prior, 
  \begin{align*} 
    p(\theta\,|\,m) & \propto \theta^{100} (1 - \theta)^{100} \cdot \binom{N}{m} \theta^m (1 - \theta)^{N-m} \\
    & \propto \theta^{m + 100} (1 - \theta)^{N - m + 100} \\
    p(\theta\,|\,m) & \propto \theta^{102} (1 - \theta)^{103}
  \end{align*}
  </li>
</ol>
Note that this rule that the posterior is $q\,|\, m \sim \text{Beta}(a + m, b + N - m)$ make sense intuitively. The effect of observing data $\mathbf{x}$ that has $m$ successes and $N-m$ failures has been to increase the value of $a$ by $m$ and the value of $b$ by $N-m$, in going from the prior to the posterior. We can reconstruct this sequentially. That is, given that we have data $\mathbf{x}$ consisting of a sequence of successes $S$ and failures $F$ (e.g. $SSFFSFS$), we can sequentially update the posteriors so that a success increases $a$ by $1$ and a failure increases $b$ by $1$. For example, consider the bus example with the uniform prior, with $\mathbf{x} = SFFSF$. Then, we have the sequence of updates 
  \[\text{Beta}(1, 1) \xrightarrow{S} \text{Beta}(2, 1) \xrightarrow{F} \text{Beta}(2, 2) \xrightarrow{F} \text{Beta}(2, 3) \xrightarrow{S} \text{Beta}(3, 3) \xrightarrow{F} \text{Beta}(3, 4)\]
Sequential methods to learning make use of observations one at a time, which can be useful for computing in real time or very large sets that may be infeasible to update in batches. 
<br>
If our goal is to predict, as best we can, the outcome of the next trial (which is Bernoulli by assumption), then we must evaulate the predictive distribution of $x$, given the observed data set $\mathbf{x}$. We can calculate it 
  \[\text{Bernoulli}(x = 1\,|\,\mathbf{x}) = \int_0^1 \text{Bernoulli}(x = 1\,|\,\theta) \, p(\theta\,|\, \mathbf{x}) \, d\theta = \int_0^1 \theta p(\theta\,|\,\mathbf{x})\,d\theta = \mathbb{E}(\theta\,|\,\mathbf{x})\]
</div>

<div class="subsection_title">Maximal Likelihood for Gaussians & Sequential Estimation</div>
<div>
Given a data set $\mathbf{X} = \{\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_N \}$ in which the observations $\{\mathbf{x}_N\}$ are drawn independently from a multivariate Gaussian distribution, we can estimate the parameters of the distributions by maximum likelihood. The log likelihood function is given by 
  \[\ln\, p(\mathbf{X}\,|\,mu, \Sigma) = -\frac{ND}{2}\ln(2 \pi) - \frac{N}{2} \ln|\Sigma| - \frac{1}{2} \sum_{n=1}^N (x_n - \mu)^T \Sigma^{-1} (x_n - \mu) \]
By simple rearrangement, we see that the likelihood function depends on the data set only through the two quantities 
  \[\sum_{n=1}^N x_n \text{ and } \sum_{n=1}^N x_n x_n^T\]
which are known as the <b>sufficient statistics</b> for the Gaussian distribution. Skipping calculations, the maximum likelihood estimate of the mean is simply the mean of the observed set of data points, while that of the covariarance matrix is: 
  \[\mu_{ML} = \frac{1}{N} \sum_{n=1}^N x_n, \;\;\;\;\; \Sigma_{ML} = \frac{1}{N} \sum_{n=1}^N (x_n - \mu_{ML}) (x_n - \mu_{ML})^T\]
If we evaluate the expectations of the maximum likelihood solutions under the true distribution, we obtain the following results 
\begin{align*}
  \mathbb{E} \big( \mu_{ML} \big) & = \mu \\
  \mathbb{E} \big( \Sigma_{ML} \big) & = \frac{N-1}{N} \Sigma
\end{align*}
The expectation of the maximum likelihood estimate for the mean is equal to the true mean. However, the maximum likelihood estimate for the covariance has an expectation that is less than the true value. (Why?)
<br>
The likelihood function can be derived sequentially, where we maximize the likelihood estimate of the distribution parameters incrementally, after every data point $\mathbf{x}_i$. Consider the result for the maximum likelihood estimator of the mean $\mu_{ML}$, which we will denote by $\mu_{ML}^{(N)}$, when based on $N$ observations. If we dissect out the contribution from the final data point $x_N$, we obtain 
\begin{align*} 
  \mu_{ML}^{(N)} & = \frac{1}{N} \sum_{n=1}^N x_n \\
  & = \frac{1}{N} x_N + \frac{1}{N} \sum_{n=1}^{N-1} x_n \\
  & = \frac{1}{N} x_N + \frac{N-1}{N} \mu_{ML}^{(N-1)} \\
  & = \mu_{ML}^{(N-1)} + \frac{1}{N} (x_N - \mu_{ML}^{(N-1)})
\end{align*}
In words, after observing $N-1$ data points we have estimated $\mu$ by $\mu_{ML}^{(N-1)}$. We now observe data point $x_N$, and we obtain our revised estimate $\mu_{ML}^{(N)}$ by moving the old estimate a small amount, proportional to $1/N$, in the direction of the <i>error signal</i> $(x_N - \mu_{ML}^{(N-1)})$. 
</div>

<div class="subsection_title">Bayesian Inference for Gaussian</div>
<div>
The maximum likelihood framework gave point estimates for the parameters $\mu$ and $\Sigma$. Now we develop a Bayesian treatment by introducing prior distributions over these parameters. Given a set of $N$ $D$-dimensional observations $\mathbf{X} = \{x_1, \ldots, x_n\}$, the likelihood function is given by (the unnormalized function of $\mu$): 
  \[p(\mathbf{X}\,|\,\mu, \Sigma) = \prod_{n=1}^N p(x_n\,|\,\mu, \Sigma) = \frac{1}{(2\pi)^{D/2}} \frac{1}{|\Sigma|^{1/2}} \exp \sum_{n=1}^N \bigg(-\frac{1}{2} (x_n - \mu)^T \Sigma^{-1} (x_n - \mu)\bigg)\]
The likelihood function takes the form of the exponential of a quadratic form in $\mu$. Thus, if we choose a prior $p(\mu)$ given by a Gaussian, it will be a conjugate distribution for this likelihood function. Taking our prior distribution to be 
  \[p(\mu, \Sigma) = \mathcal{N}(\mu, \Sigma\,|\,\mu_0, \Sigma_0\]
The similarity of the symbols $\mu, \Sigma$ with $\mu_0, \Sigma_0$ may be slightly confusing. We can think as such: $\mu, \Sigma$ are random variables that determine the paramters of some Gaussian distribution. But the values $\mu, \Sigma$ are uncertain, and their possible values with probabilities take the form of another distribution $\mathcal{N}(\mu_0, \Sigma_0)$. The posterior distribution is given by the familiar formula 
  \[p(\mu, \Sigma\,|\,\mathbf{X}) \propto p(\mathbf{X}\,|\,\mu, \Sigma) \; p(\mu, \Sigma)\]
which is another Gaussian $p(\mu\,|\,\mathbf{X}) = \mathcal{N}(\mu, \Sigma\,|\, \mu_N, \Sigma_N)$. Let us place a few conditions for simplification. Since every Gaussian density can be represented as a product of independent univariate Gaussians, we can work with univariate Gaussians. Furthermore, let us assume that the true value of $\sigma$ is known, so all we have to do is find the posterior distribution of $\mu$ using the prior density $\mathcal{N}(\mu\,|\,\mu_0, \sigma_0^2)$. We have our prior and likelihood to be the following. Note that while the likelihood distribution is pretty much given, we have the flexibility to choose what our prior distribution is. We have only set the prior as a Gaussian simply because it is a conjugate form and therefore will greatly simplify calculations. 
\begin{align*} 
  p(\mu) & = \mathcal{N}(\mu\,|\,\mu_0, \sigma_0^2) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\bigg(-\frac{1}{2\sigma^2} (\mu - \mu_0)^2) \bigg) \\
  p(\mathbf{X}\,|\,\mu) & = \prod_{n=1}^N p(x_n\,|\,\mu) = \frac{1}{(2 \pi \sigma^2)^{N/2}} \exp\bigg(-\frac{1}{2 \sigma^2} \sum_{n=1}^N (x_n - \mu)^2 \bigg)
\end{align*}
which gives a posterior $p(\mu\,|\,\mathbf{X}) = \mathcal{N}(\mu \,|\, \mu_N \sigma_N^2)$ where 
\begin{align*} 
  \mu_N & = \frac{\sigma^2}{N \sigma_0^2 + \sigma^2} \mu_0 + \frac{N \sigma_0^2}{N \sigma_0^2 + \sigma^2} \mu_{ML} \\
  \frac{1}{\sigma_N^2} & = \frac{1}{\sigma_0^2} + \frac{N}{\sigma^2}
\end{align*}
and $\mu_{ML}$ is the maximum likelihood solution for $\mu$ give by the sample mean $\mu_{ML} = \frac{1}{N} \sum_{n=1}^N x_n$. These values makes sense. We can see that the mean of the posterior distribution $\mu_N$ is a compromise between the prior mean $\mu_0$ and maximum likelihod solution $\mu_{ML}$. If the number of observed data points $N=0$, then it is simply the prior mean, but for $N \rightarrow \infty$, the posterior mean is given by the maximum likelihood solution since the data "overpowers" the prior mean assumption. 
<br>
Now, suppose that the mean of the Gaussian over the data is known and we wish to infer the variance. For convenience, let us work with the precision $\lambda = \frac{1}{\sigma^2}$ over the variance. The likelihood function for $\lambda$ is 
  \[p(\mathbf{X}\,|\,\lambda) = \prod_{n=1}^N \mathcal{N} (x_n\,|\, \mu, \lambda^{-1}) \propto \lambda^{N/2} \exp \bigg(-\frac{\lambda}{2} \sum_{n=1}^N (x_n - \mu)^2 \bigg)\]
Note that since this is a function of $\lambda$, it behaves differently than the likelihood function of $\mu$, even though they are of the same form. Since the likelihood function is proportional to the product of a power of $\lambda$ and the exponential of a linear function of $\lambda$, we must find a prior distribution $p(\lambda)$ with precisely these proportional properties identical to that of the likelihood. Fortunately, the Gamma distribution satisfies them, defined by 
  \[p(\lambda\,|\,a_0, b_0) = \text{Gamma}(\lambda\,|\, a_0, b_0) = \frac{1}{\Gamma(a_0)} b_0^{a_0} \lambda{a_0-1} \exp(-b_0 \lambda)\]
Using Bayes rule and multiplying gives the posterior density 
  \[p(\lambda\,|\,\mathbf{X}) \propto \lambda_{a_0 - 1} \lambda^{N/2} \exp\bigg( -b_0 \lambda - \frac{\lambda}{2} \sum_{n=1}^N (x_n - \mu)^2 \bigg)\]
which is indeed the density of a $\text{Gamma}(\lambda\,|\,a_N, b_N)$ distribution, where 
\begin{align*} 
  a_N & = a_0 + \frac{N}{2} \\
  b_N & = b_0 + \frac{1}{2} \sum_{n=1}^N (x_n - \mu)^2 = b_0 + \frac{N}{2} \sigma_{ML}^2
\end{align*}
where $\sigma_{ML}^2$ is the maximum likelihood estimator of the variance. Now, suppose that both the mean and precision are unknown. To find a conjugate prior, we consider the dependence of the likelihood function on $\mu$ and $\lambda$. 
\begin{align*} 
  p(\mathbf{X}\,|\,\mu, \lambda) & = \prod_{n=1}^N \bigg(\frac{\lambda}{2\pi}\bigg)^{1/2} \exp \bigg( -\frac{\lambda}{2} (x_n - \mu)^2 \bigg) \\
  & \propto \bigg(\lambda^{1/2} \exp \Big(-\frac{\lambda \mu^2}{2} \Big) \bigg)^N \, \exp \bigg( \lambda \mu \sum_{n=1}^N x_n - \frac{\lambda}{2} \sum_{n=1}^N x_n^2 \bigg)
\end{align*}
We now wish to identify a prior distributio $p(\mu, \lambda)$ that has the same functional dependence on $\mu$ and $\lambda$ as the likelihood function and that should therefore take the form 
\begin{align*} 
  p(\mu, \lambda) & \propto \bigg( \lambda^{1/2} \exp \Big(-\frac{\lambda \mu^2}{2}\Big) \bigg)^\beta \exp\big( c \lambda \mu - d \lambda \big) \\
  & = \exp \bigg( -\frac{\beta \lambda}{2} \Big(\mu - \frac{c}{\beta} \Big)^2 \bigg) \, \lambda^{\beta/2} \exp \bigg(-\Big(d - \frac{c^2}{2\beta}\Big) \lambda \bigg)
\end{align*}
where $c, d, \beta$ are constants. Since we can always write $p(\mu, \lambda) = p(\mu\,|\,\lambda) p(\lambda)$, we can find $p(\mu\,|\,\lambda)$ and $p(\lambda)$ by inspection. We have just shown that $p(\mu \,|\, \lambda)$ is a Gaussian whose precision is a linear function of $\lambda$ and that $p(\lambda)$ is a gamma distribution, so the normalized prior takes the form 
  \[p(\mu, \lambda) = \mathcal{N}(\mu \,|\, \mu_0, (\beta\lambda)^{-1}) \; \text{Gamma}(\lambda\,|\,a, b)\]
which is called the <b>Gaussian-Gamma distribution</b>. Note that this is not simply the product of an independent Gaussian prior over $\mu$ and a gamma prior over $\lambda$, because the precision of $\mu$ is a linear function of $\lambda$. The extension of this to multivariate random variables is straightforward. 
</div>

<div class="subsection_title">Inference over Periodic Distributions</div>
<div>
Although Gaussian distributions are of great significance, there are situations in which they are inappropriate as density models for continuous variables (e.g. wind direction or quantities periodic over 24 hours). Such quantities are conveniently represented using an angular (polar) coordinate $0 \leq \theta &lt; 2\pi$. Let us consider the problem of evaluating the mean of a set of observations $\mathbf{\theta} = \{\theta_1, \theta_2, \ldots, \theta_N\}$ of a periodic varialble measured in radians. The simple average $(\theta_1 + \ldots + \theta_N)/N$ is strongly coordinate dependent. To find an invariant measure of the mean, we can see that the observations can be viewed as points on the unit circle and can therefore be described instead by two-dimensional unit vectors $x_1, \ldots, x_N$, where $x_n = (\cos{\theta_n}, \sin{\theta_n})$. We can average these vectors and compute its angle to find this average angle. 
  \[\overline{x} = \frac{1}{N} \sum_{n=1}^N x_n = \Big( \frac{1}{N} \sum{n=1}^N \cos{\theta_n} , \frac{1}{N} \sum{n=1}^N \sin{\theta_n} \Big) \implies \overline{\theta} = \tan^{-1} \bigg( \frac{\sum_{n=1}^N \sin{\theta_n}}{\sum_{n=1}^N \cos{\theta_n}} \bigg)\]
In general, any distribution $p(\theta)$ that have period $2 \pi$ must be defined such that it is nonnegative, integrate to $1$, and be periodic. 
\begin{align*} 
  p(\theta) & \geq 0 \\
  \int_0^{2\pi} p(\theta)\,d\theta  & = 1 \\
  p(\theta + 2\pi) & = p(\theta)
\end{align*}
We can obtain a Guassian-like distribution that satisfies these three properties. Consider a 2-dimensional Gaussian over variables $x_1, x_2$ having mean $\mu = (\mu_1, \mu_2)$ and a covariance matrix $\Sigma = \sigma^2 I$. This gives us 
  \[p(x_1, x_2) = \frac{1}{2 \pi \sigma^2} \exp \bigg( -\frac{(x_1 - \mu_1)^2 + (x_2 - \mu_2)^2}{2 \sigma^2} \bigg)\]
Now, suppose that we consider the value of this distribution along a circle of fixed radius. Then, this distribution will be periodic , although it will not be normalized. We can determine the form of this distribution by transforming from Cartesian coordinates to polar coordinates $(r, \theta)$ (so that $x_1 = r \cos{\theta}, x_2 = r \sin{\theta}$) and keeping $r$ constant. 
  <img src="Bayesian_Pictures/circular_normal_change_of_basis.jpg" width="700px" style="max-width: 90%;"> 
This transformation from $\mathbb{R}^2 \longrightarrow [0, 2 \pi)$ defined 
  \[(x_1, x_2) \mapsto \tan^{-1} \frac{y}{x}\]
simply takes the "circular" cross section of the Gaussian and maps those values. We can roughly visualize it as such: 
  <img src="Bayesian_Pictures/Circular_Cross_Section.jpg" width="500px" style="max-width: 90%;"> 
The value of $r$ is not important so we assume $r=1$. With some algebra and trig identities, we have the <b>circular normal</b>, or <b>von Mises distribution</b>, of form 
  \[p(\theta\,|\,\theta_0, m) = \frac{1}{2 \pi I_0 (m)} \exp \big( m \cos(\theta - \theta_0) \big)\]
where the parameter $\theta_0$ corresponds to the mean of the distribution while $m$ is analogous to the precision for the Gaussian. The normalization coefficient $I_0 (m)$ is the zeroth-order Bessel function of the first kind, defined by 
  \[I_0 (m) = \frac{1}{2 \pi} \int_0^{2 \pi} \exp \big( m \cos{\theta}\big) \, d\theta\]
For large $m$, the distribution becomes approximately Gaussian. Considering the maximum likelihood estimators for the parameters $\theta_0$ and $m$ for the circular normal, the log likelihood function is given by 
  \[\ln p(\mathbf{\theta}\,|\, \theta_0, m) = - N \ln(2\pi) - N \ln \big( I_0 (m)\big) + m \sum_{n=1}^N \cos(\theta_n - \theta_0)\]
The maximum estimator for the mean is 
  \[\theta_{0}^{ML} = \tan^{-1} \bigg( \frac{\sum_n \sin{\theta_n}}{\sum_n \cos{\theta_n}} \bigg)\]
while that of $m$ can be evaluated numerically. 
</div>

<div class="subsection_title">Mixtures of Gaussians</div>
<div>
While the Gaussian distribution has important properties, it suffers from significant limitations when modeling real data sets. It is much more powerful to use <b>Gaussian mixture models</b>, which takes advantage of linear superpositions of Gaussians for better modeling. These mixtures can give rise to extremely complex densities, and they are so powerful that by using a sufficient number of Gaussians, almost any continuous density can be approximated to arbitrary accuracy. A mixture of 3 Gaussians is shown below. 
  <img src="Bayesian_Pictures/3_mixture_Gaussian.jpg" width="800px" style="max-width: 90%;">
A superposition of $K$ $D$-dimensional Gaussian densities is of the form 
  \[p(x) = \sum_{k=1}^K \pi_k \mathcal{N} (x\,|\, \mu_k, \Sigma_k)\]
each $D$-dimensional Gaussian density $\mathcal{N}(\mu_k, \Sigma_k)$ is called a <b>component</b> of the mixture, with its own mean $\mu_k$ and covariance $\Sigma_k$. The parameters $\pi_k$ are called <b>mixing coefficients</b>, with the property 
  \[\sum_{k=1}^K \pi_k = 1\]
They can be interpreted as a multinomial distribution reprsenting the probabilities of choosing one of the Gaussians. By conditioning, the marginal density is given by 
  \[p(x) = \sum_{k=1}^K p(k)\,p(x\,|\,k)\]
The right hand side saya that we can view $\pi_k = p(k)$ as the prior probability of picking the $k$th component, and the density $\mathcal{N}(x\,|\,\mu_k, \Sigma_k) = p(x\,|\,k)$ as the probability of $x$ conditioned on $k$. From Bayes rule, we have 
  \[p(k\,|\,x) = \frac{p(k)\, p(x\,|\,k)}{\sum_l p(l)\, p(x\,|\,l)} = \frac{\pi_k \mathcal{N}(x\,|\,\mu_k, \Sigma_k)}{\sum_l \mathcal{N}(x\,|\,\mu_l, \Sigma_l)}\]
The form of the Gaussian mixture distribution is governed by the parameters 
\begin{align*} 
  \pi & = \{\pi_1, \pi_2, \ldots, \pi_K\} \\
  \mu & = \{\mu_1, \mu_2, \ldots, \mu_K\} \\
  \Sigma & = \{\Sigma_1, \Sigma_2, \ldots, \Sigma_K\}
\end{align*}
The log likelihood of this distribution is 
  \[L(\pi, \mu, \Sigma) = \ln p(\mathbf{X}\,|\,\pi, \mu, \Sigma) = \sum_{n=1}^N \ln \bigg( \sum_{k=1}^K \pi_k \mathcal{N} (x_n\,|\, \mu_k, \Sigma_k) \bigg)\]
where we have observations $\mathbf{X} = \{x_1, \ldots, x_n\}$. Since this function is much more complicated, we usually rely on numerical optimization techniques. 
</div>

<div class="subsection_title">Exponential Family of Distributions</div>
<div>
The probability distributions so far are contained within the <b>exponential family</b> of distributions, which have important properties in common. The exponential family of distributions over $x \in \Omega \subset \mathbb{R}^D$, given parameters $\eta$, is defined to be the set of distributions of the form 
  \[p(x\,|\,\eta) = h(x) g(\eta) \exp\big(\eta^T u(x)\big) \]
where $x$ may be a scalar or vector, discrete or continuous. Here, $\eta$ are called the <b>natural parameters</b> of the distribution, and $u(x)$ is some function of $x$. The function $g(\eta)$ can be interpreted as the normalizing coefficient and therefore satisfies 
  \[g(\eta) \int_{x \in \Omega} h(x) \exp\big(\eta^T u(x)\big) \, dx = 1\]
with the integration replaced by a summation if $x$ is discrete. See the Machine learning GLM section <a href="Machine_Learning.html#Section6" target="_blank">here </a> for some examples. Now, consider a set of iid data denoted by $\mathbf{X} = \{x_1, \ldots, x_n\}$, for which the likelihood function is given by 
  \[p(X,|\,\eta) = \bigg( \prod_{n=1}^N h(x_n) \bigg) g(\eta)^N\, \exp \bigg( \eta^T \sum_{n=1}^N u(x_n) \bigg)\]
Setting the gradient of $\ln p(\mathbf{X}\,|\, \eta)$ with respect to $\eta$ to $0$, we can the following condition to be satisfied by the maximum likelihood estimator $\eta_{ML}$: 
  \[- \nabla \ln g(\eta_{ML}) = \frac{1}{N} \sum_{n=1}^N u(x_n)\]
which can in principle be solved to obtain $\eta_{ML}$. The solution for the maximum likelihood estimator depends on the data only through $\sum_n u(x_n)$, which is therefore called the sufficient statistic of this distribution. Therefore, we do not need to store the entire data set itself but its sufficient statistic. 
<br>
In general, for a given probability distribution $p(\mathbf{X}\,|\, \eta)$, we can seek a prior that is conjugate to the likelihood function, so that the posterior distribution has the same functional form as the prior. Given that the likelihood function is in the exponential family, there exists a conjugate prior that can be written in the form 
  \[p(\eta) = p(\eta\,|\, \chi, \nu) = f(\chi, \nu) g(\eta)^\nu \exp \big( \nu \eta^T \chi \big)\]
where $f(\chi, \nu)$ is a normalization coefficient, and $g(\eta)$ is the same function as the one appearing in the exponential family form of likelihood function. Indeed, multiplying this conjugate with the exponential family likelihood gives 
  \[p(\eta\,|\, \mathbf{X}, \chi, \nu) \propto g(\eta)^{\nu + N} \exp \Bigg( \eta^T \bigg( \sum_{n=1}^N u(x_n) + v \chi \bigg) \Bigg)\]
</div>

</div>

<p id="Section3" class="section_title">Linear Regression</p>
<a id="show_hide_3" class="show_hide" onclick="show_hide_3()">[Hide]</a>
<hr>
<div id="section_content_3">
The previous chapter has been mostly on unsupervised learning, which attempts to take some data and infer some model that best fits them. A supervised learning problem is <b>regression</b>. 

<div class="subsection_title">Bayesian Regression: Modeling with Hierarchical Priors</div>
<div>
Given a training data set $\mathcal{D} = (\mathbf{X}, \mathbf{Y})$ comprised of $N$ pairs of observations with corresponding target variables $\{(x_i, y_i)\}_{i=1}^N$ ($x_i \in \mathbb{R}^D, y_i \in \mathbb{R}$), the goal is to predict the value of $y$ for a new value of $x$. We first construct a <i>statistical model</i> (more explained in next next subsection) by assuming that there exists some function $f(x)$ of some form such that the $y_i$'s have been generated by inputting the $x_i$'s into $f$, followed by a random residual term. We assume that the data $\mathcal{D}$ has been sampled independently, but this may not always be a justifiable assumption in practice. Under this model, which we denote $\mathcal{M}_i$, we further assume that $f$ can be parameterized by a vector $\theta$, so therefore, we assume that 
  \[y = f(x, \theta) + \epsilon, \;\;\;\;\;\; \epsilon \sim \text{Residual} (\beta)\]
where $\beta$ is some collection of parameters that determine the error function. 
<ul>
  <li>The frequentist perspective reduces this problem to finding the value of $\theta$ that maximizes the likelihood. That is, we must find 
    \[\theta^* = \text{arg}\, \max_{\theta} p(\mathcal{D}\,|\,\theta) = \text{arg}\, \max_{\theta} \prod_{i=1}^N p(y_i \,|\,x_i, \theta) \]  
  and claiming that $y = f(x, \theta^*)$ is the function of best fit. This is a quite straightforward (hopefully convex) optimization problem, which can be done in many ways (e.g. batch/sequential gradient descent, solving normal equations, etc.). </li>
  <li>The Bayesian approach attempts to construct a <i>distribution</i> of the values of $\theta$. Clearly, this vector $\theta$ would be an element in some multidimensional Euclidean space, and we want to define a posterior density $p(\theta\,|\,\mathcal{D})$ across this space that tells us the probability of $\theta$. Using Bayes rule, 
    \[p(\theta\,|\,\mathcal{D}) \propto p(\mathcal{D}\,|\,\theta) \, p(\theta) \]
  we see that we must define some prior distribution $p(\theta)$ on $\theta$. We can assume that this prior is defined with some distribution
    \[\theta \sim \text{Dist}_\theta (\gamma)\]
  where $\gamma$ is a collection of parameters on $\theta$. Knowing this prior of $\theta$ will allow us to get the posterior of $\theta\,|\,\mathcal{D}$. The not-so-complete Bayesian treatment would treat this $\gamma$ as a known constant. But note that there is still uncertainty of whether $\theta$ comes from $\text{Dist}_\theta (\gamma)$ for one value of $\gamma$, compared to another value of $\gamma$. This uncertainty requires us to treat $\gamma$ as now a <b>hyperparameter</b>, that is a parameter for the distribution of a parameter, and this distribution of $\gamma$, which we can denote
    \[\gamma \sim \text{Dist}_\gamma (\xi)\]
  is called a <b>hyperprior</b>. We can construct higher and higher level hyperpriors on top of this as much as we want, which will lead to more flexibility in our model (but more computationally expensive). This is known as <b>hierarchical priors</b>. Generally, we will only go up to the level of one hyperparameter. 
  </li>
</ul>
</div>

<div class="subsection_title">Computing the Posterior Parameter Distribution by Initially Marginalizing over Hyperparameters</div>
<div>
Let us summarize how we would conduct the Bayesian method step by step. We first have to determine how many levels of hierarchical priors we are accounting for. Say that we will treat $\xi$ as a constant, and consider the parameter $\theta$ along with its hyperparameter $\gamma$. Our goal is to compute the posterior $p(\theta\,|\,\mathcal{D})$. 
<ol>
  <li>Since there is uncertainty over the value of $\theta$ depending on $\gamma$, we can marginalize over $\gamma$ to get
    \[p(\theta\,|\,\mathcal{D}) = \int p(\theta\,|\,\mathcal{D}, \gamma)\, p(\gamma\,|\,\mathcal{D})\; d\gamma\]
  If the situation calls for it, we could also compute the posterior by doing Bayes rule first to get $p(\theta\,|\,\mathcal{D}) \propto p(\mathcal{D}\,|\,\theta)\; p(\theta)$, but then we would have to calculate both $p(\mathcal{D}\,|\,\theta)$ and $p(\theta)$ by marginalizing each over $\gamma$, which would lead to complications. </li>
  <li>To calculate $p(\theta\,|\,\mathcal{D}, \gamma)$, note that the formula for the posterior density of $\theta$ given $\mathcal{D}$ is $p(\theta\,|\,\mathcal{D}) \propto p(\mathcal{D}\,|\,\theta) p(\theta)$, where $p(\theta)$ is a density function of $\theta$ and parameter $\gamma$, which means that $p(\theta\,|\,\mathcal{D})$ would be a density function of $\theta$ and parameter $\gamma$. But since $\gamma$ is fixed, the posterior 
    \[p(\theta\,|\,\mathcal{D}, \gamma) \propto p(\mathcal{D}\,|\,\theta, \gamma) p(\theta\,|\,\gamma)\]
  is a density function of $\theta$ with fixed constant $\gamma$. This can be easily calculated because the prior $p(\theta\,|\,\gamma)$ is of distribution $\text{Dist}_\theta (\gamma)$ and the likelihood d $p(\mathcal{D}\,|\,\theta, \gamma)$ is the product of densities of $y$ given fixed $\theta$. </li>
  <li>To calculate $p(\gamma\,|\,\mathcal{D})$, we first use Bayes rule to get 
    \[p(\gamma\,|\,\mathcal{D}) \propto p(\mathcal{D}\,|\,\gamma)\, p(\gamma)\]
  This can be easily calculated because the prior $p(\gamma)$ is of distribution $\text{Dist}_\gamma (\xi)$ of given $\xi$. The likelihood can be marginalized over $\theta$ to get 
    \[p(\mathcal{D}\,|\,\gamma) = \int p(\mathcal{D}\,|\,\theta, \gamma)\, p(\theta\,|\, \gamma)\; d\theta\]
  where $p(\theta\,|\,\gamma)$ is a function of $\theta$ with given parameter $\gamma$, and $p(\mathcal{D}\,|\,\theta)$ is the product of the individual likelihoods. 
  </li>
</ol>
But remember that this was all assumed under model $\mathcal{M}_i$, so the posterior density $p(\theta^i \,|\,\mathcal{D})$ of the $\theta^i $ parameterizing our best-fit function is really 
  \[p(\theta^i \,|\,\mathcal{D}, \mathcal{M}_i)\]
where we index the parameter of model $\mathcal{M}_i$ to be $\theta^i$, with a superscript (since we may mistake subscript indices to be the components of $\theta$). 
</div>
  
<div class="subsection_title">Computing the Posterior Parameter Distribution by Initially Applying Bayes Rule</div>
<div> 
There is another way we can approach to calculating the posterior $p(\theta\,|\,\mathcal{D})$. 
<ol>
  <li>We directly apply Bayes rule to get 
    \[p(\theta\,|\,\mathcal{D}) = \frac{p(\mathcal{D}\,|\,\theta)\, p(\theta)}{p(\mathcal{D})} = \frac{p(\mathcal{D}\,|\,\theta)\, p(\theta)}{\int p(\mathcal{D}\,|\,\theta)\, p(\theta)\; d\theta}\]
  Since we are working under a specific model $\mathcal{M}_i$, it would be more accurate to say 
    \[p(\theta^i \,|\,\mathcal{D}, \mathcal{M}_i) = \frac{p(\mathcal{D}\,|\,\theta^i, \mathcal{M}_i)\, p(\theta^i \,|\,\mathcal{M}_i)}{p(\mathcal{D}\,|\,\mathcal{M}_i)} = \frac{p(\mathcal{D}\,|\,\theta^i, \mathcal{M}_i)\, p(\theta^i \,|\,\mathcal{M}_i)}{\int p(\mathcal{D}\,|\,\theta^i, \mathcal{M}_i)\, p(\theta^i \,|\,\mathcal{M}_i)\; d\theta^i} \]
  </li>
  <li>Since $\mathcal{D} = \{(x_i, y_i)\}_{i=1}^N$ consists of $N$ independent observations, we can calculate 
    \[p(\mathcal{D}\,|\,\theta^i , \mathcal{M}_i) = \prod_{j=1}^N p(y_j \,|\,x_j, \theta^i, \mathcal{M}_i)\]
  since the form of the likelihood is determined by our model $\mathcal{M}_i$ that says $y = f(x, \theta^i) + \epsilon$. 
  </li>
  <li>To calculate $p(\theta^i\,|\,\mathcal{M}_i)$, we would have to condition over the hyperparameter $\gamma$, which gives 
    \[p(\theta^i \,|\,\mathcal{M}_i) = \int p(\theta^i\,|\,\gamma, \mathcal{M}_i)\, p(\gamma\,|\,\mathcal{M}_i)\; d\gamma\]
  where $p(\theta^i\,|\,\gamma, \mathcal{M}_i)$ is the density of $\text{Dist}_{\theta^i} (\gamma)$ where $\gamma$ is constant, and $p(\gamma\,|\,\mathcal{M}_i)$ is the prior distribution $\text{Dist}_\gamma (\xi)$ with fixed $\xi$. 
  </li>
</ol>
Multiplying the two would get the proportional term, and integrating them over $\theta^i$ would get the marginalization constant 
  \[p(\mathcal{D}\,|\,\mathcal{M}_i) = \int p(\mathcal{D}\,|\,\theta^i, \mathcal{M}_i) \, p(\theta^i\,|\,\mathcal{M}_i)\; d\theta^i\]
entirely defining the posterior. Upon closer inspection, these two methods of deriving the posterior parameter are not that different. One just uses Bayes rule first and then marginalizes, while the other marginalizes and then uses Bayes rule. 
</div>

<div class="subsection_title">Constructing a Predictive Function from Parameter Density</div>
<div>
We can then construct a <b>predictive distribution</b> that calculates the probability of $y$ given $x$. That is, given a new input $x$, the probability of getting a value $y$, given our dataset $\mathcal{D}$, is 
\begin{align*} 
  p(y\,|\,x, \mathcal{D}, \mathcal{M}_i) & = \int p(y\,|\,\theta^i, x, \mathcal{D}, \mathcal{M}_i) \, p(\theta^i \,|\, x, \mathcal{D}, \mathcal{M}_i)\; d\theta^i \\
  & = \int p(y\,|\,\theta^i, x, \mathcal{M}_i)\, p(\theta^i \,|\,\mathcal{D}, \mathcal{M}_i)\; d\theta^i 
\end{align*}
but $p(\theta^i\,|\,\mathcal{D}, \mathcal{M}_i)$ is completely defined by what we just calculated, and $p(y\,|\,\theta^i, x, \mathcal{D}, \mathcal{M}_i)$ is defined by the random variable generated by 
  \[y \sim f(x, \theta^i) + \epsilon\]
</div>

<div class="subsection_title">Basis Functions</div>
<div>
For <i>linear</i> regression, we usually denote the parameters $\theta$ of function $f(x, \theta)$ as $w$, so we can treat them as equivalent. The simplest linear model for regression is one that involves a linear combination of the input variables 
  \[f(x, \theta) = f(x, w) = w_0 + w_1 x_1 + \ldots + w_D x_D\]
where $x = (x_1, \ldots, x_D)^T$. The key property of this model is that it is a linear function of the parameters $w_0, \ldots, w_D$. But the fact that it linear with respect to the input variables $x_i$ imposes significant limitations. Therefore, we can extend the class of models by considering combinations of fixed nonlinear functions of the input variables of the form 
  \[f(x, w) = w_0 + \sum_{j=1}^{M-1} w_j \phi_j (x)\]
where each <b>basis function</b> $\phi_j: \mathbb{R}^D \longrightarrow \mathbb{R}$. By denoting the maximum value of the index $j$ by $M-1$, the total number of parameters in this model will be $M$. Note that the above form can be written in the form 
  \[f(x, w) = \sum_{j=0}^{M-1} w_j \phi_j (x) = w^T \phi(x)\]
by introducing a "dummy" basis function $\phi_0 (x) = 1$. The reason this is still called a linear model is because the function is linear in $w$. 
<br>
We can choose many different types of basis functions. The following examples are for 1-dimensional $x$. 
<ol>
  <li>The <b>polynomial basis functions</b> form powers of $x$ such that 
    \[\phi_j (x) = x^j\]
  One limitation of polynomial basis function is that they are global functions on the input variable, so that changes in one region of input space affect all other regions. This can be resolved by dividing up the input space up into regions and fit a different polynomial in each region, leading to <b>spline functions</b>. </li>
  <li>The <b>Gaussian basis functions</b> (which can be, but not necessarily must be interpreted in the probabilistic way), have the form 
    \[\phi_j (x) = \exp \bigg(-\frac{(x - \mu_j)^2}{2 s^2}\bigg)\]
  where the $\mu_j$ govern the locations of the basis functions in input space, and the parameter $s$ governs their spatial scale. 
  </li>
  <li>The <b>sigmoidal basis functions</b> are of form 
    \[\phi_j (x) = \sigma \bigg( \frac{x - \mu_j}{s} \bigg), \text{ where } \sigma(a) = \frac{1}{1 + e^{-a}}\]
  Rather than using the sigmoid function $\sigma$, we could also use the hyperbolic tangent $\text{tanh}(a) = 2\sigma(a) - 1$. </li>
  <li>The <b>Fourier basis functions</b> leads to an expansion in sinusoidal functions, which has specific frequency and infinite spatial extent. By contrast, basis functions that are localized to finite regions of input space necessarily comprise a spectrum of different spatial frequencies. In many signal processing applications, it is of interest to consider basis functions that are localized in both space and frequency, leading to a class of functions known as <b>wavelets</b>. </li>
</ol>
<img src="Bayesian_Pictures/Linear_basis_functions.jpg" width="800px" style="max-width: 90%;">
</div>

<div class="subsection_title">Bayesian Model Selection</div>
<div>
Note that up until now, we have assumed that we <i>knew</i> the <b>statistical model</b> describing the process of how the data $\mathcal{D}$ was generated. The definition of a model is often used loosely without explicit definition, but we can define it as such: A model completely defines the <i>form</i> of the function $f$ that we assume is generating $y$ for values of $x$. This does not mean that the model corresponds to a parameter value of $w$. It defines the <i>entire form</i> of $f$ for 
  \[y = f(x, \theta) + \epsilon\]
The model then defines the form of $p(y_i\,|\,x_i, \theta)$ according to the above, which then defines the form of the likelihood function 
  \[p(\mathcal{D}\,|\,\theta) = \prod_{i=1}^N p(y_i\,|\,x_i, \theta)\]
Here are some examples of different models for different problems. Note that for every model $\mathcal{M}_i$, the set of parameters $\theta^i$ is <i>different</i>, since the basis functions do not need to necessarily be the same for these models. 
<ol>
  <li>For linear regression, we assume that the distribution is of form 
    \[y = w^T \phi(x) + \epsilon\]
  and thus our models have different forms which are completely dependent on the basis functions $\phi_j(x)$ we choose. Assuming that we have scalar inputs $x \in \mathbb{R}$, we may choose 
  <ul>
    <li>a purely linear model of $x$, which we will call $\mathcal{M}_1$ with $\theta^1 = (w_0, w_1)$. 
      \[y = \begin{pmatrix} w_0 & w_1 \end{pmatrix} \begin{pmatrix} \phi_0 (x) \\ \phi_1 (x) \end{pmatrix} + \epsilon = \begin{pmatrix} w_0 & w_1 \end{pmatrix} \begin{pmatrix} 1 \\ x \end{pmatrix} + \epsilon \]
    Therefore the form is $f(x, w) = w_0 + w_1 x$. 
    </li>
    <li>a quadratic model of $x$, which we will call $\mathcal{M}_2$ with $\theta^2 = (w_0, w_1, w_2)$. 
      \[y = \begin{pmatrix} w_0 & w_1 & w_2 \end{pmatrix} \begin{pmatrix} \phi_0 (x) \\ \phi_1 (x) \\ \phi_2 (x) \end{pmatrix} + \epsilon = \begin{pmatrix} w_0 & w_1 & w_2 \end{pmatrix} \begin{pmatrix} 1 \\ x \\ x^2 \end{pmatrix} + \epsilon \]
    Therefore the form is $f(x, w) = w_0 + w_1 x + w_2 x^2$. 
    </li>
    <li>a cubic model of $x$ called $\mathcal{M}_3$ with form $f(x, \theta) = w_0 + w_1 x + w_2 x^2 + w_3 x^3$, and so on... </li>
  </ul></li>
  <li>More examples to be updated. </li>
</ol>
A fully Bayesian approach would condition over all possible models when predicting $y$ given $x$. Suppose that we have a finite set of Bayesian models $\{\mathcal{M}_i\}$ (each with their own parameters $\theta^i$) that we could use to explain the observed data $\mathcal{D}$. Then, as shown above, for each $i$th model, we would calculate the posterior density of the parameter $p(\theta^i\,|\,\mathcal{D}, \mathcal{M}_i)$ and then construct a predictive distribution 
  \[p(y\,|\,x, \mathcal{D}, \mathcal{M}_i)\]
for each model in $\{\mathcal{M}_i\}$. Then we calculate the posterior probabilities of the models $p(\mathcal{M}_i\,|\,\mathcal{D})$, and conditioning over all possible models, we get the ultimate predictive distribution over all models 
  \[p(y\,|\,x, \mathcal{D}) = \sum_i p(y\,|\,x, \mathcal{D}, \mathcal{M}_i)\, p(\mathcal{M}_i\,|\,\mathcal{D})\]
This is called a <b>mixture model</b> or <b>Bayesian model averaging</b>, but in practice this is not used due to computational overhead. A more common practice is simply to calculate all the $p(\mathcal{M}_i\,|\,\mathcal{D})$, pick the $\mathcal{M}_i$ that has the highest posterior probability, and build out predictive distribution assuming $\mathcal{M}_i$. This is called <b>model selection</b>, since we are throwing away all other models that are deemed to overfit or underfit and selecting the best one.  
<br>
Now, the problem of model selection (and averaging) reduces to just finding the posterior model probabilities $p(\mathcal{M}_i \,|\,\mathcal{D})$, since we know how to do everything else. We can work out the posterior probability over the models via Bayes rule 
  \[p(\mathcal{M}_i \,|\,\mathcal{D}) \propto p(\mathcal{D}\,|\,\mathcal{M}_i)\, p(\mathcal{M}_i)\]
$p(\mathcal{M}_i)$ is the prior distribution over models that we have selected, which is conventionally set to the uniform: $p(\mathcal{M}_i) \propto 1$. Therefore, calculating the posterior probability of the models reduces to calculating $p(\mathcal{D}\,|\,\mathcal{M}_i)$ which is called the <b>model evidence</b>. By marginalizing over the parameter $\theta^i$, we have 
  \[p(\mathcal{D}\,|\,\mathcal{M}_i) = \int p(\mathcal{D}\,|\,\theta^i, \mathcal{M}_i) \, p(\theta^i\,|\,\mathcal{M}_i)\; d\theta^i\]
To calculate this, we evaluate each component of the integral: 
<ul>
  <li>Remember that $\mathcal{D} = \{(x_i, y_i)\}_{i=1}^N$ is composed of independent data. So 
    \[p(\mathcal{D}\,|\,\theta^i, \mathcal{M}_i) = \prod_{i=1}^N p(y_i \,|\, x_i, \theta^i, \mathcal{M}_i)\]
  which is well-defined since we can simply use our model $y = f (x, \theta^i) + \epsilon$. </li>
  <li>Furthermore, we see that $p(\theta^i \,|\,\mathcal{M}_i)$ should be conditioned over its hyperparameter $\gamma$, so 
    \[p(\theta^i \,|\,\mathcal{M}_i) = \int p(\theta^i \,|\,\gamma, \mathcal{M}_i) \, p(\gamma\,|\,\mathcal{M}_i)\; d\gamma\]
  where $\theta^i \,|\,\gamma \sim \text{Dist}_\theta (\gamma)$ for constant $\gamma$ and $\gamma \sim \text{Dist}_\gamma (\xi)$ for constant $\xi$. 
  </li>
</ul>
Note that if we had calculated the posterior densities of the parameters $p(\theta_i\,|\,\mathcal{D}, \mathcal{M}_i)$ by applying Bayes rule first, we would have calculated the posterior as 
  \[p(\theta^i\,|\,\mathcal{D}, \mathcal{M}_i) = \frac{p(\mathcal{D}\,|\,\theta^i, \mathcal{M}_i)\, p(\theta^i \,|\,\mathcal{M}_i)}{p(\mathcal{D}\,|\,\mathcal{M}_i)} = \frac{p(\mathcal{D}\,|\,\theta^i, \mathcal{M}_i)\, p(\theta^i \,|\,\mathcal{M}_i)}{\int p(\mathcal{D}\,|\,\theta^i, \mathcal{M}_i)\, p(\theta^i \,|\,\mathcal{M}_i)\; d\theta^i} \]
Note that the marginalization term that we've already calculated is the model evidence! So this shortcut may save us a lot of computation. 
</div>

<div class="subsection_title">Intution Behind Model Evidence</div>
<div>
  Let us take a closer look at the model evidence term and try to develop an intuition for it. 
  \[p(\mathbf{Y} \,|\,\mathcal{M}_i) = \int p(\mathbf{Y}\,|\,w, \mathcal{M}_i) p(w\,|\,\mathcal{M}_i) \; dw\]
Note that the evidence tells us the probability of getting $\mathbf{Y}$ from a given model $\mathcal{M}_i$, and we want this to be as large as possible. It does this by conditioning over all possible values of $w$ for that given model. Consider first the case of a model having a single parameter $w$. Let us make two assumptions: 
<ul>
  <li>The posterior distribution $p(\mathbf{Y}\,|\,w, \mathcal{M}_i)$ is sharply peaked around the most probable value $w_{MAP}$, with width $\Delta w_{\text{posterior}}$. </li>
  <li>The prior distribution $p(w\,|,\mathcal{M}_i)$ is flat with width $\Delta w_{\text{prior}}$, so that $p(w) = 1/ \Delta w_{\text{prior}}$. </li>
</ul>
  <img src="Bayesian_Pictures/approximation.jpg" width="350px" style="max-width: 90%;">
Then, we can approximate 
  \[p(\mathbf{Y}\,|\,\mathcal{M}_i) = \int p(\mathbf{Y}\,|\,w, \mathcal{M}_i) p(w\,|\,\mathcal{M}_i)\; dw \approx p(\mathbf{Y}\,|\,w_{MAP}) \, \frac{\Delta w_{\text{posterior}}}{\Delta w_{\text{prior}}}\]
Note two things: 
<ul>
  <li>The term $p(\mathbf{Y}\,|\,w_{MAP})$ gives the fit to the data given the most probable parameter values $w_{MAP}$. If this fit is better (i.e. this term becomes larger), then the evidence also increases. </li>
  <li>However, the ratio $\Delta w_{\text{posterior}}/\Delta w_{\text{prior}}$ should be less than $1$, meaning that the more "squished" the posterior distribution is, the smaller this fraction becomes, decreasing the evidence. </li>
</ul>
For a model having a set of $M$ parameters, we can make a similar approximation. Assuming that all parameters have the same ratio $\Delta w_{\text{posterior}}/\Delta w_{\text{prior}}$, we get
  \[p(\mathbf{Y}\,|\,\mathcal{M}_i) = p(\mathbf{Y}\,|\, w_{MAP}, \mathcal{M}_i) \, \bigg(\frac{\Delta w_{\text{posterior}}}{\Delta w_{\text{prior}}} \bigg)^M\]
Therefore, we can see that the size of the complexity penalty increases with the number $M$ of adaptive parameters in the model. Therefore, given two models $\mathcal{M}_i$ and $\mathcal{M}_j$ with the latter having more parameters (e.g. higher degree polynomial model), the model evidence $p(\mathbf{Y}\,|\,\mathcal{M}_j)$ will decrease at a faster rate as the posterior gets more fine-tuned to the data. 
</div>

<div class="subsection_title">Frequentist Linear Regression Using Maximum Likelihood: Gaussian Error w/ OLS & Laplacian Error w/ LAV</div>
<div>
Now, given dataset $\mathcal{D} = (\mathbf{X}, \mathbf{Y})$, we fix a model $\mathcal{M}$ and assume that $f(x, w) = w^T \phi(x)$ for a given collection (determined by $\mathcal{M}$) and the noise is Gaussian $\epsilon \sim \mathcal{N}(0, \beta^{-1})$. Therefore, 
  \[y = w^T \phi(x) + \epsilon = \begin{pmatrix} w_1 & \ldots & w_D \end{pmatrix} \begin{pmatrix} \phi_0 (x) \\ \vdots \\ \phi_D (x) \end{pmatrix} + \epsilon \implies p(y\,|\,x, w, \beta) = \mathcal{N} \big(y\,|\, w^T \phi(x), \beta^{-1} \big)\]
Then, the likelihood function is 
  \[p(\mathcal{D}\,|\,w, \beta) = \prod_{n=1}^N p(y_i\,|\,x_i, w, \beta) = \prod_{n=1}^N \mathcal{N}\big( y_n \,|\, w^T \phi(x_n), \beta^{-1} \big)\]
Taking the logarithm of it and a bit of algebra gives 
\begin{align*} 
  \ln p(\mathcal{D}\,|\,w, \beta) & = \frac{N}{2} \ln{\beta} - \frac{N}{2} \ln{2\pi} - \beta E_D (w)\\
  & = \frac{N}{2} \ln{\beta} - \frac{N}{2} \ln{2\pi} - \beta \cdot \frac{1}{2} \sum_{n=1}^N \big(y_n - w^T \phi(x_n)\big)^2 
\end{align*}
which we can see is very dependent on the <b>sum-of-squares error term</b> $E_D(w)$. This is the motivation behind the least squares function as the cost function for modeling functions with Gaussian errors. Moving on, maximizing this likelihood gives us 
\begin{align*} 
  w_{ML} & = (\Phi^T \Phi)^{-1} \Phi^T \mathbf{Y} \\
  \beta_{ML} & = \Bigg( \frac{1}{N} \sum_{n=1}^N \big( y_n - w_{ML}^T \phi(x_n)\big)^2 \Bigg)^{-1}
\end{align*}
where $\mathbf{Y}$ is the $N$-vector of target values $y_i$ in the data $\mathcal{D}$ and $\Phi$ is the $N \times M$ matrix of basis functions evaluated for each $x_n$. 
  \[\Phi = \begin{pmatrix} 
  \phi_1 (x_1) & \phi_2 (x_1) & \ldots & \phi_{M-1} (x_1) & \phi_M (x_1) \\
  \phi_1 (x_2) & \phi_2 (x_2) & \ldots & \phi_{M-1} (x_2) & \phi_M (x_2) \\
  \vdots & \vdots & \ddots & \vdots & \vdots \\
  \phi_1 (x_{N-1}) & \phi_2 (x_{N-1}) & \ldots & \phi_{M-1} (x_{N-1}) & \phi_M (x_{N-1}) \\
  \phi_1 (x_N) & \phi_2 (x_N) & \ldots & \phi_{M-1} (x_N) & \phi_M (x_N) 
  \end{pmatrix}\]
Note that even if there were a hyperparameter of $\theta$, the frequentist approach would not care about this because all it looks at is the likelihood of $\mathcal{D}$ <i>given</i> $\theta$. Note that if we assumed that the residual noise distribution was $\epsilon \sim \text{Laplace}(0, \beta)$, the likelihood function would turn out to be 
  \[p(\mathcal{D}\,|\,w, \beta) = \prod_{n=1}^N \text{Laplace}(y_n\,|\,w^T \phi(x_n), \beta) = \prod_{n=1}^N \frac{1}{2\beta} \exp\bigg(- \frac{|y_n - w^T \phi(x_n)|}{b} \bigg)\]
and taking the logarithm of it gives 
\begin{align*} 
  \ln p( \mathcal{D}\,|\, w, \beta) & = -N \ln{(2\beta)} - \frac{2}{\beta} E_D (w) \\
  & = -N \ln{(2\beta)} - \frac{1}{\beta} \sum_{n=1}^N \big| y_n - w^T \phi(x_n) \big|
\end{align*}
which is now dependent on the <b>sum-of-residuals error term</b> $E_D(w)$.  
</div>

<div class="subsection_title">Regularization: Gaussian Parameter Prior w/ L2 Regularizers & Laplacian Parameter Prior w/ L1 Regularizers</div>
<div>
In some cases of solving the least squares problem, it may be case that our model with optimized parameters $w, \beta$ may be either
<ul>
  <li>too fine-tuned to the data, i.e. may overfit. This happens when the number of basis functions exceeds the number of observations, which makes the least squares problem ill-posed and is therefore impossible to fit because the associated optimization problem has infinitely many solutions. RLS allows the introduction of further constraints that uniquely determine the solution. </li>
  <li>suffering from poor generalization. </li>
</ul>
Therefore, we can add a <b>regularization term</b> $E_W (w)$ to our residual-squared error function $E_D (w)$. 
  \[E_D (w) + \lambda E_W (w)\]
The idea is that as the model becomes more complex and as $w$'s values increase, the $E_W (w)$ term will also increase, nullifying the minimization of $E_D (w)$. Two common regularization terms are 
<ul>
  <li>The <b>L1 regularization term</b> is 
    \[E_W (w) = \frac{1}{2} \sum_{j=0}^{M-1} |w_j|\]
  which leads us to find 
  \begin{align*} 
    \text{arg}\, \min_w \bigg\{ \frac{1}{2} \sum_{n=1}^N \big( y_n - w^T \phi(x_n) \big)^2  + \frac{\lambda}{2} \sum_{j=0}^{M-1} |w_j| \bigg\} & \text{ if } \epsilon \text{ is Gaussian} \\
    \text{arg}\, \min_w \bigg\{ \frac{1}{2 \beta} \sum_{n=1}^N \big| y_n - w^T \phi(x_n) \big| + \frac{\lambda}{2} \sum_{j=0}^{M-1} |w_j| \bigg\} & \text{ if } \epsilon \text{ is Laplacian} \\
  \end{align*}
    \[\]</li>
  <li>The <b>L2 regularization term</b>
    \[E_W (w) = \frac{1}{2} \sum_{j=0}^{M-1} w_j^2 = \frac{1}{2} ||w||^2 = \frac{1}{2} w^T w\]
  which leads us to find 
  \begin{align*} 
    \text{arg}\, \min_w \bigg\{ \frac{1}{2} \sum_{n=1}^N \big( y_n - w^T \phi(x_n) \big)^2  + \frac{\lambda}{2} \sum_{j=0}^{M-1} w_j^2 \bigg\} & \text{ if } \epsilon \text{ is Gaussian} \\
    \text{arg}\, \min_w \bigg\{ \frac{1}{2 \beta} \sum_{n=1}^N \big| y_n - w^T \phi(x_n) \big| + \frac{\lambda}{2} \sum_{j=0}^{M-1} w_j^2 \bigg\} & \text{ if } \epsilon \text{ is Laplacian} \\
  \end{align*}
  </li>
</ul>
But how do we know which regularization term $E_W (w)$ to use? 
<ul>
  <li>Remember that our <i>assumption</i> of the form of the error distribution $\epsilon$ led to least error term. A Gaussian $\epsilon$ led to a OLS cost function, and a Laplace $\epsilon$ led to a LAV cost function. </li>
  <li>Similarly, our assumption of the form of the prior density $p(w)$ will naturally lead to the form of the regularization term. A Gaussian prior $p(w)$ leads to the L2 regularizer, and a Laplace $p(w)$ leads to the L1 regularizer. </li>
</ul>
We must step out of the frequentist setting and let Bayesian statistics take over. Unlike simply getting the point estimate from the maximum likelihood, i.e. calculating $\text{arg}\, \max_w p(\mathcal{D}\,|\,w)$, we must calculate 
\begin{align*} 
  \text{arg}\, \max_w p(w\,|\,\mathcal{D}) & = \text{arg}\, \max_w p(\mathcal{D}\,|\, w)\, p(w) \\
  & = \text{arg}\, \max_w \log\big(p(\mathcal{D}\,|\, w)\, p(w)) \\
  & = \text{arg}\, \max_w \Big( \log{p(\mathcal{D}\,|\,w)} + \log{p(w)} \Big)
\end{align*}
Note that the frequentist calculations is the Bayesian approach with the prior $p(w)$ set to uniform. Previously, we have assumed that $p(w) = \mathcal{N}(w\,|\, 0, \alpha^{-1} I)$. We can simplify this assumption by further assuming that it is a product of univariate distributions for each of its parameters $w_i$, which can be done with a change of basis. So, we will write 
  \[p(w) = \prod_{j=0}^{M-1} p(w_j) = \begin{cases} 
  \prod_{j=0}^{M-1} \mathcal{N}(w_j \,|\,0, \alpha^{-1}) & \text{ if assuming Gaussian} \\
  \prod_{j=0}^{M-1} \text{Laplace}(w_j \,|\,0, \alpha^{-1}) & \text{ if assuming Laplace}
  \end{cases}\]
Remember that $\epsilon \sim \mathcal{N}(0, \beta^{-1})$, and the priors $\mathcal{N}(0, \alpha^{-1})$ and $\text{Laplace}(0, b)$ have fixed and known parameters $\alpha$ and $b$.  
<ul>
  <li>If we assume that each $p(w_j)$ is Gaussian, we have 
  \begin{align*} 
    \text{arg}\, \max_w p(w\,|\,\mathcal{D}) & = \text{arg}\, \max_w \Big( \log{p(\mathcal{D}\,|\,w)} + \log{p(w)} \Big) \\
    & = \text{arg}\, \max_w \Bigg( \log \prod_{n=1}^N \mathcal{N}\big(y_n \,|\, w^T \phi(x_n), \beta^{-1} \big) + \log \prod_{j=0}^{M-1} \mathcal{N}(w_j \,|\,0, \alpha^{-1}) \Bigg) \\
    & = \text{arg}\, \max_w \Bigg( \log \prod_{n=1}^N \frac{1}{\beta^{-1} \sqrt{2 \pi}} e^{-\frac{(y_n - w^T \phi(x_n))^2}{2 (\beta^{-1})^2}} + \log \prod_{j=0}^{M-1} \frac{1}{\alpha^{-1} \sqrt{2 \pi}} e^{-\frac{w_j^2}{2 (\alpha^{-1})^2}}\Bigg) \\
    & = \text{arg}\, \min_w \frac{1}{2 (\beta^{-1})^2} \bigg( \sum_{n=1}^N \big( y_n - w^T \phi(x_n)\big)^2 + \frac{(\beta^{-1})^2}{(\alpha^{-1})^2} \sum_{j=0}^{M-1} w_j^2 \bigg) \\
    & = \text{arg}\, \min_w \bigg( \sum_{n=1}^N \big( y_n - w^T \phi(x_n)\big)^2 + \lambda \sum_{j=0}^{M-1} w_j^2 \bigg)
  \end{align*}
  So, we can see that having a Gaussian prior of the parameter naturally leads to us minimizing the L2-regularized cost function. Furthermore, we have the optimal value $\lambda = (\beta^{-1})^2/(\alpha^{-1})^2$. 
  </li>
  <li>If we assume that each $p(w_j)$ is Laplace, we have  
  \begin{align*} 
    \text{arg}\, \max_w p(w\,|\,\mathcal{D}) & = \text{arg}\, \max_w \Big( \log{p(\mathcal{D}\,|\,w)} + \log{p(w)} \Big) \\
    & = \text{arg}\, \max_w \Bigg( \log \prod_{n=1}^N \mathcal{N}\big(y_n \,|\, w^T \phi(x_n), \beta^{-1} \big) + \log \prod_{j=0}^{M-1} \text{Laplace}(w_j \,|\,0, b) \Bigg) \\
    & = \text{arg}\, \max_w \Bigg( \log \prod_{n=1}^N \frac{1}{\beta^{-1} \sqrt{2 \pi}} e^{-\frac{(y_n - w^T \phi(x_n))^2}{2 (\beta^{-1})^2}} + \log \prod_{j=0}^{M-1} \frac{1}{2b} e^{-\frac{|w_j|}{b}} \Bigg) \\
    & = \text{arg}\, \min_w \frac{1}{2 (\beta^{-1})^2} \bigg( \sum_{n=1}^N \big( y_n - w^T \phi(x_n)\big)^2 + \frac{2 (\beta^{-1})^2}{b} \sum_{j=0}^{M-1} |w_j| \bigg) \\
    & = \text{arg}\, \min_w \bigg( \sum_{n=1}^N \big( y_n - w^T \phi(x_n)\big)^2 + \lambda \sum_{j=0}^{M-1} |w_j| \bigg)
  \end{align*}
  So, we can see that having a Laplace prior of the parameter naturally leads to us minimizing the L1-regularized cost function. Furthermore, we have the optimal value $\lambda = 2 (\beta^{-1})^2/b$. 
  </li>
</ul>
To develop an intuition for this, let us visualize what this regularization term does. Setting $w \in \mathbb{R}^2$ for visual purposes, we can visualize the (unregularized) error function $E_D (w)$ as being defined over $\mathbb{R}^2$ with contours, where darker lines represent lower values. Clearly, the minimum value of $w$ lies at the dot $w^*$. 
  <img src="Bayesian_Pictures/ED(W).jpg" width="500px" style="max-width: 90%;">
But now let's add the regularization term $E_W (w)$, scaled by $\lambda$. $E_W (w) = 0$ at the origin, but it increases as we move into circles with higher radii (according to their respective L1, L2 norms). Therefore, the value of $E_D(w) + \lambda E_W (w)$ at $w^*$ may not be the minimum because $E_W (w)$ may have increased too much by the time we reached $w^*$. This leads to a <i>new</i> minimum $w^**$ of the regularized error, located at the point where the both contours are as light as possible. 
<img src="Bayesian_Pictures/L1vsL2.jpg" width="800px" style="max-width: 90%;">
Note that as shown above, the L1 case is more likely to lead to sparse solutions (i.e. solutions $w^{**}$ closer to the axes), while L2 regularization promotes smaller coefficients in general (i.e. no one coefficient should be too large). <b>Sparsity</b> translates to some coefficients having values, while others are $0$ (or closer to $0$). In one dimension, the figure is even simpler to imagine. 
  <img src="Bayesian_Pictures/1D.jpg" width="500px" style="max-width: 90%;">
In summary, 
<ul>
  <li>The Laplace prior promotes sparsity, i.e. zeroes out some of the coefficients due to its greater peak around $0$. </li>
  <li>The Gaussian prior is more diffused around $0$, allowing non-zero values to have greater probability mass. </li>
</ul>
Other possibilities for robust priors are Cauchy or t-distributions. 
</div>

<div class="subsection_title">Bayesian Linear Regression with Gaussian Priors</div>
<div>
To perform linear regression in the Bayesian setting, let us start off with a collection of potential models $\{\mathcal{M}_i\}_{i=1}^L$ and dataset $\mathcal{D}$. For each model $\mathcal{M}_i$ with 
  \[y = w^T \phi(x) + \epsilon, \;\;\;\;\; \epsilon \sim \mathcal{N}(0, \beta^{-1})\]
We will state our unknowns: 
<ul>
  <li>The value of $\beta$ that determines the variance of the error $\epsilon$ will have a <i>fixed</i> prior distribution $p(\beta)$ (with no hyperparameter). </li>
  <li>The parameter $w$ has a (not fixed) prior distribution $p(w) = \mathcal{N}(w\,|\,0, \alpha^{-1} I)$ with hyperparameter $\alpha$. </li>
  <li>The value of $\alpha$ that determines the covariance matrix of the prior of $w$ will have a fixed prior distribution $p(\alpha)$, with no further hyperparameters. </li>
</ul>
Now, our final goal is to construct the predictive function $p(y\,|\,x, \mathcal{D})$. But since the predictive function is completely determinant on the values of $w, \beta$ (since $y = w^T \phi(x) + \epsilon \sim \mathcal{N}\big( w^T \phi(x), \beta^{-1}\big)$, we simply marginalize over the two parameters to simplify it into 
\begin{align*} 
  p(y\,|\,x, \mathcal{D}) & = \iint p(y\,|\, x, w, \beta, \mathcal{D}) \, p(w, \beta \,|\,\mathcal{D})\; dw\, d\beta \\
  & = \iint \mathcal{N}\big(y\,|\, w^T \phi(x), \beta^{-1}\big) \, p(w, \beta\,|\, \mathcal{D})\; dw\, d\beta
\end{align*}
Therefore, we now need to calculate the joint posterior distribution of $w, \beta$ given $\mathcal{D}$. To marginalize this over the proper parameters, we need more insight. 
<ul>
  <li>Let us first calculate $p(w\,|\,\mathcal{D})$ to see what parameters the posterior density is dependent on. 
  \begin{align*} 
    p(w\,|\,\mathcal{D}) & \propto p(\mathcal{D} \,|\,w)\, p(w) \\
    & = \Bigg( \int p(\mathcal{D}\,|\,w, \beta) \, p( \beta) \; d\beta \Bigg) \cdot \Bigg( \int p(w\,|\,\alpha)\, p(\alpha)\; d\alpha \Bigg) \\
    & = \int \bigg( \prod_{n=1}^N p(y_i\,|\,x_i, w, \beta)\bigg)\, p(\beta) \; d\beta \cdot \Bigg( \int \mathcal{N}(w\,|\,0, \alpha^{-1} I)\, p(\alpha)\; d\alpha \Bigg) \\
    & = \int \bigg( \prod_{n=1}^N \mathcal{N}\big( y\,|\, w^T \phi(x), \beta^{-1} \big) \bigg)\, p(\beta) \; d\beta \cdot \Bigg( \int \mathcal{N}(w\,|\,0, \alpha^{-1} I)\, p(\alpha)\; d\alpha \Bigg)
  \end{align*}
  Note that in this case, we marginalized over all $\beta$ and $\alpha$, so $p(w\,|\,\mathcal{D})$ is parameterized by both $\alpha$ and $\beta$. If we kept them fixed, we would have 
  \begin{align*} 
    p(w\,|\,\alpha, \beta, \mathcal{D}) & \propto p(\mathcal{D}\,|\,w, \alpha, \beta) \, p(w\,|\,\alpha, \beta) \\
    & = p(\mathcal{D}\,|\,w, \beta) \, p(w\,|\,\alpha) \\
    & = \bigg( \prod_{n=1}^N \mathcal{N}\big( y\,|\, w^T \phi(x), \beta^{-1} \big) \bigg) \cdot \mathcal{N}(w\,|\,0, \alpha^{-1} I) \\
    & = \mathcal{N}\big( w\,|\, m_N = \beta S_N \Phi^T \mathbf{Y}, S_N = (\alpha I + \beta \Phi^T \Phi)^{-1} \big)
  \end{align*}
  which itself is a multivariate Gaussian. 
  </li>
</ul>
Knowing this, we know we should marginalize $p(w, \beta\,|\,\mathcal{D})$ so that the term $p(w\,|\, \alpha, \beta, \mathcal{D})$ exists. We can do this by 
\begin{align*} 
  p(w, \beta\,|\,\mathcal{D}) & = \int p(w, \beta\,|\,\alpha, \mathcal{D}) \, p(\alpha\,|\,\mathcal{D})\; d\alpha \\
  & = \int p(w\,|\,\beta, \alpha, \mathcal{D}) \, p(\beta \,|\, \alpha, \mathcal{D}) \, p(\alpha\,|\,\mathcal{D}) \; d\alpha \\
  & = \int p(w\,|\, \beta, \alpha, \mathcal{D}) \, p(\alpha, \beta\,|\,\mathcal{D}) \; d\alpha
\end{align*}
where in the second row we simply used the conditional probability rule $p(a, b\,|\,c) = p(a\,|\,b, c)\, p(b\,|\,c)$. Finally, substituting this into the double integral above gives 
\begin{align*} 
  p(y\,|\,x, \mathcal{D}) & = \iint p(y\,|\,x, w, \beta, \mathcal{D}) \, \bigg( \int p(w\,|\, \beta, \alpha, \mathcal{D}) \, p(\alpha, \beta\,|\,\mathcal{D}) \; d\alpha\bigg) \; dw\, d\beta \\
  & = \iiint p(y\,|\,x, w, \beta, \mathcal{D}) \, p(w\,|\, \beta, \alpha, \mathcal{D}) \, p(\alpha, \beta\,|\,\mathcal{D}) \; d\alpha \,dw\, d\beta \\
  & = \iiint \mathcal{N}\big(y\,|\, w^T \phi(x), \beta^{-1}\big)\,\bigg( \prod_{n=1}^N \mathcal{N}\big( y\,|\, w^T \phi(x), \beta^{-1} \big) \bigg) \cdot \mathcal{N}(w\,|\,0, \alpha^{-1} I)\, p(\alpha, \beta\,|\,\mathcal{D}) \; d\alpha \, dw \, d\beta 
\end{align*}
Our Gaussian assumption on the priors will greatly simplify this term when written in vector notation. Now, the only thing to do is figure out what $p(\alpha, \beta\,|\,\mathcal{D})$ is. We will use Bayes rule and assume that the prior $p(\alpha, \beta)$ is relatively flat. 
\begin{align*} 
  p(\alpha, \beta \,|\,\mathcal{D}) & \propto p( \mathcal{D}\,|\,\alpha, \beta) \, p(\alpha, \beta) \\
  & \propto p(\mathcal{D}\,|\,\alpha, \beta)
\end{align*}
where $p(\mathcal{D}\,|\,\alpha, \beta)$ is another evidence jfunction (like the model evidence), which we will call the <i>hyperparameter evidence</i>. We can simply condition over $w$ to get the following. Hopefully, the realizations of the probabilities into the densities function make sense to the reader. 
\begin{align*} 
  p(\alpha, \beta \,|\, \mathcal{D}) & \propto p(\mathcal{D}\,|\,\alpha, \beta) \\
  & = \int p(\mathcal{D}\,|\,w, \beta) \, p(w \,|\,\alpha)\; dw \\
  & = \int \bigg(\prod_{n=1}^N \mathcal{N} \big(y_n \,|\, w^T \phi(x_n), \beta^{-1} \big)\bigg) \cdot \mathcal{N}(w\,|\, 0, \alpha^{-1} I)\; dw
\end{align*} 
If we know that $p(\alpha, \beta \,|\,\mathcal{D})$ is sharply peaked, then we can try maximizing the evidence function with respect to $\alpha, \beta$ using maximum likelihood, and simply fixing them in further calculations. 
<br>
By substituting in the densities, the evidence function reduces to 
  \[p(\mathcal{D}\,|\,\alpha, \beta) = \bigg(\frac{\beta}{2 \pi}\bigg)^{N/2} \alpha^{M/2} \exp\big( -E (m_N)\big) \,|A|^{-1/2}\]
where 
\begin{align*} 
  \Phi & = (\Phi)_{nj} = \phi_j (x_n) \\
  S_N^{-1} & = \alpha I + \beta \Phi^T \Phi \\
  m_N & = \beta S_N \Phi^T \mathbf{Y} \\
  E(m_N) & = \frac{\beta}{2} ||\mathbf{Y} - \Phi m_N||^2 + \frac{\alpha}{2} m_N^T m_N
\end{align*}
Taking the log gives us 
\[\log{p(\mathcal{D}\,|\,\alpha, \beta)} = \frac{M}{2}\log{\alpha} + \frac{N}{2} \log{\beta} - E(m_N) - \frac{1}{2} \log{|S_N^{-1}|} - \frac{N}{2} \log{2 \pi}\]
This evidence can also be used as the model evidence. Remember that given data $\mathcal{D}$, a (linear) model $\mathcal{M}_i$ determines the collection of basis function, i.e. determines $\Phi$. Let us denote the $\Phi$ determined by model $\mathcal{M}_i$ as $\Phi_{\mathcal{M}_i}$. Therefore, we can treat $p(\mathcal{D}\,|\,\alpha, \beta)$ as a function of $\Phi$ and write
  \[p(\mathcal{D}\,|\,\alpha, \beta, \Phi)\]
To determine which model from $\{\mathcal{M}_i\}_{i=1}^L$ to choose, we first fix $\Phi_{\mathcal{M}_i}$ and maximize $p(\mathcal{D}\,|\,\alpha, \beta, \Phi_{\mathcal{M}_i})$ with respect to $\alpha, \beta$, for $i = 1, \ldots, L$. 
\begin{align*} 
  \text{Assume model } \mathcal{M}_1 & \implies \text{Find } \max_{\alpha, \beta} p(\mathcal{D}\,|\,\alpha, \beta, \Phi_{\mathcal{M}_1}) \\
  \text{Assume model } \mathcal{M}_2 & \implies \text{Find } \max_{\alpha, \beta} p(\mathcal{D}\,|\,\alpha, \beta, \Phi_{\mathcal{M}_2}) \\
  \ldots & \implies \ldots \\
  \text{Assume model } \mathcal{M}_L & \implies \text{Find } \max_{\alpha, \beta} p(\mathcal{D}\,|\,\alpha, \beta, \Phi_{\mathcal{M}_L})
\end{align*}
Then, find 
  \[\text{arg}\, \max_{\mathcal{M}_i} \{ \max_{\alpha, \beta} p(\mathcal{D}\,|\,\alpha, \beta, \Phi_{\mathcal{M}_i}) \}\]
For each model $\mathcal{M}_i$, we have optimized $\alpha, \beta$ to maximize the evidence function $p(\mathcal{D}\,|\,\alpha, \beta, \Phi_{\mathcal{M}_i})$. The model with the highest max evidence should be the best model, and by Occam's razor, we should choose simpler models if their predictive powers are equal. Again, we restate the big takeaway: <i>The $\Phi$ represents the model, and therefore maximizing the evidence function $p(\mathcal{D}\,|\, \alpha, \beta, \Phi_{\mathcal{M}_i})$ with respect to the $\Phi$ will tell us what the correct model is. </i> That is, 
<ol>
  <li>$p(\mathcal{D}\,|\, \alpha, \beta, \Phi_{\mathcal{M}_i})$ interpreted as a function of $\alpha, \beta$ is the <b>hyperparameter evidence</b>. </li>
  <li>$p(\mathcal{D}\,|\, \alpha, \beta, \Phi_{\mathcal{M}_i})$ interpreted as a function of $\Phi_{\mathcal{M}_i}$ (or more accurately, of $\mathcal{M}_i$) is the <b>model evidence</b>. </li>
</ol>
For more information on this process and how it is performed computationally, look <a href="http://krasserm.github.io/2019/02/23/bayesian-linear-regression/" target="_blank">here</a>. 
</div>

<div class="subsection_title">Equivalent Kernel</div>
<div>
The posterior mean solution $m_N = \beta S_N \Phi^T \mathbf{Y}$ is a point-estimate prediction of what $w$ is. We can substitute it into the linear equation $f(x, w) = w^T \phi(x)$ to get 
  \[f(x, m_N) = m_N^T \phi(x) = \beta \phi(x)^T S_N \Phi^T \mathbf{Y} = \sum_{n=1}^N \beta \phi(x)^T S_N \phi(x_n) y_n \]
which is a linear combination of the training set target variables $y_n$, written as 
  \[f(x, m_N) = \sum_{n=1}^N k(x, x_n) y_n, \;\;\;\;\; k(x, x_n) \equiv \beta \phi(x)^T S_N \phi(x_n)\]
That is, the mean of the predictive distribution at a point $x$ is given by a linear combination of the $y_n$'s. The function $k(x, x_n)$ is known as the <b>smoother matrix</b>, or <b>equivalent kernel</b>. 
</div>

</div>


<p id="Section4" class="section_title">Bias Variance Decomposition</p>
<a id="show_hide_4" class="show_hide" onclick="show_hide_4()">[Hide]</a>
<hr>
<div id="section_content_4">

Determination of the predictive distribution $p(y\,|\,x)$ given data $\mathcal{D}$ is the goal of statistical inference, as we have seen. That is, posterior $p(y\,|\,x, \mathcal{D})$ tells us the distribution of $y$ if we have a new data point $x$. But after this inference step, we must look now at the <b>decision step</b>: we must determine a function $h(x)$ that deterministically predicts a value $y$, without predictions. That is, we must have some algorithm to make a decision. 
<br>
Let us zoom out for a better overview. Let $\mathcal{D}$ be our training data of $N$ points. We can assume that each point $(x_i, y_i) \in \mathcal{D}$ was <i>generated</i> independently by a joint distribution $p(x, y)$!. If we were to get another data point, we would just generate one from the density $p(x, y)$. Usually, we have fixed input data $x$ and knew that the output $y$ given $x$ would be $p(y\,|\,x)$. But if we loosen our constraint on $x$, we would get 
\[p(x, y) = p(y\,|\,x) p(x)\]
which states that each data point in $\mathcal{D}$ is gotten by generating a value of $x$ with probability $p(x)$, and then generating a $y$ given this $x$. Let us also denote $\mathcal{A}$ as our machine learning algorithm, which we can interpret as a function that takes in data $\mathcal{D}$ and outputs the hypothesis function $h_\mathcal{D}$. 
  \[\mathcal{A} (\mathcal{D}) = h_\mathcal{D}\]
Then, given that the next new data point $(x, y)$ is generated, we can set our <b>test error</b>, or <b>loss/cost function</b>, of $h_\mathcal{D}$ to be 
\[L\big(h_\mathcal{D}, (x, y) \big) = \big[ h_\mathcal{D} (x) - y \big]^2\]
This loss function basically calculates the inaccuracy of whatever hypothesis function $h_\mathcal{D}$ we have on the data $(x, y)$, which in this case is the square of the residual. There can be other types of loss functions, but we wil consider the squares loss function for now. Given $h_\mathcal{D}$, we can also calculate the expected test error by conditioning over all $x, y$ drawn from $P$.  
  \[\text{Expected Test Error given } h_\mathcal{D} \implies \mathbb{E}_{x, y, \sim P} \big[ (h_\mathcal{D} (x) - y)^2 \big] = \int_x \int_y (h_\mathcal{D} (x) - y)^2 \, p(x, y)\; dy \, dx\]
However, note that we can treat the $N$ data points $\mathcal{D}$ also as a random variable coming from the joint distribution of $N$ $P$'s. Therefore, we can take each possible dataset $\mathcal{D}$, calculate $h_\mathcal{D} = \mathcal{A}(\mathcal{D})$ with our algorithm, and average them out to get the expected hypothesis function $\overline{h}$. We can interpret $\overline{h}$ as the "ideal regressor" that we are trying to build, but with limited data $\mathcal{D}$, we can only build $h_\mathcal{D}$ that deviates from $\overline{h}$. 
  \[\overline{h} = \mathbb{E}_{\mathcal{D} \sim P^N} \big[ \mathcal{A}(\mathcal{D}) \big] = \int_\mathcal{D} h_\mathcal{D} P(\mathcal{D})\; d\mathcal{D}\]
So, we can take compute the expected error of the <i>entire algorithm</i> $\mathcal{A}$ by marginalizing over all $x, y$ given $h_\mathcal{D}$ and marginalizing over all $\mathcal{D}$. Remember that $D \sim P^N$ is our training data of $N$ points, and $(x, y) \sim P$ is our $(n+1)$th data point. Therefore, the expected test error of our <i>algorithm</i> for the $(n+1)$th data point is
  \[\mathbb{E}_{(x, y) \sim P, \mathcal{D} \sim P^N} \big( [ h_\mathcal{D} (x) - y]^2 \big) = \int_\mathcal{D} \int_x \int_y [ h_\mathcal{D} (x) - y]^2\, p(x, y) \, p(\mathcal{D})\; dy\,dx\,d\mathcal{D}\]
The integral above looks quite intimidating, so let us decompose it. We just have do use a trick where we subtract and add the same term $\overline{h}(x)$.
\begin{align*} 
  \mathbb{E}_{(x, y), \mathcal{D}} \big( [ h_\mathcal{D} (x) - y]^2 \big) & = \mathbb{E}_{(x, y), \mathcal{D}} \big( [ (h_\mathcal{D} (x) - \overline{h}(x)) + (\overline{h}(x) - y)]^2 \big) \\
  & = \mathbb{E}_{(x, y), \mathcal{D}} \big( [h_\mathcal{D} (x) - \overline{h} (x)]^2 \big) + 
  \mathbb{E}_{(x, y), \mathcal{D}} \big( [\overline{h} (x) - y]^2 \big) + 
  2 \mathbb{E}_{(x, y), \mathcal{D}} \big([h_\mathcal{D} (x) - \overline{h} (x)]\,[\overline{h} (x) - y] \big) 
\end{align*}
But I claim that the last term vanishes. It is easy to see why because 
\begin{align*}
	\mathbb{E}_{(x, y), \mathcal{D}} \left[\left(h_{\mathcal{D}}(x) - \bar{h}(x)\right) \left(\bar{h}(x) - y\right)\right] &= \mathbb{E}_{(x, y)} \left[E_{\mathcal{D}} \left[ h_{\mathcal{D}}(x) - \bar{h}(x)\right] \left(\bar{h}(x) - y\right) \right] \\
    &= \mathbb{E}_{(x, y)} \left[ \left( E_{\mathcal{D}} \left[ h_{\mathcal{D}}(x) \right] - \bar{h}(x) \right) \left(\bar{h}(x) - y \right)\right] \\
    &= \mathbb{E}_{(x, y)} \left[ \left(\bar{h}(x) - \bar{h}(x) \right) \left(\bar{h}(x) - y \right)\right] \\
    &= \mathbb{E}_{(x, y)} \left[ 0 \right] \\
    &= 0
\end{align*}
Therefore, we can see that the expected value of the error of an algorithm consists of two terms: the variance and and the second term.  
  \[\mathbb{E}_{(x, y), \mathcal{\mathcal{D}}} \big( [ h_\mathcal{\mathcal{D}} (x) - y]^2 \big) = \mathbb{E}_{(x, y), \mathcal{\mathcal{D}}} \big( [h_\mathcal{\mathcal{D}} (x) - \overline{h} (x)]^2 \big) + 
  \mathbb{E}_{(x, y), \mathcal{\mathcal{D}}} \big( [\overline{h} (x) - y]^2 \big)\]
The second term is the expected value of the average prediction minus the $y$-value of the new point. Now, we do the same trick: Let the expected value of $y$ given $x$ be $\overline{y}(x) = \mathbb{E}_{y\,|\,x} (y) = \int y\, p(y\,|\,x)\; dx$. This function $\overline{y}(x)$ is the ideal regressor predicting $y$ from $x$. Then, we have 
\begin{align}
	\mathbb{E}_{x, y} \left[ \left(\bar{h}(x) - y \right)^{2}\right] &= \mathbb{E}_{x, y} \left[ \left(\bar{h}(x) -\bar y(x) )+(\bar y(x) - y \right)^{2}\right]  \\
  &=\underbrace{\mathbb{E}_{x, y} \left[\left(\bar{y}(x) - y\right)^{2}\right]}_\mathrm{Noise} + \underbrace{\mathbb{E}_{x} \left[\left(\bar{h}(x) - \bar{y}(x)\right)^{2}\right]}_\mathrm{Bias^2} + 2 \mathrm{\;} \mathbb{E}_{x, y} \left[ \left(\bar{h}(x) - \bar{y}(x)\right)\left(\bar{y}(x) - y\right)\right]
\end{align}
where the third term vanishes since 
\begin{align*}
	\mathbb{E}_{x, y} \left[\left(\bar{h}(x) - \bar{y}(x)\right)\left(\bar{y}(x) - y\right)\right] &= \mathbb{E}_{x}\left[\mathbb{E}_{y \mid x} \left[\bar{y}(x) - y \right] \left(\bar{h}(x) - \bar{y}(x) \right) \right] \\
    &= \mathbb{E}_{x} \left[ \left( \bar{y}(x) - \mathbb{E}_{y \mid x} \left [ y \right]\right) \left(\bar{h}(x) - \bar{y}(x)\right)\right] \\
    &= \mathbb{E}_{x} \left[ \left( \bar{y}(x) - \bar{y}(x) \right) \left(\bar{h}(x) - \bar{y}(x)\right)\right] \\
    &= \mathbb{E}_{x} \left[ 0 \right] \\
    &= 0
\end{align*}
Therefore, the expected test error is precisely the sum of three things. 
\begin{equation*}
	\underbrace{\mathbb{E}_{x, y, \mathcal{D}} \left[\left(h_{\mathcal{D}}(x) - y\right)^{2}\right]}_\mathrm{Expected\;Test\;Error} = \underbrace{\mathbb{E}_{x, \mathcal{D}}\left[\left(h_{\mathcal{D}}(x) - \bar{h}(x)\right)^{2}\right]}_\mathrm{Variance} + \underbrace{\mathbb{E}_{x, y}\left[\left(\bar{y}(x) - y\right)^{2}\right]}_\mathrm{Noise} + \underbrace{\mathbb{E}_{x}\left[\left(\bar{h}(x) - \bar{y}(x)\right)^{2}\right]}_\mathrm{Bias^2}
\end{equation*}
To understand this term a bit deeper, recall the following: The function $\overline{y}(x)$, which outputs the expected value of $y$ given $x$, is the best possible regressor we can have. There are many different algorithms that we can choose to approximate $\overline{y}(x)$, so let us choose one learning algorithm $\mathcal{A}$. We just feed an arbitrary dataset $\mathcal{D}$ to $\mathcal{A}$, which outputs a hypothesis function $h_\mathcal{D}$. But this hypothesis function $h_\mathcal{D}$ is really just an approximation of the <i>ideal</i> hypothesis function $\overline{h}$, which is the expectation of all hypotheses $h_\mathcal{D}$ (i.e. the hypothesis that $\mathcal{A}$ should generate when we feed it an infinite amount of data). So, by feeding $\mathcal{D}$ to $\mathcal{A}$, it generates a hypothesis function $h_\mathcal{D}(x)$, which approximates $\overline{h}(x)$, which hopefully is a good estimate of $\overline{y}(x)$. 
<ol>
  <li>The difference between a generated hypothesis function $h_\mathcal{D}(x)$ and the ideal hypothesis that it is trying to estimate according to learning algorithm $\mathcal{A}$ is represented by the variance. The variance term tells us how far each generated hypothesis $h_\mathcal{D}$ deviates from the ideal $\overline{h}$. </li>
  <li>The difference between the ideal hypothesis $\overline{h}(x)$ (according to algorithm $\mathcal{A}$) and the ideal regressor <i>in general</i> $\overline{y}(x)$ is captured in the bias term. The bias term tells us how far our algorithm's ideal hypothesis deviates from the expectation of the conditional $p(y\,|\,x)$. </li>
  <li>The noise term represents the difference between the true value of $y$ and the best possible regressor $\overline{y}(x)$. But since the best we can do is find the expectation of the conditional $p(y\,|\,x)$, the deviation of the true values $y$ from the mean $\overline{y}$ is simply the noise. For example, if we have $p(y\,|\,x) = \mathcal{N} \big( y\,|\, w^T\phi(x), \epsilon \big)$, then the noise would simply be $\epsilon$. If the variance of $\epsilon$ is large, the noise would be large. Therefore, the same ideal regressor function $\overline{y}(x)$ would perform worse with a higher noise. </li>
</ol>
If we are comparing this to the throwing-darts analogy, we can imagine the ideal function $\overline{y}(x)$ to be the bull's eye that we must hit. The different algorithms $\mathcal{A}$ represent different players throwing the darts. When one algorithm (player) is chosen, their vision can be skewed (perhaps their glasses is off), leading them to think that the target $\overline{h}(x)$ is somewhere else. If their target is far away from the bull's eye (i.e. $[\overline{h}(x) - \overline{y}(x)]^2$ is high), then their bias is high. Their skills in darts may just be bad, so even if their vision is good and they have a good sense of where to hit (low bias), for each time they throw the dart (i.e. each time the regressor function $h_\mathcal{D}$ is generated from data), it may be very off from their ideal target $\overline{h}(x)$. 
<br>
Therefore, if you are a data scientist and you find that your regression function is not accurate enough, it is your job to find out whether your bias is too high, your variance is too high, or whether there is too much noise, and fix the proper component. Generally, we would try to minimize this cost function, visualized below. 
  <img src="Bayesian_Pictures/biasvariance.png" width="400px" style="max-width: 90%;">
</div>


<p id="Section5" class="section_title">Markov Chain Monte Carlo (MCMC)</p>
<a id="show_hide_5" class="show_hide" onclick="show_hide_5()">[Hide]</a>
<hr>
<div id="section_content_5">
Monte carlo algorithms is a general term for computational techniques that use random numbers, which can be used both in classical and Bayesian statistics. This is extremely important when working with distributions that are cannot be simply stated using elementary densities (Gaussian, Beta, etc.). The entire goal of Bayesian inference is to maintain a full posterior probability distribution over a set of random variables. However, maintaining and using this distribution requires computing integrals which, for most non-trivial models, is intractable. 
<br>
The basic idea of MCMC is that we want to construct a Markov chain which will travel between different possible states (e.g. the hypotheses/parameter values in a Bayesian analysis), where the amount of time spent in any particular state is proportional to the posterior probability of the state. That is, the stationary distribution of the chain is the posterior distribution. As a result, the computer explores the set of possible parameter values, spending a lot of time in the regions with high posterior probability, and only rarely visiting regions of low posterior probability. 

<div class="subsection_title">Metropolis-Hastings: General Algorithm</div>
Say that with initial distribution $p(\theta)$, we have calculated the posterior as 
  \[p(\theta\,|\,x) \propto p(\theta) \; p(x\,|\,\theta)\]
It is often the case that the set of possible values of $\theta$ is very large, so it is computationally inefficient to compute the normalizing factor 
  \[p(x) = \sum_\theta p(\theta) \; p(x\,|\,\theta)\]
Therefore, we only have this function $f(\theta) = p(\theta) \; p(x\,|\,\theta)$ that is directly proportional to $p(\theta\,|\,x)$. That is, we don't know the normalizing constant $c$ such that 
  \[p(\theta\,|\,x) = \frac{f(\theta)}{c}\]
Using this information, we wish to construct and run an algorithm that converges onto the true posterior distribution $p(\theta\,|\,x)$ at a sufficiently fast rate. 
<br>
We begin by constructing a discrete-time irreducible Markov chain with state space $\mathcal{S} = \{1, 2, \ldots, N\}$ representing the set of possible parameter values (the labels for the elements of $\mathcal{S}$ does not matter, since we can construct whatever bijection we want from the actual states to a subset of $\mathbb{N}$). Like a normal Markov chain, we will choose the next state to go to at each step, <i>but now, we will then choose to accept this proposal to go to that step with an additional probability</i>. That is, we will construct two matrices: 
<ul>
  <li>An $|\mathcal{S}| \times |\mathcal{S}|$ <b>proposal transition matrix</b> $Q_{prop}$, with 
    \[p(\text{propose } i \mapsto j) = (Q_{prop})_{ij} = q_{prop} (i, j)\]
  being the probability of getting a <i>proposal</i> to transition from state $i$ to state $j$. This matrix is constructed by the user and is completely well-defined and known; this choice may also affect the convergence rate. Note that with this formulation, the rows will sum up to $1$ and $Q^T$ is a stochastic matrix. We can also construct $Q_{prop}$ to be symmetric, that is $q_{prop}(i, j) = q_{prop} (j, i)$, for easier calculations. </li>
  <li>An $|\mathcal{S}| \times |\mathcal{S}|$ <b>acceptance probability matrix</b> $A$, with 
    \begin{align*} 
      (\text{accept proposal }i \mapsto j\,|\, \text{propose } i \mapsto j) & = (A)_{ij} = \alpha(i, j) \\
      & = \min \bigg(1, \frac{p(\theta = j \,|\, x)\; q_{prop}(j, i)}{p(\theta = i\,|\, x) \; q_{prop}(i, j)} \bigg) \\
      & = \min\bigg(1, \frac{f(\theta = j) \; q_{prop}(j, i)}{f(\theta = i) \; q_{prop}(i, j)} \bigg) \\
      & = \min\bigg(1, \frac{f(\theta = j)}{f(\theta = i)} \bigg) \;\;\;\;\;\;\; (\text{if we constructed } Q_{prop} \text{ to be symmetric})
    \end{align*}</li>
</ul>
Then, we element-wise multiply the two matries, except the diagonals, to get the <b>true transition matrix</b> $Q$ defined 
  \[(Q)_{ij} = q(i, j) = \begin{cases} 
  q_{prop} (i, j) \cdot \alpha (i, j) = q_{prop}(i, j) \cdot \min\bigg(1, \frac{f(\theta = j) \; q(j, i)}{f(\theta = i) \; q(i, j)} \bigg) & \text{ if } i \neq j \\
  1 - \sum_{j \neq i} q(i, j) & \text{ if } i = j
  \end{cases}\] 
where $q(i, j)$ represents the <b>true transition probability</b> of going from state $i$ to state $j$. Note that we have element-wise multiplied every non-diagonal element, and we have defined $(Q)_ii$ such that the sum of each row is $1$ (so that this becomes a viable transition matrix). Note also that this element-wise multiplication makes sense because 
\begin{align*} 
  p(\theta_{k+1} = j\,|\, \theta_k = i) & = p(\text{accept proposal }i \mapsto j, \; \text{propose } i \mapsto j) \\
  & = p(\text{accept proposal }i \mapsto j\,|\, \text{propose } i \mapsto j)\; p(\text{propose } i \mapsto j) \\
  & = \alpha(i, j) \cdot q_{prop} (i, j)
\end{align*}
This is the Markov chain we wish to get, where "one" step is really a two-step process of proposing and accepting/rejecting. We wish to get the steady state distribution $\pi(\theta)$ of this chain, which can be found in two well-known ways: 
<ul>
  <li>Calculate the left-eigenvector of $Q$ with eigenvalue $1$. </li>
  <li>Randomly initalize $\theta_0$ and run the chain for a sufficiently long time to record where it lands at each step 
    \[\theta_0 = i_0, \theta_1 = i_1, \theta_2 = i_2, \theta_3 = i_3, \ldots, \theta_n = i_n \]
  which can be used to approximate $\pi(\theta)$ by defining 
    \[\pi(\theta = i) = \frac{\text{proportion of states in state } i \text{ in the n-step process}}{n}\]
  </li>
</ul>
Finally, we claim that this steady state distribution $\pi(\theta)$ is precisely the posterior we are looking for: $p(\theta\,|\,x)$.  

<div class="subsection_title">Detailed Balance: Justification of the Metropolis Algorithm</div>
But why does $\pi(\theta) = p(\theta\,|\,x)$? Given a Markov chain $\theta_0$ with transition matrix $Q$, the chain is said to satisfy <b>detailed balance</b> with respect to a distribution $\pi(\theta)$ if 
  \[\pi(\theta = i) q(i, j) = \pi(\theta = j) q(j, i)\]
for all $i, j \in \mathcal{S}$. In fact, we claim that $\theta_i$ does satisfy detailed balance with respect to $p(\theta\,|\,x)$. That is, it satisfies
  \[p(\theta = i\,|\,x) q(i, j) = p(\theta = j\,|\,x) q(j, i)\]
This case is trivial for when $i=j$, so assume $i \neq j$. A transition from $i$ to a different $j$ can only be achieved with an accepted proposed step, which happens with probability 
\begin{align*} 
  q(i, j) & = q_{prop} (i, j) \cdot \alpha(i, j) \\
  & = q_{prop} (i, j) \cdot \min\bigg( 1, \frac{p(\theta = j\,|\,x)\; q_{prop} (j, i)}{p(\theta = i\,|\,x) \; q_{prop}(i, j)}\bigg) \\
  & = \frac{q_{prop} (i, j)}{p(\theta = i\,|\,x)} \min\big( p(\theta = i\,|\,x), \frac{p(\theta = j\,|\,x)\; q_{prop} (j, i)}{q_{prop}(i, j)} \big) \\
  & = \frac{1}{p(\theta = i\,|\,x)} \min \big( p(\theta = i\,|\,x)\; q_{prop} (i, j), p(\theta = j\,|\,x) \; q_{prop} (j, i) \big)
\end{align*}
Applying the same method from transitioning from $j$ to $i$ gives the same equation, but with the $i$ and $j$'s switched. 
\[q(j, i) = \frac{1}{p(\theta = j\,|\,x)} \min \big( p(\theta = j\,|\,x)\; q_{prop} (j, i), p(\theta = i\,|\,x) \; q_{prop} (i, j) \big)\]
But switching the $i$ and $j$ leaves the terms inside the minimum invariant. Therefore, we can see that 
  \[p(\theta = i\,|\,x)\; q(i, j) = \min \big( p(\theta = j\,|\,x)\; q_{prop} (j, i), p(\theta = i\,|\,x) \; q_{prop} (i, j) \big) = p(\theta = j\,|\,x)\; q(j, i)\]
proving detailed balance. Now, we can sum the left hand side of the detailed balance equation over $i$ to get 
  \[\sum_i p(\theta = i\,|\,x) q(i, j) = \sum_i p(\theta = j\,|\,x) q(j, i) = p(\theta = j\,|\,x) \sum_i q(j, i) = p(\theta = j\,|\,x)\]
which in matrix form, says 
  \[p(\theta\,|\,x) Q = p(\theta\,|\,x)\]
where $p(\theta\,|\,x) = \big( p(\theta=1\,|\,x) \ldots p(\theta=N\,|\,x)\big)$ and $Q_{ij} = q(i, j)$. This implies that $p(\theta\,|\,x)$ is a stationary distribution, and therefore, computing the stationary distribution is equivalent to computing $p(\theta\,|\,x)$. 
<br><br>
A summary of the justification of the Metropolis algorithm: 
Given a function $f(\theta)$ that was proportional to some unknown posterior $p(\theta\,|\,x)$, we have cleverly constructed a certain Markov chain $\theta_i$ using $f$, proved that the distribution $p(\theta\,|\,x)$ satisfies detailed balance in this chain, then proved that $p(\theta\,|\,x)$ is the stationary distribution $\pi(\theta)$. But stationary distributions are always unique (in an irreducible Markov chain), so we can say that $p(\theta\,|\,x)$ is the one and only stationary distribution of this chain $\theta_i$. 
\[\text{a distribution is } \pi(\theta) \iff \text{a distribution is } p(\theta\,|\,x)\]
So, since $\pi(\theta) = p(\theta\,|\,x)$, we can now sample from $\pi(\theta)$ (or the Markov chain itself), which would be equivalent to sampling from the initially hard-to-sample posterior $p(\theta\,|\,x)$ itself. 
<br>
The intuition behind detailed balance is quite easy to understand, too. Suppose we start a chain in the stationary distribution, so that the respective probabilities $\theta_0 \sim \pi(\theta)$ of starting at position are "smeared" across all states $i$. Then, the quantity $\pi(\theta = i) q (i, j)$ represents the "amount" of probability that flows down edge $i \rightarrow j$ in one time step. If detailed balance holds, then the amount of probability flowing from $i \rightarrow j$ equals the amount that flows from $j \rightarrow i$ (which is $\pi(\theta = j) q(j, i)$). Therefore, there is no <i>net</i> flux of probability along the edge $i \leftrightarrow j$ during one time step (remember this holds only for when the chain is in the stationary distribution). 
  <img src="Bayesian_Pictures/Detailed_Balance.jpg" width="450px" style="max-width: 90%;">

<div class="subsection_title">Metropolis-Hastings: Example</div>
Suppose we want a Markov chain of state space $\mathcal{S} = \{1, 2\}$ with the steady state distribution 
\[\pi = \begin{pmatrix} \frac{3}{4} & \frac{1}{4} \end{pmatrix} \iff \pi(\theta = 1) = \frac{3}{4}, \; \pi(\theta = 2) = \frac{1}{4}\]
To implement the Metropolis-Hastings algorithm, we calculate the proposal matrix and acceptance matrix
\[Q_{prop} = \begin{pmatrix} \frac{1}{2} & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{2} \end{pmatrix} \text{ and } A = \begin{pmatrix} 1 & \frac{1}{3} \\ 1 & 1 \end{pmatrix}\]
which is calculated since $\alpha(1, 2) = \min\big(1, \frac{1/4}{3/4} \big) = 1/3$ and $\alpha(2, 1) = \min \big(1, \frac{3/4}{1/4} \big) = 1$. We multiple the nondiagonal entries together and fill in the diagonals to get 
  \[\begin{pmatrix} & \frac{1}{2} \cdot \frac{1}{3} \\ \frac{1}{2} \cdot 1 & \end{pmatrix} \implies Q = \begin{pmatrix} \frac{5}{6} & \frac{1}{6} \\ \frac{1}{2} & \frac{1}{2} \end{pmatrix}\]
Which can be visualized as an object jumping between two nodes with the following transitions. 
  <img src="Bayesian_Pictures/2_state_chain.jpg" width="400" style="max-width: 90%;">

<div class="subsection_title">Monte Carlo Integration</div>



<div class="subsection_title">Gibbs Sampling: General Algorithm</div>
Gibbs Sampling is a special case of the Metropolis-Hastings in which the newly proposed state is accepted with probability one. With observed data $x$, say that we have calculated the $D$-dimensional posterior 
\[p(\theta\,|\,x) \propto f(\theta) = p(\theta) \; p(x\,|\, \theta)\]
where the parameter $\theta = (\theta^1, \ldots, \theta^D)$ is an element of the $D$-dimensional state space $\mathcal{S} = \{1, \ldots, n\}^D$ (actually, each $\theta^i$ does not need to be derived from the same $\{1, \ldots, n\}$ and we can generalize this algorithm to account for this). Remember that 
<ul>
  <li>It is hard to calculate $p(\theta\,|\,x) = p(\theta^1, \ldots, \theta^D\,|\,x)$ because calculating the constant $c$ that normalizes $f(\theta)$ is hard (since $D$ may be large). This makes it difficult to sample from the posterior. </li>
  <li>It is easy to calculate $f(\theta) = f(\theta^1, \ldots, \theta^D)$. We just don't know how to scale the individual values appropriately and so this function it useless in of itself, even though it is directly proportional to $p(\theta\,|\,x)$. </li>
</ul>
<img src="Bayesian_Pictures/unknown_posterior_vs_known_f.jpg" width="550px" style="max-width: 90%;">
With the $D$-dimensional state space $\mathcal{S}$, we construct the true transition matrix. Say that the $i$th state of the chain is located at node $\theta_i$ with given coordinates 
  \[\theta_i = (\theta_i^1, \theta_i^2, \ldots, \theta_i^D)\]
The step to transition from this given $\theta_i$ to the next $\theta_{i+1}$ consists of two parts: 
<ol>
  <li>Pick a component index $j=d \in \{1, 2, \ldots, D\}$ uniformly at random. Many algorithms also pick $d=1$ for the first step, $d=2$ for the second, and so on. 
    \[p(\text{Index }d \text{ chosen}) = \frac{1}{D}\]
  <img src="Bayesian_Pictures/choose_index.jpg" width="700px" style="max-width: 90%;">
  </li>
  <li>With this well-defined $d$, we would like to update the Markov chain from state $\theta_i$ to $\theta_{i+1}$ by updating only the $d$th component of $\theta_i$, and keeping every component fixed. When $\theta_i^d$ is updated, the new $\theta_{i+1}^d$ must take some value of $k^* \in \{1, \ldots, n\}$. As expected, it chooses which value $k^*$ to update to according to the marginal distribution of $p(\theta\,|\,x)$ given $\theta_i^1, \ldots, \theta_i^{d-1}, \theta_i^{d+1}, \ldots, \theta_i^D$. 
  \begin{align*} 
    p(\theta_i^d \mapsto \theta_{i+1}^d = k^*\,|\, \text{Index } d \text{ chosen}) & = p(\theta_{i+1}^d = k^* \,|\,\theta_i^1, \ldots, \theta_i^{d-1}, \theta_i^{d+1}, \ldots, \theta_i^D, x) \\
    & = \frac{p(\theta_i^1, \ldots, \theta_i^{d-1}, k^*, \theta_i^{d+1}, \ldots, \theta_i^D\,|\,x)}{\sum_{k=1}^n p(\theta_i^1, \ldots, \theta_i^{d-1}, k, \theta_i^{d+1}, \ldots, \theta_i^D\,|\,x)} \\
    & = \frac{f(\theta_i^1, \ldots, \theta_i^{d-1}, k^*, \theta_i^{d+1}, \ldots, \theta_i^D)}{\sum_{k=1}^n f(\theta_i^1, \ldots, \theta_i^{d-1}, k, \theta_i^{d+1}, \ldots, \theta_i^D)} 
  \end{align*}
  where the last step is justified by the proportionality of $f$ and $p$. It turns out that the probability of where $\theta_{i+1}^d$ will land on does not actually depend on where $\theta_{i}^d$ is currently. A simple example for $D=2, n=5$ is shown below. In here, we have chosen $d=1$ in step one, which leaves us with $5$ possible states (within the red line) that $\theta_{i+1}$ can go to. 
    <img src="Bayesian_Pictures/example_gibbs_1.jpg" width="550px" style="max-width: 90%;">
  Do not be daunted by the notation. Just remember that $p(\theta_{i+1}^d = k^* \,|\,\theta_i^1, \ldots, \theta_i^{d-1}, \theta_i^{d+1}, \ldots, \theta_i^D, x)$ is just the conditional probability of $p(\theta\,|\,x)$ given that every $\theta_i^j, j \neq d$ are constant. This is easily visualized by taking the 1-dimensional cross section of the density $p(\theta\,|\,x)$ defined on $\mathcal{S}$. 
    <img src="Bayesian_Pictures/Gibbs_step_2.jpg" width="700px" style="max-width: 90%;">
  </li>
</ol>
Therefore, we can construct a Markov chain with the following transition probabilities. Given two states $\theta_r, \theta_s \in \mathcal{S}$, if $\theta_s$ differs in $\theta_r$ in at most one component, call it the $d$th component (i.e. $\theta_r^j = \theta_s^j$ for all $j \neq d$), then the probability of transition from $\theta_r$ to $\theta_s$ is 
\begin{align*}
p(\theta_r, \theta_s) & = p(\theta_r^d \mapsto \theta_{s}^d\,|\, \text{Index } d \text{ chosen})\; p (\text{Index } d \text{ chosen}) \\
& = \frac{f(\theta_r^1, \ldots, \theta_r^{d-1}, \theta_s^d, \theta_r^{d+1}, \ldots, \theta_r^D)}{\sum_{k=1}^n f(\theta_r^1, \ldots, \theta_r^{d-1}, k, \theta_r^{d+1}, \ldots, \theta_r^D)} \cdot \frac{1}{D}
\end{align*}
Therefore, given that the chain is in state $\theta_i = (3, 2)$ in state space $\mathcal{S} = \{1, 2, 3, 4, 5\}^2$, it may be able to get to the point in red or blue, depending on which index $d$ was chosen. But it is impossible to go to any of the yellow states, so the transition probabilities are all $0$. 
<img src="Bayesian_Pictures/prob_0.jpg" width="450px" style="max-width: 90%;">
With this, we can calculate the stationary distribution by either: 
<ul>
  <li>Calculating the left-eigenvector of the transition matrix defined $p(\theta_r, \theta_s)$ with eigenvalue $1$. </li>
  <li>Randomly initialize $\theta_0 = (\theta_0^1, \ldots, \theta_0^D)$ and run the chain for sufficiently long time to find out the proportion of steps in which a Markov chain lands on each $\theta \in \mathcal{S}$. </li>
</ul>
Now, it is easy to see why Gibbs sampling is a special case of Metropolis-Hastings. The Gibbs transition algorithm that we just mentioned is clearly a Markov chain, and within the context of Metropolis, we can interpret it as the proposal transition matrix with acceptance probability 1. By the same justification for Metropolis, we can prove that the stationary distribution of Gibbs sampling is $p(\theta\,|\,x)$. 
</div>



<div style="display: none;">
<p id="" class="section_title">Bayesian Inference</p>
<a id="" class="show_hide" onclick="show_hide_2()">[Hide]</a>
<hr>
<div id="">

<div class="subsection_title">Bayes' Rule</div>
We have seen that Bayesian statistics depends on having some initial belief about an event. Upon some observation, we can gain some sort of information about the event, allowing us to <i>modify</i> our prior distribution to a new one, called the posterior distribution. This simple property is the reason why Bayesian statistics is so useful for machine learning. The way we do this is through <b>Bayes' Rule</b>, which states 
  \[p(H\,|\,D) = \frac{p(D\,|\,H) \; p(H)}{p(D)}\]
Note that
<ol>
  <li>$H$ is the <b>hypothesis</b> whose probability may be affected by <b>data</b> $D$, also called <b>evidence</b>. </li>
  <li>$p(H)$ is the <b>prior distribution</b>, our initial hypothesis of what the distribution would have been. </li>
  <li>$p(H\,|\,D)$ is the <b>posterior distribution</b>, which was determined upon observing the event $B$. </li>
  <li>$p(D\,|\,H)$ is the <b>likelihood</b>. If you were to assume that $A$ is true, then the likelihood tells you the probability of getting result $B$. </li>
  <li>$p(D)$ is the <b>marginal likelihood</b>, which is calculated by conditioning on $A$
    \[p(D) = \sum_H p(D\,|\,H)\; p(H) \text{ or } p(D) \int_H p(D\,|\,H)\; p(H) \, dH\]</li>
</ol>
When computing our prior, the outcomes $H$ are the <b>hypotheses</b>. We can assume that hypotheses are mutually exclusive and exhaustive (if one of these is true, it can't be some undefined third option). These assumptions are reasonable since it is almost always possible to redefine an arbitrary set of hypotheses into a set of hypotheses that <i>are</i> mutually exclusive and exhaustive. 
<br>
There are multiple ways to write Bayes rule. When attempting to calculate the posterior, we can see that $p(D)$ is really just a normalization constant and therefore does not affect the type of distribution the posterior is. So, we can in effect write the above as 
  \[p(H\,|\,D) \propto p(D\,|\,H)\; p(H)\]
or 
  \[\text{Posterior } \propto \text{ Prior } \times \text{ Likelihood}\]
where the $\propto$ symbol means "proportional to." We use this notation more often when calculating posteriors since the normalizing constant isn't as important as finding the shape of the posterior density. 

<div class="subsection_title">Bayes Box: Computing Simple Discrete Posterior</div>
If we would like to compute the entire posterior distribution of a discrete distribution, then we can use a <b>Bayes box</b>. Given that we have hypotheses $H_i$ with their respective prior probabilities $p(H_i)$ and an observation $D$, we can list all the hypotheses out in a column and fill out the box as such: 
<table>
  <tr>
    <th>Hypotheses<br>$H_i$</th>
    <th>Prior<br>$p(H_i)$</th>
    <th>Likelihood<br>$p(D\,|\,H_i)$</th>
    <th>Prior $\times$ Likelihood<br>$p(H_i) \times p(D\,|\,H_i)$</th>
    <th>Posterior<br>$p(H_i\,|\,D)$</th>
  </tr>
  <tr>
    <td>$H_1$</td>
    <td>$p(H_1)$</td>
    <td>$p(D\,|\,H_1)$</td>
    <td>$p(H_1) \times p(D\,|\,H_1)$</td>
    <td>$p(H_1\,|\,D)$</td>
  </tr>
  <tr>
    <td>$H_2$</td>
    <td>$p(H_2)$</td>
    <td>$p(D\,|\,H_2)$</td>
    <td>$p(H_2) \times p(D\,|\,H_2)$</td>
    <td>$p(H_2\,|\,D)$</td>
  </tr>
  <tr>
    <td>$H_3$</td>
    <td>$p(H_3)$</td>
    <td>$p(D\,|\,H_3)$</td>
    <td>$p(H_3) \times p(D\,|\,H_3)$</td>
    <td>$p(H_3\,|\,D)$</td>
  </tr>
  <tr>
    <td>...</td>
    <td>...</td>
    <td>...</td>
    <td>...</td>
    <td>...</td>
  </tr>
  <tr>
    <td>Totals</td>
    <td>$1$</td>
    <td></td>
    <td>$p(D)$</td>
    <td>$1$</td>
  </tr>
</table>
Suppose that we have a box with two balls inside. We know that the there are either two black balls (BB) or a black and white ball (BW) in the box. Therefore, the probability distribution is: 
\begin{align*} 
  p(BB) & = 0.5 \\
  p(BW) & = 0.5
\end{align*}
If we reach into the box and take out a black ball (B), then what is our posterior distribution? We can calculate 
\begin{align*} 
  p(BB\,|\,B) & = \frac{p(B\,|\,BB)\; p(BB)}{p(B)} \\
  & = \frac{p(B\,|\,BB)\; p(BB)}{p(B\,|\,BB)\; p(BB) + p(B\,|\, BW)\; p(BW)} \\
  & = \frac{1 \cdot 0.5}{1 \cdot 0.5 + 0.5 \cdot 0.5} = \frac{2}{3} \\
  p(BW\,|\,B) & = \frac{p(B\,|\,BW)\; p(BW)}{p(B)} \\
  & = \frac{p(B\,|\,BW)\; p(BW)}{p(B\,|\,BB)\; p(BB) + p(B\,|\, BW)\; p(BW)} \\
  & = \frac{0.5 \cdot 0.5}{1 \cdot 0.5 + 0.5 \cdot 0.5} = \frac{1}{3} \\
\end{align*}
and therefore, our posterior distribution is $p(BB) = 0.67, p(BW) = 0.33$. This makes sense, because taking out a black ball would suggest that it is more probable that there are two black balls. Note also that due to our assumption that the hypotheses are exhaustive, choosing a white ball will produce a posterior with $p(BW) = 1$ and $p(WW)$ still equal to $0$. That is, we do not expect some other unaccounted hypothesis to change from $0$ to nonzero probability. 
<br>
Let's move onto another problem. You move into a new house which has a phone installed. You can't remember the phone number, but you suspect it might be 555-3226. To test this hypothesis, you carry out an experiment by picking up the phone and dialing 555-3226. If you are correct, you will definitely hear a busy signal because you are calling youself. If you are incorrect, the probability of hearing a busy signal is $1/100$. However, all of that is true if you are assuming that the phone is working, and it might be broken. If broken, the phone will always give a busy signal. Say that you call and the outcome (the observation/data) is getting the busy signal. Therefore, we can make a set of (mutually exclusive and exhaustive) hypotheses as such, with prior probabilities: 
\begin{align*} 
  p(H_1) & = p(\text{Working, Correct}) = 0.4 \\
  p(H_2) & = p(\text{Working, Incorrect}) = 0.4 \\
  p(H_3) & = p(\text{Broken, Correct}) = 0.1 \\
  p(H_4) & = p(\text{Broken, Incorrect}) = 0.1
\end{align*}
with $D = $ Observing a busy signal upon calling. To save space, we will use a Bayes box: 
<table>
  <tr>
    <th>Hypotheses<br>$H_i$</th>
    <th>Prior<br>$p(H_i)$</th>
    <th>Likelihood<br>$p(D\,|\,H_i)$</th>
    <th>Prior $\times$ Likelihood<br>$p(H_i) \times p(D\,|\,H_i)$</th>
    <th>Posterior<br>$p(H_i\,|\,D)$</th>
  </tr>
  <tr>
    <td>$H_1$</td>
    <td>$0.4$</td>
    <td>$1$</td>
    <td>$0.4$</td>
    <td>$0.662$</td>
  </tr>
  <tr>
    <td>$H_2$</td>
    <td>$0.4$</td>
    <td>$0.01$</td>
    <td>$0.004$</td>
    <td>$0.00662$</td>
  </tr>
  <tr>
    <td>$H_3$</td>
    <td>$0.1$</td>
    <td>$1$</td>
    <td>$0.1$</td>
    <td>$0.166$</td>
  </tr>
  <tr>
    <td>$H_4$</td>
    <td>$0.1$</td>
    <td>$1$</td>
    <td>$0.1$</td>
    <td>$0.166$</td>
  </tr>
  <tr>
    <td>Totals</td>
    <td>$1$</td>
    <td></td>
    <td>$0.604$</td>
    <td>$1$</td>
  </tr>
</table>
This results in the new posterior distribution, which intuitively makes sense when observing the likelihood. 

<div class="subsection_title">Parameter Estimation</div>
Bayes theorem can also be good for <b>parameter estimation</b>. In order to make probability statements about $\theta$ given $x$, we must begin with a model providing a joint probability distribution for $\theta$ and $x$. The joint probability density function can be written as a product of two densities that are often referred to as the prior distribution $p(\theta)$ and the sampling distribution $p(x\,|\,\theta)$, respectively: 
  \[p(\theta, x) = p(\theta) \; p(x\,|\,\theta)\]
Simply conditioning on the known value of the data $x$, using Bayes rule, yields the posterior density 
  \[p(\theta\,|\,x) = \frac{p(\theta)\; p(x\,|\, \theta)}{p(x)}\]
also expressed as the unnormalized posterior density 
  \[p(\theta\,|\,x) \propto p(\theta)\; p(x\,|\, \theta)\]
The term $p(x\,|\,\theta)$ is taken here as a function of $\theta$, not of $x$. It represents the probability of $x$ happening given $\theta$, which we can vary. 

Imagine that there is some scenario where we would like to calculate a parameter $\theta$ (e.g. the probability of a random bus going to my destination). We can use <b>Bayesian inference</b> to approximate these true values. We first construct a prior distribution 
  \[\theta \sim Distribution\]
with probabilities $p(\theta)$ (note that in this case, $\theta$ acts as both a random variable and as a hypothesis). Then, upon observation $x$, we can determine another distribution for the likelihood 
  \[\theta\,|\, x \sim Distribution\]

Let us introduce an example. Say that you moved into a new city and are waiting at the bus stop. You want to find out the probability $\theta$ that any bus successfully goes to your workplace, and after taking 5 random buses, you see that 2 out of the 5 does arrive at the destination. In this case, we see that
<ul>
  <li>$\theta$ is the parameter that we wish to estimate. </li>
  <li>$x$ is the data/evidence that $x=2$ out of the $N=5$ buses arrives at the destination. </li>
</ul>
Remember, the general process is that we first have some prior density of what $\theta$ could be, which would change to a more accurate posterior distribution $\theta\,|\,x$ upon evidence $x$. Depending on how much information we have or what assumptions we're making, we can choose different priors: 
<ol>
  <li>If we have no clue what the actual proportion of buses arriving actually go to the workplace, we can assume the prior to be 
    \[\theta \sim \text{Uniform}(0, 1) \implies p(\theta) = \begin{cases} 1 & 0 \leq x \leq 1 \\ 0 & \text{else} \end{cases} \]</li>
  <li>If we think that there should be more weight to the extreme solutions for when $\theta$ is close to $0$ or $1$, then we can use the prior 
    \[p(\theta) \propto \theta^{-\frac{1}{2}} (1 - \theta)^{-\frac{1}{2}} \text{ for } 0 \leq \theta \leq 1\]
  </li>
  <li>If we already had a lot of information that suggested that $\theta$ was probably close to $0.5$, then we would use the prior 
    \[p(\theta) \propto \theta^{100} (1 - \theta)^{100} \text{ for } 0 \leq \theta \leq 1\]
  </li>
</ol>
Now given that we know $\theta$, the probability of getting $x$ out of $N$ is clearly a binomial 
  \[x\,|\,\theta \sim \text{Binomial}(N, \theta) \implies p(x\,|\,\theta) = \binom{N}{x} \theta^x (1 - \theta)^{N-x} \]
Therefore, using Bayes rule $p(\theta\,|\,x) \propto p(\theta)\; p(x\,|\,\theta)$, we get (by substituing $x=2, N=5$ from our data)
<ol>
  <li>for the uniform prior, 
  \begin{align*} 
  p(\theta\,|\,x) & \propto 1 \cdot \binom{N}{x} \theta^x (1 - \theta)^{N - x} \\
  & \propto \theta^x (1 - \theta)^{N - x} \\
  p(\theta\,|\,x) & \propto \theta^2 (1 - \theta)^{3}
  \end{align*}
  </li>
  <li>for the extreme value-weighed prior, 
  \begin{align*} 
    p(\theta\,|\,x) & \propto \theta^{-\frac{1}{2}} (1 - \theta)^{-\frac{1}{2}} \cdot \binom{N}{x} \theta^x (1 - \theta)^{N-x} \\
    & \propto \theta^{x - \frac{1}{2}} (1 - \theta)^{N - x - \frac{1}{2}} \\
    p(\theta\,|\,x) & \propto \theta^{\frac{3}{2}} (1 - \theta)^{\frac{5}{2}}
  \end{align*}
  </li>
  <li>for the average weighed prior, 
  \begin{align*} 
    p(\theta\,|\,x) & \propto \theta^{100} (1 - \theta)^{100} \cdot \binom{N}{x} \theta^x (1 - \theta)^{N-x} \\
    & \propto \theta^{x + 100} (1 - \theta)^{N - x + 100} \\
    p(\theta\,|\,x) & \propto \theta^{102} (1 - \theta)^{103}
  \end{align*}
  </li>
</ol>
Once we get this posterior distribution, we can take its expectation to find our prediction of what the true value of $\theta$ will be. 

<div class="subsection_title">Computing Posteriors with Beta Prior and Binomial Likelihood</div>
The family of <b>beta distributions</b> contain normal, Poisson, binomial, etc. densities, of the general form
  \[x \sim \text{Beta}(\alpha, \beta) \implies p(x;\, \alpha, \beta) \propto x^{\alpha - 1} (1 - x)^{\beta - 1}\]
The normalizing constant in this density is quite complicated, so we will just call it $B$: 
\[p(x; \alpha, \beta) = \frac{x^{\alpha - 1} (1 - x)^{\beta - 1}}{\int_0^1 x^{\alpha - 1} (1 - x)^{\beta - 1}\; dx} = \frac{x^{\alpha - 1} (1 - x)^{\beta - 1}}{B(\alpha, \beta)}\]
It is actually a fact that <i>a beta prior and a binomial likelihood will produce a beta posterior</i>. That is, given $\theta \sim \text{Beta}(\alpha, \beta)$ and $x \,|\, \theta \sim \text{Binomial}(N, \theta)$, we have
\begin{align*} 
  p(\theta\,|\,x) & \propto p(\theta) \, p(x\,|\,\theta) \\
  & \propto \theta^{\alpha - 1} (1 - \theta)^{\beta - 1} \cdot \theta^x (1 - \theta)^{N-x} \\
  & = \theta^{\alpha + x - 1} (1 - \theta)^{\beta + N - x - 1} 
\end{align*}
which implies that $\theta \,|\,x \sim \text{Beta}(\alpha + x, \beta + N - x)$ distribution. That is, 
<ol>
  <li>Prior $\theta \sim \text{Beta}(1, 1)$ implies posterior $\theta\,|\,x \sim \text{Beta}(3, 4)$. </li>
  <li>Prior $\theta \sim \text{Beta}(\frac{1}{2}, \frac{1}{2})$ implies posterior $\theta\,|\,x \sim \text{Beta}(\frac{5}{2}, \frac{7}{2})$. </li>
  <li>Prior $\theta \sim \text{Beta}(101, 101)$ implies posterior $\theta\,|\,x \sim \text{Beta}(103, 104)$. </li>
</ol>
If the posterior distribution $p(\theta\,|\,x)$ is in the same probability distribution family as the prior distribution $p(\theta)$, the prior and posterior are then called <b>conjugate distributions</b>, and the prior is called a <b>conjugate prior</b> for the likelihood function $p(x\,|\,\theta)$. 

<div class="subsection_title">How Important is the Choice of the Prior? </div>
We can see that the choice of the prior distribution has an impact on the posterior. However, with more data, the discrepancy between the posteriors will vanish at infinity. This is reassuring. However, all this is under the assumption that the likelihood function is the same! Having different likelihood functions means that we will interpret the data differently, since the same data $x$ can appear with different probabilities under different likelihood densities $p(x\,|\,\theta)$. Therefore, we can think of the choice of the likelihood function as how we <i>interpret</i> the data. 

<div class="subsection_title">Summarizing Posteriors: Point Estimates & Credible Intervals</div>
The posterior distribution is the full answer to any Bayesian problem. It gives a complete description of our state of knowledge and our uncertainty about the value(s) of unknown parameters. But if we were to answer the question of what number $\hat{\theta}$ is the best estimator, we can say that the <b>point estimate</b> of this posterior is simply its expectation: 
  \[\hat{\theta} = \mathbb{E}(\theta\,|\,x) = \int \theta\, p(\theta\,|\,x)\; d\theta\]
Another way that we can concisely represent data from a posterior is by finding some <b>credible interval</b>. That is, we use the posterior distribution to find an interval $[a, b]$ such that 
  \[p(a \leq \theta \leq b \,|\,x) = 0.95\]
or some other value. Usually, this interval would be "centered" around $\theta$ in the way that $p(\theta \leq a) = p(b \leq \theta) = 0.025$. 
<br>
Finally, if we ever have data that looks even remotely like a Gaussian, it is conventional to summarize it in form: 
  \[\text{Mean} \pm \text{Standard Deviation}\]
</div>
</div>

<script src="/JS/html_notes.js"></script>
</body>
</html>
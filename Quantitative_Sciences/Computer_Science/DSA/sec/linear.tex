\section{Linear Programming}

  Basically, you want to solve a constrained optimization problem. Some examples are 
  \begin{enumerate}
    \item Shortest path: select a set of edges of minimum cost such that $s$ and $t$ are connected. 
    \item Minimum Spanning Tree: select a set of edges of minimum cost such that all vertices are connected. 
    \item Maximum Flow: select a flow on every edge such that capacity and balance constraints are respected. 
  \end{enumerate} 


  \begin{definition}[Linear Programming]
    In linear programming both the constraints and optimization objectives are linear functions. It is usually written in \textit{canonical form}, of the form 
    \begin{equation}
      \min c^T x \text{ subject to } Ax \geq b, x \geq 0
    \end{equation}
    or 
    \begin{equation}
      \max c^T x \text{ subject to } Ax \leq b, x \geq 0
    \end{equation}
    where we have $n$ variables $x \in \mathbb{R}^n$, $c \in \mathbb{R}^n$ are the optimization coefficients, and $A \in \mathbb{R}^{m \times n}$ are the matrix of $m$ constraint equations. 
  \end{definition} 

  \begin{example}[Simple LP]
    We can have the problem of maximizing $2x + y$ subject to 
    \begin{align}
      x + y & \leq 1 \\ 
      x & \geq 0  \\
      y & \geq 0
    \end{align}
    In this case, we can write it in canonical form as 
    \begin{equation}
      \max \begin{pmatrix} 2 & 1 \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} \text{ subject to } \begin{pmatrix} 1 & 1 \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} \leq 1, \begin{pmatrix} x \\ y \end{pmatrix} \geq \begin{pmatrix} 0 \\ 0 \end{pmatrix}
    \end{equation}
  \end{example}

  \begin{theorem}[Optimality of LP]
    An optimal solution to a linear program can be obtained in weakly polynomial time in the number of variables, constraints, and poly-logarithmic in the constants in the LP.\footnote{Obtaining a strongly polynomial algorithm for LPs remains a major open problem.} 
  \end{theorem} 

  This is very similar to integer programs, which solve for integer solutions rather than real, but in general we don't know of algorithms that can solve integer programs in polynomial time. However, integer programs can be ``relaxed'' to linear programs by removing the restriction on variables being integers. However, the optimal solution for the relaxed LP may be different from that of the original LP. 

  \begin{example}[Max Flow as LP]
    
  \end{example}

  \begin{example}[Bipartite Matching as LP]
    
  \end{example} 

  \begin{example}[Primal vs Dual LP]
    Now given a maximization LP problem, such as 
    \begin{equation}
      \max{2x + y} \text{ subject to } \begin{cases} x + y \leq 3 \\ x - y \leq 1 \\ x, y \geq 0 \end{cases} 
    \end{equation}
    we can just find \textit{one} solution satisfying the constraints and use it as a lower bound on the optimum. Say we find one, e.g. $x = y = 0$, which does satisfy all constraints. Now if we want to find an \textit{upper bound} on the value of the solution, we can use duality. We create new variables $a, b \geq 0$, and observe that 
    \begin{align}
      x + y \leq 3 & \implies a (x + y) \leq 3a \\
      x - y \leq 1 & \implies b (x - y) \leq b
    \end{align}
    Thus by adding the two we have $(a + b) x + (a - b) y \leq 3a + b$. 

    Now if we suppose $a + b \geq 2$ and $a - b \geq 1$ (which are the coeffients of $2x + y$), then since $x, y \geq 0$, we have 
    \begin{equation}
      2x + y \leq (a + b) x + (a - b) y \leq 3a + b
    \end{equation} 
    so any $a, b$ satisfying the above inequalities give an upper bound of $3a + b$ on the value of the optimal solution. Say we have $a = 2, b = 1$, which does satisfy. Then $3a + b = 7$ is an upper bound of $2x + y$. Therefore, to get the tightest upper bound, we can minimize the dual problem 
    \begin{equation}
      \min{3a + b} \text{ subject to } \begin{cases} a + b \geq 2 \\ a - b \geq 1 \\ a, b \geq 0 \end{cases}
    \end{equation}
    This is called the \textbf{dual LP}.
  \end{example}
  
  Generally, given the primal $\max{c^T x}$ subject to $Ax \leq b$ and $x \geq 0$, the dual problem is to compute $\min{b^T y}$ subject to $A^T y \geq c$ and $y \geq 0$. Every dual variable corresponds to a primal constraint and vice versa. If a primal has $n$ variables and $m$ constraints, the dual has $m$ variables and $n$ constraints. 

  \begin{theorem}
    The dual of a dual is the primal. 
  \end{theorem}

  \begin{theorem}[Weak Duality]
    The following properties are called \textbf{weak duality}. 
    \begin{enumerate}
      \item The optimal value for a dual maximization LP is a lower bound on the optimal value for a primal minimization LP. 
      \item The optimal value for a dual minimization LP is an upper bound on the optimal value for a primal maximization LP. 
    \end{enumerate}
  \end{theorem}

  \begin{theorem}[Strong Duality]
    The optimal value of a primal and dual LP are exactly identical, called \textbf{strong duality}. 
  \end{theorem}

  \begin{example}[Vertex Cover]
    You are given an undirected graph $G = (V, E)$. A vertex cover is a subset of vertices $S \subseteq V$ such that every edge in $E$ has at least one endpoint in $S$. That is, for each edge $(u, v) \in E$, either $u \in S$ or $v \in S$ (or both). The size of a vertex cover $S$ is the number of vertices in $S$, $|S|$. The minimum vertex cover is the vertex cover with the minimum size.
    \begin{enumerate}
      \item Write an integer program to compute the minimum vertex cover in $G$.  Clearly define all variables and provide a brief English explanation of the objective and each constraint.
      \item Relax your program from part $a$ to a linear program, and take it's dual. Interpret your result as another algorithmic optimization problem.
    \end{enumerate} 

    The relaxed primal LP problem is finding 
    \begin{equation}
      \min \sum_{v \in V} x_v \text{ subject to } \begin{cases} 
        x_u + x_v \geq 1 & \text{ for all } (u, v) \in E \\
        x_u \geq 0 & \text{ for all } u \in V
      \end{cases}
    \end{equation}
    To introduce the dual problem, we can introduce the variables for each edge $x_{uv} \geq 0$. Therefore, we can take each constraint $x_u + x_v \geq 1$ and multiply it by $x_{uv}$ to get $x_{uv} (x_u + x_v) \geq x_{uv}$ for all $(u, v) \in E$. Therefore, summing it up, we get 
    \begin{equation}
      \sum_{(u, v) \in E} x_{uv} (x_u + x_v) \geq \sum_{(u, v) \in E} x_{uv}
    \end{equation}
    Now we rephrase the left summation. Summing over all edges is the same as summing over all nodes, and for each node summing over all adjacent edges to that node, and so we can get 
    \begin{equation}
      \sum_{v \in V} \bigg( \sum_{(u, v) \in E} x_{uv} \bigg) x_v \geq \sum_{(u, v) \in E} x_{uv} 
    \end{equation}
    To make the LHS a lower bound of $\sum_{v \in V} x_v$, we can introduce the constraints that all coefficients are $\leq 1$, so we have the dual constraints 
    \begin{equation}
      \sum_{(u, v)} x_{uv} \leq 1 \text{ for all } v \in V
    \end{equation}
    Since $\sum_{(u, v) \in E} x_{uv}$ is a lower bound, we want to maximize the lower bound and so our dual optimization problem is 
    \begin{equation}
      \max \sum_{(u, v) \in E} x_{uv}
    \end{equation}
  \end{example}

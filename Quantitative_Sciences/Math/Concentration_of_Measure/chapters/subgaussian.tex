\section{Subgaussian Concentration and log-Sobolev Inequalities}

\subsection{Subgaussian Variables and Chernoff Bounds}
  
  We should first consider how one might go about proving that a random variable satisfies a Gaussian tail bound. Most tail bounds in probability theory are proved using some form of Markov's inequality. 
  \begin{lemma}[Markov's Inequality]
  Given a nonnegative random variable $X$, we have 
  \[\mathbb{P}(X > \alpha) \leq \frac{\mathbb{E}[X]}{\alpha}\]
  which means that the probability that $X > \alpha$ goes down at least as fast as $1/\alpha$. 
  \end{lemma}

  Markov's inequality is very conservative but very general, too. If we make further assumptions about the random variable $X$, we can often make stronger bounds. Chebyshev's inequality assumes a (possibly negative) random variable with finite variance and states that the probability will go down as $1/x^2$. 

  \begin{theorem}[Chebyshev Inequality]
  Given (possibly negative) random variable $X$, if $\mathbb{E}[X] = \mu < +\infty$ and $\Var(X) = \sigma^2 < +\infty$, then for all $\alpha > 0$, 
  \[\mathbb{P} \big( |X - \mu| > k \sigma \big) \leq \frac{1}{k^2} \iff \mathbb{P}(|X - \mu| > \alpha) \leq \frac{\mathrm{Var}[X]}{\alpha^2}\]
  That is, the probability that $X$ takes a value further than $k$ standard deviations away from $\mu$ goes down by $1/k^2$. Therefore, if $\sigma$ is small, then this bound will be small since there is more concentration in the mean. 
  \end{theorem}
  \begin{proof}
  We apply Markov's inequality to the non-negative random variable $|X - \mu|$. 
  \[\mathbb{P}(|X - \mu| > \alpha) = \mathbb{P}(|X - \mu|^2 > \alpha^2) \leq \frac{\mathbb{E}(|X - \mu|^2)}{\alpha^2} = \frac{\mathrm{Var}[X]}{\alpha^2}\]
  since the numerator on the RHS is the definition of variance. 
  \end{proof}

  Using higher powers, we can obtain better and better bounds, but not exponential ones. To obtain these Gaussian tail bounds, we must use more sophisticated methods. 

  \begin{lemma}[Chernoff Bound]
  Define the log-moment generating function $\psi$ of a random variable $X$ and its Legendre dual $\psi^*$ as 
  \[\psi_X (\lambda) \coloneqq \log \mathbb{E}[ e^{\lambda (X - \mathbb{E}[X])} ] = \mathbb{E}[ e^{\lambda X} ] - \lambda \mathbb{E}[X] \;\;\;\;\; \psi_X^* (t) = \sup_{\lambda \geq 0} \{ \lambda t - \psi_X(\lambda)\}\]
  Then, the following is known as the \textbf{Chernoff bound}. 
  \[\mathbb{P}[X - \mathbb{E}[X] \geq t] \leq e^{-\psi_X^* (t)}\]
  for all $t \geq 0$. We can lower bound it too with 
  \[\mathbb{P}[X - \mathbb{E}[X] \leq -t] \leq e^{-\psi_X^* (t)}\]
  and union bounding them gives 
  \[\mathbb{P}(|X - \mathbb{E}[X] | \geq t ] \leq 2e^{- \psi_X^* (t)}\]
  \end{lemma}
  \begin{proof}
  We take some $\lambda \geq 0$ and given that the map $x \mapsto e^{\lambda x}$ is nondecreasing, we can exponentiate and then use Markov's inequality: 
  \[\mathbb{P}[X - \mathbb{E}[X] \geq t ] = \mathbb{P}[ e^{\lambda(X - \mathbb{E}[X])} \geq e^{\lambda t}] \leq e^{-\lambda t} \mathbb{E}[e^{\lambda (X - \mathbb{E}[X])}] = e^{- (\lambda t - \psi_X (\lambda))} \leq e^{-\psi_X^* (t)}\]
  as the left hand does not depend on the choice of $\lambda$, we have the additional flexibility of tuning $\lambda$ to get potentially better bounds. We can also use Chernoff bound on the random variable $-X$ to bound \begin{align*}
      \mathbb{P}(X - \mathbb{E}[X] \leq -t) & = \mathbb{P}(-X - \mathbb{E}[-X] \geq t) \\
      & = \mathbb{P}(e^{\lambda(-X + \mathbb{E}[X])} \geq e^{\lambda t} ] \\
      & \leq e^{-\lambda t} \mathbb{E}[ e^{\lambda (-X + \mathbb{E}[X])}] \\
      & = e^{-(\lambda t - \psi_{-X}(\lambda))} \leq e^{-\psi_{-X}^* (t)} 
  \end{align*}
  There seems to be a minor problem in the fact that $-\psi^*_X$ and $-\psi^*_{-X}$ are different, and so provide different bounds for the upper and lower tail. But note that $\psi_X (\lambda) = \psi_{-X}(-\lambda)$, and so their maximum will coincide and $\psi_X^* (t) = \psi_{-X}^* (t)$, allowing us to get the union bound. 
  \[\mathbb{P}(|X - \mathbb{E}[X] | \geq t ] \leq 2e^{- \psi^* (t)}\]
  \end{proof}

  To observe how the Chernoff bound can give rise to Gaussian tail bounds, let us first consider the case of an actual Gaussian random variable. 

  \begin{example}
  Let $X \sim N(\mu, \sigma^2)$. Then, $\mathbb{E}[ e^{\lambda (X - \mathbb{E}[X])} ] = e^{\lambda^2 \sigma^2 / 2}$, so 
  \[\psi(\lambda) = \frac{\lambda^2 \sigma^2}{2}, \;\;\;\;\; \psi^* (t) = \sup_{\lambda \geq 0} \big\{ \lambda t - \frac{\lambda^2 \sigma^2}{2} \big\} = \frac{t^2}{2 \sigma^2}\]
  and by the Chernoff bound, we have $\mathbb{P}(X - \mathbb{E}[X] \geq t ] \leq e^{-t^2 / 2\sigma^2}$. 
  \end{example}

  Note that in order to get the tail bound, the fact that $X$ is Gaussian was not actually important. It would suffice to assume that the log-MGF is bouded from above by a Gaussian. 

  \begin{definition}[Subgaussian Random Variables]
  A random variable is called $\sigma^2$-\textbf{subgaussian} if its log-MGF satisfies 
  \[\psi(\lambda) \leq \frac{\lambda^2 \sigma^2}{2}\]
  for all $\lambda \in \mathbb{R}$. The constant $\sigma^2$ is called the \textbf{variance proxy}. 
  \end{definition}

  Remember that if $\psi(\lambda)$ is the log-MGF of a random variable $X$, then $\psi(-\lambda)$ is the log-MGF of the random variable $-X$. For a $\sigma^2$-subgaussian random variable $X$, we can therefore apply the Chernoff bound to both the upper and lower tails and union bound to obtain 
  \[\mathbb{P}(|X - \mathbb{E}[X]| \geq t ) \leq 2 e^{-t/2\sigma^2}\]

  We have only worked with Gaussians, which are trivially subgaussian. A nontrivial results is that every bounded random variable is subgaussian. 

  \begin{lemma}[Hoeffding's Lemma]
  Let $a \leq X \leq b$ a.s. for some $a, b \in \mathbb{R}$. Then, 
  \[\mathbb{E}[e^{\lambda(X - \mathbb{E}[X])}] \leq \exp \bigg( \frac{\lambda^2 (b - a)^2}{8} \bigg)\]
  That is, $X$ is $(b-a)^2 /4$-subgaussian. 
  \end{lemma}
  \begin{proof}
  We assume without loss of generality that $\mathbb{E}[X] = 0$. Then, we have $\psi(\lambda) = \log \mathbb{E}[ e^{\lambda X}]$, and we can compute 
  \[\psi^\prime (\lambda) = \frac{\mathbb{E}[X e^{\lambda X}]}{\mathbb{E}[e^{\lambda X}]}, \;\;\;\;\; \psi^{\prime\prime} (\lambda) = \frac{\mathbb{E}[X^2 e^{\lambda X}]}{\mathbb{E}[e^{\lambda X}]} - \bigg( \frac{\mathbb{E}[X e^{\lambda X}]}{\mathbb{E}[e^{\lambda X}]} \bigg)^2\]
  and thus 
  \[\psi^{\prime\prime} (\lambda) = \int_\Omega X^2 \, \frac{e^{\lambda X}}{\mathbb{E}[e^{\lambda X}]} \,d\mathbb{P} - \bigg( \int_\Omega X \, \frac{e^{\lambda X}}{\mathbb{E}[e^{\lambda X}]} \,d\mathbb{P} \bigg)^2 \] 
  can be interpreted as the variance of the random variable $X$ under the twisted probability measure $d\mathbb{Q} = \frac{e^{\lambda X}}{\mathbb{E}[e^{\lambda X}]} \,d\mathbb{P}$. But $a \leq X \leq b$, so we can bound the variance by its infimum and suprememum $\psi^{\prime\prime} (\lambda) = \Var_\mathbb{Q} [X] \leq (b-a)^2 / 4$, and the fundamental theorem of calculus yields 
  \[\psi(\lambda) = \int_0^\lambda \int_0^\mu \psi^{\prime\prime} (\rho) \, d\rho \, d\mu \leq \frac{\lambda^2 (b - a)^2}{8}\]
  using $\psi(0) = 0$ and $\psi^\prime (0)$. 
  \end{proof}

  \begin{exercise}[Subgaussian Variables]
  There are several different notions of random variables with a Gaussian tail that are all essentialy equivalent up to constants. The aim of this problem is to obtain some insight into these notions. 
  \begin{enumerate}
      \item Show that if $X$ is $\sigma^2$-subgaussian, then $\Var[X] \leq \sigma^2$. 
      \item Show that for any increasing and differentiable function $\Phi$, 
      \[\mathbb{E}[ \Phi(|X|)] = \Phi(0) + \int_0^\infty \Phi^\prime (t) \, \mathbb{P}(|X| \geq t) \, dt\]
  \end{enumerate}
  In the following, we will assume for simplicity that $\mathbb{E}[X] = 0$. We now prove that the following three properties are equivalent for suitable constants $\sigma, b, c$: (1) $X$ is $\sigma^2$-subgaussian; (2) $\mathbb{P}(|X| \geq t) \leq 2 e^{-b t^2}$; and (3) $\mathbb{E}[e^{c X^2}] \leq 2$. 
  \begin{enumerate}[resume]
      \item Show that if $X$ is $\sigma^2$-subgaussian , then $\mathbb{P}(|X| \geq t) \leq 2 e^{-t^2 / 2 \sigma^2}$ 
      \item Show that if $\mathbb{P}(|X| \geq t) \leq 2 e^{- t^2 / 2 \sigma^2}$, then $\mathbb{E}[e^{X^2 / 6 \sigma^2} ] \leq 2$. 
      \item Show that if $\mathbb{E}[e^{X^2 / 6 \sigma^2}] \leq 2$, then $X$ is $18 \sigma^2$-subgaussian. 
  \end{enumerate}
  In addition, the subgaussian property of $X$ is equivalent to the fact that the moments of $X$ scale as is the case for the Gaussian distribution. 
  \begin{enumerate}[resume]
      \item Show that if $X$ is $\sigma^2$-subgaussian, then $\mathbb{E}[X^{2q}] \leq (4 \sigma^2)^q q!$ for all $q \in \mathbb{N}$. 
      \item Show that if $\mathbb{E}[X^{2q}] \leq (4 \sigma^2)^q q!$ for all $q \in \mathbb{N}$, then $\mathbb{E}[e^{X^2 / 8 \sigma^2}] \leq 2$. 
  \end{enumerate}
  \end{exercise}

  \begin{solution}
  Listed. 
  \begin{enumerate}
      \item We can expand out 
      \begin{align*}
          \mathbb{E}[e^{\lambda (X - \mathbb{E} X}] & = \mathbb{E} \bigg[ 1 + \lambda( X - \mathbb{E}X) + \frac{\lambda^2}{2} (X - \mathbb{E} X)^2 + \ldots \bigg] \\
          & = 1 + \frac{\lambda^2}{2} \Var[X] + o(\lambda^2) \\
          & \leq e^{\lambda^2 \sigma^2 / 2} = 1 + \frac{\lambda^2 \sigma^2}{2} + o (\lambda^2)
      \end{align*}
      which is true for all $\lambda$. Setting $\lambda = 0$, we get $\Var[X] \leq \sigma^2$. 
      
      \item Unfinished. 
      
      \item Since $X$ is $\sigma^2$ subgaussian, its log-MGF satisfies $\psi(\lambda) = \log \mathbb{E}[e^{\lambda X}] \leq \frac{\lambda^2 \sigma^2}{2} \implies - \psi(\lambda) \geq - \frac{\lambda^2 \sigma^2}{2}$. Then, its Legendre dual is 
      \[\psi^\ast (t) = \sup_{\lambda \geq 0} \{ \lambda t - \psi(\lambda)\} \geq \sup_{\lambda \geq 0} \{ \lambda t - \frac{\lambda^2 \sigma^2}{2} \} = \frac{t^2}{2 \sigma^2}\]
      where we optimize the quadratic w.r.t. $\lambda$. Therefore, $-\psi^* (t) \leq - \frac{t^2}{2\sigma^2} \implies \mathbb{P}(X \geq t) \leq e^{ - \psi^* (t)} \leq e^{- t^2/ 2 \sigma^2}$. 
      
      
      \item By using the identity above with $\Phi(t) = e^{t^2 / 6 \sigma^2}$, we have 
      \begin{align*}
          \mathbb{E}[e^{X^2 / 6 \sigma^2}] & = \mathbb{E}[e^{|X|^2 / 6 \sigma^2}] \\
          & = e^{0^2 / 6 \sigma^2} + \int_0^\infty e^{t^2 / 6 \sigma^2} \, \frac{t}{3 \sigma^2} \mathbb{P}(|X| \geq t) \, dt \\
          & \leq 1 + \frac{1}{3t^2} \int_0^\infty t e^{t^2 / 6 \sigma^2} \, 2 e^{-t^2 / 2\sigma^2} \, dt\\
          & = 1 + \frac{2}{3 \sigma^2} \int_0^\infty t e^{-\frac{1}{3} \frac{t^2}{\sigma^2}} \, dt \\
          & = 1 - \frac{1}{\sigma^2} \int_0^\infty \Big( - \frac{2}{3 \sigma} t \Big) \, e^{- \frac{t^2}{3 \sigma^2}} \, dt \\
          & = 1 - e^{- \frac{t^2}{3 \sigma^2}} \bigg|_0^\infty \\
          & = 1 - (0 - 1) = 2 
      \end{align*}
      
      \item Unfinished. 
      
      \item We know $X^{2q} = |X|^{2q}$ for all $q \in \mathbb{N}$. By setting $\Phi(t) = t^{2q}$ from the identity above, we can get 
      \[\mathbb{E}[|X|^{2q}] = 0^{2q} + \int_0^\infty (2q) t^{2q - 1} \mathbb{P}(|X| \geq t) \,dt\]
      and from (3), we get the first line, where we can just keep doing integration by parts: 
      \begin{align*}
          \mathbb{E}[|X|^{2q}] & \leq \int_0^\infty (2q) t^{2q - 1} e^{- t^2 / 2 \sigma^2} \,dt \\
          & = 2 (4q \sigma^2) \int_0^\infty (2q - 2) t^{2q - 3} e^{-t^2 / 2 \sigma^2} \, dt \\
          & = 2 (4q \sigma^2) (4 (q - 1) \sigma^2) \int_0^\infty (2q - 4) t^{2q - 5} e^{-t^2 / 2\sigma^2} \,dt \\
          & = \ldots \\
          & = 2 (4q \sigma^2) \ldots (4 \cdot 2\sigma^2) \int_0^\infty 2t e^{-t^2 / 2 \sigma^2} \,dt \\
          & = \prod_{k=1}^q (4 k \sigma^2) = (4 \sigma^2)^q q! 
      \end{align*}
      
      \item We can expand and from the inequality above, we get 
      \begin{align*}
          \mathbb{E}[e^{X^2 / 8 \sigma^2}] = \mathbb{E} \bigg[ 1 + \frac{X^2}{8 \sigma^2} + \frac{1}{2} \bigg( \frac{X^2}{8 \sigma^2}\bigg)^2 + \ldots \bigg] \\
          & = 1 + \sum_{q = 1}^\infty \frac{1}{(8 \sigma^2)^q q!} \mathbb{E}[X^{2q}] \\
          & \leq 1 + \sum_{q = 1}^\infty \frac{1}{(8 \sigma^2)^q q!} (4 \sigma^2)^q q! \\
          & = 1 + \sum_{q=1}^\infty \frac{1}{2^q} = 2 
      \end{align*}
  \end{enumerate}


  \end{solution}

  \begin{exercise}[Tightness of Hoeffding's Lemma]
  Show that the bound on Hoeffding's lemma is the best possible by consider $\mathbb{P}(X = a) = \mathbb{P}(X = b) = \frac{1}{2}$. 
  \end{exercise}
  \begin{solution}
  From computing the expectation 
  \[\mathbb{E}[ e^{\lambda (X - \mathbb{E} X)}] = e^{\lambda (a - \frac{a + b}{2})} \mathbb{P}(X = a) + e^{\lambda (b - \frac{a + b}{2})} \mathbb{P}(X = b) = \frac{1}{2} e^{ \lambda \frac{a - b}{2}} + \frac{1}{2} e^{\lambda \frac{b - a}{2}}\]
  we know that this is always less than $\lambda^2 (b - a)^2/ 8$ for all $\lambda$. But setting $\lambda = 0$ satisfies equality. 
  \end{solution}

\subsection{The Martingale Method}

  In this section, we will use the martingale method to derive useful results. Recall that in order to derive some property (like tensorization of variance) of $f(X_1, \ldots, X_n) - \mathbb{E}[f(X_1, \ldots, X_n)]$, we can expand it as a telescoping sum of martingale differences 
  \[f(X_1, \ldots, X_n) - \mathbb{E}[f(X_1, \ldots, X_n)] = \sum_{k=1}^n \Delta_k\]
  where 
  \[\Delta_k = \mathbb{E}[f(X_1, \ldots, X_n) \mid X_1, \ldots, X_k] - \mathbb{E}[f(X_1, \ldots, X_n) \mid X_1, \ldots, X_{k-1}]\]
  and then deriving bounds on each difference. Note that these are martingale differences because given the filtration $\mathbb{F} = \{\mathcal{F}_k = \sigma(X_1, \ldots, X_k)\}$, the stochastic process
  \[Y_k = \sum_{i=1}^k \Delta_i = \mathbb{E}[f(X_1, \ldots, X_n) \mid X_1, \ldots, X_k] - \mathbb{E}[f(X_1, \ldots, X_n)]\]
  is a martingale. 

  \begin{lemma}[Azuma]
  Let $\mathbb{F} = \{\mathcal{F}_k\}_{k \leq n}$ be any filtration, and $\Delta_1, \ldots, \Delta_n$ be random variables that satisfy the following properties for $k = 1, \ldots, n$. 
  \begin{enumerate}
      \item Martingale Difference Property: $\Delta_k$ is $\mathcal{F}_k$-measurable and $\mathbb{E}[\Delta_k \mid \mathcal{F}_{k-1}] = 0$ 
      \item Conditional Subgaussian Property: $\mathbb{E}[e^{\lambda \Delta_k} \mid \mathcal{F}_{k-1}] \leq e^{\lambda^2 \sigma^2_k / 2}$ a.s. 
  \end{enumerate}
  Then, the sum $\sum_{k=1}^n \Delta_k$ is subgaussian with variance proxy $\sum_{k=1}^n \sigma_k^2$. 
  \end{lemma}
  \begin{proof}
  For any $1 \leq k \leq n$, we can compute 
  \[\mathbb{E}[ e^{\lambda \sum_{i=1}^k \Delta_i} ] = \mathbb{E}[e^{\lambda \sum_{i=1}^{k-1} \Delta_i} \mathbb{E}[e^{\lambda \Delta_k} \mid \mathcal{F}_{k-1}]] \leq e^{\lambda^2 \sigma_k^2 / 2} \, \mathbb{E}[e^{\lambda \sum_{i=1}^{k-1} \Delta_i}]\]
  and by induction, this proof is finished. Note that $\mathbb{E}[e^{\lambda \Delta_k} \mid \mathcal{F}_{k-1}] \leq e^{\lambda^2 \sigma^2_k / 2}$ can only hold if $\mathbb{E}[\Delta_k \mid \mathcal{F}_{k-1}] = 0$. 
  \end{proof}

  What this lemma basically says is that if we decompose a random variable into martingale differences, and each martingale difference is conditionally subgaussian, then their sum is also subgaussian. Now, if we just assume that each of these martingale differences are bounded, then we can use Hoeffding's lemma on each of them to make them subgaussian, and then use Azuma's lemma to show that their sum is subgaussian. This is exactly what we do here. 

  \begin{theorem}[Azuma-Hoeffding Inequality]
  Let $\mathbb{F} = \{ \mathcal{F}_k \}_{k \leq n}$ be any filtration, and let $\Delta_k, A_k, B_k$ satisfy the following properties for $k = 1, \ldots, n$. 
  \begin{enumerate}
      \item Martingale Difference Property: $\Delta_k$ is $\mathcal{F}_k$-measurable and $\mathbb{E}[\Delta_k \mid \mathcal{F}_{k-1}] = 0$ 
      \item Predictable bounds: $A_k, B_k$ are $\mathcal{F}_{k-1}$-measurable and $A_k \leq \Delta_k \leq B_k$ a.s. 
  \end{enumerate}
  Then, $\sum_{k=1}^n \Delta_k$ is subgaussian with variance proxy $\frac{1}{4} \sum_{k=1}^n ||B_k - A_k||^2_\infty$. In particular, we obtain for every $t \geq 0$ the tail bound 
  \[\mathbb{P} \bigg( \sum_{k=1}^n \Delta_k \geq t \bigg) \leq \exp \bigg( - \frac{2t^2}{\sum_{k=1}^n ||B_k - A_k||_\infty^2} \bigg)\]
  \end{theorem}

  The Azuma-Hoeffding's inequality is often applied in the following setting. Let $X_1, \ldots, X_n$ be independent random variables s.t. $a \leq X_i \leq b$ for all $i$ (we can interpret $a$ and $b$ as simply constant random variables). Then, let $\Delta_k = (X_k - \mathbb{E}[X_k])/n$ be martingale differences, which we can show that $\Delta_k$ is clearly $\mathcal{F}_k$-measurable and that by independence of $X_i$'s,  $\mathbb{E}[\Delta_k \mid \mathcal{F}_{k-1}] = \mathbb{E}[\Delta_k] = 0$. Therefore, we can show that its sum satisfies
  \[\mathbb{P} \bigg( \frac{1}{n} \sum_{k=1}^n \{X_k - \mathbb{E}[X_k]\} \geq t \bigg) \leq e^{-2n t^2 / (b - a)^2}\]
  which is consistent with the central limit theorem. 

  Now we can return to the case of functions $f(X_1, \ldots, X_n)$ of independent random variables. Recall that the discrete derivative is defined 
  \[D_k f(x) = \sup_z f(x_1, \ldots, x_{k-1}, z, x_{k+1}, \ldots, x_n) - \inf_z f(x_1, \ldots, x_{k-1}, z, x_{k+1}, \ldots, x_n)\]

  \begin{theorem}[McDiarmid]
  For $X_1, \ldots, X_n$ independent, $f(X_1, \ldots, X_n)$ is subgaussian with variance proxy $\frac{1}{4} \sum_{k=1}^n ||D_k f||^2$. That is, 
  \[\mathbb{P}\big[ f(X_1, \ldots, X_n) - \mathbb{E}[f(X_1, \ldots, X_n)] \geq t \big] \leq \exp \bigg( -\frac{2 t^2}{\sum_{k=1}^n ||D_k f||^2_\infty} \bigg)\]
  \end{theorem}
  \begin{proof}
  We use the martingale method again to write 
  \[f(X_1, \ldots, X_n) - \mathbb{E}[f(X_1, \ldots, X_n)] = \sum_{k=1}^n \Delta_k\]
  where 
  \[\Delta_k = \mathbb{E}[f(X_1, \ldots, X_n) \mid X_1, \ldots, X_k] - \mathbb{E}[f(X_1, \ldots, X_n) \mid X_1, \ldots, X_{k-1}]\]
  What we want to do is set some upper and lower bound on $\mathbb{E}[f(X_1, \ldots, X_n) \mid X_1, \ldots, X_k]$, which will set bounds on $\Delta_k$. We can do this by bounding $f$ by the infimum and supremum w.r.t. each element, getting 
  \begin{align*}
      &\mathbb{E}[ \inf_z f(X_1, \ldots, X_{k-1}, z, X_{k+1}, \ldots, X_n) \mid X_1, \ldots, X_k] \\
      &\;\;\;\;\;\leq \mathbb{E}[f (X_1, \ldots, X_n) \mid X_1, \ldots, X_k] \\
      &\;\;\;\;\;\;\;\;\;\; \leq \mathbb{E}[ \sup_z f(X_1, \ldots, X_{k-1}, z, X_{k+1}, \ldots, X_n) \mid X_1, \ldots, X_k]
  \end{align*}
  but by independence of $X_k$'s, we have 
  \[\mathbb{E}[ \inf_z f(X_1, \ldots, z, \ldots, X_n) \mid X_1, \ldots, X_k] = \mathbb{E}[ \inf_z f(X_1, \ldots, X_{k-1}, z, X_{k+1}, \ldots, X_n) \mid X_1, \ldots, X_{k-1}]\]
  So, setting 
  \begin{align*}
      A_k & = \mathbb{E}[ \inf_z f(X_1, \ldots, X_{k-1}, z, X_{k+1}, \ldots, X_n) - f(X_1, \ldots, X_n) \mid X_1, \ldots, X_{k-1}] \\
      B_k & = \mathbb{E}[ \sup_z f(X_1, \ldots, X_{k-1}, z, X_{k+1}, \ldots, X_n) - f(X_1, \ldots, X_n) \mid X_1, \ldots, X_{k-1}]
  \end{align*}
  we have $A_k \leq \Delta_k \leq B_k$ for all $k$, and by Azuma-Hoeffding's inequality along with the fact that $||B_k - A_k|| \leq ||D_k f||_\infty$, we get 
  \[\mathbb{P}[ f(X_1, \ldots, X_n) - \mathbb{E}[ f(X_1, \ldots, X_n)] \geq t] \leq \exp \bigg(- \frac{2t^2}{\sum_{k=1}^n ||B_k - A_k||^2_\infty} \bigg) \leq \exp \bigg(- \frac{2t^2}{\sum_{k=1}^n ||D_k f||^2_\infty} \bigg)\]
  \end{proof}

  We should treat McDiarmid's inequality as a subgaussian form of the bounded difference inequality 
  \[\Var[ f(X_1, \ldots, X_n)] \leq \frac{1}{4} \mathbb{E} \bigg[ \sum_{k=1}^n \big(D_k f (X_1, \ldots, X_n)\big)^2 \bigg]\]
  The bounded difference inequality says that the variance is controlled by the expectation of the square gradient of the function $f$. In contrast, McDiarmid's inequality asserts the stronger subgaussian inequality, but under the stronger condition that the variance proxy is controlled by a uniform upper bound on the square gradient rather than its expectation. This will be a recurring theme: 
  \begin{enumerate}
      \item the expectation of the square gradient controls the variance 
      \item a uniform bound on the square gradient controls the subgaussian property
  \end{enumerate}
  Note that McDiarmid's theorem is not satisfactory. The appropriate notion of a square gradient in both inequalities is the random variable $\sum_{k=1}^n |D_k f|^2$. To control the variance, we want to take its expectation $\mathbb{E} [\sum_{k=1}^n |D_k f|^2]$, and to control the upper bound of the square gradient, we simply want to take its supremum $|| \sum_{k=1}^n |D_k f|^2||_\infty$. However, McDiarmid's inequality only yields control in terms of the larger quantity $\sum_{k=1}^n || D_k f||^2_\infty$ (by triangle inequality), which gets worse in higher dimensions. Rather than taking the supremum of square gradient, we just take the supremum of each (squared) component and add them up, which may be much greater than the actual upper bound. Therefore, the martingale method is far too crude to capture this idea, and we will need new techniques for more refined bounds. 

  \begin{exercise}[Bin Packing]
  For the Bin packing problem previoulsly, show that the variance bound $\Var[B_n] \leq n/4$ can be strengthened to a Gaussian tail bound 
  \[\mathbb{P}(|B_n - \mathbb{E} B_n| \geq t) \leq 2e^{-2t^2/n}\]
  \end{exercise}
  \begin{solution}
  We can see that 
  \[D_k f(X_1, \ldots, X_n) = f(X_1, \ldots, X_{k-1}, 1, X_{k+1}, \ldots, X_n) - f(X_1, \ldots, X_{k-1}, 1, X_{k+1}, \ldots, X_n) = 1\]
  and by McDiarmid's inequality, we are done. 
  \end{solution}

  \begin{exercise}[Rademacher Processes]

  \end{exercise}

  \begin{exercise}[Sums in Hilbert Space]
  Let $X_1, \ldots, X_n$ be independent random variables with zero mean that map to a Hilbert space, and suppose that $||X_k|| \leq C$ a.s. for every $k$. 
  \begin{enumerate}
      \item Show that for all $t \geq 0$, 
      \[\mathbb{P} \bigg[ \bigg| \bigg| \frac{1}{n} \sum_{k=1}^n X_k \bigg| \bigg| \geq \mathbb{E} \bigg| \bigg| \frac{1}{n} \sum_{k=1}^n X_k \bigg| \bigg| + t \bigg] \leq e^{-nt^2 / 2C^2} \]
      
      \item Show that 
      \[\mathbb{E} \bigg| \bigg| \frac{1}{n} \sum_{k=1}^n X_k \bigg| \bigg| \leq C n^{-1/2}\]
      
      \item Conclude that for all $t \geq C n^{-1/2}$, 
      \[\mathbb{P} \bigg[ \bigg| \bigg| \frac{1}{n} \sum_{k=1}^n X_k \bigg| \bigg| \geq t \bigg] \leq e^{-nt^2 / 8C^2}\]
      
      \item Finally, argue that for all $t \geq 0$, 
      \[\mathbb{P} \bigg[ \bigg| \bigg| \frac{1}{n} \sum_{k=1}^n X_k \bigg| \bigg| \geq t \bigg] \leq e^{-nt^2 / 8C^2}\]
  \end{enumerate}
  \end{exercise}

\subsection{The Entropy Method}

  In order to develop more sophisticated concentration inequalities, let us introduce another term that is used to measure the deviation of a random variable. 

  \begin{definition}[Entropy]
  The \textbf{entropy} of a nonnegative random variable $Z$ is defined 
  \[\Ent[Z] \coloneqq \mathbb{E}[Z \log Z] - \mathbb{E}[Z] \log \mathbb{E}[Z]\]
  \end{definition}

  \begin{lemma}[Herbst]
  Suppose that random variable $X$ satisfies
  \[\Ent[e^{\lambda X}] \leq \frac{\lambda^2 \sigma^2}{2} \mathbb{E}[e^{\lambda X}]  \text{ for all } \lambda \geq 0\]
  Then, $X$ is $\sigma^2$-subgaussian. That is, 
  \[\psi(\lambda) \coloneqq \log\mathbb{E}[e^{\lambda (X - \mathbb{E}[X])}] \leq \frac{\lambda^2 \sigma^2}{2} \text{ for all } \lambda \geq 0\]
  \end{lemma}
  \begin{proof}
  As $\psi(\lambda) = \log\mathbb{E}[ e^{\lambda X}] - \lambda \mathbb{E}[X]$, we have 
  \[\frac{d}{d \lambda} \frac{\psi(\lambda)}{\lambda} = \frac{1}{\lambda} \frac{\mathbb{E}[X e^{\lambda X}]}{\mathbb{E}[e^{\lambda X}]} - \frac{1}{\lambda^2} \log \mathbb{E}[e^{\lambda X}] = \frac{1}{\lambda^2} \frac{\Ent [e^{\lambda X}]}{\mathbb{E}[e^{\lambda X}]} \leq \frac{\sigma^2}{2}\]
  where the last inequality yields from the assumption. By the fundamental theorem of calculus, we have 
  \[\frac{\psi (\lambda)}{\lambda} = \lim_{\lambda \downarrow 0} \frac{\psi(\lambda)}{\lambda} + \int_0^\lambda \frac{1}{t^2} \frac{\Ent[e^{t X}]}{\mathbb{E}[e^{t X}]} \,dt \leq \frac{\lambda \sigma^2}{2} \implies \psi(\lambda) \leq \frac{\lambda^2 \sigma^2}{2}\]
  \end{proof}

  \begin{exercise}
  It turns out that the converse is true up to a constant: If $X$ is $\frac{\sigma^2}{4}$-subgaussian, then 
  \[\Ent [e^{\lambda X}] \leq \frac{\lambda^2 \sigma^2}{2} \mathbb{E}[e^{\lambda X}]\]
  \end{exercise}
  \begin{solution}
  We know that by Jensen's inequality and concavity of the logarithm, 
  \[\log \mathbb{E}[e^{\lambda(X - \mathbb{E} X)}] \geq \mathbb{E}[\lambda (X - \mathbb{E} X)] = 0 \implies \mathbb{E}[e^{\lambda(X - \mathbb{E} X)}] \geq 1\]
  Furthermore, note that given $Z = e^{\lambda X} / \mathbb{E}[e^{\lambda X}]$, we have 
  \begin{align*}
      \mathbb{E}[Z \log{Z}] & = \mathbb{E} \bigg[ \frac{e^{\lambda X}}{\mathbb{E}[e^{\lambda X}]} \, \log \bigg( \frac{e^{\lambda X}}{\mathbb{E}[e^{\lambda X}]} \bigg) \bigg] \\
      & = \frac{1}{\mathbb{E}[e^{\lambda X}]} \mathbb{E}\big[ e^{\lambda X} \big( \log e^{\lambda X} - \log \mathbb{E}[e^{\lambda X}] \big) \big] \\
      & = \frac{1}{\mathbb{E}[e^{\lambda X}]} \mathbb{E} \big[ e^{\lambda X} \lambda X - e^{\lambda X} \log \mathbb{E}[e^{\lambda X}] \big] \\
      & = \frac{1}{\mathbb{E}[e^{\lambda X}]} \Big( \mathbb{E} [ e^{\lambda X} \lambda X ] - \mathbb{E}[ e^{\lambda X}] \, \log \mathbb{E}[e^{\lambda X}] \Big) \\
      & = \frac{\Ent [e^{\lambda X}]}{\mathbb{E}[e^[{\lambda X}]} 
  \end{align*}

  \end{solution}

  Since this theorem assumes a bound on $\Ent[e^{\lambda X}]$ rather than $\Ent[X]$, we will mainly be working with the entropy of exponentials of a random variable. 

  It turns out that entropy behaves very similarly to variance and extends nicely into the subgaussian setting. Just like variance, we define the partial entropy of function $f(x_1, \ldots, x_n)$ as 
  \[\Ent_k f (x_1, \ldots, x_n) \coloneqq \Ent[ f(x_1, \ldots, x_{k-1}, X_k , x_{k+1}, \ldots, x_n)]\]
  That is, $\Ent[f(X_1, \ldots, X_n)]$ is the entropy of $f(X_1, \ldots, X_n)$ with respect to the variable $X_k$ only, the remaining variables kept fixed. 

  \begin{theorem}[Tensorization of Entropy]
  Given that $X_1, \ldots, X_n$ are independent, 
  \[\Ent[ f(X_1, \ldots, X_n)] \leq \mathbb{E} \bigg[ \sum_{k=1}^n \Ent_k f (X_1, \ldots, X_n) \bigg]\]
  \end{theorem}

  Recall that the basic method for deriving Poincare inequalities is that we have some bound on the variance of a single random variable 
  \[\Var_\mu [g] \leq \mathbb{E}[|\nabla g|^2]\]
  and by tensorization, we can take the multivariate function $f$ and derive 
  \[\Var_\mu [f] \leq \mathbb{E}[ ||\nabla g||^2 ]\]
  In here, we derive modified log-Sobolev inequalities by bounding the entropy of the form 
  \[\Ent_\mu [e^g] \leq \mathbb{E}[ |\nabla g|^2 \, e^g ]\]
  and then using tensorization to bound 
  \[\Ent_\mu [e^{\lambda f}] \leq \mathbb{E} [ ||\nabla (\lambda f)||^2 \, e^{\lambda f} ]\]

  \begin{lemma}[Discrete Modified log-Sobolev]
  Let $D^- f \coloneqq f - \inf f$. Then, 
  \[\Ent[e^f] \leq \Cov[f, e^f] \leq \mathbb{E}[|D^- f|^2 e^f]\]
  \end{lemma}
  \begin{proof}
  Note that $\log \mathbb{E}[e^f] \geq \mathbb{E}[f]$ by Jensen's inequality. Therefore, 
  \[\Ent[e^f] = \mathbb{E}[f e^f] - \mathbb{E}[e^f] \, \log \mathbb{E}[e^f] \leq \mathbb{E}[f e^f] - \mathbb{E}[f] \mathbb{E}[e^f] = \Cov[f, e^f]\]
  To prove the second part, we have 
  \[\Cov[f, e^f] = \mathbb{E}[(f - \mathbb{E}[f]))(e^f - \mathbb{E}[e^f])] \leq \mathbb{E}[(f - \inf f)(e^f - e^{\inf f})] \]
  and since $e^x$ is convex, the first-order condition gives 
  \[e^{\inf f} \geq e^f + e^f (\inf f - f) \implies e^f - e^{\inf f} \leq e^f (f - \inf f)\]
  and substituting above gives the result. 
  \end{proof}

  Now, by defining the one-sided differences 
  \begin{align*}
      D_k^- f (x) & = f(x_1, \ldots, x_n) - \inf_z f (x_1, \ldots, x_{k-1}, z, x_{k+1}, \ldots, x_n) \\
      D_k^+ f (x) & = \sup_z f (x_1, \ldots, x_{k-1}, z, x_{k+1}, \ldots, x_n) - f(x_1, \ldots, x_n) 
  \end{align*}
  we can use the discrete modified log-Sobolev inequality on each of them and then tensorize to get the following. 

  \begin{theorem}[Bounded Difference Inequality]
  For all $t \geq 0$, 
  \begin{align*}
      \mathbb{P}[ f(X_1, \ldots, X_n) - \mathbb{E}[f(X_1, \ldots, X_n) \geq t] & \leq \exp \bigg( -\frac{t^2}{4 || \sum_{k=1}^n |D_k^- f|^2||_\infty} \bigg) \\
      \mathbb{P}[ f(X_1, \ldots, X_n) - \mathbb{E}[f(X_1, \ldots, X_n) \leq -t] & \leq \exp \bigg( -\frac{t^2}{4 || \sum_{k=1}^n |D_k^+ f|^2||_\infty} \bigg) 
  \end{align*}
  whenever $X_1, \ldots, X_n$ are independent. In particular, $f(X_1, \ldots, X_n)$ is subgaussian with variance proxy $2 ||\sum_{k=1}^n |D_k f|^2 ||_\infty$, where $D_k f = \sup_z f - \inf_z f$. 
  \end{theorem}

\subsection{Modified log-Sobolev Inequalities}

  \begin{theorem}[Modified log-Sobolov Inequality]
  Let $P_t$ be a Markov semigroup with stationary measure $\mu$. The following are equivalent: 
  \begin{enumerate}
      \item $\Ent_\mu [f] \leq c \mathcal{E}(\log f, f)$ for all $f$ (modified log-Sobolev inequality). 
      \item $\Ent_\mu [P_t f] \leq e^{-t/c} \Ent_\mu [f]$ for all $f, t$ (entropic exponential ergodicity). 
  \end{enumerate}
  Moreover, if $\Ent_\mu [P_t f] \rightarrow 0$ as $t \rightarrow +\infty$, then 
  \[\mathcal{E}(\log P_t f, P_t f) \leq e^{-t/c} \mathcal{E}(\log f, f) \text{ for all } f, t\]
  implies $1$ and $2$ above. 
  \end{theorem}


In frequentist inference, we would treat some true parameter $\theta$ as \textit{fixed}, albeit unknown. We would do density estimation by maximizing the likelihood over some data $\mathcal{D}$ with respect so some parameters $\theta$. 
\begin{equation}
  \hat{\theta} = \argmax_{\theta} p(\mathcal{D} \mid \theta)
\end{equation}
This maximum likelihood estimation, along with the method of moments, were two paradigms of estimation that we have seen. As long as we can solve this, we could get a point estimate, and there are a lot of ways to solve this. By assuming independence, we can decompose the probability term into a product of the likelihoods of each sample, which we can hopefully solve analytically or with numerical optimizers. We can even create confidence sets that give us some information about the uncertainty in our estimates. However, there is an Achilles heel of Frequentist inference, as depicted in the following example. 

\begin{example}[Conflicting Results in Significance Tests]
  Let $\theta$ be the probability of a particular coin landing on heads, and suppose we want to test the hypotheses
  \begin{align}
    H_0: \theta = 1/2, \quad H_1: \theta > 1/2
  \end{align}
  at a significance level of $\alpha = 0.05$. Now suppose we observe the following sequence of flips:
  \begin{center}
    heads, heads, heads, heads, heads, tails \quad (5 heads, 1 tails)
  \end{center}
  
  To perform a frequentist hypothesis test, we must define a random variable to describe the data. The proper way to do this depends on exactly which of the following two ways the experiment was performed:
  
  \begin{itemize}
    \item Suppose that the experiment was ``Flip six times and record the results.'' In this case, the random variable $X$ counts the number of heads, and $X \sim \text{Binomial}(6, \theta)$. The observed data was $x = 5$, and the p-value of our hypothesis test is
    \begin{align}
      \text{p-value} &= P_{\theta=1/2}(X \geq 5) \\
      &= P_{\theta=1/2}(X = 5) + P_{\theta=1/2}(X = 6) \\
      &= \frac{6}{64} + \frac{1}{64} = \frac{7}{64} = 0.109375 > 0.05.
    \end{align}
    So we fail to reject $H_0$ at $\alpha = 0.05$.
    
    \item Suppose instead that the experiment was ``Flip until we get tails.'' In this case, the random variable $X$ counts the number of the flip on which the first tails occurs, and $X \sim \text{Geometric}(1 - \theta)$. The observed data was $x = 6$, and the p-value of our hypothesis test is
    \begin{align}
      \text{p-value} &= P_{\theta=1/2}(X \geq 6) \\
      &= 1 - P_{\theta=1/2}(X < 6) \\
      &= 1 - \sum_{x=1}^{5} P_{\theta=1/2}(X = x) \\
      &= 1 - \left(\frac{1}{2} + \frac{1}{4} + \frac{1}{8} + \frac{1}{16} + \frac{1}{32}\right) = \frac{1}{32} = 0.03125 < 0.05.
    \end{align}
    So we reject $H_0$ at $\alpha = 0.05$.
  \end{itemize}
  
  The conclusions differ, which seems absurd. Moreover the p-values aren't even close---one is 3.5 times as large as the other. Essentially, the result of our hypothesis test depends on whether we would have stopped flipping if we had gotten a tails sooner. In other words, the frequentist approach requires us to specify what we would have done had the data been something that we already know it wasn't. 
\end{example}

Note that despite the different results, the likelihood for the actual value of $x$ that was observed is the same for both experiments (up to a constant):
\begin{align}
  p(x|\theta) \propto \theta^5(1-\theta).
\end{align}

A Bayesian approach would take the data into account only through this likelihood and would therefore be guaranteed to provide the same answers regardless of which experiment was being performed. Therefore, Bayesian modeling on the other hand treats the true $\theta$ as intrinsically \textit{random}. Therefore, we can ``solve'' for $\theta$ by completely characterizing its distribution given the data $\mathcal{D}$. This is a much harder problem than finding a point estimate, but we can relate this to the often computable likelihood using Bayes rule. 
\begin{equation}
  p(\theta \mid \mathcal{D}) = \frac{p(\mathcal{D} \mid \theta) \, p(\theta)}{p(\mathcal{D})}
\end{equation}
Note that if we take the maximum of this, it is equivalent to finding 
\begin{equation}
  \hat{\theta} = \argmax_{\theta} p(\mathcal{D} \mid \theta) \, p(\theta)
\end{equation}
which is called a \textit{maximum a-posteriori (MAP)} estimate and is similar to MLE, but with an extra term $p(\theta)$. This term, called the \textbf{prior distribution}, does not depend on the data and is usually thought of as an initial guess for $\theta$ before we see any data. Upon observing the data, our prior gets updated to the \textbf{posterior distribution} $p(\theta \mid \mathcal{D}) \propto p(\mathcal{D} \mid \theta) p (\theta)$. Since the posterior must integrate to $1$, we have the normalizing constant $p(\mathcal{D})$, known as our \textit{marginal likelihood}. 

The general framework is very simple but there are two big questions. 
\begin{enumerate}
  \item How do we know \textit{which} prior to put? In the beginning, we will work with some friendly priors---called \textit{conjugate priors}---that have nice analytic posteriors. In practice, this is an art. 

  \item How can we compute the normalizing constant $p(\mathcal{D})$? For very simple distributions, we can analytically solve it, but in most cases, computing it is impossible. Therefore, we are only given some scaled version of the posterior density. Fortunately, there are a range of \textit{Markov Chain Monte Carlo (MCMC)} samplers that remarkably only require these values to sample from the distributions. Therefore, we use the following notation more often when calculating posteriors since the normalizing constant isn't as important as finding the shape of the posterior density.

  \begin{equation}
    \text{Posterior } \propto \text{ Prior } \times \text{ Likelihood}
  \end{equation}

  The MCMC techniques are covered in my sampling notes. 
\end{enumerate}

Once we have a distribution of $\theta$, we can do prediction or density estimation by naively using only the posterior defined by our MAP estimate $p(x \mid \mathcal{D}) = p(x \mid \hat{\theta})$. But for a full Bayesian treatment, we should condition over the $\theta$. 

\begin{definition}[Posterior Predictive Distribution]
  The \textbf{posterior predictive distribution} is defined 
  \begin{equation}
    p(x \mid \mathcal{D}) = \int p(x \mid \theta, \mathcal{D}) p(\theta \mid \mathcal{D}) d\, \theta
  \end{equation}
\end{definition}

A lot of this is based off of \cite{2009hoff}. 


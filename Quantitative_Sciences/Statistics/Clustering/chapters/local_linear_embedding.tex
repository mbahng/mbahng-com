\section{Local Linear Embedding} 

  PCA and MDS are linear embedding methods. Let's move onto nonlinear ones. The first nonlinear models that we work with again use the idea of locality (remember kernel regression). You have data that is globally nonlinear, but if you look at a point and its local neighborhood around it, then it is approximately linear since we assume that it lives in some smooth manifold. 

  \begin{figure}[H]
    \centering 
    \includegraphics[scale=0.4]{img/local_linear_embedding.png}
    \caption{Local linear embedding assumes that the data is locally linear. } 
    \label{fig:local_linear_embedding}
  \end{figure}

  The concept of neighborhood can be defined in two ways. You can either just fix an $\epsilon$ and take the $\epsilon$-ball around each point $x_i$. Or you can fix a $k$ and take the $k$ nearest neighbors of each point. The general idea of using kernel PCA is to take a local neighborhood of the data and construct some linear approximation of it. 


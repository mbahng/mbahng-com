\section{Talagrand's Gaussian Inequality}

  \begin{lemma}[Gaussian Integration by Parts Formula]
  For Gaussian random variables $x, x_1, \ldots, x_n$ and a function $F$ of moderate growth at infinity, we have 
  \[\mathbb{E}\big[ x \, F(x_1, \ldots, x_n) \big] = \sum_{i=1}^n \mathbb{E}[x \, x_i] \; \mathbb{E}\bigg[ \frac{\partial F}{\partial x_i} (x_1, \ldots, x_n) \bigg]\]
  \end{lemma}

  \begin{theorem}[Talagrand's Gaussian Inequality]
  Consider a Lipschitz function $F: \mathbb{R}^N \longrightarrow \mathbb{R}$ (with Lipschitz constant $A$). Let $x_1, \ldots, x_N \sim \mathcal{N}(0, 1)$ be iid, and let $\mathbf{x} = (x_1, \ldots, x_N)$. Then, for each $t > 0$, we have 
  \[\mathbb{P} \big( | F(\mathbf{x}) - \mathbb{E} F(\mathbf{x}) | \geq t \big) \leq 2 \exp \bigg(- \frac{t^2}{4A^2} \bigg)\]
  \end{theorem}
  \begin{proof}
  For this proof, we assume that $F$ is not only Lipschitz, but $C^2$. This is the case in most applications of this theorem, and if it is not the case, then we can regularize $F$ by convolving with a smooth function to solve the problem. We begin with a parameter $s$ and consider the function $G: \mathbb{R}^{2N} \longrightarrow \mathbb{R}$ defined 
  \[G(z_1, \ldots, z_{2N}) = \exp \Big( s \big[ F ( z_1, \ldots, z_N) - F(z_{N+1}, \ldots, z_{2N}) \big] \Big)\]
  For clarity, we will denote variables of $F$ with $x_i$ and variables of $G$ with $z_i$. Let $u_1, \ldots, u_{2N} \sim \mathcal{N}(0, 1)$ be iid, and let $v_1, \ldots, v_n \sim \mathcal{N}(0, 1)$ be iid, with $v_{N+1}, \ldots, v_{2N}$ copies of the first $N$. For shorthand, we can denote the collection as $\mathbf{u}$ and $\mathbf{v}$. Then, we have 
  \[\mathbb{E} [u_i u_j] - \mathbb{E}[ v_i v_j] = 0\]
  except when $j = i + M$ or $i = j + M$, in which case we have 
  \[\mathbb{E} [u_i u_j] - \mathbb{E}[ v_i v_j] = 0 - 1 = -1\]
  since $v_i v_j = X^2$, where $X \sim \mathcal{N}(0, 1) = \chi^2_1$, a Chi-Squared distribution with 1 degree of freedom. We consider the transformed random variable
  \[\mathbf{f}(t) \coloneqq \sqrt{t} \, \mathbf{u} + \sqrt{1 - t} \, \mathbf{v} \sim \mathcal{N}(0, 1) \text{ for all } t\]
  that is essentially some smooth path from $\mathbf{f}(0) = \mathbf{u}$ and $\mathbf{f}(1) = \mathbf{v}$. Note that given some $t \in [0, 1]$, $\mathbf{f}(t)$ is some random vector, $G ( \mathbf{f}(t))$ is some random variable, and $\mathbb{E}[ G(\mathbf{f}(t))]$ is some number. We can define the function $\phi: [0, 1] \longrightarrow \mathbb{R}$ as 
  \begin{align*}
      \phi(t) = \mathbb{E} [G (\mathbf{f}(t))] & = \int_\mathbb{R} x \; p_{G(f(t))} (x) \,dx \\
      & = \int_{\mathbb{R}^{2N}} G(y) \; p_{f(t)} (y) \,dy 
  \end{align*}
  where $p_X$ is the PDF of the distribution $X$. Take the derivative with respect to $t$ to get the first line, and we can simplify using Gaussian integration by parts 
  \begin{align*}
      \phi^\prime (t) & \mathbb{E}\bigg[ \sum_{i=1}^{2N} \frac{d}{dt} f_i (t) \; \frac{\partial G}{\partial z_i} \big( \mathbf{f}(t)\big) \bigg] \\
      & = \sum_{i=1}^{2N} \mathbb{E} \bigg[ \frac{d}{dt} f_i (t) \, \frac{\partial G}{\partial z_i} \big( \mathbf{f}(t)\big)\bigg] \\
      & = \sum_{i=1}^{2N} \sum_{j=1}^{2N} \mathbb{E} \bigg[ \Big( \frac{\partial}{\partial t} f_i (t) \Big) \, f_i (t) \bigg] \; \mathbb{E} \bigg[ \frac{\partial^2 G}{\partial z_i \partial z_{j}} \mathbf{f} (t) \bigg] 
  \end{align*}
  But we can simplify 
  \begin{align*}
      \mathbb{E} \bigg[ \Big( \frac{\partial}{\partial t} f_i (t) \Big) \, f_i (t) \bigg] & = \mathbb{E} \bigg[ \Big( \frac{1}{2 \sqrt{t}} u_i - \frac{1}{2 \sqrt{1 - t}} v_i \Big) \big( \sqrt{t} u_j - \sqrt{1 - t} \, v_j \big) \bigg] \\
      & = \frac{1}{2} \big(\mathbb{E}[ u_i u_j] - \mathbb{E}[v_i v_j] \big) = \begin{cases} -1 & \text{ if } j = i + M , i = j + M \\
      0 & \text{ else} \end{cases} 
  \end{align*}
  So, we can simplify the above to
  \[\phi^\prime (t) = - \mathbb{E} \bigg[  \sum_{i=1}^N \frac{\partial^2 G}{\partial z_i \, \partial z_{i + M}} \big( \mathbf{f}(t)\big) \bigg]\]
  and computing the second derivative using the chain rule gives 
  \begin{align*}
      \frac{\partial G}{\partial z_i} (\mathbf{z}) & = \frac{\partial G}{\partial F} \frac{\partial F}{\partial x_i} (z_1, \ldots, z_N) \\
      & = s \; G(\mathbf{z}) \, \frac{\partial F}{\partial x_i} (z_1, \ldots, z_N) \\
      \frac{\partial^2 G}{\partial z_i \partial z_{i + N}} (\mathbf{z}) & = - s^2 \, G(\mathbf{z}) \, \frac{\partial F}{\partial x_i} (z_1, \ldots, z_N) \, \frac{\partial F}{\partial x_i} (z_{N+1}, \ldots, z_{2N}) 
  \end{align*}
  for all $\mathbf{z}$. So we have for all $t \in [0, 1]$, 
  \begin{align*}
      \phi^\prime (t) & = s^2 \, \mathbb{E} \bigg[ \sum_{i=1}^N G(\mathbf{f}(t)) \, \frac{\partial F}{\partial x_i} \big( f_1 (t), \ldots, f_N (t) \big) \, \frac{\partial F}{\partial x_i} \big( f_{N+1} (t), \ldots, f_{2N} (t) \big) \bigg] \\ 
      & \leq s^2 \mathbb{E} \bigg[ G(\mathbf{f}(t)) \sum_{i=1}^N \frac{\partial F}{\partial x_i} \big( f_1 (t), \ldots, f_N (t) \big) \, \frac{\partial F}{\partial x_i} \big( f_{N+1} (t), \ldots, f_{2N} (t) \big) \bigg] \\ 
      & \leq s^2 \mathbb{E} \big[ G(\mathbf{f}(t) \big) \, A^2 \big] \\
      & \leq s^2 A^2 \mathbb{E}[G(\mathbf{f}(t))] = s^2 A^2 \phi(t)
  \end{align*}
  Solving the inequality for $\phi$ gives 
  \begin{align*}
      \phi^\prime (t) / \phi(t) \leq s^2 A^2 & \implies \int \phi^\prime (t) / \phi(t) \,dt \leq \int s^2 A^2 \,dt \\
      & \implies \log{\phi(t)} \leq s^2 A^2 t + C \\
      & \implies \phi(t) \leq e^{s^2 A^2 t} \leq e^{s^2 A^2}
  \end{align*}
  Recalling that $\mathbf{f}(1) = \mathbf{u}$, we have 
  \[\mathbb{E}[\exp\{ s ( F(u_1, \ldots, u_N) - F(u_{N+1}, \ldots, u_{2N})) \}] \leq e^{s^2 A^2}\]
  and by independence of the $u_i$'s, the LHS equals $\mathbb{E}[e^{s F(u_1, \ldots, u_N)}]\, \mathbb{E}[e^{-s F(u_{N+1}, \ldots, u_{2N})}]$ and by Jensen's inequality, we have $\mathbb{E}[e^{-s F(u_{N+1}, \ldots, u_{2N})}] \geq e^{-s \mathbb{E}[F(u_{N+1}, \ldots, u_{2N})]}$. We can derive as follows: 
  \begin{align*}
      e^{s^2 A^2} & \geq \mathbb{E}[e^{s F(u_1, \ldots, u_N)}]\, \mathbb{E}[e^{-s F(u_{N+1}, \ldots, u_{2N})}] \\
      & \geq \mathbb{E}[e^{s F(u_1, \ldots, u_N)}]\, e^{-s \mathbb{E}[F(u_{N+1}, \ldots, u_{2N})]} \\
      & = \mathbb{E}[e^{s F(u_1, \ldots, u_N)}]\, \mathbb{E}[e^{-s \mathbb{E}[F(u_{N+1}, \ldots, u_{2N})]}] \\
      & = \mathbb{E}[e^{s F(u_1, \ldots, u_N) -s \mathbb{E}[F(u_{N+1}, \ldots, u_{2N})]}] \\
      & = \mathbb{E}[\exp \big( s F(u_1, \ldots, u_N) -s \mathbb{E}[F(u_{N+1}, \ldots, u_{2N})] \big) ]
  \end{align*}
  and by Markov's inequality, we get for a random vector of standard Gaussian random variables $\mathbf{x}$
  \begin{align*}
      \mathbb{P} \big( F(\mathbf{x}) - \mathbb{E}[F(\mathbf{x})] \geq t) & = \mathbb{P} \big( e^{s( F(\mathbf{x}) - \mathbb{E}[F(\mathbf{x})]} \geq e^{st} \big) \\
      & \leq \frac{\mathbb{E}[e^{s( F(\mathbf{x}) - \mathbb{E}[F(\mathbf{x})]}]}{e^{st}} \\
      & \leq e^{s^2 A^2 - st} \\
      & = e^{- t^2 / 4A^2} \text{ when } s = t / 2A^2
  \end{align*}
  \end{proof}


\section{Variance Bounds and Poincare Inequalities}

  Let us first describe this concentration phenomenon by investigating bounds on the variance 
  \[\mathrm{Var}[f(x_1, \ldots, x_n)] \coloneqq \mathbb{E}\big[ \big( f(x_1, \ldots, x_n) - \mathbb{E}[f(x_1, \ldots, x_n)] \big)^2 \big] \]
  We can first bound 
  \[\Var[f(X_1, \ldots, X_n)] = \mathbb{E}\big[ \big( f(X_1, \ldots, X_n)\big)^2 \big] - \mathbb{E}\big[ f(X_1, \ldots, X_n) \big]^2 \leq \mathbb{E}\big[ \big( f(X_1, \ldots, X_n)\big)^2 \big]\]
  and since adding a constant term to $f$ doesn't affect the variance, we can utilize this to get our first variance bound. 

  \begin{lemma}
  Let $\mathbf{X}$ be a random variable or vector. Then, 
  \[\mathrm{Var}[f(\mathbf{X})] \leq \mathbb{E} \big[ \big( f(\mathbf{X}) - \inf f \big)^2 \big] \text{ and } \mathrm{Var}[f(\mathbf{X})] \leq \mathbb{E} \big[ \big(\sup f - f(\mathbf{X}) \big)^2 \big]\]
  and 
  \[\mathrm{Var}[ f(\mathbf{X})] \leq \frac{1}{4} ( \sup f - \inf f)^2\]
  \end{lemma}
  \begin{proof} 
  Since $\mathrm{Var}[\mathbf{X}] = \mathbb{E}[\mathbf{X}^2] - \mathbb{E}[\mathbf{X}]^2$ from above, we have 
  \[\mathrm{Var}[ f(\mathbf{X})] = \mathrm{Var}[f(\mathbf{X}) - a] = \mathbb{E}[(f(\mathbf{X}) - a)^2] - \mathbb{E}[f(\mathbf{X}) - a]^2 \leq \mathbb{E}[(f(\mathbf{X}) - a)^2]\]
  By letting $a = \inf f$, we get the first inequality. By letting $a = (\sup f + \inf f) /2$ be the "middle" of $f$, we have $|f(\mathbf{X}) - a| \leq (\sup f - \inf f)/2 \implies [f(\mathbf{X}) - a]^2 \leq (\sup f - \inf f)^2/4$, and so 
  \[\Var[ f(\mathbf{X})] \leq \mathbb{E}[(f(\mathbf{X}) - a)^2] \leq \frac{1}{4} (\sup f - \inf f)^2\]
  which gives our third inequality. We can also see that 
  \[\mathrm{Var}[ f(\mathbf{X})] = \Var[ -f (\mathbf{X})] = \Var[ b - f(\mathbf{X})] \leq \mathbb{E}[ (b - f(\mathbf{X}))^2]\]
  to get our second. 
  \end{proof} 

  This allows us to bound the random vector $f(\mathbf{X})$ if $f$ itself is bounded, no matter what $\mathbf{X}$ is. But this generally turns out to be a very conservative bound, which is unsurprising since we assume so little about $\mathbf{X}$. For example, if we let $X_1, \ldots, X_n$ be iid random variables taking values in $[-1, 1]$, and let $f(x_1, \ldots, x_n) = \frac{1}{n} \sum_{i=1}^n x_i$. Then, $f$ takes values in $[-1, 1]$, and by the previous lemma, we have
  \[\mathrm{Var}[f(X_1, \ldots, X_n)] \leq \frac{1}{4} (1 - (-1))^2 = 1\]
  which looks good, until we see that we can derive a better bound from direct computation (which becomes much better as $n$ increases). 
  \[\mathrm{Var}[f(X_1, \ldots, X_n)] = \frac{1}{n^2} \sum_{i=1}^n \mathrm{Var}[X_i] = \frac{1}{n}\]
  However, this computation assumes independence of $X_i$'s, which the previous lemma doesn't. This is the reason we're able to get a better bound, since if we took $n$ copies of the same $X$, we would have 
  \[\mathrm{Var}[f(X_1, \ldots, X_n)] = \mathrm{Var}[n X / n] = \mathrm{Var}[X] = 1\]
  Therefore, we will capitalize on the independence of these random variables in high dimensions to obtain better bounds. Now in the next result, we shall show that the variance of a high dimensional $f(X_1, \ldots, X_n)$ can be bounded by the variances of each random variable. Those quantities, like the variance, that behave well in high dimensions is said to \textit{tensorize}. 

  Consider independent random variables $X_1, \ldots, X_n$ and a function $f: \mathbb{R}^n \longrightarrow \mathbb{R}$. If we fix values $x_1, \ldots, x_n$, then we can define for all $k = 1, \ldots, n$ the function $g_{k}(x_1, \ldots, x_{k-1}, x_{k+1}, \ldots, x_n): \mathbb{R} \rightarrow \mathbb{R}$ as 
  \[g_{k}(x_1, \ldots, x_{k-1}, x_{k+1}, \ldots, x_n)(z) = f(x_1, \ldots, x_{k-1}, z, x_{k+1}, \ldots, x_n)\]
  where 
  \[(g_{k}(x_1, \ldots, x_{k-1}, x_{k+1}, \ldots, x_n))^\prime (z) = \frac{\partial}{\partial x_k} f(x_1, \ldots, x_{k-1}, z, x_{k+1}, \ldots, x_n)\]
  and $g_k (x_1, \ldots, x_{k-1}, x_{k+1}, \ldots, x_n) (X_k)$ is a random variable of $X_k$. Then, we can define 
  \begin{align*}
      \Var_k f(x_1, \ldots, x_n) & = \Var_{X_k} [ f(x_1, \ldots, x_{k-1}, X_k, x_{k+1} \ldots, x_n)] \\ 
      & = \mathbb{E}_{X_k} \big[ \big( f (x_1, \ldots, x_{k-1}, X_k, x_{k+1}, \ldots, x_n) - \mathbb{E}_{X_k} [f (x_1, \ldots, x_{k-1}, X_k, x_{k+1}, \ldots, x_n)] \big)^2 \big] \\
      & = \Var[g_{k} (x_1, \ldots, x_{k-1}, x_{k+1}, \ldots, x_n) (X_k)] \\
      & = \Var_{X_k} [g(x_1, \ldots, x_{k-1}, x_{k+1}, \ldots, x_n)] 
  \end{align*}
  which takes the variance of $f$ with respect to $X_k$, keeping all other variables fixed. However, this value will change for different $x_1, \ldots, x_n$'s, and so we can loosen the restriction that they are fixed. We can take 
  \[g_k (X_1, \ldots, X_{k-1}, X_{k+1}, \ldots, X_n) (z) = f(X_1, \ldots, X_{k-1}, z, X_{k+1}, \ldots, X_n)\]
  where $g_k (X_1, \ldots, X_{k-1}, X_{k+1}, \ldots, X_n) (X_k)$ is a random variable of $X_1, \ldots, X_n$. Now if we calculate its partial variance, we get 
  \begin{align*}
      \Var_k f(X_1, \ldots, X_n) & = \Var_{X_k} [f(X_1, \ldots, X_k, \ldots, X_n)]\\
      & = \Var [g_k (X_1, \ldots, X_{k-1}, X_{k+1}, \ldots, X_n) (X_k)] \\
      & = \Var_{X_k} [g_k (X_1, \ldots, X_{k-1}, X_{k+1}, \ldots, X_n)]
  \end{align*}
  which is now a random variable of all $X_i$'s, $i \neq k$, that outputs the variance of $f$ with respect to $X_k$. \textbf{But is it true that }
  \[\mathbb{E}_{X_k} [ f(X_1, \ldots, X_n)] = \mathbb{E}[ f(X_1, \ldots, X_n) \mid X_1, \ldots, X_{k-1}, X_{k+1}, \ldots, X_n] ?\]


  Now, we can show a very useful property of variance: that the variance of some arbitrary function can be bounded by the expected sum of the partial variances. 

  \begin{theorem}[Tensorization of Variance]
  That is, $\Var_i f(\mathbf{x})$ is the variance of $f(X_1, \ldots, X_n)$ w.r.t. the variable $X_i$ only, the remaining variables kept fixed. Then, we have 
  \[\Var[f(X_1, \ldots, X_n)] \leq \mathbb{E} \bigg[ \sum_{i=1}^n \Var_i f(X_1, \ldots, X_n) \bigg] \]
  \end{theorem}
  \begin{proof}
  We try to mimic the fact that the variance of the sum of independent random variables is the sum of the variances. At first sight, the general function $f(x_1, \ldots, x_n)$ need not look anything like a sum, but we can expand it as a telescoping sum of random variables. We will prove this using the \textit{martingale method}, which constructs this random variable $f(X_1, \ldots, X_n)$ as a sum of finer and finer increments starting from the "coarse" constant function $\mathbb{E}[f(X_1, \ldots, X_n)]$. We define the random variable 
  \[\Delta_k \coloneqq \mathbf{E}[ f(X_1, \ldots, X_n) \mid X_1, \ldots X_k] - \mathbb{E}[ f(X_1, \ldots, X_n) \mid X_1, \ldots, X_{k-1}]\]
  Then, we can express 
  \[f( X_1, \ldots, X_n) - \mathbf{E}[ f(X_1, \ldots, X_n)] = \sum_{k=1}^n \Delta_k\]
  Note that $\mathbb{E}[\Delta_k \mid X_1, \ldots, X_{k-1}] = 0$ (i.e. $\Delta_k$'s are martingale increments). In particular, even though the $\Delta_k$'s are not independent, if we have $l < k$, then 
  \begin{align*}
      \mathbb{E}[ \Delta_k \Delta_l] & = \mathbb{E}[ \mathbb{E}[\Delta_k \Delta_l \mid X_1, \ldots, X_{k-1}]] \\
      & = \mathbb{E}[ \mathbb{E}[\Delta_k \mid X_1, \ldots X_{k-1} ] \, \mathbb{E}[\Delta_l \mid X_1, \ldots X_{k-1} ]] \\
      & = \mathbb{E}[ \mathbb{E}[\Delta_k \mid X_1, \ldots X_{k-1} ] \, \Delta_l] \\
      & = \mathbb{E}[0 \cdot \Delta_l] = 0
  \end{align*}
  and so, the variance can be expanded into terms that vanish. 
  \begin{align*}
      \Var[ f(X_1, \ldots, X_n)] & = \mathbb{E} \big[ \big( f( X_1, \ldots, X_n) - \mathbf{E}[ f(X_1, \ldots, X_n)] \big)^2\big] \\
      & = \mathbb{E} \bigg[ \bigg( \sum_{k=1}^n \Delta_k \bigg)^2 \bigg] = \sum_{k=1}^n \mathbb{E}[ \Delta_k^2]
  \end{align*}
  Now it remains to show that $\mathbb{E}[\Delta_k^2] \leq \mathbb{E}[\Var_k f(X_1, \ldots, X_n)]$ for every $k$. Let us define 
  \[\Tilde{\Delta}_k = f(X_1, \ldots, X_n) - \mathbb{E}[f(X_1, \ldots, X_n) \mid X_1, \ldots, X_{k-1}, X_{k+1}, \ldots, X_n]\]
  to be the approximation of $f(X_1, \ldots, X_n)$ "one step" before the final increment. Then, we have 
  \[\Delta_k = \mathbb{E}[\Tilde{\Delta}_k \mid X_1, \ldots, X_k]\]
  and as $X_k$ and $X_1, \ldots, X_{k-1}, X_{k+1}, \ldots, X_n$ are independent, we have 
  \[\Var_k f(X_1, \ldots, X_n) = \mathbb{E}[\Tilde{\Delta}_k^2 \mid X_1, \ldots, X_{k-1}, X_{k+1}, \ldots, X_n] \]
  and therefore using Jensen's inequality we can prove 
  \[\mathbb{E}[\Delta_k^2] = \mathbb{E}[\mathbb{E}[ \Tilde{\Delta}_k \mid X_1, \ldots, X_k]^2 ] \leq \mathbb{E}[\Tilde{\Delta}_k^2] = \mathbb{E}[\Var_k f(X_1, \ldots, X_n)]\]
  \end{proof}

  What we want to eventually do is prove an inequality of the form where for any function $h: \mathbb{R} \rightarrow \mathbb{R}$ and some $X \sim \mu$, 
  \[\Var_\mu[h] = \Var [h(X)] \leq ||\mathcal{L}(h)||^2_{L^2 (\mu)}\]
  where $\mathcal{L}$ is an operator on $h$. This will allow us to bound 
  \[\Var [g_k (x_1, \ldots, x_{k-1}, x_{k+1}, \ldots, x_n)(X_k)] \leq ||\mathcal{L}(g_k (x_1, \ldots, x_{k-1}, x_{k+1}, \ldots, x_n))||^2\]
  for all $x_1, \ldots, x_n$, simply by taking $h = g(x_1, \ldots, x_{k-1}, x_{k+1}, \ldots, x_n)$. Since this works for all $x_1, \ldots, x_n$, we can claim that this inequality holds for all $X_1 (\omega), \ldots, X_n (\omega)$ for all $\omega \in \Omega$. That is, we can loosen the fixed values into random variables. 
  \begin{align*}
      \Var_{k} f(X_1, \ldots, X_n) & = \Var[g_k (X_1, \ldots, X_{k-1}, X_{k+1}, \ldots, X_n)(X_k)] \\
      & \leq || \mathcal{L}(g_k (X_1, \ldots, X_{k-1}, X_{k+1}, \ldots, X_n))||^2_{L^2 (\mu)} 
  \end{align*}
  Note that all terms are random variables of $X_1, \ldots, X_n$, and so the same inequality holds for their expectations over the entire joint measure. 
  \[\mathbb{E}[ \Var_{k} f(X_1, \ldots, X_n) ] \leq \mathbb{E} \big[ || \mathcal{L}(g_k (X_1, \ldots, X_{k-1}, X_{k+1}, \ldots, X_n))||^2_{L^2 (\mu)} \big] \]
  and so by tensorization (i.e. summing them up), we get 
  \[\Var[f(X_1, \ldots, X_n)] \leq \sum_{i=1}^n \mathbb{E} \big[ \Var_i f(X_1, \ldots, X_n) \big] \leq \sum_{i=1}^n \mathbb{E} \big[ || \mathcal{L}(g_k (X_1, \ldots, X_{k-1}, X_{k+1}, \ldots, X_n))||^2_{L^2 (\mu)} \big] \]

  Furthermore, this bound is sharp when $f$ is linear. Let us demonstrate this by letting $f(x_1, \ldots, x_n) = a_1 x_1 + \ldots + a_n x_n$. On the left hand side, we have 
  \[\Var[ f(X_1, \ldots, X_n)] = \Var\bigg[ \sum_{i=1}^n a_i X_i \bigg] = \sum_{i=1}^n a_i^2 \Var[X_i] \]
  and on the right hand side, each component divides up to 
  \begin{align*}
      \Var_i f(x_1, \ldots, x_n ) & = \Var[ f(x_1, \ldots, X_i, \ldots, x_n)] \\
      & = \Var[ a_1 x_1 + \ldots + a_i X_i + \ldots a_n x_n] \\
      & = \Var[a_i X_i] \\
      & = a_i^2 \Var[X_i]
  \end{align*}
  \textbf{Then?} Note that since $f$ is linear, the values of all $x_j, j \neq i$ have no effect on the variance of $X_i$, and so $\Var_i f(X_1, \ldots, X_n)$, which is originally a random variable of $X_1, \ldots, X_{i-1}, X_{i+1}, \ldots, X_n$, is really just the constant (random variable) $a_i^2 \Var[X_i]$. This is because no matter what values $X_1, \ldots, X_{i-1}, X_{i+1}, \ldots, X_n$ are realized, these values will only contribute to a translation of the random variable $f(X_1, \ldots, X_n)$, and hence will not affect the variance w.r.t. $X_i$. So, the right hand side also becomes 
  \[\mathbb{E} \bigg[ \sum_{i=1}^n \Var_i f(X_1, \ldots, X_n) \bigg] = \mathbb{E} \bigg[ \sum_{i=1}^n a_i^2 \Var[X_i] \bigg] = \sum_{i=1}^n a_i^2 \Var[X_i]\]
  which is the same as the LHS. 

  We can view the tensorization of the variance in itself as an expression of the concentration phenomenon. $\Var_i f (\mathbf{x})$ quantifies the sensitivity of the function $f(\mathbf{x})$ of the coordinate $x_i$ in a distribution-dependent manner. If this sensitivity w.r.t. each coordinate ($\mathbb{E}[ \Var_i f(X_1, \ldots, X_n)]$) is small, then $f(X_1, \ldots, X_n)$ is close to its mean. However, it might not be so straightforward to compute $\Var_i f$, since it depends on both the function $f$ and on the distribution of $X_i$. So, we can try combining this with a suitable bound on the component-wise variance. 

  Let us define the quantities: 
  \[D_i f (\mathbf{x}) \coloneqq \sup_z f(x_1, \ldots, x_{i-1}, z, x_{i+1}, \ldots, x_n) - \inf_z f(x_1, \ldots, x_{i-1}, z, x_{i+1}, \ldots, x_n)\]
  and 
  \[D_i^- f(\mathbf{x}) \coloneqq f(x_1, \ldots, x_n) - \inf_z f(x_1, \ldots, x_{i-1}, z, x_{i+1}, \ldots, x_n)\]
  which quantifies the sensitivity of the function $f$ to the coordinate $x_i$ in a distribution-independent manner. Now we can introduce the following bounds. 

  \begin{corollary}
  We have 
  \[\Var[ f(X_1, \ldots, X_n)] \leq \frac{1}{4} \mathbb{E} \bigg[ \sum_{i=1}^n \big( D_i f(X_1, \ldots, X_n) \big)^2 \bigg] \]
  \end{corollary}
  \begin{proof}
  We start off with 
  \begin{align*}
      \Var_i f (X_1, \ldots, X_n) & = \Var[ f(X_1, \ldots, X_i, \ldots, X_n)] \\
      & \leq \frac{1}{4} \big( D_i f (X_1, \ldots, X_n)\big)^2 
  \end{align*}
  Since these a random variables follow this inequality (for all $\omega \in \Omega$), we can attach an expectation on them to get 
  \[\mathbb{E}[\Var_i f (X_1, \ldots, X_n)] \leq \mathbb{E} \bigg[ \frac{1}{4} \big( D_i f (X_1, \ldots, X_n)\big)^2\bigg] \]
  and substituting in the previous theorem gives 
  \begin{align*}
      \Var[f(X_1, \ldots, X_n)] & \leq \mathbb{E} \bigg[ \sum_{i=1}^n \Var_i f(X_1, \ldots, X_n) \bigg] \\
      & = \sum_{i=1}^n \mathbb{E}\big[ \Var_i f(X_1, \ldots, X_n) \big] \\
      & \leq \sum_{i=1}^n \mathbb{E} \bigg[ \frac{1}{4} \big( D_i f (X_1, \ldots, X_n)\big)^2\bigg] \\
      & = \frac{1}{4} \mathbb{E} \bigg[ \sum_{i=1}^n \big( D_i f(X_1, \ldots, X_n) \big)^2 \bigg] 
  \end{align*}
  \end{proof}

  \begin{example}[Random Matrices]

  \end{example}



  \begin{exercise}[Banach-Valued Sums]
  Let $X_1, X_2, \ldots, X_N$ be independent random variables with values in a Banach space $(B, ||\cdot ||_B)$. Suppose these random variables are bounded in the sense that $||X_i||_B \leq C$ a.s. for every $i$. Show that 
  \[\Var\bigg( \bigg| \bigg| \frac{1}{n} \sum_{k=1}^n X_k \bigg| \bigg|_B \bigg) \leq \frac{C^2}{n}\]
  This is a simple vector-valued variant of the elementary fact that the variance of $\frac{1}{n} \sum_{k=1}^n X_k$ for real-valued random variables $X_k$ is of order $\frac{1}{n}$. 
  \end{exercise}
  \begin{solution}
  We can tensorize the variance to get 
  \begin{align*}
      \Var_k \bigg| \bigg| \frac{1}{n} \sum_{k=1}^n X_k \bigg| \bigg|_B & = \Var \bigg| \bigg| \frac{1}{n} X_k \bigg| \bigg|_B = \frac{1}{n^2} \Var ||X_k||_B \\
      & \leq \frac{1}{n^2} \bigg( \frac{1}{4} (C - (-C))^2 \bigg) = \frac{C^2}{n^2} 
  \end{align*}
  and so letting $f(X_1, \ldots, X_n) = \big| \big| \frac{1}{n} \sum_{k=1}^n X_k \big| \big|_B$, we get 
  \begin{align*}
      \Var [f(X_1, \ldots X_n)] & \leq \sum_{k=1}^n \mathbb{E}[ \Var_k f (X_1, \ldots, X_n)] \\
      & \leq \sum_{k=1}^n \frac{C^2}{n^2} = \frac{C^2}{n} 
  \end{align*}
  \end{solution}

  \begin{exercise}[Rademacher Processes]
  Let $\epsilon_1, \ldots, \epsilon_n$ be independent symmetric Bernoulli random variables $\mathbb{P}(\epsilon_i = \pm 1) = \frac{1}{2}$ (also called Rademacher variables), let $T \subset \mathbb{R}^n$. The following identity is completely trivial: 
  \[\sup_{t \in T} \Var \bigg[ \sum_{k=1}^n \epsilon_k t_k \bigg] = \sup_{t \in T} \sum_{k=1}^n t_k^2\]
  Prove the following nontrivial fact: 
  \[\Var \bigg[ \sup_{t \in T} \sum_{k=1}^n \epsilon_k t_k \bigg] \leq 4 \sup_{t \in T} \sum_{k=1}^n t_k^2\]
  \end{exercise}
  \begin{solution}
  Let us consider a fixed $\boldsymbol{\epsilon} = (\epsilon_1, \ldots, \epsilon_n)$ and index $i \in [n]$. Then, consider the random variable formed by taking the value $f(\epsilon_1, \ldots, \epsilon_n)$ and loosening $\epsilon_i$ to be an random variable. That is, 
  \begin{align*}
      \mathbb{P} \Big[ f(\epsilon_1, \ldots, \epsilon_n) = \sup_{t \in T} \{\epsilon_1 t_1 + \ldots + 1 t_i + \ldots + \epsilon_n t_n\} \Big] = \frac{1}{2} \\
      \mathbb{P} \Big[ f(\epsilon_1, \ldots, \epsilon_n) = \sup_{t \in T} \{\epsilon_1 t_1 + \ldots - 1 t_i + \ldots + \epsilon_n t_n\} \Big] = \frac{1}{2} 
  \end{align*}
  Then, we compute 
  \[D_i^- f (\epsilon_1, \ldots, \epsilon_n) = \inf_{\epsilon_i \in \{-1, 1\}} \sup_{t \in T} \sum_{k=1}^n \epsilon_k t_k\]
  and we can estimate 
  \begin{align*}
      D_i^- f(\boldsymbol{\epsilon}) & = f(\epsilon_1, \ldots, \epsilon_n) - D_i f (\epsilon_1, \ldots, \epsilon_n) \\
      & = \sup_{t \in T} \sum_{k=1}^n \epsilon_k t_k - \inf_{\epsilon_i \in \{-1, 1\}} \sup_{t \in T} \sum_{k=1}^n \epsilon_k t_k \\
      & \leq \sup_{t \in T} 2 |t_i| 
  \end{align*}
  We can finally bound 
  \begin{align*}
      \Var[ f(\epsilon_1, \ldots, \epsilon_n)] & \leq \mathbb{E} \bigg[ \sum_{i=1}^n \big( D_i^- f(\boldsymbol{\epsilon})\big)^2 \bigg] \\
      & \leq 4 \mathbb{E} \bigg[ \sum_{i=1}^n \sup_{t \in T} t_i^2 \bigg] \\
      & = 4 \sup_{t \in T} \sum_{i=1}^n t_i^2 
  \end{align*}
  \end{solution}

  \begin{exercise}[Bin Packing]
  This is a classical application of bounded difference inequalities. Let $X_1, \ldots, X_n$ i.i.d. random variables with values in $[0, 1]$. Each $X_i$ represents the size of a package to be shipped. The shipping containers are bins of size $1$ (so each bin can hold a set packages whose sizes sum to at most $1$). Let $B_n = f(X_1, \ldots, X_n)$ be the minimal number of bins needed to store the packages. Note that computing $B_n$ is a hard combinatorial optimization problem, but we can bound its mean and variance by easy arguments. 
  \begin{enumerate}
      \item Show that $\Var[B_n] \leq n/4$
      \item Show that $\mathbb{E}[B_n] \geq n \mathbb{E}[X_1]$
  \end{enumerate}
  Thus the fluctuations $\sim \sqrt{n}$ of $B_n$ are much smaller than its magnitude $\sim n$. 
  \end{exercise}
  \begin{solution}
  Listed. 
  \begin{enumerate}
      \item Given fixed sizes $X_1, \ldots, X_n$ and some $i \in [n]$, we can see that a property of $f$ is that 
      \[f(X_1, \ldots, X_{i-1}, 0, X_{i+1}, \ldots, X_n) + 1 = f(X_1, \ldots, X_{i-1}, 1, X_{i+1}, \ldots, X_n)\]
      since for an extra package with size $1$, you would for sure need one more bin. So the maximum difference of $f$ based on the $x_i$ value is the constant random variable 
      \begin{align*}
          D_i f(X_1, \ldots, X_n) & = \sup_{z \in [0, 1]} f(X_1, \ldots, z, \ldots, X_n) - \inf_{z \in [0, 1]} f(X_1, \ldots, z, \ldots, X_n)\\
          & = f(X_1, \ldots, 1, \ldots, X_n) - f(X_1, \ldots, 0, \ldots, X_n) = 1
      \end{align*}
      and so by the bounded difference inequalities, 
      \begin{align*}
          \Var[B_n] = \Var[f(X_1, \ldots, X_n)] & \leq \frac{1}{4} \mathbb{E} \bigg[ \sum_{i=1}^n \big( D_i f(X_1, \ldots, X_n) \big)^2 \bigg] \\
          & = \frac{1}{4} \sum_{i=1}^n \mathbb{E} \big[ \big( D_i f(X_1, \ldots, X_n) \big)^2 \big] \\
          & \leq \frac{n}{4} 
      \end{align*}
      \item Given the sizes $X_1, \ldots, X_n$, $B_n$ must satisfy 
      \[B_n = f(X_1, \ldots, X_n) \geq X_1 + \ldots + X_n\] 
      since the total volume of bins $B_n$ must exceed the total volume $X_1 + \ldots + X_n$ of packages. So, 
      \[\mathbb{E}[B_n] \geq \mathbb{E}\bigg[ \sum_{k=1}^n X_k \bigg] = n \mathbb{E}[X_1]\]
  \end{enumerate}
  \end{solution}

  \begin{exercise}[Order Statistics and Spacings]
  Let $X_1, \ldots, X_n$ be independent random variables, and denote by $X_{(1)} \geq \ldots \geq X_{(n)}$ their decreasing rearrangement ($X_{(1)} = \max_i X_i$, $X_{(n)} = \min_i X_i$, etc.). Show that 
  \[\Var[X_{(k)}] \leq k \, \mathbb{E}[(X_{(k)} - X_{(k+1)})^2] \text{ for } 1 \leq k \leq n/2\]
  and that 
  \[\Var[X_{(k)}] \leq (n - k + 1)\, \mathbb{E}[(X_{(k-1)} - X_{(k)})^2] \text{ for } n/2 < k \leq n\]
  \end{exercise}

  \begin{exercise}[Convex Poincare Inequality]
  Let $X_1, \ldots, X_n$ be independent random variables taking values in $[a, b]$. The bounded difference inequalities estimate the variance $\Var[f(X_1, \ldots, X_n)]$ in terms of \textit{discrete} derivatives $D_i f$ or $D_i^- f$ of the function $f$. The goal of this problem is to show that if the function $f$ is convex, then one can obtain a similar bound in terms of the ordinary notion of derivative $\nabla_i f(x) = \partial f(x)/\partial x_i$ in $\mathbb{R}^n$. 
  \begin{enumerate}
      \item Show that if $g: \mathbb{R} \longrightarrow \mathbb{R}$ is convex, then 
      \[g(y) - g(x) \geq g^\prime (x)\, (y - x) \text{ for all } x, y \in \mathbb{R}\]
      
      \item Show using part (a) and the bounded difference inequalities that if $f: \mathbb{R}^n \rightarrow \mathbb{R}$ is convex, then 
      \[\Var[f(X_1, \ldots, X_n)] \geq (b - a)^2 \mathbb{E}[ ||\nabla f (X_1, \ldots, X_n)||^2]\]
      
      \item Conclude that if $f$ is convex and $L$-Lipschitz, i.e. $|f(x) - f(y)| \leq L ||x - y||$ for all $x, y \in [a, b]^n$, then $\Var[f(X_1, \ldots, X_n)] \geq L^2 (b - a)^2$. 
  \end{enumerate}
  \end{exercise}
  \begin{solution}
  Listed. 
  \begin{enumerate}
      \item Assuming $g$ is differentiable, let us choose any $x, y \in \mathbb{R}$ and define some $z = \lambda x + (1 - \lambda)y$ in between. Then, pictorially, we would like to formally show that 
      \[\frac{f(z) - f(x)}{z - x} \leq \frac{f(y) - f(x)}{y - x}\]
      and take the limit as $z \rightarrow x$ to get $f^\prime(x)$ on the LHS. By definition, we have 
      \[f(z) = f\big( \lambda x + (1 - \lambda) y\big) \leq \lambda f(x) + (1 - \lambda) f(y)\]
      Subtracting $f(x)$ and then dividing by $1 - \lambda > 0$ on both sides gives 
      \[\frac{f(z) - f(x)}{1 - \lambda} \leq f(y) - f(x)\] 
      Note that $z - x = \lambda x + (1 - \lambda y) - x = (1 - \lambda)(y - x)$. So, dividing by $y - x > 0$ on both sides gives 
      \[\frac{f(z) - f(x)}{z - x} \leq \frac{f(y) - f(x)}{y - x}\]
      and taking the limit on the LHS gives 
      \[f^\prime (x) = \lim_{z \rightarrow x} \frac{f(z) - f(x)}{z - x} \leq \frac{f(y) - f(x)}{y - x}\]
      Since $y - x > 0$, we can multiply both on the same side to get 
      \[f(y) - f(x) \geq f^\prime (x) \, (y - x)\]
      If $y < x$, then the proof is the same, and the inequality sign ends up getting switched around twice, leading to the same conclusion. 
      
      \item Note that from the above result, we can multiply both sides by $-1$ to get that $g(x) - g(y) \leq g^\prime (x) (x - y)$ for all $x, y \in \mathbb{R}$, and then swap the two variables to get $g(y) - g(x) \leq g^\prime (y) (y - x)$. Let us consider fixed $x_1, \ldots, x_n$ and some $i \in [n]$. Given $f: \mathbb{R}^n \rightarrow \mathbb{R}$, we define $f_i (\mathbf{x}): \mathbb{R} \rightarrow \mathbb{R}$ by unfixing the $i$th variable. Then, given some $\alpha, \beta \in [a, b]$, 
      \[f_i (\mathbf{x}) (\beta) - f_i (\mathbf{x}) (\alpha) \leq g^\prime (\beta) (\beta - \alpha)\]
      or equivalently, 
      \[f(x_1, \ldots, \beta, \ldots, x_n) - f(x_1, \ldots, \alpha, \ldots, x_n) \leq \frac{\partial f}{\partial x_i} (x_1, \ldots, \beta, \ldots, x_n) \; (\beta - \alpha)\]
      Now let $z^\ast \in [a, b]$ be the value s.t. 
      \[z^\ast = \arg \min_{z \in [a, b]} f(x_1, \ldots, z, \ldots, x_n) \]
      Then, 
      \[D_i^- f(\mathbf{x}) = f(x_1, \ldots, x_i, \ldots x_n) - f(x_1, \ldots, z^\ast, \ldots, x_n) \leq \frac{\partial f}{\partial x_i} (x_1, \ldots, x_i, \ldots, x_n) \; (x_i - z^\ast)\]
      and so 
      \[\big( D_i^- f(\mathbf{X}) \big)^2 \leq \nabla_i f (\mathbf{x})^2 \, (x_i - z^\ast)^2 \leq \nabla_i f (\mathbf{x})^2 \, (b - a)^2\]
      which gives from the bounded difference inequality 
      \begin{align*}
          \Var[f(X_1, \ldots, X_n)] & \leq \mathbb{E} \bigg[ \sum_{i=1}^n \big( D_i^- f(X_1, \ldots, X_n) \big)^2 \bigg] \\
          & \leq \mathbb{E} \bigg[ \sum_{i=1}^n \nabla_i f (\mathbf{x})^2 \, (b - a)^2 \bigg] \\
          & = (b - a)^2 \mathbb{E} \big[ \big| \big| \nabla f(\mathbf{X})\big| \big|^2 \big]
      \end{align*}
      
      \item If $f$ is $L$-lipschitz, then $||\nabla f(\mathbf{X})|| \leq L$, and so  
      \[\Var[f(X_1, \ldots, X_n)] \leq (b - a)^2 L^2\]
  \end{enumerate}
  \end{solution}

\subsection{Markov Semigroups}

  \begin{definition}[Markov Process]
  Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space and $(S, \mathcal{S})$ be a measurable space. A homogeneous Markov process $\{X_t\}_{t \geq 0}$ is a stochastic process that satisfies the \textbf{Markov property}: for every bounded measurable function $f$ and $s, t \geq 0$, there exists a bounded measurable function $P_s f$ satisfying 
  \[\mathbb{E}[f (X_{t + s}) \mid \{X_r\}_{r \leq t}] = (P_s f) (X_t) = \mathbb{E}[ f(X_{t + s}) \mid X_t]\]
  \end{definition}

  \begin{definition}[Stationary Measure]
  A probability measure $\mu$ is called \textbf{stationary} or \textbf{invariant} if 
  \[\mathbb{E}_\mu[f] = \mathbb{E}_\mu [P_t f] \text{ i.e. } \int_S f \,d \mu = \int_S P_t f d\mu\]
  for all $t \geq 0$ and bounded measurable $f$. By abusing notation, this is conventionally written 
  \[\mu(f) = \mu(P_t f)\]
  \end{definition}

  To interpret this notion, suppose that $X_0 \sim \mu$. Then, 
  \[\mathbb{E}[f(X_t)] = \mathbb{E}[\mathbb{E}[f(X_t) \mid X_0]] = \mathbb{E}[P_t f (X_0)] = \mathbb{E}_\mu [P_t f]\]
  and if $\mu$ is stationary, then we have $\mathbb{E}[f(X_t)] = \mathbb{E}_\mu [f]$. If $f = 1_A$ for some measurable $A \subset S$, then $\mathbb{E}[1_A (X_t)] = \mathbb{P}(X_t \in A)$, and 
  \[\mathbb{P}(X_t \in A) = \mathbb{E}_\mu [1_A] = \int_S 1_A \,d\mu = \int_A d\mu = \mu(A) = \mathbb{P}(X_0 \in A)\]
  which means that the probability that for all $A \in \mathcal{S}$ and all $t \geq 0$, the probability of $X_t$ realizing in $A$ is equivalent to the initial probability of $X_0$ realizing in $A$. This means that the process remains distributed according to the stationary measure $X_t \sim \mu$ for every time $t$. In summary, stationary measures describe the equilibrium or steady-state behavior of the Markov process.  

  From now, given the state space $(S, \mathcal{S})$ we can put a measure $\mu$ on it to get a measure space $(S, \mathcal{S}, \mu)$. The Banach space of all $\mu$-measurable functions $f: (S, \mathcal{S}, \mu) \rightarrow (\mathbb{R}, \mathcal{R})$ (i.e. for every Borel $B \in \mathcal{R}$, $f^{-1}(B) \in \mathcal{S}$) will be denoted $L^p (\mu)$, equipped with the norm 
  \[||f||_{L^p(\mu)} \coloneqq \mathbb{E}_\mu [f^p]^{1/p} = \bigg( \int_S |f|^p \,d\mu \bigg)^{1/p}\]
  If $p = 2$, then we can define the inner product 
  \[\langle f, g \rangle_\mu \coloneqq \mathbb{E}_\mu [f g] = \int_S f g \, d\mu\]

  \begin{lemma}
  Let $\mu$ be a stationary measure. Then, the following hold for all $p \geq 1$, $t, s \geq 1$, $\alpha, \beta \in \mathbb{R}$, and bounded measurable functions $f, g$. 
  \begin{enumerate}
      \item Contraction: 
      \[||P_t f||_{L^p(\mu)} \leq ||f||_{L^p (\mu)} = \mathbb{E}_\mu [f^p]^{1/p}\]
      
      \item Linearity: 
      \[P_t (\alpha f + \beta g) = \alpha P_t f + \beta P_t g\] 
      
      \item Semigroup Property: 
      \[P_{t + s} f = P_t P_s f\]
      
      \item Conservativeness: 
      \[P_t 1 = 1\]
  \end{enumerate}
  \end{lemma}

  \begin{lemma}
  Let $\mu$ be a stationary measure. Then, $t \mapsto \Var_\mu [P_t f]$ is a decreasing function of time for every function $f \in L^2 (\mu)$. 
  \end{lemma}
  \begin{proof}
  Note that 
  \begin{align*}
      \Var_\mu [P_t f] & = ||P_t f - \mu f||^2_{L^2(\mu) } =  ||P_t (f - \mu f)||^2_{L^2 (\mu)} = ||P_{t - s} P_s (f - \mu f)||^2_{L^2 (\mu)} \\
      & \leq ||P_s (f - \mu f)||^2_{L^2 (\mu)} = ||P_s f - \mu f||^2_{L^2 (\mu)} = \Var_\mu (P_s f)
  \end{align*}
  \end{proof}

  We now define the analogous operator to the transition rate matrix in discrete time chains with a finite state space. 

  \begin{definition}[Generator]
  The generator $\mathscr{L}$ is defined as 
  \[\mathscr{L} f \coloneqq \lim_{t \downarrow 0} \frac{P_t f - f}{t}\]
  for every $f \in L^2 (\mu)$ for which the above limit exists in $L^2 (\mu)$. The set of $f$ for which $\mathscr{L}f$ is defined is called the domain $\mathrm{Dom}(\mathscr{L})$ of the generator, and $\mathscr{L}$ defines a linear operator from $\mathrm{Dom}(\mathscr{L}) \subset L^2 (\mu)$ to $L^2 (\mu)$. 
  \end{definition}

  We have defined the generator $\mathscr{L}$ from the Markov semigroup $\{P_t\}_{t \geq 0}$. Now, let's try to define the semigroup in terms of the generator $\mathscr{L}$. Given that we have some map $\mathscr{L})$, can we define some semigroup $\{P_t\}$ satisfying the definition? To do this, we must solve the differential equation: 
  \[\frac{d}{dt} P_t = \lim_{\delta \downarrow 0} \frac{P_{t + \delta} - P_t}{\delta} = \lim_{\delta \downarrow 0} \frac{P_t P_\delta - P_t}{\delta} = P_t \lim_{\delta \downarrow 0} \frac{P_\delta - I}{\delta} = P_t \mathscr{L}\]
  For function $P_t$ to satisfy this differential equation, we have the solution 
  \[P_t = e^{t \mathscr{L}}\]
  which also implies that $\mathscr{L}$ and $P_t$ must commute. 

  \begin{definition}[Reversibility]
  The Markov semigroup $\{P_t\}_{t \geq 0}$ with stationary measure $\mu$ is called \textbf{reversible} if 
  \[\langle f, P_t g \rangle_\mu = \langle P_t f, g \rangle_\mu\]
  for every $f, g \in L^2 (\mu)$. Equivalently, we can say that $P_t$ is self-adjoint on $L^2 (\mu)$, or since $P_t = e^{t \mathscr{L}}$, we have $\mathscr{L}$ is self-adjoint. 
  \end{definition}

  \begin{definition}[Ergodicity]
  The Markov semigroup $\{P_t\}_{t \geq 0}$ with stationary measure $\mu$ if called \textbf{ergodic} if 
  \[P_t f \rightarrow \mu f\]
  in $L^2 (\mu)$ as $t \rightarrow +\infty$ for every $f \in L^2 (\mu)$. Note that $\mu f = \mu(f)$ is the constant function in $L^2 (\mu)$. 
  \end{definition}

  \begin{exercise}[Elementary Identities]
  Let $P_t$ be a Markov semigroup with generator $\mathscr{L}$ and stationary measure $\mu$. Prove the following elementary facts. 
  \begin{enumerate}
      \item Show that $\mu( \mathscr{L} f) = 0$ for every $f \in L^2 (\mu)$ 
      \item If $\phi : \mathbb{R} \rightarrow \mathbb{R}$ is convex, then $P_t \phi (f) \geq \phi (P_t f)$ when $f, \phi(f) \in L^2(\mu)$ 
      \item If $\phi : \mathbb{R} \rightarrow \mathbb{R}$ is convex, then $\mathscr{L} \phi(f) \geq \phi^\prime (f) \mathscr{L} f$ when $f, \phi(f) \in L^2 (\mu)$ 
      \item Let $f \in L^2 (\mu)$. Show that the following process is a martingale. 
      \[M_t^f \coloneqq f(X_t) - \int_0^t \mathscr{L} f(X_s) \,ds\]
  \end{enumerate}
  \end{exercise}
  \begin{solution}
  Listed. 
  \begin{enumerate}
      \item This is simply a property of the generator. Not worrying about interchanging limits and integrals, we have 
      \begin{align*}
          \mu(\mathscr{L} f) = \mathbb{E}_\mu [\mathscr{L} f] & = \int_S \lim_{t \downarrow 0} \frac{P_t f - P_0 f}{t} \,d\mu \\ 
          & = \lim_{t \downarrow 0} \int_S \frac{P_t f - P_0 f}{t} \,d\mu \\
          & = \lim_{t \downarrow 0} \frac{1}{t} \big( \mathbb{E}_\mu [P_t f] - \mathbb{E}_\mu [f] \big) = \lim_{t \downarrow 0} \frac{1}{t} \cdot 0 = 0 
      \end{align*}
      
      \item By Jensen's inequality, 
      \begin{align*}
          P_s \phi(f) & = \mathbb{E} [ \phi(f) (X_{t + s}) \mid X_t] \\
          & \geq \phi \bigg( \mathbb{E}[f(X_{t + s} \mid X_t] \big) = \phi(P_s f)
      \end{align*}

  \end{enumerate}
  \end{solution}


\subsection{Poincare Inequalities}

  Recall that a Poincare inequality for $\mu$ is, informally, of the form 
  \[\mathrm{variance}(f) \leq \mathbb{E}_\mu[ ||\mathrm{gradient}(f)||^2 ]\]
  At first sight, such an inequality has nothing to do with Markov processes. However, the validity of a Poincare inequality for $\mu$ turns out to be related to the rate of convergence of an ergodic Markov process for which $\mu$ is the stationary distribution. That is, a measure $\mu$ satisfies a Poincare inequality for a certain notion of gradient if and only if an ergodic Markov semigroup associated to this gradient converges exponentially fast to $\mu$. 

  \begin{definition}[Dirichlet Form]
  Given a Markov process with generator $\mathscr{L}$ and stationary measure $\mu$, the corresponding Dirichlet form is defined as 
  \[\mathcal{E}(f, g) \coloneqq - \langle f, \mathscr{L} g \rangle_\mu\]
  \end{definition}

  \begin{theorem}[Poincare Inequality]
  Let $P_t$ be a reversible ergodic Markov semigroup with stationary measure $\mu$. The following are equivalent given $c \geq 0$. 
  \begin{enumerate}
      \item $\mathrm{Var}_\mu (f) \leq c \mathcal{E}(f, f)$ for all $f$ (Poincare Inequality) 
      \item $||P_t f - \mu f||_{L^2 (\mu)} \leq e^{-t /c} ||f - \mu f||_{L^2 (\mu)}$
      \item $\mathcal{E}(P_t f, P_t f) \leq e^{-2t /c} \mathcal{E}(f, f)$ for all $f, t$
      \item For every $f$ there exists $\kappa (f)$ s.t. $||P_t f - \mu f||_{L^2 (\mu)} \leq \kappa(f) e^{-t/c}$
      \item For every $f$ there exists $\kappa (f)$ s.t. $\mathcal{E}(P_t f, P_t f) \leq \kappa(f) e^{-2t/c}$ 
  \end{enumerate}
  \end{theorem}

  We should view properties 2 through 5 as different notions of exponential convergence of the Markov semigroup $P_t$ to the stationary measure $\mu$. Properties 2 and 4 directly measure the rate of convergence of $P_t f$ to $\mu f$ in $L^2 (\mu)$, while properties 3 and 5 measure the rate of convergence of the "gradient" (now depicted as $\mathcal{E}$) of $P_t f$ to $0$. 

\subsubsection{The Gaussian Poincare Inequality}

  \begin{definition}[Ornstein-Uhlenbeck Process]
  Given standard Brownian motion $(W_t)_{t \geq 0}$, the \textbf{Ornstein-Uhlenbeck process} is defined as 
  \[X_t = e^{-t} X_0 + e^{-t} W_{e^{2t} - 1}\]
  \end{definition}

  \begin{lemma}[Gaussian Integration by Parts]
  If $\xi \sim \mathcal{N}(0, 1)$, then 
  \[\mathbb{E}[ \xi f(\xi)] = \mathbb{E}[f^\prime (\xi)]\]
  \end{lemma}
  \begin{proof}
  Assuming that $f$ is smooth with compact support, we have by integration by parts 
  \begin{align*}
      \mathbb{E}[f^\prime (\xi)] & = \int_{-\infty}^\infty f^\prime(x) \frac{e^{-x^2 / 2}}{\sqrt{2\pi}} \,dx \\ 
      & = \frac{e^{-x^2 / 2}}{\sqrt{2\pi}} \, f(x) \bigg|_{-\infty}^\infty - \int_{-\infty}^\infty f(x) \frac{d}{dx} \bigg(\frac{e^{-x^2 / 2}}{\sqrt{2\pi}}\bigg) \,dx \\
      & = - \int_{-\infty}^\infty -x f(x) \frac{e^{-x^2 / 2}}{\sqrt{2\pi}} \,dx \\
      & = \int_{-\infty}^\infty \big( x f(x)\big) \frac{e^{-x^2 / 2}}{\sqrt{2\pi}}\,dx = \mathbb{E}[\xi f(\xi)]
  \end{align*}
  \end{proof}

  \begin{theorem}
  The Ornstein-Uhlenbeck Process $(X_t)_{t \geq 0}$ 
  \begin{enumerate}
      \item is a Markov process with semigroup 
      \[P_t f(x) = \mathbb{E} \big[ f(e^{-t} x + \sqrt{1 - e^{-2t}} \xi) \big] \text{ with } \xi \sim \mathcal{N}(0, 1)\]
      \item admits $\mu = \mathcal{N}(0, 1)$ as its stationary measure
      \item is ergodic
      \item has generator and Dirichlet form given by 
      \[\mathscr{L} f(x) = -x f^{\prime} (x) + f^{\prime\prime} (x), \;\;\;\;\; \mathcal{E}(f, g) = \langle f^\prime , g^\prime \rangle_\mu\]
      \item is reversible
  \end{enumerate}
  \end{theorem}
  \begin{proof}
  Let $s \geq t$. 
  \begin{enumerate}
      \item By definition of $X_t$, we have $X_t = e^{-t} X_0 + e^{-t} W_{e^{2t - 1}}$ and 
      \[X_s = e^{-s} X_0 + e^{-s} W_{e^{2s} - 1} \implies X_0 = (X_s - e^{-s} W_{e^{2s} - 1} ) e^{s}\]
      Substituting in the equation for $X_s$ gives 
      \begin{align*}
          X_t & = e^{-(t - s)} X_s + e^{-t} (W_{e^{2t} - 1} - W_{e^{2s} - 1}) \\
          & = e^{-(t - s)} X_s + \sqrt{1 - e^{-2 (t - s)}} \xi
      \end{align*}
      where $\xi = (W_{e^{2t} - 1} - W_{e^{2s} - 1}) / \sqrt{e^{2t} - e^{2s}} \sim N(0, 1)$ is independent of $\{X_r\}_{r \leq s}$. Therefore, we can write 
      \[\mathbb{E}[ f(X_t) \mid \{X_r\}_{r \leq s}] = P_{t - s} f (X_s) = \mathbb{E}\big[f \big( e^{-(t - s)} X_s + \sqrt{1 - e^{-2 (t - s)}} \xi \big) \big]\]
      which proves the Markov property and gives the semigroup. 
      
      \item We can clearly see that if $X_t \sim N(0, 1)$, then $X_{t + s} = e^{-s} X_t + \sqrt{1 - e^{-2s}}\xi$ is a sum of Gaussians, one with variance $e^{-2s}$ and the other with variance $1 - e^{-2s}$, and so their sum has variance $1$. 
      
      \item We will take for granted that this is ergodic. 
      
      \item To compute the generator, we use the chain rule (and not worry about whether we take the derivative within the expectation integral) and then use Gaussian integration by parts to get 
      \begin{align*}
          \frac{d}{dt} P_t f(x) & = \mathbb{E} \bigg[ f^\prime (e^{-t} x + \sqrt{1 - e^{-2t}} \xi) \bigg( \frac{e^{-2t}}{\sqrt{1 - e^{-2t}}} \xi - e^{-t} x \bigg) \bigg] \\
          & = \mathbb{E} \big[ e^{-t} x f^\prime (e^{-t} x + \sqrt{1 - e^{-2t}} \xi) + e^{-2t} f^{\prime\prime} (e^{-t} x + \sqrt{1 - e^{-2t}} \xi ) \big]
      \end{align*}
      and therefore have 
      \[\frac{d}{dt} P_t f (x) = \bigg( -x \frac{d}{dx} + \frac{d^2}{dx^2} \bigg) P_t f (x)\]
      The Dirichlet form can be simplified using the Gaussian integration by parts as 
      \begin{align*}
          \mathcal{E} (f, g) & = - \langle f, \mathscr{L} g \rangle_\mu \\
          & = \mathbb{E}[ f(\xi) \big( x g^\prime (\xi) - g^{\prime\prime} (\xi) \big)] \\
          & = \mathbb{E}[\xi f(\xi) g^\prime(\xi)] - \mathbb{E}[f(\xi) g^{\prime\prime} (\xi)] \\
          & = \mathbb{E}[f^\prime (\xi) g^\prime (\xi) + f(\xi) g^{\prime\prime} (\xi)] - \mathbb{E}[f(\xi) g^{\prime\prime} (\xi)] \\
          & = \mathbb{E}[f^\prime (\xi) g^\prime (\xi) ]
      \end{align*} 
      
      \item Since $\mathcal{E}(f, g) = \mathbb{E}[f^\prime(\xi) g^\prime (\xi)]$, it is symmetric and so $\mathscr{L}$ is self-adjoint. 
  \end{enumerate}

  \end{proof}

  From the previous theorem part 4, we can see that 
  \[\mathcal{E}(f, f) = \langle f^\prime, f^\prime \rangle_\mu = ||f^\prime||_{L^2(\mu)}^2 = \mathbb{E}_\mu[ f^{\prime 2} ]\]
  which means that the Dirichlet form of an Ornstein-Uhlenbeck process is precisely the expected square gradient of function $f$! Therefore, with the Poincare inequality, we can bound the variance of $f$ with the Dirichlet form, which is the expected square gradient of $f$. 

  \begin{theorem}
  Let $\mu = \mathcal{N}(0, 1)$. Then, 
  \[\mathrm{Var}_\mu [f] \leq ||f^\prime||_{L^2(\mu)}^2\]
  \end{theorem}
  \begin{proof}
  We have from the properties of the Ornstein-Uhlenbeck process that
  \begin{align*}
      \frac{d}{dx} P_t f(x) & = \frac{d}{dx} \mathbb{E}[ f(e^{-t} x + \sqrt{1 - e^{-2t}} \xi)] \\
      & = \mathbb{E} \bigg[ \frac{d}{dx} f(e^{-t} x + \sqrt{1 - e^{-2t}} \xi)] \\
      & = \mathbb{E}[f^\prime (e^{-t} x + \sqrt{1 - e^{-2t}} \xi) \; e^{-t}] \\
      & = e^{-t} \mathbb{E}[f^\prime (e^{-t} x + \sqrt{1 - e^{-2t}} \xi)] \\
      & = e^{-t} P_t f^\prime (x) 
  \end{align*}
  Thus
  \[\mathcal{E}(P_t f, P_t f) = ||(P_t f)^\prime||_{L^2 (\mu)}^2 = e^{-2t} || P_t f^\prime ||^2_{L^2(\mu)} \leq e^{-2t} ||f^\prime||^2_{L^2(\mu)} = e^{-2t} \mathcal{E}(f, f) \]
  where the inequality follows from contraction. 
  \end{proof}

  By tensorization, we can prove the following. 

  \begin{corollary}[Gaussian Poincare Inequality]
  Let $X_1, \ldots, X_n \sim N(0, 1)$ be iid. Then, 
  \[\Var[ f(X_1, \ldots, X_n)] \leq \mathbb{E}[ || \nabla f (X_1, \ldots, X_n)||^2 ]\]
  \end{corollary}
  \begin{proof}
  Computation. 
  \begin{align*}
      \mathrm{Var}[f(X_1, \ldots, X_n)] & \leq \mathbb{E} \bigg[ \sum_{i=1}^n \mathrm{Var}_i f(X_1, \ldots, X_n) \bigg] \\
      & \leq \mathbb{E} \bigg[ \sum_{i=1}^n \bigg| \bigg| \frac{d}{dx_i} f(X_1, \ldots, X_n)\bigg|\bigg|^2 \bigg] \\
      & = \mathbb{E}[ ||\nabla f (X_1, \ldots, X_n) ||^2 ]
  \end{align*}
  \end{proof}

  So what have we done so far? If we have some distribution $\mu$ and want to prove an inequality that bounds $\Var_\mu [f]$, then we should choose some (reversible ergodic) Markov process that has a stationary distribution $\mu$. We can identify its semigroup, generator, and ultimately its Dirichlet form $\mathcal{E}(f, g)$, which will allow us to invoke the Poincare inequality to bound 
  \[\Var_\mu [f] \leq c \mathcal{E}(f, f)\]
  and since $\mu = N(0, 1)$, we have shown above using both the properties of the generator of the Ornstein-Uhlenbeck process and Gaussian integration by parts that this Dirichlet form is precisely the norm of $f^\prime$. This is clear since the Dirichlet form $\langle f, \mathscr{L} g\rangle_\mu$ only depends on $\mathscr{L}$ and $\mu$. However, the Dirichlet form does not have to be this form. 
  \begin{enumerate}
      \item If $\mu$ is some other distribution, we would not be able to reduce $\mathcal{E}(f, f)$ to the norm of its derivative, and so it make take on a different form. 
      \item If we choose a different Markov process, even with the same stationary measure $\mu = N(0, 1)$, the generator may be different and so will the Dirichlet form. 
  \end{enumerate}

  \begin{exercise}[Carre du Champ]
  We have interpreted the Dirichlet form $\mathcal{E}(f, f)$ as a general notion of “expected square gradient” that arises in the study of Poincare inequalities. There is an analogous quantity $\Gamma(f, f)$ that plays the role of “square gradient” in this setting (without the expectation). In good probabilistic tradition, it is universally known by its French name carre du champ (literally, “square of the field”). The carre du champ is defined as
  \[\Gamma(f, g) \coloneqq \frac{1}{2} \big[ \mathscr{L}(f g) - f \mathscr{L} g - g \mathscr{L} f \big] \]
  in terms of the generator $\mathscr{L}$ of a Markov process with stationary measure $\mu$. 
  \begin{enumerate}
      \item Show that $\mathcal{E}(f, f) = \int \Gamma(f, f) \, d\mu$ and that $\mathcal{E}(f, g) = \int \Gamma(f, g) \,d\mu$ if the Markov process is in addition reversible. 
      \item Show that $\Gamma(f, f) \geq 0$ so it can indeed by interpreted as a square. 
      \item Prove the Cauchy-Schwartz inequality $\Gamma(f, g)^2 \leq \Gamma(f, f) \, \Gamma(g, g)$ 
      \item Compute the carre du champ of the Ornstein-Uhlenbeck process and confirm that it should indeed be interpreted as the appropriate notion of "square gradient." 
  \end{enumerate}
  \end{exercise}
  \begin{solution}
    Listed. 
    \begin{enumerate}
      \item By stationarity, we have 
      \begin{equation}
        \mu ( \mathscr{L} f) = \int_S \mathscr{L} f \, d\mu = 0
      \end{equation}
      for all $f \in L^2 (\mu)$, which reduces the first term below to $0$. So, we can reduce the carre du champ to 
      \begin{align}
        \int_S \Gamma(f, f) \, d\mu & = \frac{1}{2} \bigg( \int_S \mathscr{L} (f^2) \, d\mu - 2 \int_S f \mathscr{L} f \, d\mu \bigg) \\
        & = - \int_S f \mathscr{L} f \, d\mu = - \langle f, \mathscr{L} f \rangle_\mu = \mathcal{E}(f, f)
      \end{align}
      Furthermore, assuming that $P_t$ is reversible, we have 
      \begin{equation}
        \mathcal{E}(f, g) = - \langle f, \mathscr{L} g \rangle_\mu = -\langle \mathscr{L} f, g \rangle_\mu = - \langle g, \mathscr{L} f \rangle_\mu = \mathcal{E}(g, f)
      \end{equation}
      and so 
      \begin{align}
        \int \Gamma (f, g) \, d\mu & = \frac{1}{2} \bigg( \int \mathscr{L}(f g) \, d\mu - \int f \mathscr{L} g \, d\mu - \int g \mathscr{L} f \, d\mu \bigg) \\
        & = \frac{1}{2} \big( - \langle f, \mathscr{L} g \rangle_\mu - \langle g, \mathscr{L} f\rangle_\mu \big) \\
        & = - \langle f, \mathscr{L} g \rangle_\mu = \mathcal{E}(f, g)
      \end{align}
      
      \item Since $\Gamma(f, f) = \frac{1}{2} \big( \mathscr{L} (f^2) - 2 f \mathscr{L}f \big)$, the problem now reduces to proving that $\mathscr{L} (f^2) \geq 2 f \mathscr{L}f$. By Jensen's inequality, we have $P_t (f^2) \geq (P_t f)^2$, and so 
      \begin{align}
        \mathscr{L}(f^2) & = \lim_{t \downarrow 0} \frac{P_t (f^2) - f^2}{t} \geq \lim_{t \downarrow 0} \frac{(P_t f)^2 - f^2}{t} \\
        & = \frac{d}{dt} (P_t f)^2 \bigg|_{t = 0} = \bigg( 2 (P_t f) \cdot \frac{d}{dt} (P_t f) \bigg)\bigg|_{t = 0} = 2 f \mathscr{L} f
      \end{align}
      
      \item We know that $\Gamma(f + t g, f + tg) \geq 0$ from above, and so if we expand out, we get
      \begin{align}
        \Gamma(f + t g, f + tg) & = \frac{1}{2} \Big[ \mathscr{L} \big( (f + t g)^2 \big) - 2 (f + t g) \mathscr{L}(f + t g) \Big] \\
        & = \Gamma(g, g) t^2 + 2 \Gamma (f, g) t + \Gamma(f, f) \geq 0 
      \end{align}
      for all $t$. Since this quadratic is nonnegative, its discriminant must be $\leq 0$, and so 
      \begin{equation}
        \Delta = \big( 2 \Gamma (f, g) \big)^2 - 2 \Gamma(g, g) \Gamma(f, f) \leq 0 \implies \Gamma(f, g)^2 \leq \Gamma(f, f) \Gamma(g, g)
      \end{equation}
      
      \item The generator of the Ornstein-Uhlenbeck process is $\mathscr{L}f(x) = -x f^\prime(x) + f^{\prime\prime} (x)$. Therefore, 
      \begin{align}
        \Gamma (f, g) & (x) = \frac{1}{2} \big[ \mathscr{L} (f g)(x) - f(x) \mathscr{L} g(x) - g(x) \mathscr{L} f (x) \big] \\
        & = \frac{1}{2} \Big[ \big( -x (f g)^\prime (x) + (f g)^{\prime\prime} (x) \big) - f(x) \big( -x g^\prime(x) + g^{\prime\prime} (x) \big) - g(x) \big( -x f^\prime(x) + f^{\prime\prime} (x) \big) \Big]
      \end{align}
      which simplifies down to $f^\prime(x) g^\prime(x)$, and so $\Gamma(f, f) = [ f^\prime(x)]^2$ can be interpreted as the square gradient of $f$. 
    \end{enumerate}
  \end{solution}

\subsection{Variance Identities and Exponential Ergodicity}

  Now, let us develop some intuition on the connection between Markov semigroups, $\Var_\mu [f]$ and the Dirichlet form $\mathcal{E}(f, f)$. 

  \begin{lemma}
  The following identity holds. 
  \[\frac{d}{dt} \Var_\mu [P_t f] = -2 \mathcal{E} (P_t f, P_t f)\]
  \end{lemma}
  \begin{proof}
  By stationarity, $\mu (P_t f) = \mu(f)$, and so 
  \begin{align*}
      \frac{d}{dt} \Var_\mu [P_t f] & = \frac{d}{dt} \big\{ \mu((P_t f)^2) - \mu(P_t f)^2\big\} \\
      & = \frac{d}{dt} \big\{ \mu((P_t f)^2) - \mu( f)^2\big\} = \frac{d}{dt} \mu((P_t f)^2) \\
      & = \frac{d}{dt} \int_S (P_t f)^2 \, d\mu = \int_S \frac{d}{dt} (P_t f)^2 \,d\mu = 2 \int_S (P_t f) \, \frac{d}{dt} P_t f \, d\mu \\
      & = 2 \mathbb{E}_\mu [P_t f, \mathscr{L} (P_t f) ] = 2 \langle P_t f, \mathscr{L} P_t f \rangle_\mu = -2 \mathcal{E}(P_t f, P_t f) 
  \end{align*}
  \end{proof}

  \begin{theorem}
  $\mathcal{E}(f, f) \geq 0$ for every $f$. 
  \end{theorem}
  \begin{proof}
  We know that $t \mapsto \Var_\mu [P_t f]$ is a decreasing function of $t$ (by contraction of $P_t$), so 
  \[\frac{d}{dt} \Var_\mu [P_t f] = - 2 \mathcal{E}(P_t f, P_t f) \leq 0\]
  \end{proof}

  \begin{theorem}
  Suppose that the Markov semigroup is ergodic. Then, we have for every $f$ 
  \[\Var_\mu [f] = 2 \int_0^\infty \mathcal{E}(P_t f, P_t f) \, dt\]
  \end{theorem}


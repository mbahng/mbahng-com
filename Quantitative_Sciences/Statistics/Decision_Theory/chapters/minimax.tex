\section{Minimax Risk}

  The foundational work was done by Wald in 1949 \cite{1949wald}. In practice, minimax estimation isn't something that you actually write out for a model and then solve for an estimator that achieves this minimax risk. It's not really the point of minimax estimation, nor is it really practical. It is a more abstract diagnosis of determining how difficult a certain problem is, similar to classify problems in P or NP in computer science. 

  \begin{definition}[Minimax Risk]
    Given a family of probability distributions $\mathscr{P}$ and an estimator $\theta$, the \textbf{minimax risk} is defined 
    \begin{equation}
      R = \inf_{\hat{\theta}} \sup_{P \in \mathscr{P}} \mathbb{E}_P \left[ L \big( \hat{\theta}, \theta(P) \big)\right] 
    \end{equation}
  \end{definition}

  In essence, we want to do best (inf) in the worst case (sup) scenario. This gives you more insight and quantifies this difficulty. Ideally, this risk might go to $0$ under some asymptotic conditions, and we would like to know the rate at which it does. We will see that for nonparameteric problems, the rate goes something like 
  \begin{equation}
    \left( \frac{1}{n} \right)^{\frac{2 \beta}{2 \beta + d}}
  \end{equation}
  where $\beta$ is some measure of smoothness and $d$ is the dimension. 

  \begin{example}[Gaussian]
    Let $\mathscr{P} = \{N(\theta, 1), \theta \in \mathbb{R}\}$, a family of Gaussians. Then, the sample mean $\hat{\theta} = \bar{X}$ minimizes 
    \begin{equation}
      \inf_{\hat{\theta}} \sup_{\theta} \mathbb{E}[ (\hat{\theta} - \theta)^2 ] 
    \end{equation}
  \end{example}

  \begin{definition}[Kullback-Leibler Divergence]
    Recall that the KL divergence is 
    \begin{equation}
      \mathrm{KL}(P, Q) = \int p \log \frac{p}{q} 
    \end{equation}
  \end{definition}

  \begin{definition}[Total Variation Distance]
    The total variation distance
    \begin{equation}
      \mathrm{TV}(P, Q) = \sup_{A} | P(A) - Q(A) | 
    \end{equation}
    which can be rewritten as 
    \begin{equation}
      \frac{1}{2} \int |P(x) - Q(x)| \,dx
    \end{equation}
    if we take the measurable set $A = \{x \mid p(x) > q(x) \}$. 
  \end{definition}

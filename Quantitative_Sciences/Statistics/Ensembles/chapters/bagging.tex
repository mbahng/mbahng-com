\section{Bagging}

  Let's start off with the simpler of the two. 

  \begin{definition}[Bootstrap Aggregating]
    Given a dataset $\mathcal{D}$ of $N$ samples and a model $\mathcal{M}$, \textbf{bagging} is an ensemble method done with two steps: 
    \begin{enumerate}
      \item \textit{Bootstrap}. Sample $\Tilde{N}$ data points with replacement from $\mathcal{D}$ to get a dataset $\mathcal{D}_1$, and do this $M$ times to get 
      \[\mathcal{D}_1, \mathcal{D}_2, \ldots, \mathcal{D}_M \subset \mathcal{D}\]
      \item \textit{Aggregate}. For each sub dataset $\mathcal{D}_m$, train our model to get the optimal hypothesis $h_{\mathcal{D}_m}^\ast$. We should have $M$ different hypothesis functions, each trained on each sub dataset. 
      \[h_{\mathcal{D}_1}^\ast, h_{\mathcal{D}_2}^\ast, \ldots, h_{\mathcal{D}_M}^\ast\]
    \end{enumerate}
    To predict the output on a new value $\hat{\mathbf{x}}$, we can evaluate all the $h_{\mathcal{D}_m}^\ast (\hat{\mathbf{x}})$ .
  \end{definition}

  Note that the bootstrapping step could be expanded to different types of subsampling. 

  \begin{definition}[Pasting]
    If random subsets (without replacement) are sampled from the original dataset $\mathcal{D}$, then this method is known as \textbf{pasting}. 
  \end{definition}

  \begin{definition}[Random Subspaces]
    When random subsets of the data are drawn as random subsets of the features, then this is known as \textbf{random subspaces}. 
  \end{definition}

  \begin{definition}[Random Patches]
    When random subsets of both the data and the features are chosen, then this is known as \textbf{random patches}. 
  \end{definition}

  Since the whole point of this algorithm is to reduce variance, bagging does not really overfit. 

\subsection{Random Forests}

  \begin{definition}[Random Forests]
    A \textbf{random forest} is a (random patch) bagging algorithm where the component models are decision trees.  
  \end{definition}


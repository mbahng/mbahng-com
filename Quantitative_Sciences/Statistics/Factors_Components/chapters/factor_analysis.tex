\section{Factor Analysis} 

  As we have constantly seen, there are specific themes that run between models. In PCA, we have taken some data $x$ in high-dimension $d$ and reduced it to a lower-dimensional orthogonal representation in $\mathbb{R}^k$. In other words, for some sample $x \in \mathbb{R}^{1 \times d}$, the projection onto its component space $x V_k \in \mathbb{R}^{1 \times k}$ is a more parsimonious representation with respect to some other basis vectors. The $v_1, \ldots, v_k$ are new features that are linear combinations of the old vectors. Are they interpretable? In some cases yes and in most cases no, which is why we also call them \textit{latent variables} that live in a \textit{latent space}. 

  Another type of model that encodes covariates in a latent space are factor models, which was developed by Spearman in 1904 \cite{1904spearman}. The general idea was that we have some $d$-dimensional random vector $x$, and we would like to encode it in a $k$-dimensional random vector $f$, called the \textit{factors}. Since we are trying to compress the data, generally $k < d$. The first thing that comes to mind is to try and compare how the variables $x_i$ and $f_j$ correlate to each other, and this is exactly what Spearman did. 

  Before we get into factor models, let's step back and talk more about latent variable models. Colloquially, we would like to find the distribution of some data, whether it'd be $(x, y)$ supervised tasks or $x$ for unsupervised. For the unsupervised case, say that we have some covariates $x$ and we want to find its true distribution $p^\ast$. In density estimation so far, what we have done is define a family of distributions $\{p_\theta\}$ and optimize the loss by maximizing the MLE or something else. 
  \begin{equation}
    \min_\theta L(p_\theta, p^\ast) = \max_\theta \prod_{i} p_\theta(x^{(i)})
  \end{equation}
  In order to do this we work with explicitly parameterized distribution families (e.g. Gaussian, Gamma, multinomial, etc.), but this is too simple to model complex things in real like (e.g. the distribution of faces).

  Therefore, we consider \textit{implicitly parameterized} probability distributions by ``adding'' a latent distribution $z$, creating the joint distribution $(x, z)$. This may look more complicated, but it captures a much richer family of distributions. For example, we might try modeling $x$ as a function of $z$, and try to learn some function $x = f(z)$. If we have an accurate function $f$, we can do many things. 
  \begin{enumerate}
    \item Given an $x$, we might find the closest point on the image of $f$, perhaps some manifold, as low-rank approximation of $x$. This dimensionality reduction is essentially what PCA does with projections.\footnote{We will in fact extend PCA to probabilistic PCA soon to make it generative.} 
    \item If we can sample from $z$, then we can forward it through $f$ and can sample from $x$, making this is a generative model. 
  \end{enumerate}

  Like we do with everything else in math, we take a look at the simplest case when the class of functions are linear. This is known as \textit{linear latent variable modeling}. 
  \begin{equation}
    X = \mu + W Z + \epsilon
  \end{equation}
  where the noise $\epsilon$ is typically Gaussian and diagonal (but not necessarily the same component-wise variances). 

  Remember that given data matrix $X \in \mathbb{R}^{n \times d}$, we can take the SVD of it $X = U \Sigma V^T$ where the columns of $V$ represent the principal axes. Since $V$ is orthogonal, $V^T V = V V^T = I_d$, and so $X = X V V^T$. This may seem pretty trivial, but in PCA, we did data compression by removing the last columns of $V$ to get $X \approx X V_k V_k^T$. In a sense we can think of $XV_k \in \mathbb{R}^{n \times k}$ as the projection into the component space and then $(X V_k) V_k^T \in \mathbb{R}^{n \times d}$ as the embedding back into the original space. 

  In terms of random variables, we have $x$ having a distribution over $\mathbb{R}^{d}$. Then, $V V^T x$ is our projected random variable that minimizes the expected $L^2$ distance between $x$ and $V V^T x$. 
  
  \begin{definition}[Factor Analysis] 
    Let $x$ be a random variable in $\mathbb{R}^d$. We would like to model it as 
    \begin{equation}
      x - \mu = \Lambda \eta + \epsilon
    \end{equation}
    where $\mu = \mathbb{E}[x]$ simply normalizes the distribution, $\eta \in \mathbb{R}^k$ are the \textbf{factors} that act as latent variables, $\Lambda \in \mathbb{R}^{d \times k}$ is the \textbf{loading matrix} that maps the factors to the samples, and $\epsilon$ is an error term.\footnote{$\eta$ is analogous to $V^T x \in \mathbb{R}^k$. $\Lambda$ is analogous to the embedding $V^T: \mathbb{R}^k \to \mathbb{R}^d$.} We have the following assumptions. 
    \begin{enumerate}
      \item $\mathbb{E}[\eta] = 0$. 
      \item $\Var[\eta] = I_k$ so that the factors are uncorrelated. 
      \item $\Lambda$ and $\epsilon$ are independent. 
      \item Sometimes, $\epsilon$ is assumed to be an isotropic Gaussian. 
    \end{enumerate}
  \end{definition} 

  For simplicity, we will omit the mean term, and it is also common notation to use $L$ and $f$ as the loading matrix and factors. 

  If we were working with the data matrix $X \in \mathbb{R}^{n \times d}$, then we would be using right matrix multiplication, and so our model will look like 
  \begin{equation}
    X = \eta \Lambda + \epsilon
  \end{equation}
  where $\eta \in \mathbb{R}^{n \times k}$, $\Lambda \in \mathbb{R}^{k \times d}, \epsilon \in \mathbb{R}^{n \times d}$. 

  \begin{theorem}[Likelihood]
    
  \end{theorem}

  \begin{lemma} 
    It should be clear to us that $X$ should be Gaussian\footnote{Since linear transformations of Gaussians are Gaussian} and that $\mathbb{E}[X] = \mu$, with 
    \begin{align} 
        \mathrm{Var}[X] & = \mathbb{E}[ (X - \mu)(X - \mu)^T ] \\
                        & = \mathbb{E}[ (W Z + \epsilon) (Z^T W^T + \epsilon^T)] \\
                        & = \mathbb{E}[W z z^T W^T] + \mathbb{E}[ \epsilon \epsilon^T] \\
                        & = W \mathbb{E}[ z z^T] W^T + \mathbb{E}[ \epsilon \epsilon^T] \\
                        & = W W^T + \mathrm{diag}(\sigma_1^2, \ldots, \sigma_d^2) 
    \end{align} 
    The $W, \mu$, and $\sigma_i$'s can be estimated using MLE methods. 
  \end{lemma}

\subsection{Degrees of Freedom}

  
\subsection{Differences from Linear Regression and PCA} 

  

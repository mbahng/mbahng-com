\section{Hypothesis Testing}

  This was done for the first time as early as 1710, when a researcher tried to measure sex ratios in a population \cite{1710arbuthnot}. 
  
  A significance test is a method used to decide whether the data at hand sufficiently supports a particular hypothesis. The hypothesis to be tested is called the \textbf{alternative hypothesis}, denoted $H_1$ or $H_a$, and the status quo is called the \textbf{null hypothesis}, denoted $H_0$. Assuming that $H_0$ is true, we compute the likelihood of the data happening. If the sample is not too unlikely (past some significance level), we fail to reject $H_0$, and if there is strong evidence, we reject $H_0$. $H_0$ and $H_a$ can be devised in countless ways. 

  \begin{example}
    There are countless test statistics we can build, but here are some common examples, 
    \begin{enumerate}
      \item Proportion: Company A produces circuit boards, but 10\% of them are defective. Company B claims that they produce fewer defective circuit boards. 
      \begin{equation}
        H_0 : \, p = 0.10 \text{ versus } H_a : \, p < 0.10
      \end{equation}
      
      \item Means: It is known that the average height of boys in KIS is 176cm. Ben claims that the average height is lower than this. 
      \begin{equation}
        H_0 : \, \mu = 176 \text{ versus } H_a : \, \mu < 176
      \end{equation}
      
      \item Difference of Means: If $\mu_1$ and $\mu_2$ denote the true average breaking strengths of the same type of twine produced by two different companies. Jenny claims that the $\mu_1 - \mu_2 > 5$. 
      \begin{equation}
        H_0 : \, \mu_1 - \mu_2 = 0 \text{ versus } H_a : \, \mu_1 - \mu_2 > 5
      \end{equation}
    \end{enumerate}
  \end{example}

\subsection{One Sample Z and T Tests}

  Let us have some population $X \sim P$ and a null hypothesis that claims $H_0 : \, \mu = \theta_0$. Since we are interested in the mean, we would like to use CLT or some other theorem to determine what the distribution of the mean of $n$ samples $\overline{x}_n$ looks like (either Normal or Student T centered around $\theta_0$ and scaled down by factor of $\sqrt{n}$). When we actually sample, the value $\overline{x}_n = \hat{\theta}$ is realized, and we would like to see if sampling $\hat{\theta}$ from the distribution centered around $\theta_0$ is likely, usually after normalizing. If it isn't, then we reject $H_0$. 

  How do we decide whether to use the z-test or the t-test? It is known that $\mathrm{StudentT}(n-1)$ converges to $\mathcal{N}(0, 1)$ in distribution as $n \rightarrow +\infty$. Therefore, depending on the context of the problem, at a certain point $N$ (usually $N = 30$ or perhaps higher for skewed distributions), the difference between these two are negligible. 
  \begin{enumerate}
    \item Z-test: if we know the population variance $\sigma^2$, but it is rarely the case that we actually know $\sigma^2$. 
    \item T-test: if we do not know the population variance $\sigma^2$, which we then substitute for the sample variance $S^2$. 
    \item Z-test: if we do not know the population variance (which we substitute for $S^2$), but our sample size is greater than $N$, then we can approximate the $t$-distribution with our normal, allowing us to use the Z-test again. 
  \end{enumerate}

  In general, the alternative to the null hypothesis $H_0 : \, \theta = \theta_0$ will looks like one of the following three assertions: 
  \begin{enumerate}
    \item Two-Sided Test: $H_a : \, \theta \neq \theta_0$ 
    \item One-Sided Test: $H_a : \, \theta > \theta_0$ (in which case the null hypothesis is $\theta \leq \theta_0$) 
    \item One-Sided Test: $H_a : \, \theta < \theta_0$ (in which case the null hypothesis is $\theta \geq \theta_0$) 
  \end{enumerate}

  Now we must still quantify \textit{how} unlikely our sample mean $\theta$ must be compared to $\theta_0$ in order to reject the null hypothesis. This is where we specify our \textbf{significance level}, denoted by $\alpha$ (common values $0.10, 0.05, 0.01$). This specifies the tail-regions in which $\theta$ will land in with probability $\alpha$. Usually, working with general normal/t distributions is tedious, so we can rescale them and use their z/t-scores. 

  \begin{definition}[Z-score]
    Given a value $x$ sampled from distribution $X \sim \mathcal{N}(\mu, \sigma^2)$, its \textbf{z-score} is defined to be the number of standard deviations away from the mean. 
    \begin{equation}
      z \coloneqq \frac{x - \mu}{\sigma}
    \end{equation}
    Now given a significance level $\alpha \in [0, 1]$, let $z_\alpha$ be the value such that the measure of a standard normal distribution past $z_\alpha$ is $1 - \alpha$ (i.e. the $100\alpha$ percentile). $z_\alpha$ is called the \textbf{critical z-value}.
  \end{definition}

  \begin{definition}[T-score]
    Given a value $x$ sampled from distribution $X \sim \mathrm{StudentT}(n)$, its \textbf{t-score} is defined to be the number of standard deviations away from the mean. 
  \end{definition}

  \begin{example}
    A factory has a machine that dispenses 80mL of fluid in a bottle. An employee believes the average amount of fluid is not 80mL. Using 40 samples, he measures the average amount dispensed by the machine to be 78mL with a sample standard deviation of 2.5. 
    \begin{enumerate}
      \item Let the true mean be $\mu$ and true standard deviation be $\sigma$. The null hypothesis is $H_0 : \, \mu = 80$ and the alternative is $H_1 : \, \mu \neq 80$, making this a two-sided test. 
      
      \item We don't know the true standard deviation $\sigma$, so we must use the sample standard deviation $S$. This requires us to use the $t$-test, but since $n > 30$, we can invoke CLT and state that $\overline{x}_{40}$ is (approximately) Gaussian with mean $\mu$ and standard deviation $S / \sqrt{n}$. So, we use the $z$-test. 
      
      \item At a 95\% confidence level, we have $\alpha = 0.05$, and our rejection region is $(-\infty, z_{0.025}] \cup [z_{0.975}, +\infty)$. Since we are looking at a standard Gaussian, we have by symmetry $z_{0.025} = -1.96$ and $z_{0.975} = 1.96$, and our critical z-value is $z^\ast = 1.96$. 
      
      \item So the z-score for $78$ is 
      \begin{equation}
        z = \frac{\overline{x} - \mu_0}{s / \sqrt{n}} = \frac{78 - 80}{2.5 / \sqrt{40}} = -5.06
      \end{equation}
      which is definitely in the reject region. So this tells us that we can reject the null hypothesis with a 95\% level of confidence. 
    \end{enumerate}
  \end{example}

  \begin{example}
    A company manufactures car batteries with an average life span of 2 or more years. An engineer believes this value to be less. Using 10 samples, he measures the average life span to be 1.8 years with a standard deviation of 0.15. 
    \begin{enumerate}
      \item Let the true mean be $\mu$ and true standard deviation be $\sigma$. The null hypothesis is $H_0: \, \mu \geq 2$ and the alternative is $H_1 : \, \mu < 2$, making this a one-sided test. 
      
      \item We don't know the true standard deviation $\sigma$, so we must use the sample standard deviation $S$. This requires us to use the $t$-test, especially since $n = 10$ is not large enough for us to invoke CLT. 
      
      \item At a 99\% confidence level, we have $\alpha = 0.01$, and our rejection region is $(-\infty, t_{0.01}] = (-\infty, -2.82]$. 
      
      \item The t-score for the observed mean value is 
      \begin{equation}
        t = \frac{\overline{x} - \mu_0}{s / \sqrt{n}} = \frac{1.8 - 2}{0.15 / \sqrt{10}} = -4.22
      \end{equation}
      which is definitely in the reject region. So this tells us that we can reject the null hypothesis with a 99\% level of confidence. 
    \end{enumerate}
  \end{example}

  We may have to account for errors. There is always a chance that our evidence leads us to an incorrect conclusion, and we have names for this. 

  \begin{definition}[Errors]
    Given a hypothesis test where we look for evidence supporting our alternative claim, 
    \begin{enumerate}
      \item A \textbf{type 1 error} is when the null hypothesis is rejected, but it is true (false positive). 
      \item A \textbf{type 2 error} is when we fail to reject the null hypothesis, when it is false (false negative). 
    \end{enumerate}
  \end{definition}

\subsection{Power of a test}

\subsection{Common tests (t-test, z-test, chi-square test, F-test)}

\subsection{Multiple testing problem}


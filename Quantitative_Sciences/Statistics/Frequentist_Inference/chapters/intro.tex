Statistics and probability seem like the same topic, but there are very fundamental differences. In probability, we are given some distribution and must compute certain probabilities. In statistics, we are given the results (the data) and must infer what distribution it came from. Some notes to put later. 

The three pillars of frequentist inference is hypothesis testing, point estimation, and confidence intervals. After we establish some fundamental with hypothesis testing, we talk about point estimation, which is the most popular problem in statistics and machine learning. In estimation, the bias variance tradeoff is a central idea. We introduce three methods in creating good point estimators, starting with the most naive method of moments. This is improved with the more modern maximum likelihood estimation, which is most popular for parameteric models. For nonparameteric models, minimax estimation is popular. Then we finish off with confidence intervals for uncertainty quantification. 

Next, we talk about model selection, focusing on the cross validation and information criteria. 


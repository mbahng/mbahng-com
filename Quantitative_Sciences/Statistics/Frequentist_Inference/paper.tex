\documentclass{article}

  % packages
    % basic stuff for rendering math
    \usepackage[letterpaper, top=1in, bottom=1in, left=1in, right=1in]{geometry}
    \usepackage[utf8]{inputenc}
    \usepackage[english]{babel}
    \usepackage{amsmath} 
    \usepackage{amssymb}
    % \usepackage{amsthm}

    % extra math symbols and utilities
    \usepackage{mathtools}        % for extra stuff like \coloneqq
    \usepackage{mathrsfs}         % for extra stuff like \mathsrc{}
    \usepackage{centernot}        % for the centernot arrow 
    \usepackage{bm}               % for better boldsymbol/mathbf 
    \usepackage{enumitem}         % better control over enumerate, itemize
    \usepackage{hyperref}         % for hypertext linking
    \usepackage{fancyvrb}          % for better verbatim environments
    \usepackage{newverbs}         % for texttt{}
    \usepackage{xcolor}           % for colored text 
    \usepackage{listings}         % to include code
    \usepackage{lstautogobble}    % helper package for code
    \usepackage{parcolumns}       % for side by side columns for two column code
    

    % page layout
    \usepackage{fancyhdr}         % for headers and footers 
    \usepackage{lastpage}         % to include last page number in footer 
    \usepackage{parskip}          % for no indentation and space between paragraphs    
    \usepackage[T1]{fontenc}      % to include \textbackslash
    \usepackage{footnote}
    \usepackage{etoolbox}

    % for custom environments
    \usepackage{tcolorbox}        % for better colored boxes in custom environments
    \tcbuselibrary{breakable}     % to allow tcolorboxes to break across pages

    % figures
    \usepackage{pgfplots}
    \pgfplotsset{compat=1.18}
    \usepackage{float}            % for [H] figure placement
    \usepackage{tikz}
    \usepackage{tikz-cd}
    \usepackage{circuitikz}
    \usetikzlibrary{arrows}
    \usetikzlibrary{positioning}
    \usetikzlibrary{calc}
    \usepackage{graphicx}
    \usepackage{caption} 
    \usepackage{subcaption}
    \captionsetup{font=small}

    % for tabular stuff 
    \usepackage{dcolumn}

    \usepackage[nottoc]{tocbibind}
    \pdfsuppresswarningpagegroup=1
    \hfuzz=5.002pt                % ignore overfull hbox badness warnings below this limit

  % New and replaced operators
    \DeclareMathOperator{\Tr}{Tr}
    \DeclareMathOperator{\Sym}{Sym}
    \DeclareMathOperator{\ARMA}{ARMA}
    \DeclareMathOperator{\Span}{span}
    \DeclareMathOperator{\std}{std}
    \DeclareMathOperator{\Cov}{Cov}
    \DeclareMathOperator{\Var}{Var}
    \DeclareMathOperator{\Corr}{Corr}
    \DeclareMathOperator{\pos}{pos}
    \DeclareMathOperator*{\argmin}{\arg\!\min}
    \DeclareMathOperator*{\argmax}{\arg\!\max}
    \newcommand{\ket}[1]{\ensuremath{\left|#1\right\rangle}}
    \newcommand{\bra}[1]{\ensuremath{\left\langle#1\right|}}
    \newcommand{\braket}[2]{\langle #1 | #2 \rangle}
    \newcommand{\qed}{\hfill$\blacksquare$}     % I like QED squares to be black

  % Custom Environments
    \newtcolorbox[auto counter, number within=section]{question}[1][]
    {
      colframe = orange!25,
      colback  = orange!10,
      coltitle = orange!20!black,  
      breakable, 
      title = \textbf{Question \thetcbcounter ~(#1)}
    }

    \newtcolorbox[auto counter, number within=section]{exercise}[1][]
    {
      colframe = teal!25,
      colback  = teal!10,
      coltitle = teal!20!black,  
      breakable, 
      title = \textbf{Exercise \thetcbcounter ~(#1)}
    }
    \newtcolorbox[auto counter, number within=section]{solution}[1][]
    {
      colframe = violet!25,
      colback  = violet!10,
      coltitle = violet!20!black,  
      breakable, 
      title = \textbf{Solution \thetcbcounter}
    }
    \newtcolorbox[auto counter, number within=section]{lemma}[1][]
    {
      colframe = red!25,
      colback  = red!10,
      coltitle = red!20!black,  
      breakable, 
      title = \textbf{Lemma \thetcbcounter ~(#1)}
    }
    \newtcolorbox[auto counter, number within=section]{theorem}[1][]
    {
      colframe = red!25,
      colback  = red!10,
      coltitle = red!20!black,  
      breakable, 
      title = \textbf{Theorem \thetcbcounter ~(#1)}
    } 
    \newtcolorbox[auto counter, number within=section]{proposition}[1][]
    {
      colframe = red!25,
      colback  = red!10,
      coltitle = red!20!black,  
      breakable, 
      title = \textbf{Proposition \thetcbcounter ~(#1)}
    } 
    \newtcolorbox[auto counter, number within=section]{corollary}[1][]
    {
      colframe = red!25,
      colback  = red!10,
      coltitle = red!20!black,  
      breakable, 
      title = \textbf{Corollary \thetcbcounter ~(#1)}
    } 
    \newtcolorbox[auto counter, number within=section]{proof}[1][]
    {
      colframe = orange!25,
      colback  = orange!10,
      coltitle = orange!20!black,  
      breakable, 
      title = \textbf{Proof. }
    } 
    \newtcolorbox[auto counter, number within=section]{definition}[1][]
    {
      colframe = yellow!25,
      colback  = yellow!10,
      coltitle = yellow!20!black,  
      breakable, 
      title = \textbf{Definition \thetcbcounter ~(#1)}
    } 
    \newtcolorbox[auto counter, number within=section]{example}[1][]
    {
      colframe = blue!25,
      colback  = blue!10,
      coltitle = blue!20!black,  
      breakable, 
      title = \textbf{Example \thetcbcounter ~(#1)}
    } 
    \newtcolorbox[auto counter, number within=section]{code}[1][]
    {
      colframe = green!25,
      colback  = green!10,
      coltitle = green!20!black,  
      breakable, 
      title = \textbf{Code \thetcbcounter ~(#1)}
    } 

    \BeforeBeginEnvironment{example}{\savenotes}
    \AfterEndEnvironment{example}{\spewnotes}
    \BeforeBeginEnvironment{lemma}{\savenotes}
    \AfterEndEnvironment{lemma}{\spewnotes}
    \BeforeBeginEnvironment{theorem}{\savenotes}
    \AfterEndEnvironment{theorem}{\spewnotes}
    \BeforeBeginEnvironment{corollary}{\savenotes}
    \AfterEndEnvironment{corollary}{\spewnotes}
    \BeforeBeginEnvironment{proposition}{\savenotes}
    \AfterEndEnvironment{proposition}{\spewnotes}
    \BeforeBeginEnvironment{definition}{\savenotes}
    \AfterEndEnvironment{definition}{\spewnotes}
    \BeforeBeginEnvironment{exercise}{\savenotes}
    \AfterEndEnvironment{exercise}{\spewnotes}
    \BeforeBeginEnvironment{proof}{\savenotes}
    \AfterEndEnvironment{proof}{\spewnotes}
    \BeforeBeginEnvironment{solution}{\savenotes}
    \AfterEndEnvironment{solution}{\spewnotes}
    \BeforeBeginEnvironment{question}{\savenotes}
    \AfterEndEnvironment{question}{\spewnotes}
    \BeforeBeginEnvironment{code}{\savenotes}
    \AfterEndEnvironment{code}{\spewnotes}

    \definecolor{dkgreen}{rgb}{0,0.6,0}
    \definecolor{gray}{rgb}{0.5,0.5,0.5}
    \definecolor{mauve}{rgb}{0.58,0,0.82}
    \definecolor{lightgray}{gray}{0.93}

    % default options for listings (for code)
    \lstset{
      autogobble,
      frame=ltbr,
      language=C,                           % the language of the code
      aboveskip=3mm,
      belowskip=3mm,
      showstringspaces=false,
      columns=fullflexible,
      keepspaces=true,
      basicstyle={\small\ttfamily},
      numbers=left,
      firstnumber=1,                        % start line number at 1
      numberstyle=\tiny\color{gray},
      keywordstyle=\color{blue},
      commentstyle=\color{dkgreen},
      stringstyle=\color{mauve},
      backgroundcolor=\color{lightgray}, 
      breaklines=true,                      % break lines
      breakatwhitespace=true,
      tabsize=3, 
      xleftmargin=2em, 
      framexleftmargin=1.5em, 
      stepnumber=1
    }

  % Page style
    \pagestyle{fancy}
    \fancyhead[L]{Frequentist Statistics}
    \fancyhead[C]{Muchang Bahng}
    \fancyhead[R]{December 2022} 
    \fancyfoot[C]{\thepage / \pageref{LastPage}}
    \renewcommand{\footrulewidth}{0.4pt}          % the footer line should be 0.4pt wide
    \renewcommand{\thispagestyle}[1]{}  % needed to include headers in title page

\begin{document}

\title{Frequentist Statistics}
\author{Muchang Bahng}
\date{December 2022}

\maketitle
\tableofcontents
\pagebreak

  We assume the reader is familiar with measure-theoretic probability, and unlike in introductory probability, we throw away the convention that random variables are written with capital Latin letters (so $x$ can also denote a random variable, which is useful if samples are not fixed). Statistics and probability seem like the same topic, but there are very fundamental differences. In probability, we are given some distribution and must compute certain probabilities. In statistics, we are given the results (the data) and must infer what distribution it came from. 

\section{Foundations}

  \subsection{Sampling Distributions}

    \begin{definition}[Population, Parameters]
      When conducting a statistical study, there is a set of items or events which is of interest for some experiment. This can be modeled with some probability space $(\Omega, \mathcal{F}, \mathbb{P})$. Usually, we are interested in some numerical property of this population, and so we implicitly define a random variable $X: \Omega \longrightarrow \mathbb{R}$ that induces some distribution $X \sim P$, which we call the \textbf{population}. A statistical population can be a group of existing objects or a hypothetical and potentially infinite group of objects conceived as a generalization from experience. 

      With this, we can interpret the population $X \sim P$ as a random variable, and we often call this the \textbf{parent distribution}. We are often interested in its population \textbf{parameters}, which can be any measured quantity of a population that summarizes or describes an aspect of it. In generality, the parameter of population $X$ is denoted $\theta$, and it is a fixed value. 
    \end{definition} 

    \begin{example}[Populations]
      Here are some examples of populations: 
      \begin{enumerate}
        \item We let $\Omega$ be the discrete sample space of all hands in poker, and our random variable will assign a numerical ranking to each hand $0$ for no hand, $1$ for pairs, $2$ for two pairs, etc. 
        \item $\Omega$ is the sample space of all individuals in the U.S. and we can construct a random variable $X$ that assigns to each individual their height. Even though $\Omega$ is finite, we can interpret it as continuous, which leads to a continuous distribution $X \sim P$. 
      \end{enumerate}
    \end{example}

    \begin{example}[Parameters]
      Some population parameters can be: 
      \begin{enumerate}
        \item The true mean of the population $\mu_X = \mathbb{E} [X]$
        \item The true variance of the population $\sigma^2_X = \Var[X]$ 
        \item In the height example, we can set $X = (X_1, X_2, X_3)$ and try to construct a linear regression model that predicts $X_3$ from $X_1, X_2$. Theoretically, this is $\mathbb{E}[X_3 \mid X_1, X_2]$, and we must find the best function of form $x_3 = a + b_1 x_1 + b_2 x_2$ that is closest to the conditional expectation. There does exist a unique one, and so $a, b_1, b_2$ are all population parameters. 
      \end{enumerate}
    \end{example}

    In general, the population is the total set of all relevant things that we are interested in. The specific quantity of the actual population is called the \textbf{population parameter}, e.g. the true mean $\mu$ or the true variance $\sigma^2$ of $X$, usually denoted with $\theta$. But usually, these parameters are not known since the population is too big to experimentally measure, so we must try and estimate it with samples. This is the entire point of statistics; otherwise, we would already know everything we want to know. 

    \begin{definition}[Samples]
      From the population $X \sim P$ (which still has unknown distribution), we can take $n$ \textbf{samples} by considering iid $x_1, x_2, \ldots, x_n \sim P$. 
      \begin{enumerate}
        \item We should note that the samples $x_i$ are random variables themselves. Not fixing them yet and still considering them in generality as random objects allows us to do more theoretical calculations. 
        \item Once these samples have been realized (i.e. $\omega \in \Omega$ is realized, and all $x_i$'s are also realized), we can treat them as fixed values. 
      \end{enumerate}
      Sometimes, we may not assume independence, but for most cases we do. A common rule is that if the sample size $n$ is less than 10\% of the population size, then we can assume independence. 
    \end{definition}

    \begin{definition}[Empirical Distribution]
      Now given that we have these iid samples, we can construct the \textbf{empirical distribution} $\widehat{X} \sim \widehat{P}$, defined as the discrete distribution that assigns probability $1/n$ to each value $x_i$ for $i \in [n]$. In other words, we have 
      \begin{equation}
        \mathbb{P}(\widehat{X} = x) = \frac{1}{n} \text{ for } x \in \{x_1, \ldots, x_n\}
      \end{equation}
      We can write the CDF of the empirical distribution, called the \textbf{empirical distribution function}, as the sum of indicators
      \begin{equation}
        F_X (x) = \frac{1}{n} \sum_{i=1}^n \mathbb{I}_{[x_i, +\infty)} (x)
      \end{equation}
    \end{definition}

    As expected, we would expect the empirical distribution to converge to the actual distribution. 

    \begin{theorem}[Glivenkoâ€“Cantelli theorem]
      The empirical distribution of iid samples $x_1, \ldots, x_n \sim P_n$ converges almost surely to $X \sim P$ as $n \rightarrow \infty$. More specifically, given that the CDF of $X$ is $F$ and the CDF of $P_n$ is the step function $F_n$, we have 
      \begin{equation}
        ||F_n - F||_{\infty} = \sup_{x \in \mathbb{R}} |F_n (x) - F(x)| \rightarrow 0
      \end{equation}
      almost surely as $n \rightarrow \infty$. 
    \end{theorem}

    \begin{example}[Empirical Distribution of Standard Gaussian]
      We expect the empirical distribution of the standard Gaussian to converge. Indeed, numerical results show that for 10 and 100 samples, the empirical CDF does converge to the true CDF. 

      \begin{center}
        \includegraphics[scale=0.4]{img/empirical_distribution.png}
      \end{center}
    \end{example}

  \subsection{Concentration of Measure}

    Now let's move on to concentration inequalities, which say that the probability that a random variable is greater than something is bounded by something. These probability bounds are extremely useful in of themselves. It allows us to talk about convergence theory, which tells us what happens to a statistic, such as $\overline{X}$, as I get more and more data. The first inequality exploits the fact that the tails of a Gaussian RV decay very quickly, and a lot of concentration inequalities attempt to mimic this exponential bound but for non-Gaussian distributions. 

    \begin{theorem}[Gaussian Tail Inequality]
    Given $X \sim \mathcal{N}(0, 1)$, the inequality says that the probability of $X$ taking values past a certain $t$ decays exponentially. 
    \[\mathbb{P} \big( |X| > t \big) \leq \frac{2 e^{-t^2/2}}{t}\]
    If we have $x_1, \ldots, x_n \sim \mathcal{N}(0, 1)$, then 
    \[\mathbb{P} \big( |\overline{X}| > t \big) \leq \frac{2}{\sqrt{n} t} e^{-n t^2/2}\]
    We can assume that the coefficient is less than $1$ if $n$ is large. The above tells us that this bound exponentially decays with $t$ but also with the number of samples $n$. 
    \end{theorem}

    \begin{theorem}[Markov's Inequality]
    Given a nonnegative random variable $X > 0$, we have 
    \[\mathbb{P}(X > t) \leq \frac{\mathbb{E}[X]}{t}\]
    \end{theorem}
    \begin{proof}
    We have 
    \begin{align*}
        \mathbb{E}[X] & = \int_0^\infty x p_X (x)\,dx \\
        & \geq \int_t^\infty x p_X (x) \,dx \\
        & = t \int_t^\infty p_X (x) \,dx \\
        & = t \mathbb{P}(X > t)
    \end{align*}
    \end{proof}

    \begin{theorem}[Chebyshev's Inequality]
    Given a random variable $X$ with mean $\mu = \mathbb{E}[X]$, we have 
    \[\mathbb{P}\big( |x - \mu| > t\big) \leq \frac{\mathrm{Var}(X)}{t^2}\]
    \end{theorem}

    \begin{theorem}[Hoeffding's Inequality]
    Let $x_1, x_2, \ldots, x_n$ be independent (not necessarily identical) random variables s.t. $a_i \leq X_i \leq b_i$ almost surely. Consider the random variable $\overline{X} = \frac{1}{n} (x_1 + \ldots + x_n)$. Then, for all $t > 0$, 
    \[\mathbb{P}\big( \big| \overline{X} - \mathbb{E}[\overline{X}] \big| \geq t \big) \leq 2 \exp \bigg( -\frac{2 n^2 t^2}{\sum_{i=1}^n (b_i - a_i)^2} \bigg)\]
    \end{theorem}

    Now in addition to bounding probabilities, we would like to bound expectations. 

    \begin{theorem}[Cauchy-Schwartz]
    Given random variables $X, Y$, it is often hard to compute the expectation of $X Y$ since it is hard to compute the distribution of it (sums are easy). But we can bound it as 
    \[|\mathbb{E}[XY]| \leq \mathbb{E}[ |XY| ] \leq \sqrt{\mathbb{E}[X] \, \mathbb{E}[Y]}\]
    \end{theorem}

    \begin{theorem}[Jensen's Inequality]
    Given $g$ a convex function and $X$ a random variable, we have 
    \[\mathbb{E}[ g(X)] \geq g (\mathbb{E}[X])\]
    \end{theorem}

    \subsection{Kullback Leibler Divergence}

    Now a popular metric between PMFs/PDFs is the KL divergence. 

    \begin{definition}[Kullback-Leibler Divergence]
    Given random variable $X$ and $Y$, 
    \begin{enumerate}
        \item If they are discrete with PMFs $P$ and $Q$, the KL-divergence is defined 
        \[D_{KL} (P \mid\mid Q) \coloneqq \sum_{x} P(x) \, \log \bigg(\frac{P(x)}{Q(x)} \bigg) = \mathbb{E} \bigg[ \log \frac{P(x)}{Q(x)} \bigg] \]
        where we can interpret the expectation as $X \sim p$. 
        \item If they are continuous with PDFs $p$ and $q$, the KL-divergence is defined 
        \[D_{KL} (p \mid\mid q) \coloneqq \int p(x) \, \log \bigg( \frac{p(x)}{q(x)} \bigg)\,dx = \mathbb{E} \bigg[ \log \frac{P(x)}{Q(x)} \bigg] \]
        where we can interpret the expectation as $X \sim p$. 
    \end{enumerate}
    \end{definition}

    We should prove that this is indeed a metric. 
    \begin{enumerate}
        \item The fact that $D_{KL} (p \mid p) = 0$ is obvious. 
        \item To prove that $D_{KL} (p \mid q) \geq 0$, we use Jensen's inequality
        \[-D_{KL} (p \mid q) = \mathbb{E} \log \frac{q(X)}{p(X)} \leq \log \mathbb{E} \frac{q(X)}{p(X)} = \log \int \frac{q(x)}{p(x)} \, p(x) \, dx = \log(1) = 0\]
        It is a common trick to switch the log and the expectation using Jensen's. 
    \end{enumerate}

    \subsection{Bounding Maximum of Random Variables}

    Given that we have $n$ samples $x_1, \ldots, x_n \sim P$, it is conventional to index them with open brackets to denote order 
    \[X_{(1)} \leq X_{(2)} \leq \ldots \leq X_{(n)}\]
    Our goal is now to find the distribution of $X_{(n)} = \max_i X_i$. If we know the distribution of $P$, $\mathbb{P}(X_{(n)} \leq x)$ is just the probability that all the $X_i$'s are less than $x$, and by independence we can product out the CDFs, differentiate to get the PDF, and compute. So it's not too hard to do this theoretically, but in practice this is hard to do since we don't exactly know $P$. 

    So if you didn't know the $X_i$'s, the best you can assume is that 
    \[\mathbb{E} \max\{x_1, \ldots, x_n\}\]
    is going to grow like $n$. But if we can bound the MGF with $\mathbb{E} e^{t X} \leq e^{t^2 \sigma^2}{2}$, then we can show that $\mathbb{E} \max X_i$ doesn't grow like $n$, but rather like $\log{n}$. 
    \[\mathbb{E} \max{X_i} \leq \sigma \sqrt{2 \log{n}}\]

    \subsection{Big-O, Little-O Notation}

    Going back to calculus, if we have a function $f: X \longrightarrow \mathbb{R}$, we can say that 
    \begin{enumerate}
        \item $f(x) = O(g(x))$ if $f$ is of the same order as $g$. That is, they grow at the same rate 
        \[\frac{f(x)}{g(x)} \rightarrow c \text{ as } x \rightarrow \infty\]
        for some constant $c$. 
        \item $f(x) = o(g(x))$ if $f$ is negligible w.r.t. $g$. That is, $f$ is infinitesimal w.r.t. $g$. 
        \[\frac{f(x)}{g(x)} \rightarrow 0 \text{ as } x \rightarrow \infty\]
        for some constant $c$. 
    \end{enumerate}
    Now there is a probabilistic notation as well. The concept of boundedness translates to being able to capture most of the mass of the random variable within some interval, and infinitesimality translates to the probability mass concentrating around $0$. 

    \begin{definition}[$O_p, o_p$ Notation]
    Let $x_1, x_2, \ldots $ be a sequence of random variables. 
    \begin{enumerate}
        \item $x_n = o_p (1)$ if 
        \[\mathbb{P}(|Y_n| > \epsilon) \rightarrow 0 \text{ as } n \rightarrow \infty\]
        for all $\epsilon > 0$. This means that $x_n$ gets more and more concentrated around $0$. 

        \item $x_n = O_p(1)$ if for all $\epsilon > 0$, then there exists a $C$ s.t. 
        \[\mathbb{P}(|x_n| > C) \leq \epsilon\]
        for all large $n$. That is, we can always trap the majority of the probability mass of $x_n$ within the interval $[-C, C]$. This must hold for all $x_n$ with $n > N$, so the mass can't "escape" to infinity. We can think of it as the distribution is "settling down" and not shooting off to somewhere. 

        \item $x_n = o_p (a_n)$ means that 
        \[\frac{x_n}{a_n} = o_p (1)\]

        \item $x_n = O_p (a_n)$ means that 
        \[\frac{x_n}{a_n} = O_p(1)\]
    \end{enumerate}
    \end{definition}

    \begin{theorem}
    Given $Y_1, \ldots Y_n \sim \mathrm{Bernoulli}(p)$, let $\widehat{p} = \frac{1}{n} \sum Y_i$. Then 
    \[\widehat{p}_n - p = o_p (1)\]
    which is also written $\widehat{p}_n = p + o_p (1)$, which means that the random variable $\widehat{p}_n$ is some constant $p$ plus a random variable that is going to $0$. 
    \end{theorem}
    \begin{proof}
    By Hoeffding's inequality, 
    \[\mathbb{P}(| \widehat{p}_n - p | > \epsilon) \leq 2 e^{-2n \epsilon^2}\]
    which goes to $0$ as $n \rightarrow \infty$. 
    \end{proof}

    \begin{example}
    Given $Y_1, \ldots Y_n \sim \mathrm{Bernoulli}(p)$, let $\widehat{p} = \frac{1}{n} \sum Y_i$. Then, 
    \[\widehat{p} - p = O_p \Big( \frac{1}{\sqrt{n}} \Big)\]
    \end{example}

\section{Point Estimation}

    \begin{definition}[Sample Statistic, Estimators and Estimates]
      Now given a population $X$, we would like to use the $n$ iid samples $x_1, \ldots, x_n$ to estimate a parameter $\theta$ of interest with our own random variable/value $\widehat{\theta}_n$, called a \textbf{sample statistic}. We must note the dual nature of the sample statistic as a random variable and a value is similar to that of samples. 
      \begin{enumerate}
        \item The statistic $\widehat{\theta}_n$ is a random variable itself, referred to as the \textbf{estimator}. More specifically, it is a function $\widehat{\theta}_n: \mathbb{R}^n \longrightarrow \mathbb{R}$ of the $n$ samples, i.e. a transformation of random variables 
        \begin{equation}
          \widehat{\theta}_n = \widehat{\theta}_n (x_1, x_2, \ldots, x_n)
        \end{equation}
        This makes $\widehat{\theta}_n$ also a random variable, which attempts to estimate the true $\theta$, which is some unknown fixed value. Since $\widehat{\theta}_n$ is a random variable, it has its own distribution, called the \textbf{sampling distribution} of $\widehat{\theta}_n$. 
        
        \item Once these samples $x_i$ have been realized, the estimator realizes and the value realized is now called the \textbf{estimate}. 
      \end{enumerate}
      This sampling distribution is a distribution of the statistic $\widehat{\theta}_n$, and this forms a separate distribution with its own mean and variance. 
      \begin{enumerate}
        \item the mean of the sampling distribution is denoted $\mu_{\widehat{\theta}_n}$
        \item the standard deviation of the sampling distribution is denoted $\sigma^2_{\widehat{\theta}_n}$, also called the \textbf{standard error}. 
      \end{enumerate}
    \end{definition}

    We would want these estimators to have three properties: 
    \begin{enumerate}
      \item unbiasedness
      \item consistency 
      \item efficiency
    \end{enumerate}

    We would like the sampling distribution of our statistic to give us good estimate in two ways. $\widehat{\theta}_n$ should not be too far off from the actual parameter $\theta$ (bias is small), and $\widehat{\theta}_n$ should not fluctuate too widely (variance of $\widehat{\theta}_n$ should be small). 

    \begin{definition}[Bias, Variance of Estimator]
      Given an estimator $\widehat{\theta}$ of a sample $x_1, \ldots, x_n$ estimating population parameter $\theta$, the \textbf{sampling bias} refers to 
      \begin{equation}
        \mathrm{Bias}(\widehat{\theta}) = \big| \mathbb{E}[\widehat{\theta}] - \theta \big|
      \end{equation}
      and the \textbf{sampling variance} refers to 
      \begin{equation}
        \mathrm{Var}(\widehat{\theta}) = \mathbb{E} \big[ (\widehat{\theta} - \mathbb{E}[\widehat{\theta}])^2 \big]
      \end{equation}
    \end{definition}

    A good rule of thumb to remember is that statistics is about replacing expectations with averages. 
    \begin{equation}
      \mathbb{E} \mapsto \frac{1}{n} \sum_i
    \end{equation}
    This is really the fundamental quality of statistics. Then after that we can do some fancy things, like minimizing something or manipulating another, but every single time we see an expectation just replace it with an average. 

    \begin{definition}[Sample Mean]
      Given a population $X$ with $\mu = \mathbb{E}[X]$ and $\sigma^2 = \mathrm{Var}(X)$, our estimator for $\mu$ is simply the average of the $n$ samples $x_1, \ldots, x_n$, called the \textbf{sample mean} or the \textbf{sampling distribution of the sample mean}. 
      \begin{equation}
        \overline{x}_n = \widehat{\mu}_n = \frac{1}{n} (x_1 + \ldots + x_n)
      \end{equation}
      This gives us the sampling distribution of the sample means. The mean and standard deviation (i.e. standard error) of $\overline{x}_n$ is denoted $\mu_{\overline{x}_n}$ and $\sigma_{\overline{x}_n}$. 
      \begin{enumerate}
        \item The mean of $\overline{x}_n$ is $\mu$. 
        \begin{equation}
          \mu_{\overline{x}_n} = \mu
        \end{equation}
        because
        \begin{equation}
          \mathbb{E}[\overline{x}_n] = \mathbb{E} \bigg[ \frac{1}{n} \sum_{i=1}^n x_i \bigg] = \frac{1}{n} \sum_{i=1}^n \mathbb{E}[x_i] = \mathbb{E}[x] = \mu
        \end{equation}
        
        \item The variance of $\overline{x}_n$ is $\sigma^2 / n$, i.e. the standard error of $\overline{x}_n$ is $\sigma_{\overline{x}_n} = \sigma / \sqrt{n}$. 
        \begin{equation}
          \sigma_{\overline{x}_n} = \frac{\sigma}{\sqrt{n}}
        \end{equation}
        because 
        \begin{equation}
          \sigma^2_{\overline{x}_n} = \frac{1}{n^2} \sum_{i=1}^n \mathrm{Var}(x_i) = \frac{1}{n} \mathrm{Var}(x) = \frac{\sigma^2}{n}
        \end{equation}
         Practically, this tells us that when trying to estimate the value of a population mean, due to the factor of $1/\sqrt{n}$, reducing the error on the estimate by a factor of $2$ requires acquiring $4$ times as many observations in the sample. But realistically, the true standard deviation $\sigma$ is unknown, and so the standard error of the mean is usually estimated by replacing $\sigma$ with the sample standard deviation $S$ instead. 
        \begin{equation}
          \sigma_{\overline{x}_n} \approx \frac{S}{\sqrt{n}}
        \end{equation}

        \item By CLT, $\overline{x}_n$ converges to $\mathcal{N}(\mu, \sigma^2/n)$ in distribution as $n \rightarrow +\infty$ (but in practicality, we assume this for $n \geq 30$). The fact that its mean and variance is $\mu$ and $\sigma^2 /n$ isn't that impressive. What is really impressive is that no matter what the distribution of $x$ is, the sampling distribution of the mean will be Gaussian. 
      \end{enumerate}
    \end{definition}

    \begin{example}[Sample Means]
      Here are some figures of sample means. Note that with a uniform parent distribution, the sampling distribution of its mean looks like a Gaussian even without a large $n$. However, this is not necessarily true for different parent distributions, such as the exponential. 
      \begin{figure}[H]
        \centering
        \begin{subfigure}[b]{0.48\textwidth}
        \centering
          \includegraphics[width=\textwidth]{img/sample_mean_uniform.png}
          \caption{We plot the PDF of an $X \sim \mathrm{Uniform}[0, 2]$ random variable by taking 100k samples. We also take 100k samples from the sampling distribution of the mean $\overline{X}_{3}, \overline{X}_{10}, \overline{X}_{30}$. We can see that the standard deviation decreases by a factor of $\sqrt{n}$.}
          \label{fig:sample_mean_uniform}
        \end{subfigure}
        \hfill 
        \begin{subfigure}[b]{0.48\textwidth}
        \centering
          \includegraphics[width=\textwidth]{img/sample_mean_exp.png}
          \caption{We plot the PDF of an $X \sim \mathrm{Exponential}(1.5)$ random variable by taking 100k samples. We also take 100k samples from the sampling distribution of the mean $\overline{X}_{3}, \overline{X}_{10}, \overline{X}_{30}$. }
          \label{fig:sample_mean_exp}
        \end{subfigure}
        \caption{}
        \label{fig:sample_mean_examples}
      \end{figure}
    \end{example}

    If the parent distribution is normal, then we don't even need CLT to claim that the sampling distribution of the sample mean is normal, since sums of normals are normal. 

    Now the variance of the population is defined to be $\sigma^2 = \mathbb{E}[ (X - \mathbb{E}[X])^2 ]$, and by our rule of thumb, we can replace the expectations with sample means, by first setting $\mathbb{E}[X] = \widehat{\mu}$ and averaging out the values $(X - \widehat{\mu})^2$. 

    \begin{definition}[Sample Variance]
      Given a population $X$, our estimator for $\sigma^2 = \mathbb{E}[ (X - \mathbb{E}[X])^2 ]$ is simply the average of the squared distances of the $n$ samples $\{(x_i - \widehat{\mu})^2\}_{i=1}^n$. 
      \begin{equation}
        S^2_n = \widehat{\sigma}^2_n = \frac{1}{n} \sum_{i=1}^n ( x_i - \overline{x}_n)^2
      \end{equation}
      The mean and standard deviation of $S^2_n$ is denoted $\mu_{S^2_n}$ and $\sigma_{S^2_n}$. Note that there is a small difference that the sum for variance is divided by $n-1$ rather than $n$, since we want it to be unbiased, but we will correct this later. 
    \end{definition}

    While the CLT states that the sampling distribution of the sample mean will look approximately Gaussian, we do not have this luxury when looking at the sampling distribution of sample variance. 

    \begin{example}[Sample Variance]
      Take a look at the following sampling distributions of the sample variance. There does not seem to be strong signs of convergence to a Gaussian. Their means do not align either. 
      \begin{figure}[H]
        \centering
        \begin{subfigure}[b]{0.48\textwidth}
        \centering
          \includegraphics[width=\textwidth]{img/sample_variance_uniform.png}
          \caption{}
          \label{fig:sample_variance_uniform}
        \end{subfigure}
        \hfill 
        \begin{subfigure}[b]{0.48\textwidth}
        \centering
          \includegraphics[width=\textwidth]{img/sample_variance_exp.png}
          \caption{}
          \label{fig:sample_variance_exp}
        \end{subfigure}
        \caption{}
        \label{fig:sample_variance_examples}
      \end{figure}
    \end{example}

  \subsection{Sampling from Gaussians}

    Now if we assume that the parent distribution is Gaussian, then we can conclude some extra things and more kinds of distributions arise. Let $x_1, \ldots, x_n \sim \mathcal{N}(\mu, \sigma^2)$, with $\overline{x}_n$ the sample mean and $S^2_n$ the sample variance. Say that we want to find the distribution of $\overline{x}_n$. 
    \begin{enumerate}
      \item In the unrealistic case where we know the true $\sigma^2$, we don't even need to consider the sample variance. From the basic property of Gaussians, we know that $\overline{x}_n \sim \mathcal{N}(\mu, \sigma^2/n)$, or after standardizing, 
      \begin{equation}
        \frac{\overline{x}_n - \mu}{\sigma/\sqrt{n}} \sim \mathcal{N}(0, 1)
      \end{equation}
      \item In the realistic case where we don't know the true $\sigma^2$, we should replace it with our sample variance $S^2$, and it turns out that because of this extra uncertainty in the variance, our sampling distribution follows the student-t distribution, which can be interpreted as a mixture of Gaussians with differing variances. 
      \begin{equation}
        \frac{\overline{x}_n - \mu}{S/\sqrt{n}} \sim \mathrm{StudentT}(n-1)
      \end{equation}
    \end{enumerate}
    Now if we are interested in finding the distribution of $S^2_n$: 
    \begin{enumerate}
      \item In the unrealistic case where the know the true $\mu$, we don't need to consider the sampling distribution of $\overline{x}_n$. We have 
      \begin{equation}
        S^2_n = \frac{1}{n} \sum_{i=1}^n (x_i - \mu)^2 \sim \mathrm{Gamma}\Big( \frac{n}{2}, \frac{n}{2 \sigma^2} \Big)
      \end{equation}
      \item In the realistic case where we don't know $\mu$, we have 
      \begin{equation}
        \frac{n-1}{\sigma^2} S^2_n = \frac{1}{\sigma^2} \sum_{i=1}^n (x_i - \overline{x}_n )^2 \sim \chi^2 (n-1)
      \end{equation}
    \end{enumerate}

  \subsection{Method of moments}

  \subsection{Maximum likelihood estimation}

  \subsection{Least squares estimation}

\section{Confidence Intervals}

    Recall that the central limit theorem says that given a sequence of iid random variables $x_1, \ldots, x_n$ coming from a random variable with true mean $\mu$ and variance $\sigma^2$, the sample mean is similar to a $\mathcal{N}(\mu, \sigma^2 / n)$ random variable. That is, the sample mean converges in distribution 
    \begin{equation}
      \overline{X}_n \xrightarrow{dist} \mathcal{N} \Big( \mu, \frac{\sigma^2}{n} \Big)
    \end{equation}
    as $n \rightarrow \infty$. Another way to state it is that the normalized sample mean is similar to a standard Gaussian. 
    \begin{equation}
      \frac{\overline{x}_n - \mu}{\sigma_{\overline{x}_n}} = \frac{\overline{x}_n - \mu}{\sigma / \sqrt{n}} \xrightarrow{dist} \mathcal{N}(0, 1)
    \end{equation}
    So, given that we have enough samples, I will perfectly understand its fluctuations. Now let's introduce some definitions that will allow us to unify some ideas into simpler notation: the realized value $x$, the number of standard deviations it is away from the mean, and the probability that it takes that value (or more extreme). 

    \begin{definition}[z-score]
      Given a $\mathcal{N}(\mu, \sigma^2)$ distribution, the \textbf{z-score} of a number $x \in \mathbb{R}$ is defined to be the number of standard deviations away from the mean. 
      \begin{equation}
        z = \frac{x - \mu}{\sigma}
      \end{equation}
    \end{definition}

    \begin{definition}[Percentile]
      Given $X \sim \mathcal{N}(0, 1)$ and significance level $\alpha \in [0, 1]$, let us define $q_{\alpha} \in \mathbb{R}$ as the point where 
      \begin{equation}
        \mathbb{P}(X \geq q_{\alpha}) = \alpha
      \end{equation}
      i.e. the $100\alpha$th percentile of the standard normal. Note that given $X \sim \mathcal{N}(0, 1)$, we have 
      \begin{equation}
        \mathbb{P} (|X| > q_{\alpha/2}) = \alpha
      \end{equation}
    \end{definition}

    Now given $x_1, \ldots, x_n$ from a population $X$ with mean $\mu$ and standard deviation $\sigma$, let $\overline{x}_n$ be the sampling distribution of the mean. By virtue of the central limit theorem, we can write
    \begin{equation}
      \mathbb{P} \bigg( \bigg| \frac{\overline{X}_n - \mu}{\sigma / \sqrt{n}} \bigg| \geq q_{\alpha/2} \bigg) \approx \alpha \iff \mathbb{P} \bigg( \bigg| \frac{\overline{X}_n - \mu}{\sigma \sqrt{n}} \bigg| \leq q_{\alpha/2} \bigg) \approx 1 - \alpha
    \end{equation}
    which implies that with probability $1 - \alpha$, we have 
    \begin{equation}
      \overline{X}_n \in \bigg[ \mu - q_{\alpha/2} \frac{\sigma}{\sqrt{n}}, \mu + q_{\alpha/2} 
      \frac{\sigma}{\sqrt{n}} \bigg] \iff \mu \in \bigg[ \overline{X}_n - q_{\alpha/2} \frac{\sigma}{\sqrt{n}}, \overline{X}_n + q_{\alpha/2} \frac{\sigma}{\sqrt{n}} \bigg]
    \end{equation}
    This is how we construct a confidence interval. In other words, as $n$ becomes large (ideally at least $30$), the probability that an interval around our sample mean contains the actual mean $\mu$ can be approximated by a Gaussian. But note that CI requires to know the actual standard deviation $\sigma$. There are three ways to deal with this: 
    \begin{enumerate}
      \item This may actually be known from the start, especially if we are working with calibrated devices with standard devices that have been experimentally verified.

      \item We can simply bound $\sigma$, depending on what kind of random variable we are working with. For example, given $X \sim \mathrm{Bernoulli}(p)$, its standard deviation is bounded by $\sigma = \sqrt{p (1 - p)} \leq \frac{1}{2}$, so we can create a confidence interval that is larger than any other confidence interval we can make if we had known the true $\sigma$. 
      \begin{equation}
        p \in \bigg[ \overline{X}_n - q_{\alpha/2} \, \frac{1}{2 \sqrt{n}}, \overline{X}_n + q_{\alpha/2} \,\frac{1}{2 \sqrt{n}} \bigg]
      \end{equation}

      \item We can approximate $\sigma$ with the sample standard deviation $S$, which turns out to be an unbiased estimator. 
    \end{enumerate}

    \begin{example}[Proportion of Right-Side Kissers]
      We have observed $80$ out of $124$ right-side kisses, resulting in a sample estimate of $\widehat{p} = 0.645$. Given that we want a confidence interval of $95\%$, we want an $\alpha = 0.05$, implying a the value $q_{\alpha/2} = q_{0.025} = 1.96$. So, with probability $0.95$, we have 
      \begin{equation}
        p \in \bigg[ 0.645 - \frac{1.96}{2 \sqrt{124}}, 0.645 + \frac{1.96}{2 \sqrt{124}} \bigg] = [ 0.56, 0.73 ]
      \end{equation}
      If we had, say $3$ observations, rather than $124$, we would have a $95\%$ confidence interval of $p \in [0.10, 1.23]$, which is terrible, but in this case even CLT is not valid. 
    \end{example}

    \begin{example}[Proportion of Voters]
      Given that we sample $n = 100$ people from a city's population to ask whether they support candidate A or B, we have $54$ people who support candidate $A$, so $\widehat{p} = 0.54$. Say that we want a 95\% confidence interval, which leads to $q_{\alpha /2} = q_{0.025} = 1.96$. So, with probability $0.95$, we have 
      \begin{equation}
        p \in \bigg[ 0.54 - 1.96\,\frac{\sigma}{\sqrt{100}}, 0.54 + 1.96\,\frac{\sigma}{\sqrt{100}} \bigg]
      \end{equation}
      and by substituting $\sigma$ for $S = \sqrt{0.54 (1 - 0.54)} \approx 0.5$, we get 
      \begin{equation}
        p \in \bigg[ 0.54 - 1.96\,\frac{0.284}{\sqrt{100}}, 0.54 + 1.96\,\frac{0.284}{\sqrt{100}} \bigg] = [0.44, 0.64]
      \end{equation}
    \end{example}

    An interpretation of confidence intervals is that if you keep on sampling $\overline{x}$ or $\widehat{p}$ and construct 95\% CIs, then 95\% of the time these intervals will contain the true mean $\mu$ or proportion $p$ (or more if we had bounded the CI with a bigger interval). 

    \begin{example}
      We survey 6250 teachers to ask whether they think computers are essential for teaching. 250 were randomly selected and 142 felt that they were essential. Let's construct a 99\% confidence interval for the proportion of teachers who felt that computers were essential. We would like to construct a CI for the true $\mu = p$, and we have $\overline{x} = 142/250 = 0.568$. 
      \begin{enumerate}
        \item 99\% confidence corresponds to $\alpha = 0.01$, which corresponds to a z-score of $q_{\alpha/2} = 2.576$. 
        \item The parent distribution is $\mathrm{Bernoulli}(p)$, with $\mu = p$ and $\sigma = \sqrt{p (1 - p)}$. The sampling distribution of $\overline{x}$ has $\mu_{\overline{x}} = p$ also and $\sigma_{\overline{x}} = \sigma / \sqrt{n}$. 
        \item We need to know the details of the sampling distribution, but we don't know $\sigma$, which is needed to calculate $\sigma_{\overline{x}}$. However, we can estimate it using the sample standard deviation $S = \sqrt{0.568 (1 - 0.568)} = 0.5$. 
        \item Our sampling distribution has standard deviation $\sigma_{\overline{x}} \approx S / \sqrt{n} = 0.5 / \sqrt{250} = 0.031$, and our z-score was $2.576$, so our 99\% confidence interval is $2.576$ standard deviations from our mean. That is, with probability $0.99$, 
        \begin{equation}
          p \in \big[ 0.568 - 2.576 \cdot 0.031, 0.568 + 2.576 \cdot 0.031 \big] = \big[ 0.488144, 0.647856 \big]
        \end{equation}
      \end{enumerate}
    \end{example}

  \subsection{CIs for means, proportions, and variances}

  \subsection{Bootstrap confidence intervals}

\section{Hypothesis Testing}

    A significance test is a method used to decide whether the data at hand sufficiently supports a particular hypothesis. The hypothesis to be tested is called the \textbf{alternative hypothesis}, denoted $H_1$ or $H_a$, and the status quo is called the \textbf{null hypothesis}, denoted $H_0$. Assuming that $H_0$ is true, we compute the likelihood of the data happening. If the sample is not too unlikely (past some significance level), we fail to reject $H_0$, and if there is strong evidence, we reject $H_0$. $H_0$ and $H_a$ can be devised in countless ways. 

    \begin{example}
      There are countless test statistics we can build, but here are some common examples, 
      \begin{enumerate}
        \item Proportion: Company A produces circuit boards, but 10\% of them are defective. Company B claims that they produce fewer defective circuit boards. 
        \begin{equation}
          H_0 : \, p = 0.10 \text{ versus } H_a : \, p < 0.10
        \end{equation}
        
        \item Means: It is known that the average height of boys in KIS is 176cm. Ben claims that the average height is lower than this. 
        \begin{equation}
          H_0 : \, \mu = 176 \text{ versus } H_a : \, \mu < 176
        \end{equation}
        
        \item Difference of Means: If $\mu_1$ and $\mu_2$ denote the true average breaking strengths of the same type of twine produced by two different companies. Jenny claims that the $\mu_1 - \mu_2 > 5$. 
        \begin{equation}
          H_0 : \, \mu_1 - \mu_2 = 0 \text{ versus } H_a : \, \mu_1 - \mu_2 > 5
        \end{equation}
      \end{enumerate}
    \end{example}

  \subsection{One Sample Z and T Tests}

    Let us have some population $X \sim P$ and a null hypothesis that claims $H_0 : \, \mu = \theta_0$. Since we are interested in the mean, we would like to use CLT or some other theorem to determine what the distribution of the mean of $n$ samples $\overline{x}_n$ looks like (either Normal or Student T centered around $\theta_0$ and scaled down by factor of $\sqrt{n}$). When we actually sample, the value $\overline{x}_n = \hat{\theta}$ is realized, and we would like to see if sampling $\hat{\theta}$ from the distribution centered around $\theta_0$ is likely, usually after normalizing. If it isn't, then we reject $H_0$. 

    How do we decide whether to use the z-test or the t-test? It is known that $\mathrm{StudentT}(n-1)$ converges to $\mathcal{N}(0, 1)$ in distribution as $n \rightarrow +\infty$. Therefore, depending on the context of the problem, at a certain point $N$ (usually $N = 30$ or perhaps higher for skewed distributions), the difference between these two are negligible. 
    \begin{enumerate}
      \item Z-test: if we know the population variance $\sigma^2$, but it is rarely the case that we actually know $\sigma^2$. 
      \item T-test: if we do not know the population variance $\sigma^2$, which we then substitute for the sample variance $S^2$. 
      \item Z-test: if we do not know the population variance (which we substitute for $S^2$), but our sample size is greater than $N$, then we can approximate the $t$-distribution with our normal, allowing us to use the Z-test again. 
    \end{enumerate}

    In general, the alternative to the null hypothesis $H_0 : \, \theta = \theta_0$ will looks like one of the following three assertions: 
    \begin{enumerate}
      \item Two-Sided Test: $H_a : \, \theta \neq \theta_0$ 
      \item One-Sided Test: $H_a : \, \theta > \theta_0$ (in which case the null hypothesis is $\theta \leq \theta_0$) 
      \item One-Sided Test: $H_a : \, \theta < \theta_0$ (in which case the null hypothesis is $\theta \geq \theta_0$) 
    \end{enumerate}

    Now we must still quantify \textit{how} unlikely our sample mean $\theta$ must be compared to $\theta_0$ in order to reject the null hypothesis. This is where we specify our \textbf{significance level}, denoted by $\alpha$ (common values $0.10, 0.05, 0.01$). This specifies the tail-regions in which $\theta$ will land in with probability $\alpha$. Usually, working with general normal/t distributions is tedious, so we can rescale them and use their z/t-scores. 

    \begin{definition}[Z-score]
      Given a value $x$ sampled from distribution $X \sim \mathcal{N}(\mu, \sigma^2)$, its \textbf{z-score} is defined to be the number of standard deviations away from the mean. 
      \begin{equation}
        z \coloneqq \frac{x - \mu}{\sigma}
      \end{equation}
      Now given a significance level $\alpha \in [0, 1]$, let $z_\alpha$ be the value such that the measure of a standard normal distribution past $z_\alpha$ is $1 - \alpha$ (i.e. the $100\alpha$ percentile). $z_\alpha$ is called the \textbf{critical z-value}.
    \end{definition}

    \begin{definition}[T-score]
      Given a value $x$ sampled from distribution $X \sim \mathrm{StudentT}(n)$, its \textbf{t-score} is defined to be the number of standard deviations away from the mean. 
    \end{definition}

    \begin{example}
      A factory has a machine that dispenses 80mL of fluid in a bottle. An employee believes the average amount of fluid is not 80mL. Using 40 samples, he measures the average amount dispensed by the machine to be 78mL with a sample standard deviation of 2.5. 
      \begin{enumerate}
        \item Let the true mean be $\mu$ and true standard deviation be $\sigma$. The null hypothesis is $H_0 : \, \mu = 80$ and the alternative is $H_1 : \, \mu \neq 80$, making this a two-sided test. 
        
        \item We don't know the true standard deviation $\sigma$, so we must use the sample standard deviation $S$. This requires us to use the $t$-test, but since $n > 30$, we can invoke CLT and state that $\overline{x}_{40}$ is (approximately) Gaussian with mean $\mu$ and standard deviation $S / \sqrt{n}$. So, we use the $z$-test. 
        
        \item At a 95\% confidence level, we have $\alpha = 0.05$, and our rejection region is $(-\infty, z_{0.025}] \cup [z_{0.975}, +\infty)$. Since we are looking at a standard Gaussian, we have by symmetry $z_{0.025} = -1.96$ and $z_{0.975} = 1.96$, and our critical z-value is $z^\ast = 1.96$. 
        
        \item So the z-score for $78$ is 
        \begin{equation}
          z = \frac{\overline{x} - \mu_0}{s / \sqrt{n}} = \frac{78 - 80}{2.5 / \sqrt{40}} = -5.06
        \end{equation}
        which is definitely in the reject region. So this tells us that we can reject the null hypothesis with a 95\% level of confidence. 
      \end{enumerate}
    \end{example}

    \begin{example}
      A company manufactures car batteries with an average life span of 2 or more years. An engineer believes this value to be less. Using 10 samples, he measures the average life span to be 1.8 years with a standard deviation of 0.15. 
      \begin{enumerate}
        \item Let the true mean be $\mu$ and true standard deviation be $\sigma$. The null hypothesis is $H_0: \, \mu \geq 2$ and the alternative is $H_1 : \, \mu < 2$, making this a one-sided test. 
        
        \item We don't know the true standard deviation $\sigma$, so we must use the sample standard deviation $S$. This requires us to use the $t$-test, especially since $n = 10$ is not large enough for us to invoke CLT. 
        
        \item At a 99\% confidence level, we have $\alpha = 0.01$, and our rejection region is $(-\infty, t_{0.01}] = (-\infty, -2.82]$. 
        
        \item The t-score for the observed mean value is 
        \begin{equation}
          t = \frac{\overline{x} - \mu_0}{s / \sqrt{n}} = \frac{1.8 - 2}{0.15 / \sqrt{10}} = -4.22
        \end{equation}
        which is definitely in the reject region. So this tells us that we can reject the null hypothesis with a 99\% level of confidence. 
      \end{enumerate}
    \end{example}

    We may have to account for errors. There is always a chance that our evidence leads us to an incorrect conclusion, and we have names for this. 

    \begin{definition}[Errors]
      Given a hypothesis test where we look for evidence supporting our alternative claim, 
      \begin{enumerate}
        \item A \textbf{type 1 error} is when the null hypothesis is rejected, but it is true (false positive). 
        \item A \textbf{type 2 error} is when we fail to reject the null hypothesis, when it is false (false negative). 
      \end{enumerate}
    \end{definition}

  \subsection{Power of a test}

  \subsection{Common tests (t-test, z-test, chi-square test, F-test)}

  \subsection{Multiple testing problem}

\section{Regression Analysis}

    Now we will talk about regression analysis from a statistical point of view. Regression can be used to approximate the relationship between two random variables (through a smooth function) and can be used for casual inference. Essentially, linear regression attempts to model the conditional distribution $Y \mid X$. 

  \subsection{Ordinary Least Squares}

    If we use a squared loss function, this is called \textbf{ordinary least squares}. It is a well known fact that the true regressor that minimizes this loss is 
    \begin{equation}
      f^\ast (x) = \mathbb{E}[Y \mid X = x]
    \end{equation}
    which is the conditional expectation of $Y$ given $X$. This is the true regressor function, which is the best approximation of $Y$ over the $\sigma$-algebra generated by $X$. This may or may not be linear. 

    \begin{theorem}[Least Squares Solution For Linear Regression]
      Given the design matrix $\mathbf{X}$, we can present the linear model in vectorized form: 
      \begin{equation}
        \mathbf{Y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}, \; \boldsymbol{\epsilon} \sim N(\mathbf{0}, \sigma^2 \mathbf{I})
      \end{equation}
      The solution that minimizes the squared loss is 
      \begin{align*}
        \boldsymbol{\beta} & = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{Y} \in \mathbb{R}^d \\
        \Var(\hat{\boldsymbol{\beta}}) & = \hat{\sigma}^2 (\mathbf{X}^T \mathbf{X})^{-1} \in \mathbb{R}^{d \times d}
      \end{align*}
    \end{theorem}
    \begin{proof}
      The errors can be written as $\boldsymbol{\epsilon} = \mathbf{Y} - \mathbf{X} \boldsymbol{\beta}$, and you have the following total sum of squared errors: 

      \[S(\boldsymbol{\beta}) = \boldsymbol{\epsilon}^T \boldsymbol{\epsilon} = (\mathbf{Y} - \mathbf{X} \boldsymbol{\beta})^T (\mathbf{Y} - \mathbf{X} \boldsymbol{\beta})\]

      We want to find the value of $\boldsymbol{\beta}$ that minimizes the sum of squared errors. In order to do this, remember the following matrix derivative rules when differentiating with respect to vector $\mathbf{x}$. 
      \begin{enumerate}
        \item $\mathbf{x}^T \mathbf{A} \mapsto \mathbf{A}$
        \item $\mathbf{x}^T \mathbf{A} \mathbf{x} \mapsto 2 \mathbf{A} \mathbf{x}$
      \end{enumerate}
      Now this should be easy. 
      \begin{align*}
          S(\boldsymbol{\beta}) & = \mathbf{Y}^T \mathbf{Y} - \boldsymbol{\beta}^T \mathbf{X}^T \mathbf{Y} - \mathbf{Y}^T \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\beta}^T \mathbf{X}^T \mathbf{X} \boldsymbol{\beta} \\
          & = \mathbf{Y}^T \mathbf{Y} - 2 \mathbf{Y}^T \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\beta}^T \mathbf{X}^T \mathbf{X} \boldsymbol{\beta} \\
          \frac{\partial}{\partial \boldsymbol{\beta}} S(\boldsymbol{\beta}) & = - 2 \mathbf{X}^T \mathbf{Y} + 2 \mathbf{X}^T \mathbf{X} \boldsymbol{\beta}
      \end{align*}
      and setting it to $\mathbf{0}$ gives 
      \[2 \mathbf{X}^T \mathbf{X} \boldsymbol{\beta} - 2 \mathbf{X}^T \mathbf{Y} = 0 \implies \mathbf{X}^T \mathbf{X} \boldsymbol{\beta} = \mathbf{X}^T \mathbf{Y}\]
      and the variance of $\boldsymbol{\beta}$, by using the fact that $\Var[\mathbf{A} \mathbf{X}] = \mathbf{A} \Var[X] \mathbf{A}^T$, is
      \[\Var(\hat{\boldsymbol{\beta}}) =
       (\mathbf{X}^{\prime} \mathbf{X})^{-1} \mathbf{X}^{\prime}
       \;\sigma^2 \mathbf{I} \; \mathbf{X}  (\mathbf{X}^{\prime} \mathbf{X})^{-1}
      = \sigma^2 (\mathbf{X}^{\prime} \mathbf{X})^{-1} (\mathbf{X}^{\prime}
       \mathbf{X})  (\mathbf{X}^{\prime} \mathbf{X})^{-1}
      = \sigma^2  (\mathbf{X}^{\prime} \mathbf{X})^{-1}\]
      But we don't know the true $\sigma^2$, so we estimate it with $\hat{\sigma}^2$ by taking the variance of the residuals. Therefore, we have 
      \begin{align*}
          \boldsymbol{\beta} & = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{Y} \in \mathbb{R}^d \\
          \Var(\hat{\boldsymbol{\beta}}) & = \hat{\sigma}^2 (\mathbf{X}^T \mathbf{X})^{-1} \in \mathbb{R}^{d \times d}
      \end{align*}
    \end{proof}

    Note that we have assumed that $\mathbf{X}^T \mathbf{X}$ was invertible in order for such a solution to be unique, i.e. $\mathbf{X}$ must be full rank. This process breaks down when it isn't invertible, e.g. if there are repetitions in the features (one feature is a linear combination of the others and hence not full column rank). We will talk more about this soon. 

    \begin{definition}[Hat Matrix]
      For convenience of notation, let's call 
      \begin{equation}
        \mathbf{H} = \mathbf{X} (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T 
      \end{equation}
      the $n \times n$ \textbf{hat matrix}, which is essentially a projection of the observed $y_i$'s to the predictions. 
      \begin{equation}
        \hat{\mathbf{y}} = \mathbf{H} \mathbf{y}
      \end{equation}
    \end{definition}

    \begin{lemma}[Properties]
      The hat matrix is an orthogonal projection matrix that projects to the column space of $\mathbf{X}$. 
    \end{lemma}

    Note that this parallels the orthogonal projection of conditional expectation to the true function onto the subspace of $X$ measurable functions. Except that we are not doing this in function space, but rather the sample space $\mathbb{R}^n$. 

    We can also see that the residuals $\hat{\epsilon}_i = y_i - \hat{y}_i$ has the property that 
    \begin{equation}
      \hat{\boldsymbol{\epsilon}} = \mathbf{y} - \hat{\mathbf{y}} = (\mathbf{I}_n - \mathbf{H}) \mathbf{y} 
    \end{equation}

    Now if we look back to the derivative of the loss $S$, we really want to set 
    \begin{equation}
      \mathbf{X}^T (\mathbf{y} - \mathbf{X} \hat{\boldsymbol{\beta}}) = \mathbf{X}^T \hat{\boldsymbol{\epsilon}} = \mathbf{0}
    \end{equation}

  \subsection{Gauss-Markov Theorem}

    At this point, we have only talked about the mathematical properties of the least squares regression, but now let's talk about some statistical properties. In machine learning, we talk about some assumptions (homoscedacity, uncorrelated residuals, etc.), and we now formalize their need. 

    \begin{theorem}[Gauss-Markov Theorem]
      Given a dataset with 
      \begin{enumerate}
        \item mean zero residuals $\mathbb{E}[\epsilon_i] = 0$, i.e. $\mathbb{E}[\mathbf{Y} \mid \mathbf{X}] = \mathbf{X} \boldsymbol{\beta}$. 
        \item homoscedacity $\mathrm{Var}[\epsilon_i] = \sigma^2 < \infty$ for all $i$, 
        \item uncorrelated residuals $\mathrm{Cov}(\epsilon_i, \epsilon_j) = 0$ for all $i \neq j$. This and the previous assumption can be combined into $\mathrm{Cov}[\mathbf{Y} \mid \mathbf{X}] = \sigma^2 \mathbf{I}_n$. 
      \end{enumerate}
      We were concerned with estimating the parameters $\beta_1, \ldots, \beta_d$. Now let's generalize this and consider the problem of estimating, for some known constants $c_1, \ldots, c_{d+1}$, the point estimator 
      \begin{equation}
        \theta = c_1 \beta_1 + c_2 \beta_2 + \ldots + c_d \beta_d + c_{d+1}
      \end{equation}
      Then the estimator 
      \begin{equation}
        \hat{\theta} = c_1 \hat{\beta}_1 + c_2 \hat{\beta}_2 + \ldots + c_d \hat{\beta}_d + c_{d+1}
      \end{equation}
      where $\hat{\beta_i}$ is clearly an unbiased estimator of $\theta$ and it is a linear estimator of $\theta$, i.e. 
      \begin{equation}
        \hat{\theta} = \sum_{i=1}^n b_i y_i
      \end{equation}
      for some known (given $\mathbf{X}$) constants $b_i$. Then, the Gauss-Markov theorem states that the estimator $\hat{\theta}$ has the smallest (best) variance among \textit{all} linear unbiased estimators of $\theta$, i.e. $\hat{\theta}$ is BLUE. 
    \end{theorem}

  \subsection{Analysis of variance (ANOVA)}

\section{Time Series Analysis}

    If we try sticking to linear algebra, we hope to model time series of the form 
    \begin{equation}
      X_t = f(t) + w_t
    \end{equation}
    so that we can decompose to a deterministic process followed by some white noise. There are several ways to approach this, including kernel smoothing, moving average smoothing, or cubic spline smoothing. However, this falls short when you look the residuals. They will follow some pattern that must be removed due to autocorrelation. 

    In linear regression, one of the fundamental assumptions was  independence of errors. Ideally, we would also like independence of features, but this is usually not true (in fact, in extreme cases, multicollinearity can screw us up). The relaxation of these assumptions helps us transition from linear regression to time series analysis. Let's go over some basic things with new terms. 

    \begin{definition}[Time Series]
      A stochastic process 
      \begin{equation}
        \{X_1, \ldots, X_t, \ldots \}
      \end{equation}
      of random variables indexed by time $t$ is a \textbf{time series}. The stochastic behavior of $\{X_t\}$ is determined by specifying the PDF/PMF 
      \begin{equation}
        p(x_{t_1}, \ldots, x_{t_m}) 
      \end{equation}
      for all finite collections of time indices 
      \begin{equation}
        \{(t_1, \ldots, t_m), m < \infty \}
      \end{equation}
      i.e. all finite-dimensional distributions of $X_t$. 
    \end{definition}

    \begin{definition}[White Noise]
      \textbf{White noise} $w_t$ is a random variable indexed by time $t$ satisfying 
      \begin{enumerate}
        \item $\mathbb{E}[w_t] = 0$
        \item $\Var[w_t] = \sigma^2$
        \item $\Cov[w_t, w_s] = 0$ for $s \neq t$. That is, they are uncorrelated but not necessarily independent. 
      \end{enumerate}
      Note that this third condition can be strengthened to independence or uncorrelated Gaussians, which automatically imply independence. 
    \end{definition}

  \subsection{Properties of Processes}

    Now let's define some properties. We will start with the time series analogue of covariance and correlation. 

    \begin{definition}[Autocovariance]
      The \textbf{autocovariance} between two time steps $t, s$ of process $\{X_t\}$ is defined 
      \begin{equation}
        K_X (s, t) = \Cov(X_t, X_s)
      \end{equation}
    \end{definition}

    \begin{definition}[Autocorrelation]
      The \textbf{autocorrelation} is 
      \begin{equation}
        \rho_X (s, t) = \frac{K_X (s, t)}{\sqrt{K_X (s, s) \, K_X (t, t)}}
      \end{equation}
    \end{definition}

    \begin{definition}[Cross Covariance]
      Given two stochastic processes $\{X_t\}, \{Y_t\}$, the \textbf{cross covariance} is 
      \begin{equation}
        K_{XY} (t, s) = \Cov(X_t, Y_s)
      \end{equation}
      and the \textbf{cross correlation} is 
      \begin{equation}
        \rho_{XY} (t, s) = \frac{K_{XY}(t, s)}{K_X (t, s) \, K_Y(s, s)}
      \end{equation}

      It is used to model the correlations between two related products with a certain time lag perhaps.  
    \end{definition}

    \begin{definition}[Stationarity]
      There are two types of stationarity. 
      \begin{enumerate}
        \item A \textbf{weakly stationary} or \textbf{covariance stationary} process means that its mean and autocovariance are invariant to time shifts. That is, for all $r$, 
          \begin{align}
            \mathbb{E}[X_t] & = \mathbb{E}[X_{t+r}] = \mu \\
            \Var[X_t] & = \Var[X_{t+r}] = \sigma_X^2 \\
            K_X (t, s) & = K_X (t + r, s + r) \\
          \end{align}

        \item A \textbf{strongly stationary} process means that any joint distribution function of a finite set of time steps is invariant to time shifts. That is, for any $r > 0$ and  finite collection of time points $t_1, \ldots, t_k$, 
          \begin{equation}
            F(X_{t_1}, \ldots, X_{t_k}) = F(X_{t_1 + r}, \ldots, X_{t_k + r})
          \end{equation}
          where $F$ is the joint pdf and equality means almost everywhere equality. 
      \end{enumerate}
      Clearly, weakly stationary implies strongly stationary, and the difference is that weakly stationary has invariance in the first two moments while strongly stationary holds for all moments. 
    \end{definition}

    \begin{theorem}
      It immediately follows that for a stationary process $X_t$, the autocovariance function can be defined 
      \begin{equation}
        K_X (s, t) = K_X(s - t, 0) = K_X (\tau)
      \end{equation}
      for some difference between the time points, called the lag. From this, we can see that $\Var[X_t] = K_X (0)$, so the autocorrelation can be defined as 
      \begin{equation}
        \rho_X(\tau) = \frac{K_X (\tau)}{K_X (0)}
      \end{equation}
    \end{theorem}

    Stationary time series are very desirable, since if we do parameter estimation, we don't want to estimate parameters that are always changing. For example, in stationary processes, we know that the mean never changes, so we have a bunch of sample points to choose from, and if every wasn't stationary, then every $X_t$ would have its own mean and we won't be able to estimate it. Similarly, we also know that for some fixed $\tau$, the autocorrelation does not change, so we can estimate $K_X (\tau)$ with a bunch of fixed intervals of length $\tau$. Therefore, if we want to test for stationary of a fixed time process, we want to conduct a test where we want to find whether the autocovariance is relatively invariant. This gives us a bit of intuition. 

    \begin{theorem}
      Note the following properties. 
      \begin{enumerate}
        \item $K_X (\tau) = K_X (- \tau)$ 
        \item By Cauchy-Schwartz, $K_X (0)^2 = \Var[X_t] \Var[X_{t+r}] \geq \Cov(X_t, X_{t+r}) = K_X (r)^2$, so $|K_X (\tau)| \leq K_X (0)$. 
      \end{enumerate}
    \end{theorem}

    Therefore, we would like to decompose a general time series to a stationary component and a nonstationary simple component, and do some statistics on the stationary one. 

    \begin{definition}[Joint Stationarity]
      Two processes $X_t, Y_t$, are said to be jointly stationary if both are individually stationary and also if the cross covariance function is also stationary. That is, for all $r$, 
      \begin{equation}
        K_{XY} (t, s) = K_{XY}(t + r, s + r)
      \end{equation}
    \end{definition}

    \begin{definition}[Backshift Operator]
      The backshift operator $B$ acts on time series by 
      \begin{equation}
        B X_t = X_{t-1}
      \end{equation}
      It can be iterated to get $B^k X_t = X_{t-k}$ and can also be inverted to get a forward shift $B^{-k} X_t  = X_{t+k}$. We can just think of this as (not necessarily linear?) operators between the function space of $X$-measurable functions. 
    \end{definition}

    \subsubsection{Estimation}

      We should now try to estimate some parameters of a weakly stationary process. 

      \begin{theorem}[Sampling Distribution of Mean]
        We can already estimate the mean. We should get the mean of the mean and the variance of the mean. 
        \begin{enumerate}
          \item The mean is trivial, since by linearity of expectation we can get 
            \begin{equation}
              \hat{\mu} = \bar{X} = \frac{1}{T} \sum_{t=1}^T X_t
            \end{equation}

          \item The variance is a bit more involved since there are covariance terms, so 
            \begin{align}
              \Var[\bar{X}] & = \Var \bigg( \frac{1}{T} \sum_{t=1}^T X_t \bigg) \\
                            & = \frac{1}{T^2} \sum_{t=1}^T \sum_{s=1}^T \Cov(X_t, X_s) \\
                            & = \frac{1}{T^2} \sum_{t=1}^T \sum_{s=1}^T K_X (|t - s|) \\
                            & = \frac{1}{T} K_X (0) + \frac{2}{T} \sum_{z=1}^{T-1} \Big(1 - \frac{z}{T} \Big) K_X (z)
            \end{align} 
            In the unrealistic situation where the $X_t$'s are uncorrelated, we have $K_X (0) = \sigma^2$ and $K_X (z) = 0$ for all $z > 0$, leaving us with $\sigma^2 / T$. 
        \end{enumerate}
      \end{theorem}

      \begin{theorem}[Sampling Distribution of Autocovariance]
        To estimate the autocovariance of a weakly stationary process, we can define the sample autocovariance function to be
        \begin{equation}
          \hat{K}_X (h) = \frac{1}{T} \sum_{t=1}^{T-h} (X_{t+h} - \bar{X}) (X_t - \bar{X})
        \end{equation}
        Note that we divide by $T$ rather than $T - h$ so that this covariance is positive semidefinite. Note that as $h$ gets bigger, the number of terms in the sum decreases giving less accurate estimation. Similarly, the sample autocorrelation function is 
        \begin{equation}
          \hat{\rho} (h) = \frac{\hat{K}_X (h)}{\hat{K}_X (0)}
        \end{equation}
      \end{theorem}

      The sample cross covariance and cross correlation are 
      \begin{equation}
        \hat{K}_{XY} (h) = \frac{1}{T} \sum_{t=1}^{T-h} (X_{t+h} - \bar{X}) (Y_{t} - \bar{Y}) \text{ and } \hat{\rho}_{XY} (h) = \frac{\hat{K}_{XY}(h)}{\sqrt{\hat{K}_X (0) \, \hat{K}_Y (0)}}
      \end{equation}

      Note that even though we can just plug these formulas and get the sample estimators for any time series, these don't mean anything if they are not stationary. 

    \subsubsection{Detecting White Noise}

      Ultimately, the main goal of time series analysis is to transform the data into a white noise process. We want to first identify trends and patterns in the process, remove them, and hopefully get white noise. To actually detect if we have white noise, one way to do this is to look at the estimated autocorrelation function across $h$. Note that for white noise, we have a spike at $h = 0$ to be $1$ (since it is just the correlation of a variable with itself), and then it drops to $0$ immediately (since by definition, $w_s, w_t$ are uncorrelated). We would like to see this behavior within a certain confidence interval. 

  \subsection{Autoregressive (AR) Processes}

    The assumptions are: 
    \begin{enumerate}
      \item the data must be stationary (though it is not always stationary as it may contain a unit root)
      \item the relationship between the variables and their lagged values must be linear (nonlinear gives large language models like LSTMs)
      \item the error term should be white noise
    \end{enumerate}

    \begin{definition}[Autoregressive Process]
      An \textbf{AR(p)} process encodes causality\footnote{on how a random variable $Y$ is \textit{caused} by another RV $X$.} into the white noise process. It is a stochastic process with mean $0$ and of the form 
      \begin{equation}
        X_t = w_t + \sum_{i=1}^p \phi_i X_{t - i}
      \end{equation}
      where $p$ is the hyperparameter of steps to look back, $w_t$ is white noise with variance $\sigma^2$, and $\phi_i$ are constants $\neq 0$. Using the backshift operator $B$, we can write the AR(p) process as 
      \begin{equation}
        \Phi(B) X_t = w_t
      \end{equation}
      where 
      \begin{equation}
        \Phi(B) = \bigg( 1 - \sum_{i=1}^p \phi_i B^i \bigg)
      \end{equation}
    \end{definition}

    In fact, we have already seen this process many times. 
    
    \begin{example}[AR(p) Processes]
      Consider the following. 
      \begin{enumerate}
        \item AR(O) is simply a white noise process 
          \begin{equation}
            X_t = w_t
          \end{equation}
        \item AR(1) with $\theta = 1$ gives us the formula 
          \begin{equation}
            X_t = X_{t-1} + w_t
          \end{equation}
          which is a random walk. It is also a Markov process and a martingale. 
        \item AR(1) of the form 
          \begin{equation}
            X_t = a + X_{t-1} + w_t
          \end{equation}
          is a random walk with drift. 
        \item AR(2) can be of form 
          \begin{equation}
            X_t = X_{t-1} - 0.2 X_{t-2} + w_t
          \end{equation}
        \item AR(3) can be of form 
          \begin{equation}
            X_t = X_{t-1} - 0.2 X_{t-2} + 0.13 X_{t-3} + w_t
          \end{equation}
      \end{enumerate}
      Occasionally, it may be hard to determine the difference between the difference of AR(p) processes. 
    \end{example}

    \begin{example}[AR(1) Processes]
      Let's focus on the AR(1) process. Later on in linear processes, we see that the AR(1) process has a causal representation as a linear process. 
      \begin{equation}
        X_t = \phi_1 X_{t-1} + w_t = \sum_{i=0}^\infty \phi_1^i w_{t-i}
      \end{equation}
      This is stationary under certain conditions. 
      \begin{enumerate}
        \item If $\phi < 1$, then the series is stationary. 
        \item If $\phi = 1$, this is a random walk which is not stationary. 
        \item If $\phi > 1$, then this process grows exponentially fast. 
      \end{enumerate}
    \end{example}

    Now to determine weak stationarity, let's go back to the equation. Talk about unit root test. 

    \begin{definition}[Augmented Dicky-Fuller Test]
      The Augmented Dickey-Fuller (ADF) test is a statistical test used to determine whether a time series is stationary or not. Here's a step-by-step explanation of how the ADF test is typically implemented:

      \begin{enumerate}
        \item \textbf{Model Specification}. The ADF test is based on an autoregressive model. The general form is:
        
        \begin{equation}
        \Delta Y_t = \alpha + \beta t + \gamma Y_{t-1} + \delta_1 \Delta Y_{t-1} + \cdots + \delta_{p-1} \Delta Y_{t-p+1} + \varepsilon_t
        \end{equation}
        
        Where:
        \begin{itemize}
            \item $\Delta Y_t$ is the first difference of the series at time $t$
            \item $\alpha$ is the constant term
            \item $\beta t$ is the time trend term
            \item $\gamma$ and $\delta$ are coefficients
            \item $\varepsilon_t$ is the error term
            \item $p$ is the lag order
        \end{itemize}

        \item \textbf{Determine the lag order ($p$):}
        \begin{itemize}
            \item This can be done using information criteria like AIC or BIC
            \item Or by starting with a maximum lag and testing down
        \end{itemize}

        \item \textbf{Estimate the model:}

        \begin{equation}
        \Delta X_t = \alpha + \beta t + \gamma X_{t-1} + \sum_{i=1}^{p-1} \delta_i \Delta X_{t-i} + \varepsilon_t
        \end{equation}

        Where $\Delta X_t = X_t - X_{t-1}$ is the first difference of the series. To apply OLS, we rewrite this in matrix form:

        \begin{equation}
        Y = X\beta + \varepsilon
        \end{equation}

        Where:
        \begin{itemize}
            \item $Y$ is an $(n-p) \times 1$ vector of $\Delta X_t$ values
            \item $X$ is an $(n-p) \times (p+2)$ matrix of explanatory variables
            \item $\beta$ is a $(p+2) \times 1$ vector of coefficients $(\alpha, \beta, \gamma, \delta_1, \ldots, \delta_{p-1})$
            \item $\varepsilon$ is an $(n-p) \times 1$ vector of error terms
            \item $n$ is the number of observations
            \item $p$ is the lag order
        \end{itemize}

        The OLS estimator for $\beta$ is given by:

        \begin{equation}
        \hat{\beta} = (X'X)^{-1}X'Y
        \end{equation}

        This estimator minimizes the sum of squared residuals:

        \begin{equation}
        \sum_{t=p+1}^n \varepsilon_t^2 = (Y - X\beta)'(Y - X\beta)
        \end{equation}
        \begin{itemize}
            \item Use Ordinary Least Squares (OLS) to estimate the coefficients of the model
        \end{itemize}

        \item \textbf{Calculate the test statistic:}
        \begin{itemize}
            \item The test statistic is the t-statistic for $\gamma$:
            \begin{equation}
            t = \frac{\hat{\gamma} - 0}{SE(\hat{\gamma})}
            \end{equation}
            Where $\hat{\gamma}$ is the estimated coefficient and $SE(\hat{\gamma})$ is its standard error
        \end{itemize}

        \item \textbf{Determine the critical values:}
        \begin{itemize}
            \item These depend on the sample size and the model specification (whether it includes a constant and/or trend)
            \item They're typically obtained from statistical tables or through simulation
        \end{itemize}

        \item \textbf{Compare the test statistic to the critical values:}
        \begin{itemize}
            \item If the test statistic is less than (more negative than) the critical value, reject the null hypothesis
            \item The null hypothesis is that the series has a unit root (is non-stationary)
        \end{itemize}

        \item \textbf{Interpret the results:}
        \begin{itemize}
            \item If we reject the null, we conclude the series is stationary
            \item If we fail to reject the null, we cannot conclude the series is stationary
        \end{itemize}
      \end{enumerate} 
    \end{definition}

    Once this is settled, our job is now to estimate the parameters. We can use MLE. 
    

  \subsection{Moving Average (MA) Processes}

    The key assumptions are: 
    \begin{enumerate}
      \item The random shocks are white noise, mutually independent and coming from the same distribution with mean $0$ and constant variance. 
    \end{enumerate}

    \begin{definition}[Moving Average Process]
      The \textbf{MA(q)} process is a smoother type of noise than the white noise process. It is expressed by the formula 
      \begin{equation}
        X_t = \sum_{j=1}^q \phi_j w_{t-j} + w_t
      \end{equation}
      for $\phi_j \in \mathbb{R}$. Compared to the AR formula, the MA formula averages over the noise terms $w_t$. It focuses on the ripples of the process; if there is a shock to the process $w_{t-1}$, then that shock is still felt at time $t$ by the term $\phi_1 t_{t-1}$. 

      Alternatively, the MA model can be written as an overall average of both the past and future white noise. 
      \begin{equation}
        X_t = \sum_{j=-q/2}^{q/2} \phi_j w_{t +j}
      \end{equation}
    \end{definition}

    \begin{theorem}
      A nice property of MA(q) is that autocovariance vanishes beyond a certain point. More specifically, it decays \textit{linearly} and vanishes after $q$ steps behind. 
    \end{theorem}

  \subsection{Linear Processes}

    Many time series fall under the category of linear processes. 

    \begin{definition}[Linear Processes]
      A \textbf{linear process} is defined as 
      \begin{equation}
        X_t = \mu + \sum_{j=-\infty}^{+\infty} \theta_j w_{t-j}
      \end{equation}
      which means that every $X_t$ is a linear combination of the terms in the white noise process with some mean $\mu$ added on. To ensure that this series doesn't blow up, we add the constraint that 
      \begin{equation}
        \sum_{j} \theta_j^2 < \infty
      \end{equation}
      However, since we are more interested in causal inference, to use the past to predict the future, we use the form 
      \begin{equation}
        X_t = \mu + \sum_{j=0}^{\infty} \theta_j w_{t-j}
      \end{equation}
    \end{definition}

    In fact, some AR processes are linear processes. 

    \begin{example}[AR(1) as a Linear Process]
      Note that AR(1) has a causal representation as a linear process. We can use the formula $X_t = \theta X_{t-1} + w_t$ and recursively define 
      \begin{equation}
        X_t = \theta(\theta X_{t-2} + w_{t-1}) + w_t = \ldots = \sum_{j=0}^\infty \theta^j w_{t-j}
      \end{equation}
      Going back to analysis, infinite series are just limits. 
      \begin{equation}
        \lim_{N \rightarrow \infty} \sum_{j=0}^N \theta^j w_{t-j} 
      \end{equation}
      So this sum may not converge. Letting $S_N (\theta)$ be defined as above, we can compute that 
      \begin{equation}
        \mathbb{E}[ S_N (\theta)] = 0 \text{ and } \Var[S_N] = \sigma^2 \sum_{j=0}^N \theta^{2j} = \sigma^2 \bigg( \frac{1 - \theta^{2N + 2}}{1 - \theta^2}\bigg)
      \end{equation}
      Thus, if $|\theta| < 1$, then $\Var[S_N (\theta)] \rightarrow \sigma^2 / (1 - \theta^2)$, and if $w_t$ is Gaussian noise, then 
      \begin{equation}
        S_N (\theta) \xrightarrow{d} \mathcal{N} \big( 0, \sigma^2 / (1 - \theta^2) \big)
      \end{equation}
      If $|\theta| = 1$, the series does not converge and is not stationary, and if $|\theta| > 1$, then the random talk will grow exponentially fast. 
    \end{example}

  \subsection{ARMA}

    We can combine both the AR and MA processes to make a more sophisticated model. 

    \begin{definition}[ARMA]
      The time series $X_t$ is an $\ARMA(p, q)$ process if $X_t$ has $0$-mean and if we can write it as 
      \begin{equation}
        X_t = w_t + \sum_{i=1}^p \phi_i X_{t-i} + \sum_{j=1}^q \theta_j w_{t-j}
      \end{equation}
      where $w_t$ is white noise with variance $\sigma^2$ and $\boldsymbol{\phi}, \boldsymbol{\theta}$ do not have any zero elements. Using the backshift operator, we can write it as 
      \begin{equation}
        \Phi(B) X_t = \Theta(B) w_t
      \end{equation}
      where 
      \begin{equation}
        \Phi(B) = \bigg( 1 + \sum_{i=1}^p \phi_i B^i \bigg) \text{ and } \Theta(B) = \bigg( 1 + \sum_{j=1}^q \theta_j B^j \bigg)
      \end{equation}
    \end{definition}

  \subsection{ARIMA}

  \subsection{Other}

    \begin{theorem}[Wold Representation Theorem]
      Any $0$-mean covariance stationary time series $\{X_t\}$ can be decomposed into two time series 
      \begin{equation}
        X_t = V_t + S_t
      \end{equation}
      where 
      \begin{enumerate}
        \item $V_t$ is a linear combination of past variables of $V_t$ with constant coefficients. 
        \item $S_t = \sum_{i=0}^\infty \psi_i \eta_{t-i}$ is an infinite moving average process of error terms, where 
          \begin{enumerate}
            \item $\psi_0 = 1$, $\sum_{i=0}^\infty \psi_i^2 < \infty$. 
            \item $\{\eta_t\}$ is linearly unpredictable white noise, i.e. 
              \begin{align}
                \mathbb{E}[\eta_t] & = 0 \\
                \mathbb{E}[\eta_t^2] & = \sigma^2 \\
                \mathbb{E}[\eta_t \eta_s] & = 0 \text{ for } s \neq t 
              \end{align}
              and $\eta_t$ is uncorrelated with $\{V_t\}$.  
              \begin{equation}
                \mathbb{E}[\eta_t V_s] = 0 \text{ for all } t, s
              \end{equation}
          \end{enumerate}
      \end{enumerate}
    \end{theorem}

    \begin{example}[Construction on Dataset]
      Say that we have data $\{X_t\}_{t=1}^T$ that we want to model and we have evidence that it is covariance stationary. We can do the following. 
      \begin{enumerate}
        \item Initialize a parameter $p$, the number of parameters in the linearly deterministic term of the Wold decomposition of $\{X_t\}$. 
        \item By assumption we would like to estimate the linear projection of $X_t$ on $(X_{t-1}, X_{t-2}, \ldots, X_{t-p})$. 
      \end{enumerate}
      Therefore, let us index the $n$ subseries of length $p+1$ by $y$ and we can write the OLS equation 
      \begin{equation}
        \mathbf{y} = \begin{bmatrix} y_1 \\ \vdots \\ y_n \end{bmatrix}, \;\;\; \mathbf{Z} = \begin{bmatrix} 
          1 & y_0 & y_{-1} & \ldots & y_{-(p-1)} \\
          1 & y_1 & y_{0} & \ldots & y_{-(p-2)} \\
          \vdots & \vdots & \vdots & \ddots & \vdots \\ 
          1 & y_{n-1} & y_{n-2} & \ldots & y_{n-p}
        \end{bmatrix}
      \end{equation}
      and we apply OLS to the problem $\mathbf{y} = \mathbf{Z} \boldsymbol{\beta}$ to give 
      \begin{align}
        \hat{\mathbf{y}} & = \mathbf{Z} (\mathbf{Z}^T \mathbf{Z})^{-1} \mathbf{Z} \mathbf{y} \\
                         & = \hat{P}(Y_t \mid Y_{t-1}, \ldots, Y_{t-p}) \\
                         & = \hat{\mathbf{y}}^{(p)}
      \end{align}
      We can compute the projection residuals 
      \begin{equation}
        \boldsymbol{\epsilon}^{(p)} = \mathbf{y} - \hat{\mathbf{y}}^{(p)}
      \end{equation}
      and apply time series analysis to the sequence $\boldsymbol{\epsilon}^{(p)} = \{ \epsilon^{(p)}_t \}$ to specify a moving average model. 
      \begin{equation}
        \epsilon^{(p)}_t = \sum_{i=0}^\infty \psi_i \eta_{t-i}
      \end{equation}
      yielding $\{\hat{\psi}_j\}$ and $\{\hat{\eta}_t \}$ estimates of parameters and innovations. We then check these estimates and see if they are consistent with the model assumptions. If not, we can add additional legs or modify $p$. 
    \end{example}

    Theoretically, as we increase $p$, the projection of $Y_t$ over the past $p$th history should approach the true linear projection $Y_t$ over the whole history. 
    \begin{equation}
      \lim_{p \rightarrow \infty} \hat{\mathbf{p}}^{(p)} = \hat{\mathbf{y}}
    \end{equation}
    But if $p$ is too large compared to $n$, you run out of freedom to estimate your models. You generally want to have more data than the number of parameters. 

    \begin{definition}[Lag Operator]
      The \textbf{lag operator} $L^k$ simply maps 
      \begin{equation}
        L^k (X_t)= X_{t-k}
      \end{equation}
      Inverses also exist, so $L^{-k} (X_t) = X_{t+k}$. 
    \end{definition}

    Therefore, the Wold representation for a covariance stationary time series $\{X_t\}$ can be expressed as 
    \begin{align}
      X_t & = \sum_{i=0}^\infty \psi_i \eta_{t-i} + V_t \\
          & = \sum_{i=0}^\infty \psi_i L^i (\eta_{t}) + V_t \\
          & = \psi(L) \eta_t + V_t 
    \end{align}
    where $\psi(L) = \sum_{i=0}^\infty \psi_i L^i$. 

  \subsection{Components of time series}

  \subsection{Stationarity and tests for stationarity (including ADF test)}

  \subsection{Autoregressive (AR) models}

  \subsection{Moving average (MA) models}

  \subsection{ARIMA models}

  \subsection{Forecasting techniques}

\section{Advanced Topics}

  \subsection{Generalized Linear Models}

  \subsection{Survival analysis}

  \subsection{Nonparametric methods}

  \subsection{Resampling methods (jackknife, bootstrap)}

\section{Practical Considerations}

  \subsection{Experimental design}

  \subsection{Sample size and power analysis}

  \subsection{Dealing with assumptions violations}

  \subsection{Interpretation and reporting of results}

\section{Cross Validation} 

    We have understood the theoretical foundations of overfitting and underfitting with the bias variance decomposition. But in practice, we don't have an ensemble of datasets; we just have one. Therefore, we don't actually know what the bias, the variance, or the noise is at all. Therefore, how do we actually \textit{know} in practice when we are underfitting or overfitting? Easy. We just split our dataset into 2 different parts: the training set and testing sets. 
    \begin{equation}
      \mathcal{D} = \mathcal{D}_{train} \sqcup \mathcal{D}_{test}
    \end{equation}
    What we usually have is a \textbf{training set} that allows us to train the model, and then to check its performance we have a \textbf{test set}. We would train the model on the training set, where we will always minimize the loss, and then we would look at the loss on the test set. Though we haven't made a testing set, since we know the true model let us just generate more data and use that as our testing set. For each model, we can calculate the optimal $\boldsymbol{\theta}$, which we will denote $\boldsymbol{\theta}^\ast$, according to the \textbf{root mean squared loss}
    \begin{equation}
      h_{\boldsymbol{\theta}^\ast} = \argmin_{h_{\boldsymbol{\theta}}} \sqrt{ \frac{1}{N} \sum_{i=1}^N \big( y^{(i)} - h_{\boldsymbol{\theta}} (\mathbf{x}^{(i)}) \big)^2 }
    \end{equation}
    where division of $N$ allows us to compare different sizes of datasets on equal footing, and the square root ensures that this is scaled correctly. Let us see how well these different order models perform on a separate set of data generated by the same function with Gaussian noise. 

    \begin{figure}[H]
      \centering
      \includegraphics[scale=0.4]{img/Degree_vs_RMS.png}
      \caption{We can see that the RMS decreases monotonically on the training error as more complex functions become more fine-tuned to the data. However, when we have a $9$th degree polynomial the RMS for the testing set dramatically increases, meaning that this model does not predict the testing set well, and performance drops. }
      \label{fig:poly_deg_vs_rms}
    \end{figure}

    Now we know that a more complex model (i.e. that captures a greater set of functions) is not necessarily the best due to overfitting. Therefore, researchers perform \textbf{cross-validation} by taking the training set $(\mathcal{X}, \mathcal{Y})$. We divide it into $S$ equal pieces 
    \begin{equation}
      \bigcup_{s=1}^S D_s = (\mathcal{X}, \mathcal{Y})
    \end{equation}
    Then, we train the model $\mathcal{M}$ on $S-1$ pieces of the data and then test it across the final piece, and do this $S$ times for every test piece, averaging its perforance across all $S$ test runs. Therefore, for every model $\mathcal{M}_k$, we must train it $S$ times, for all $K$ models, requiring $KS$ training runs. If data is particularly scarce, we set $S = N$, called the \textbf{leave-one-out} technique. Then we just choose the model with the best average test performance. 

    The following result shows that cross-validation (data splitting) leads to an estimator with risk nearly as good as the best model in the class. 


    \begin{theorem}[Gyorfi, Kohler, Krzyak, Walk (2002)]
      Let $\mathcal{M} = \{m_h\}$ be a finite class of regression estimators indexed by a parameter $h$, with $m$ being the true risk minimizer, $m_{\hat{h}}$ being the empirical risk minimizer over the whole dataset $\mathcal{D}$, and $m_{H}$ being the empirical risk minimizer over the test set $\mathcal{D}_{\mathrm{test}}$ for ordinary least squares loss. 
      \begin{align}
        m_H & = \argmin_{m_h} \frac{1}{N} \sum_{i \in \mathcal{D}_{\mathrm{test}}} (y_i - m_h(x_i))^2 \\ 
        m_{\hat{h}} & = \argmin_{m_h} \frac{1}{N} \sum_{i \in \mathcal{D}} (y_i - m_h(x_i))^2 
      \end{align}
      If the data $Y_i$ and estimators are bounded by $L$, then for any $\delta > 0$, we have 
      \begin{equation}
        \mathbb{E} \int |m_H (x) - m(x)|^2 \,d\mathbb{P}(x) \leq (1 + \delta) \mathbb{E} \int |m_{\hat{h}} (x) - m(x) |^2 \,d \mathbb{P}(x) + \frac{C (1 + \log{|M|})}{n}
      \end{equation}
      where $c = L^2 (16/\delta + 35 + 19\delta)$. 
    \end{theorem}

    \begin{code}[Minimal Example of Train Test Split in scikit-learn]
      To implement this in scikit-learn, we want to use the \texttt{train\_test\_split} class. We can also set a random state parameter to reproduce results. 
      \begin{lstlisting}
        from sklearn.model_selection import train_test_split 

        # Split into training (80\%) and test (20\%) data 
        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=66)
      \end{lstlisting}
    \end{code}

    However, this process requires a lot of training runs and therefore may be computationally infeasible. Therefore, various \textbf{information criterion} has been proposed to efficiently select a model. 

  \subsection{Leave 1 Out Cross Validation} 

    \subsubsection{Generalized (Approximate) Cross Validation} 

    \subsubsection{Cp Statistic}

  \subsection{K Fold Cross Validation}

  \subsection{Data Leakage}

  \subsection{Information Criterion}



\bibliography{./bibfile}
\bibliographystyle{alpha}
\end{document}

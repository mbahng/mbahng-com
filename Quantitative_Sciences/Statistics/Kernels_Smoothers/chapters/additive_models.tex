\section{Additive Models and Naive Bayes}

  Additive models and naive bayes are both nonparameteric methods, but they are very similar. 

\subsection{Additive Models}

  In the most general case, we want to create nonparametric regression functions of the form 
  \begin{equation}
    Y = f(x_1, \ldots, x_d) + \epsilon 
  \end{equation}
  We've done this for one dimensional case, but we can extend this to multiple dimensions through additive models of the form 
  \begin{equation}
    Y = \sum_j f_j (x_j)  + \epsilon
  \end{equation}
  This gives us very interpretable models where we can clearly see the effect of each covariate on $Y$. Clearly, this is not as flexible as the previous model since they can't capture dependencies, but we can create sub-dependency functions and replace the form above to something like 
  \begin{equation}
    Y = \sum_{i, j} f_{i, j} (x_i, x_j) + \epsilon
  \end{equation}
  giving us more flexible models. 

\subsection{Naive Bayes} 

  Say we are doing a binary classification problem. We treat the features as independent (which is very unrealistic) and model the probability distribution as 
  \begin{equation}
    p(x \mid y = 0) = \prod_{j=1}^d p_j (x_j \mid y = 0), \qquad p(x \mid y = 1) = \prod_{j=1}^d p_j (x_j \mid y = 1)
  \end{equation}

  When we take the log, we can see that it is like the additive model. This turns out to be surprisingly successful. 

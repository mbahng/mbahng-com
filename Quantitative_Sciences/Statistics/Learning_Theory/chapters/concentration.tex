\section{Concentration of Measure}

  Concentration of measure is a tool used to prove a lot of theorems in statistical machine learning. I have another series of notes on this, but we'll stick to the key points. 

  \begin{theorem}[Hoeffding's Inequality]
    Given $X_1, \ldots, X_n$ are iid random variables with $a \leq X_i \leq b$, then for any $\epsilon > 0$, 
    \begin{equation}
      \mathbb{P} \bigg( \bigg| \frac{1}{n} \sum_{i=1}^n X_i - \mathbb{E}[X] \bigg| \geq \epsilon \bigg) \leq 2 \exp \bigg( - \frac{2 n \epsilon^2}{(b - a)^2} \bigg)
    \end{equation}
  \end{theorem}
  \begin{proof}
    
  \end{proof}

  \begin{theorem}[Bernstein's Inequality]
    Let $X_1, \ldots, X_n$ be independent Rademacher random variables. Then for every $\epsilon > 0$, 
    \begin{equation}
      \mathbb{P}\left(\left|\frac{1}{n}\sum_{i=1}^{n}X_i\right| > \varepsilon\right) \leq 2\exp\left(-\frac{n\varepsilon^2}{2(1+\frac{\varepsilon}{3})}\right)
    \end{equation} 
  \end{theorem}

  \begin{definition}[Subgaussian Random Variables]
    A random variable $X$ is \textbf{subgasussian} if 
    \begin{equation}
      \mathbb{E}[e^{\lambda X}] \leq e^{\frac{\lambda^2 \sigma^2}{2}}
    \end{equation}
    Gaussians and bounded random variables are subgaussian.
  \end{definition}

  \begin{lemma}[Bound on Subgaussian Random Variables]
    Given a set of iid subgaussian random variables $X_1, \ldots, X_n$ 
    \begin{equation}
      \mathbb{E}\big[ \max_{1 \leq i \leq d} X_i \big] \leq \sigma \sqrt{2 \log d}
    \end{equation}
  \end{lemma}


  Use Hoeffding to show this. 
  \begin{equation}
    \mathbb{P} (|\hat{R} (f) - R(f)| \geq \epsilon) \leq 2e^{- 2 n \epsilon^2}
  \end{equation}
  In 705 in CMU course. 


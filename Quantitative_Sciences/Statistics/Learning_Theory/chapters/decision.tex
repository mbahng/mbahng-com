\section{Decision Theory}

  How can we choose our loss functions? There are two ways of doing this, either through model assumptions or with domain knowledge. When talking about model assumptions, we assume that the residual distribution is of certain form, and the maximum likelihood formulation leads to a certain loss function. For example, assuming that the residuals are normally distributed leads to the squared loss or Laplacian residuals leads to the absolute value loss. These are just modeling assumptions, and if there are no specific assumptions, we are lost. The other way is through domain expertise which allows us to construct our own loss functions. Fortunately, there is a deeper theory behind the choice of loss functions, known as decision theory, which allows us to define loss functions from the get go rather than assume distributions taking particular forms.\footnote{Credits to Edric for telling me this.}

  \begin{definition}[Misclassification Loss]
    The \textbf{misclassification loss} is defined as 
    \begin{equation}
      L(y, \hat{y}) = \begin{cases} 0 & \text{if } y = \hat{y} \\ 1 & \text{if } y \neq \hat{y} \end{cases}
    \end{equation}
  \end{definition}

  \begin{example}[Misclassification Risk]
    Substituting the misclassification loss function into the risk gives the \textbf{misclassification risk}. 
    \begin{equation}
      R(f) = \mathbb{E} [\mathbbm{1}_{\{Y \neq f(X)\}}] = \mathbb{P}(Y \neq f(X)) 
    \end{equation}
    and therefore our empirical risk is 
    \begin{equation}
      \hat{R} (f) = \frac{1}{n} \sum_{i=1}^n \mathbbm{1}_{\{y^{(i)} \neq f(x^{(i)})\}}
    \end{equation}
    which is just the number of misclassifications over the total number of samples. 
  \end{example}

  However, depending on the context, the loss for misclassification one one label can be quite different from that of another label. Consider the medical example where you're trying to detect cancer. Falsely detecting a non-cancer patient as having cancer is not as bad as falsely detecting a cancer patient as not having cancer. 

  \begin{definition}[Weighted Misclassification Loss]
    The \textbf{loss matrix} $K$ defines the loss that we incur when predicting the $i$th class on a sample with true label $j$. 
    \begin{equation}
      L(y, \hat{y}) = \begin{cases} 0 & \text{ if } y = \hat{y} \\ K_{ij} & \text{ if } y = i \neq j = \hat{y} \end{cases}
    \end{equation}
  \end{definition}

  \begin{definition}[Squared Loss]
    The \textbf{squared loss} is defined as 
    \begin{equation}
      L(y, \hat{y}) = (y - \hat{y})^2
    \end{equation}
  \end{definition}

  \begin{example}[Mean Squared Risk]
    Substituting the squared loss function into the risk gives the \textbf{mean squared risk}. 
    \begin{equation}
      R(f) = \mathbb{E}[(Y - f(X))^2]
    \end{equation}
    and therefore our empirical risk is 
    \begin{equation}
      \hat{R} (f) = \frac{1}{n} \sum_{i=1}^n (y^{(i)} - f(x^{(i)})^2
    \end{equation}
  \end{example}

  \begin{definition}[Absolute Loss]
    The \textbf{absolute loss} is defined as 
    \begin{equation}
      L(y, \hat{y}) = |y - \hat{y}|
    \end{equation}
  \end{definition}


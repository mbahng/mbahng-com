In machine learning, we model our entire method as follows. We start off with a general probability space $(\Omega, \mathcal{F}, \mathbb{P})$. This is our model of the world and everything that we are interested in. There is a measurable function that extracts a set of \textit{variables} from $\Omega$. In unsupervised learning, we simply have $X: \Omega \to \mathcal{X}$, which induces a probability measure on $\mathcal{X}$---say $\mathbb{P}_X$. In supervised learning, this function is the joint random variable $X \times Y: \Omega \to \mathcal{X} \times \mathcal{Y}$. We call the $X$ the \textit{covariates}/\textit{explanatory variable} and $Y$ is called the \textit{response}, \textit{labels}, or \textit{predictor variable}. This also induces a probability measure $\mathbb{P}_{X \times Y}$. At this point, we only care about the variables, and we can forget about $\Omega$. Our job is to estimate the \textit{true data generating distribution} $\mathbb{P}$---$\mathbb{P}_{X}$ or $\mathbb{P}_{X \times Y}$. 

We want to do some statistical inference to estimate $\mathbb{P}_{X \times Y}$. It all boils down to trying to find some function $f$--called a \textit{model}---that does this well, but what this function represents can be very flexible. In density estimation, $M$ can be a family of Gaussians. In regression and classification, it can be a parameteric function $f: \mathcal{X} \to \mathcal{Y}$ that estimates $y$ given $x$. For kernel regression, $\mathcal{M}$ can be a RKHS. To find a function, we must optimize over some set, and so we start off with a \textit{class} of functions $\mathcal{F}$. 

For each $f \in \mathcal{F}$, we need some sort of measure of how good our model is. This is where we can define a loss function with respect to a specific sample. 

\begin{definition}[Loss]
  The \textbf{loss} function between our model $f$ and a sampled point from our true data generating distribution $\mathbb{P}$ is 
  \begin{enumerate}
    \item for supervised learning, 
    \begin{equation}
      L: \mathcal{F} \times (\mathcal{X} \times \mathcal{Y}) \to \mathbb{R}, \qquad L(f(x), y)
    \end{equation} 
    \item for unsupervised learning, 
    \begin{equation}
      L: \mathcal{F} \times \mathcal{X}, \qquad L(f(x)) \to \mathbb{R}
    \end{equation}
  \end{enumerate}
  It is basically a measure of how ``well'' our model fits the data. 
\end{definition} 

So how do we choose our loss? For parameteric models, we can often compute the \textit{likelihood} of a sample $x$ or $(x, y)$ with respect to a model $f \in \mathcal{F}$, and then do some slight modifications (e.g. taking the negative logarithm) to turn it into a proper loss function. In nonparameteric estimation, we use \textit{minimax theory}. The problem of how to choose such a loss is the main topic of study in \textit{statistical decision theory}. 

Now let's assume that we have chosen a loss,. Then, we can take the expected loss over the true data generating distribution, giving us our risk. 

\begin{definition}[Risk]
  The \textbf{risk}, or \textbf{expected risk}, of function $f$ is defined as 
  \begin{equation}
    R(f) = \mathbb{E}_{X \times Y} [ L(Y, f(X))] = \int_{\mathcal{X} \times \mathcal{Y}} L(y, f(x)) \,d\mathbb{P}(x, y)
  \end{equation}
\end{definition}

However, we have integrated over the true data generating distribution, which we do not even know! We must try to approximate this risk. Fortunately, we have a dataset $\mathcal{D} = \{(x^{(i)}, y^{(i)})\}$. We assume that $\mathcal{D}$ is generated by sampling \textit{independently and identically (iid)} from $\mathbb{P}$.\footnote{Now this assumption is quite strong and is almost always not the case, as different data can be correlated, but we will relax this assumption later.} This gives us our \textit{empirical risk}. 

\begin{definition}[Empirical Risk]
  The \textbf{empirical risk} of function $f$ is defined as 
  \begin{equation}
    \hat{R}_n(f) = \frac{1}{n} \sum_{i=1}^n L(y^{(i)}, f(x^{(i)}))
  \end{equation}
\end{definition}

Now it would be great if minimizing the empirical risk---which is all we have---allows us to approximate the true risk with high probability. Note that there are two stages of estimates here. 
\begin{enumerate}
  \item First is that this is an \textit{approximation}. We must justify whether the empirical risk is actually a good approximation of the true risk. There are many theorems---some simple and others convoluted---that provides results on this. 
  \item Second, this approximation occurs with \textit{high probability}. This can be dealt with using concentration of measure. 
\end{enumerate}
This regime is known as \textit{probably approximately correct (PAC) learning}, and this is essentially what statistical learning theory does. We try to find some inequality like 
\begin{equation}
  \mathbb{P}( | \sup_{f \in \mathcal{F}} |R(f) - \hat{R}(f)| > \epsilon ) \leq \delta
\end{equation}
where $\delta$ is ideally decreasing exponentially with respect to $\epsilon$, plus some other parameters like the size of our dataset $n$ or the dimension $d$. This basically says that the probability of the empirical risk of our model $f$ deviating from the true risk is low, and so minimizing the empirical risk is indeed a good thing to do. 

\begin{definition}[Generalize]
  A function $f$ is said to \textbf{generalize} if 
  \begin{equation}
    \lim_{n \rightarrow +\infty} \hat{R}_n (f) = R(f)
  \end{equation}
\end{definition}


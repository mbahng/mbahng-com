Unlike unsupervised learning, which comes in many different shapes and forms (anomaly detection, feature extraction, density estimation, dimensionality reduction, etc.), supervised learning comes in a much cleaner format. In supervised learning, we consider an input space $\mathcal{X}$ and an output space $\mathcal{Y}$. We assume that there exists some unknown measure $\mathbb{P}$ over $\mathcal{X} \times \mathcal{Y}$, making this some probability space. We then assume that some data $\mathcal{D} = \{(x^{(i)}, y^{(i)})\}$ is generated sampled \textit{independently and identically (iid)} from $\mathbb{P}$. Now this assumption is quite strong and is almost always not the case, as different data can be correlated, but we will relax this assumption later. Let's formally construct this from the bottom up. 

\begin{enumerate}
  \item We start off with a general probability space $(\Omega, \mathcal{F}, \mathbb{P})$. This is our model of the world and everything that we are interested in. 

  \item A measurable function $X: \Omega \rightarrow \mathcal{X}$ extracts a set of features, which we call the \textbf{covariates} and induces a probability measure on $\mathcal{X}$, say $\mathbb{P}_X$. 

  \item Another measurable function $Y: \Omega \rightarrow \mathcal{Y}$ extracts another set of features called the \textbf{labels} and induces another probability measure on $\mathcal{Y}$, the \textbf{label set}, say $\mathbb{P}_Y$. 

  \item At this point the function $X \times Y$ is all we are interested in, and we throw away $\Omega$ since we only care about the distribution over $\mathcal{X} \times \mathcal{Y}$. 

  \item We model the generation of data from $\Omega$ by sampling $N$ samples from $\mathbb{P}_{X \times Y}$, which we assume to be iid (this assumption will be relaxed later). This gives us the \textbf{dataset} 
    \[\mathcal{D} = \{(\mathbf{x}^{(i)}, \mathbf{y}^{(i)}) \}_{i=1}^N\]
\end{enumerate}

Now our goal is to construct a function $f: \mathcal{X} \rightarrow \mathcal{Y}$ that predicts $Y$ from $X$, but we want to define some measure of how good our function is. We can use a loss function $L$ to talk about this. 

\begin{definition}[Risk]
  The \textbf{risk}, or \textbf{expected risk}, of function $f$ is defined as 
  \begin{equation}
    R(f) = \mathbb{E}_{X \times Y} [ L(Y, f(X))] = \int_{\mathcal{X} \times \mathcal{Y}} L(y, f(x)) \,d\mathbb{P}(x, y)
  \end{equation}
\end{definition}

Clearly, we don't know what this risk is since we don't know the true measure $\mathbb{P}$, so we try to approximate it with the \textit{empirical risk}. 

\begin{definition}[Empirical Risk]
  The \textbf{empirical risk} of function $f$ is defined as 
  \begin{equation}
    \hat{R}_n(f) = \frac{1}{n} \sum_{i=1}^n L(y^{(i)}, f(x^{(i)}))
  \end{equation}
\end{definition}

Now it would be great if minimizing the empirical risk---which is all we have---allows us to approximate the true risk with high probability. Note that there are two stages of estimates here. 
\begin{enumerate}
  \item First is that this is an \textit{approximation}. We must justify whether the empirical risk is actually a good approximation of the true risk. There are many theorems---some simple and others convoluted---that provides results on this. 
  \item Second, this approximation occurs with \textit{high probability}. This can be dealt with using concentration of measure. 
\end{enumerate}
This regime is known as \textit{probably approximately correct (PAC) learning}, and with asymptotic analysis, we can prove this with some assumptions. 

\begin{definition}[Generalize]
  A function $f$ is said to \textbf{generalize} if 
  \begin{equation}
    \lim_{n \rightarrow +\infty} \hat{R}_n (f) = R(f)
  \end{equation}
\end{definition}

A final point is to consider how one even chooses the correct loss function $L$. This can be analyzed with \textit{statistical decision theory}. 


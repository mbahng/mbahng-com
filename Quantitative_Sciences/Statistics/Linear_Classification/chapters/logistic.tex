\section{Logistic Regression} 

  We can upgrade from a discriminant function to a discriminative probabilistic model with \textbf{logistic regression}. In linear regression, we assumed that the targets are linearly dependent with the covariates as $y = w^T x + b$. However, this means that the hypothesis $f_w$ is unbounded. Since we have two classes (say with labels $0$ and $1$), we must have some sort of \textit{link function} $\sigma$ that takes the real numbers and compresses it into the domain $[0, 1]$. Technically, we can choose any continuous, monotinically increasing function from $\mathbb{R}$ to $(0, 1)$. However, the following property of the \textit{sigmoid function} $\sigma(x) \coloneqq \frac{1}{1 + e^{x}}$ makes derivation of gradients very nice. \begin{equation}
    \sigma^\prime (x) = \sigma(x) \, \big(1 - \sigma(x) \big)
  \end{equation}

  \begin{definition}[Logistic Regression]
    The \textbf{logistic regression} model is probabilistic discriminative classification model  
    \begin{equation}
      Y \mid X = x \sim \mathrm{Bernoulli}(f(x)), \qquad f(x) = \sigma(\beta^T x)
    \end{equation}
    where $\sigma$ is the sigmoid function. It has parameters $\beta$. 
  \end{definition} 

  One important observation to make is that notice that the output of our hypothesis is used as a parameter to define our residual distribution. 
  \begin{enumerate}
    \item In linear regression, the $f$ was used as the \textit{mean} $\mu$ of a Gaussian. 
    \item In logistic regression, the $f$ is used also as the mean $p$ of a Bernoulli. 
  \end{enumerate}
  The reason we want this sigmoid is so that we make the domains of the means of the residuals match the range of the outputs of our model. It's simply a manner of convenience, and in fact we could have really chose any function that maps $\mathbb{R}$ to $(0, 1)$.\footnote{Some questions may arise, such as ``why isn't the variance parameter of the Gaussian considered in the linear model?" or ``what about other residual distributions that have multiple parameters?" This is all answered by generalized linear models, which uses the output of a linear model as a \textit{natural parameter} of the canonical exponential family of residual distributions. }

  Note that logistic regression has a \textit{nonlinear regression function}, but a \textit{linear decision boundary}. That is, by running this model through the plugin classifier, this becomes a linear classifier. 

  \begin{theorem}[Decision Boundary is Linear]
    The decision boundary of logistic regression is linear. 
  \end{theorem}
  \begin{proof}
    We can solve 
    \begin{equation}
      \frac{1}{1 + e^{\beta^T x}} = \frac{1}{2} \implies 2 = 1 + e^{\beta^T x} \implies 1 = e^{\beta^T x} \implies 0 = \beta^T x
    \end{equation}
    which defines a linear hyperplane orthogonal to vector $\beta$. 
  \end{proof} 

\subsection{Maximum Likelihood Estimation} 

  Our next job is to define a loss to optimize. The pdf of a Bernoulli is not well-defined, but we can smoothly interpolate between the two points at $x = 0$ and $x = 1$ to get a smooth surrogate likelihood.  

  \begin{lemma}[Likelihood for Bernoulli]
    A surrogate likelihood of a $\mathrm{Bernoulli}(p)$ random variable is 
    \begin{equation}
      \mathbb{P}(X = x) = p^x (1 - p)^{1 - x}
    \end{equation} 
    TBD. Why do we express it in this form? Just for convenience? 
  \end{lemma} 
  \begin{proof}
    Just substitute $\mathbb{P}(X = 1) = p^1 (1 - p)^{0} = p$ and $\mathbb{P}(X = 0) = p^0 (1 - p)^1 = 1 - p$. 
  \end{proof}

  Therefore, by taking the negative log-likelihood, we can get our loss function. 

  \begin{definition}[Binary Cross Entropy Loss as Surrogate Loss]
    The surrogate loss for logistic regression is the \textbf{binary cross entropy loss}, which is defined as
    \begin{equation}
      L(y, \hat{y}) = -y \log \hat{y} - (1 - y) \log (1 - \hat{y})
    \end{equation}
  \end{definition}

  Now we can define our risk. 

  \begin{theorem}[Risk]
    The expected risk of logistic regression on the binary cross entropy loss is 
    \begin{equation}
      R(f) = \mathbb{E}_{x, y} \left[ -y \log(\sigma(\beta^T x)) - (1 - y) \log(1 - \sigma(\beta^T x)) \right]
    \end{equation} 
    and the empirical risk on a dataset $\mathcal{D} = \{x^{(i)}, y^{(i)}\}_{i=1}^n$ is 
    \begin{equation}
      \hat{R}(f) = \frac{1}{n} \sum_{i=1}^n -y^{(i)} \log(\sigma(\beta^T x^{(i)})) - (1 - y^{(i)}) \log(1 - \sigma(\beta^T x^{(i)}))
    \end{equation}
  \end{theorem}

  Unfortunately, there is no closed form solution for logistic regression like the least squares solution in linear regression. Therefore, we can only resort to numerically optimizing it. 

  \begin{theorem}[Gradients of Empirical Risk]
    The gradient of the empirical risk is 
    \begin{equation}
      \nabla_\beta \hat{R}(\beta) = \sum_{i=1}^n (y^{(i)} - f(x^{(i)})) x^{(i)}
    \end{equation}
  \end{theorem}
  \begin{proof}
    Take the negative sign out for a moment. The gradient for just a single sample $(x^{(i)}, y^{(i)})$ gives 
    \begin{align}
      \frac{\partial \ell}{\partial \beta}  & = \bigg( \frac{y^{(i)}}{\sigma(\beta^T x^{(i)})} - \frac{1 - y^{(i)}}{1 - \sigma(\beta^T x^{(i)})} \bigg) \, \frac{\partial}{\partial \beta} \sigma (\beta^T x^{(i)}) \\
      & = \frac{\sigma(\beta^T x^{(i)}) - y^{(i)}}{\sigma(\beta^T x^{(i)}) \, \big( 1 - \sigma(\beta^T x^{(i)}) \big)} \sigma(\beta^T x^{(i)}) \, \big( 1 - \sigma(\beta^T x^{(i)}) \big) x^{(i)} \\
      & = \big( f (x^{(i)}) - y^{(i)} \big) x
    \end{align} 
    and now adding in the negative sign and summing it gives the result. 
  \end{proof}

\subsection{Significance Tests and Confidence Sets}

\subsection{Concentration Bounds}


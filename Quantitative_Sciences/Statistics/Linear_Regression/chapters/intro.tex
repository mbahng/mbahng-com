In introductory courses, we start with linear predictors since it is easy to understand. We still start with linear predictors because in high-dimensional machine learning, even linear prediction can be hard as we will see. Low dimensional linear regression is what statisticians worked in back in the early days, where data was generally low dimensional.\footnote{Quoting Larry Wasserman, even 5 dimensions was considered high and 10 was considered massive. } Generally, we had $d < n$, but these days, we are in the regime where $d > n$. For example, in genetic data, you could have a sample of $n = 100$ people but each of them have genetic sequences at $d = 10^6$. When the dimensions become high, the original methods of linear regression tend to break down, which is why I separate low and high dimensional linear regression. The line tends to be fuzzy between these two regimes, but we will not worry about strictly defining that now. 

When you learn linear regression for the first time, you are really learning a very specific part of linear regression called ordinary least squares, which gives you a lot of assumptions. Let's introduce the most general variant of linear regression. 

\begin{definition}[Linear Regression Model]
  A \textbf{linear regression model} is a probabilistic model that predicts the conditional distribution of $y$ given $x$ as 
  \begin{equation}
    y = b + w^T x + \epsilon
  \end{equation}
  It has the following assumptions. 
  \begin{enumerate}
    \item \textit{Linearity in Parameters}. Note that this does not mean linearity in the \textit{covariates}.\footnote{Therefore you could build a regression using non-linear transformations of the covariates, for instance, $Y = X_1 \beta_1 + X_1^2 \beta_2 + \log(X_1) \beta_3$. }
    \item \textit{Weak exogeneity}. The covariates are observed without error. 
    \item $\epsilon$ is $0$-mean. 
    \item \textit{Homoscedasticity}: $\epsilon$ has constant variance. 
    \item The $\epsilon$'s are uncorrelated with each other. 
    \item \textit{No multicolinearity}: Assume that the covariates aren't perfectly correlated. 
  \end{enumerate}
\end{definition} 

Let's go through these assumptions in detail.  
\begin{enumerate}
  \item \textit{Linearity}: Remember again that the linear regression model only assumes linearity in the parameters, not the covariates. If you need to further relax the assumption, you are better off using non-linear modeling. 

  \item \textit{Weak exogeneity}: The sensitivity of the model can be tested to the assumption of weak exogeneity by doing bootstrap sampling for the covariates and seeing how the sampling affects the parameter estimates. Covariates measured with error used to be a difficult problem to solve, as they required errors-in-variables models, which have very complicated likelihoods. In addition, there is no universal fitting library to deal with these. But nowadays, with the availability of Markov Chain Monte Carlo (MCMC) estimation through probabilistic programming languages, it is a lot easier to deal with these using Bayesian hierarchical models (or multilevel models, or Bayesian graphical models---these have many names).

  \item \textit{Constant variance}: the simplest fix is to do a variance-stabilising transformation on the data. Assuming a constant coefficient of variation rather than a constant mean could also work. Some estimation libraries (such as the \verb+glm+ package in R) allow specifying the variance as a function of the mean.

  \item \textit{Independence of errors}: this is dangerous because in fields like finance, things are usually highly correlated in times of crisis. The most important thing is to understand how risky this assumption is for your setting. If necessary, add a correlation structure to your model, or do  a multivariate regression. Both of these require significant resources to estimate parameters, not only in terms of computational power but also in the amount of data required. Another field that looks for correlation between samples is time series analysis, which we will get to in another set of notes. 

  \item \textit{No Multicollinearity}. Assume that two variables are perfectly correlated. Then, there would be pairs of parameters that are indistinguishable if moved in a certain linear combination. This means that the variance of $\hat{\boldsymbol{\beta}}$ will be very ill conditioned, and you would get a huge standard error in some direction of the $\beta_i$'s. We can fix this by making sure that the data is not redundant and manually removing them, standardizing the variables, or making a change of basis to remove the correlation. If we just leave the model as is, numerical problems might occur depending on how the fitting algorithms invert the matrices involved. The t-tests that the regression produces can no longer be trusted.\footnote{I suggest reading this Wikipedia article on multicollinearity, as it contains useful information: \url{https://en.wikipedia.org/wiki/Multicollinearity}. Multicollinearity is a favorite topic of discussion for quant interviewers, and they usually have strong opinions about how it should be handled. The model's intended use will determine how sensitive it is to ignoring the error distribution. In many cases, fitting a line using least-squares estimation is equivalent to assuming errors have a normal distribution. If the real distribution has heavier tails, like the t-distribution, how risky will it make decisions based on your outputs? One way to address this is to use a technique like robust-regression. Another way is to think about the dynamics behind the problem and which distribution would be best suited to model them---as opposed to just fitting a curve through a set of points.} 

  In order to check multicollinearity, we compute the correlation matrix, and see if there are any off-diagonal entries that are very close to $1$, then there is multicollinearity. 
\end{enumerate}

Some more terminology: \textbf{multiple linear regression} assumes that we have several covariates and one response. If we extend this to multiple responses (i.e. a response vector), this is called \textbf{multivariate linear regression}. The simple case for one response is called \textbf{simple linear regression}, and we will mention some nice formulas and intuition that come out from working with this. 


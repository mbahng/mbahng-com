\section{Nonparametric Regression}

  \subsection{K Nearest Neighbors Regression}

    When we want to do nonparametric regression, i.e. when dealing with nonlinear functions, we can construct a function that uses local averaging of its nearby points. 

    \begin{example}[Local Averaging]
      Say that we want to fit some function through a series of datapoints in simple regression (one covariate). Then, what we can do is take some sliding window and our vale of the function at a point $x$ is the average of all values in the window $[x - \delta, x + \delta]$. 
      \begin{figure}[H]
        \centering 
        \includegraphics[scale=0.6]{img/kmeans_smoother.png}
        \caption{K means smoother} 
        \label{fig:kmeans_smoother}
      \end{figure}
    \end{example}

    \begin{code}[MWS of K Nearest Neighbor Regression in scikit-learn]
      Local averaging is implemented as the K nearest neighbor regressor in scikit learn. It is slightly different in the way that it doesn't use the points within a certain $\delta$ away but rather the $K$ nearest points. Either way, a minimal working example of this is 
      \begin{lstlisting}
        X = [[0], [1], [2], [3]]
        y = [0, 0, 1, 1]
        from sklearn.neighbors import KNeighborsRegressor
        neigh = KNeighborsRegressor(n_neighbors=2)
        neigh.fit(X, y)
        print(neigh.predict([[1.5]])) 
      \end{lstlisting}
    \end{code}

    Note that since $\hat{f}$ is a combination of step functions, this makes it discontinuous at points. 

  \subsection{Kernel Regression and Linear Smoothers} 

    K nearest neighbor regression puts equal weights on both near and far points, as long as they are in the window. This may not be ideal, so a simple modification is to \textit{weigh} these points according to their distance from the middle $x$. We can do this with a kernel, as the name suggests. Now this is not the same thing as a Mercer kernel in RKHS, so to distinguish that I will call it a \textit{local averaging kernel}. 

    \begin{definition}[Local Averaging Kernel]
      A \textbf{kernel} is any smooth, symmetric, and non-negative function $K : \mathbb{R} \to \mathbb{R}$.  
    \end{definition}

    \begin{definition}[Kernel Regression]
      Given some datapoints, $X$, our fitted regressor is of form 
      \begin{equation}
        \hat{f} (X) = \frac{\sum_{i} Y_i K \bigg( \frac{||X - X_i||}{h} \bigg)}{\sum_{i} K \bigg( \frac{||X - X_i||}{h} \bigg)}
      \end{equation}
      where $h$ is the \textbf{bandwidth} and the denominator is made sure so that the coefficients sum to $1$. To get a clearer picture, we are really taking the weighted average of the $Y_i$'s. 
      \begin{equation}
        \hat{f} (X) = \sum_{i} Y_i \ell_i (X) \text{ where } \sum_{i} \ell_i (X) = 1
      \end{equation}
      Denoting $Y = (Y_1, \ldots, Y_n) \in \mathbb{R}^n$ and the vector $f(X) = (f(X_1), \ldots, f(X_n))$, if we can write the kernel function as $\hat{Y} = \hat{f}(X) = S Y$, which in matrix form, is 
      \begin{equation}
        \begin{bmatrix} \hat{Y}_1 \\ \vdots \\ \hat{Y}_n \end{bmatrix} = \begin{bmatrix} \hat{f}(X_1) \\ \vdots \\ \hat{f} (X_n) \end{bmatrix} = \begin{bmatrix} \ell_1 (X_1) & \cdots & \ell_n (X_1) \\ \vdots & \ddots & \vdots \\ \ell_1 (X_n) & \cdots & \ell_n (X_n) \end{bmatrix} \begin{bmatrix} Y_1 \\ \vdots \\ Y_n \end{bmatrix} 
      \end{equation}
      then we say that we have a \textbf{linear smoother}, with stochastic matrix $S$ being our \textbf{smoothing matrix}. 
    \end{definition}

    The reason we'd like to have the weights to sum to $1$ is that if we had data that was constant (i.e. all $Y_i$'s are the same), then the fitted function should be constant at that value as well. Furthermore, the theme of linearity is important and will be recurring. The kernel estimator is defined for all $X$, but it's important to see its behavior at the training points $X_i$. The estimator $\hat{Y} = \hat{f}(X)$ is a linear combination of the $Y_i$'s, and the coefficients $\ell_i (X_j)$ depend on the values of $X_j$. Therefore, we have $\hat{Y} = S Y$, which is very similar to the equation $\hat{Y} = H Y$ in linear regression, where $H$ is the hat matrix that projects $Y$ onto the column space of $X$. Nonparametric regression has the same form, but rather than being a projection, it is a linear smoothing matrix. Therefore, this theme unifies both linear regression and nonparametric regression. Linear smoothers, spline smoother, Gaussian processes, are all just different choices of the smoothing matrix $S$. However, not all nonparametric estimators are linear smoothers, as we will see later. 

    Here are some popular kernels. 

    \begin{definition}[Gaussian Kernel]
      The \textbf{Gaussian kernel} is defined as 
      \begin{equation}
        K(x) = \frac{1}{\sqrt{2 \pi}} e^{-x^2/2}
      \end{equation}
      \begin{figure}[H]
        \centering 
        \includegraphics[scale=0.6]{img/gaussian_smoother.png}
        \caption{Gaussian kernel. } 
        \label{fig:gaussian_smoother}
      \end{figure}
    \end{definition}

    \begin{definition}[Box-Car Kernel]
      The \textbf{Box-Car kernel} is defined as 
      \begin{equation}
        K(x) = \frac{1}{2} \mathbbm{1}(|x| \leq 1)
      \end{equation}
      \begin{figure}[H]
        \centering 
        \includegraphics[scale=0.6]{img/boxcar_smoother.png}
        \caption{Boxcar kernel. } 
        \label{fig:boxcar_smoother}
      \end{figure}
    \end{definition}

    \begin{definition}[Epanechnikov Kernel]
      The \textbf{Epanechnikov kernel} is defined as 
      \begin{equation}
        K(x) = \frac{3}{4} (1 - x^2) \mathbbm{1}(|x| \leq 1)
      \end{equation}
      \begin{figure}[H]
        \centering 
        \includegraphics[scale=0.6]{img/epanechnikov_smoother.png}
        \caption{Epanechnikov kernel.} 
        \label{fig:epanechnikov_smoother}
      \end{figure}
    \end{definition}

    It turns out that from a theoretical point of view, the choice of the kernel doesn't really matter. What really matters is the bandwidth $h$ since that is what determines the bias variance tradeoff. To see why, if $h = 0$, then it will simply interpolate the points and variance is extremely high, and if $h = \infty$, then the fitted function will be constant at $\bar{Y}$, leading to high bias. The following theorem formalizes this.  

    \begin{theorem}[Bias Variance Tradeoff of Kernel Regression]
      Suppose that $d = 1$ and that $m^{\prime\prime}$ is bounded. Also suppose that $X$ has a nonzero, differentiable density $p$ and that the support is unbounded. Then, the risk is 
      \begin{align}
        R_n & = \frac{h_n^4}{4} \bigg( \int x^2 K(x) \bigg)^2 \int \bigg( m^{\prime\prime} (x) + 2m^\prime (x) \frac{p^\prime (x)}{p(x)} \bigg)^2 \,dx \\
            & \;\;\; + \frac{\sigma^2 \int K^2(x)\,dx} {n h_n} \int \frac{dx}{p(x)} + o \bigg( \frac{1}{n h_n} \bigg) + o(h_n^4) 
      \end{align}
      The first term is the squared bias and the second term is the variance. 
    \end{theorem}
    \begin{proof}
      We first denote 
      \begin{equation}
        \hat{f}(X) = \frac{\frac{1}{nh} \sum_{i=1}^n K \bigg( \frac{X - X_i}{h} \bigg) Y_i}{\frac{1}{nh} \sum_{i=1}^n K \bigg( \frac{X - X_i}{h} \bigg)} 
      \end{equation}
      where the denominator is the kernel density estimator $\hat{p}(X)$. Therefore, we rewrite
      \begin{align}
        \hat{f} (x) - f(x) & = \frac{\hat{a}(x)}{\hat{p}(x)} - f(x) \\
                           & = \bigg( \frac{\hat{a}(x)}{\hat{p}(x)} - f(x) \bigg) \bigg( \frac{\hat{p}(x)}{p(x) + 1 - \frac{\hat{p}(x)}{p(x)}} \bigg) \\
                           & = \frac{\hat{a}(x) - f(x) \hat{p}(x)}{p(x)} + \frac{(\hat{f}(x) - f(x)) (p(x) - \hat{p}(x))}{p(x)}
      \end{align}
      as $n \rightarrow \infty$ both $\hat{f}(x) - f(x)$ and $p(x) - \hat{p}(x)$ going to $0$, and since they're multiplied in the second term, it will go to $0$ very fast. So the dominant term is the first term, and we can write the above as approximately 
      \begin{equation}
        \hat{f}(x) - f(x) \approx  \frac{\hat{a}(x) - f(x) \hat{p}(x)}{p(x)}
      \end{equation}
      TBD continued. Wasserman lecture 6, 10:00. 
    \end{proof}

    From the theorem above, we can see that if the bandwidth is small, then $h^4$ is small and the bias decreases. However, there is a $h$ term in the denominator of the variance term, which also trades it off. We can furthermore see that the bias is sensitive to $p^\prime / p(x)$. This means that if the density is steep, then the bias will be high. This is known as \textit{design bias}, which is an underlying weakness in smoothing kernel regression. Another problem that is not contained in the theorem is the \textit{boundary bias}, which states that if you're near the boundary of the distribution (i.e. near the boundary of its support), then the bias also explodes. This happens to be very nasty especially in high dimensions where most of the data tends to be near the boundary. It turns out that this can be easily fixed with local polynomial regression, which gets rid of this term in the bias without any cost to variance. This means that this is unnecessary bias. 

    Then you can apply regularization on this to get kernel ridge regression. 

    \begin{code}[MWS of Kernel Ridge Regression in scikit learn]
      \begin{lstlisting}
        from sklearn.kernel_ridge import KernelRidge
        import numpy as np
        n_samples, n_features = 10, 5
        rng = np.random.RandomState(0)
        y = rng.randn(n_samples)
        X = rng.randn(n_samples, n_features)
        krr = KernelRidge(alpha=1.0)
        krr.fit(X, y)
      \end{lstlisting}
    \end{code}

  \subsection{Local Polynomial Regression}

    Now another way to think about the kernel estimator is as such. Suppose that you're doing linear regression on a bunch of points and you want to choose a $c$ that minimizes the loss. 
    \begin{equation}
      \sum_i (Y_i - c)^2
    \end{equation}
    You would just pick $c = \hat{Y}$. But if you are given some sort of locality condition, that the value of $c$ should depend more on the values closer to it, you're really now minimizing 
    \begin{equation}
      \sum_i (Y_i - c(x))^2 K \bigg( \frac{X_i - x}{h} \bigg)
    \end{equation}
    Minimizing this by setting the derivative equal to $0$ and solving gives us the kernel estimator. Therefore you're fitting some sort of local constant at a point $X$. But why fit a local constant, when you can fit a local line or polynomial? This is the idea behind local polynomial regression.

    \begin{figure}[H]
      \centering 
      \includegraphics[scale=0.4]{img/local_linear_estimator.png}
      \caption{Rather than using a local constant, we can use a local linear estimator.} 
      \label{fig:local_linear_estimator}
    \end{figure}

    Therefore, we can minimize the modified loss. 

    \begin{definition}[Local Polynomial Estimator]
      The \textbf{local polynomial estimator} is a local linear kernel smoother that estimates the function $\hat{f}$ that minimizes the following loss. 
      \begin{equation}
        \argmin_{\beta} \sum_i K \bigg( \frac{X_i - x}{h} \bigg) \big( Y_i - (\beta_0 (x) - \beta_1 (x) (x- X_i) + \ldots + \beta_k (x) (x - X_i)^k )\big)
      \end{equation}
    \end{definition}

    So we can fit a line 
    \begin{equation}
      f(\mu) \approx \hat{\beta}_0 (x) + \hat{\beta}_1 (x) (\mu - x)
    \end{equation}
    and simply remove the intercept term to get the local linear estimator. 
    \begin{equation}
      \hat{f}(x) = \hat{\beta}_0 (x)
    \end{equation}
    Note that this is not the same as taking the constant estimate. We are extracting the fitted intercept term and so $\hat{\beta}_0(x) \neq c(x)$. 

    \begin{theorem}[Weighted Least Squares]
      The solution to the local linear estimator is similar to the weighted least squares solution. 
      \begin{equation}
        \hat{\beta}(x) = \begin{pmatrix} \hat{\beta}_0 (x) \\ \hat{\beta}_1 (x) \end{pmatrix} = (X^T W X)^{-1} X^T W Y
      \end{equation}
      where 
      \begin{equation}
        X = \begin{pmatrix} 1 & X_1 - x \\ \vdots & \vdots \\ 1 & X_n - x \end{pmatrix} \qquad W = \begin{pmatrix} K \bigg( \frac{X_1 - x}{h} \bigg) & 0 & \cdots & 0 \\ 0 & K \bigg( \frac{X_2 - x}{h} \bigg) & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & K \bigg( \frac{X_n - x}{h} \bigg) \end{pmatrix}
      \end{equation}
    \end{theorem}

    Computationally, it's similar to kernel regression and it gets rid of both the boundary and design bias. 

  \subsection{Regularized: Spline Smoothing}

    This is not local, but it's a linear smoother. 

  \subsection{Regularized: RKHS Regression}

    This is not local, but it's a linear smoother. 

  \subsection{Additive Models}

    In the most general case, we want to create nonparametric regression functions of the form 
    \begin{equation}
      Y = f(x_1, \ldots, x_d) + \epsilon 
    \end{equation}
    We've done this for one dimensional case, but we can extend this to multiple dimensions through additive models of the form 
    \begin{equation}
      Y = \sum_j f_j (x_j)  + \epsilon
    \end{equation}
    This gives us very interpretable models where we can clearly see the effect of each covariate on $Y$. Clearly, this is not as flexible as the previous model since they can't capture dependencies, but we can create sub-dependency functions and replace the form above to something like 
    \begin{equation}
      Y = \sum_{i, j} f_{i, j} (x_i, x_j) + \epsilon
    \end{equation}
    giving us more flexible models. 
  
  \subsection{Nonlinear Smoothers, Trend Filtering} 

    Tough example of the Dobbler function (like topologists sine curve). It's a pretty good fit but it's not too good since it's using a linear smoother (homogeneous). So we might need to fit it with nonlinear smoothers. 

  \subsection{High Dimensional Nonparametric Regression}

  \subsection{Regression Trees}


\section{Ridge Regression} 

  We use all the covariates, but instead of least squares, we shrink the coefficients towards $0$. This is called \textit{ridge regression} and is an example of a \textit{shrinkage model}. 

  Ridge regression is used both in the high dimensional case or when our function space is too large/complex, which leads to overfitting. In the overfitting case, we have seen that either decreasing our function space or getting more training data helps. Another popular way is to add a \textbf{regularizing term} to the error function in order to discourage the coefficients from reaching large values, effectively limiting the variance over $\mathcal{D}$. 

  \begin{definition}[Ridge Regression]
    In \textbf{ridge regression}, we minimize 
    \begin{equation}
      L(\beta) = ||Y - X \beta||^2 + \lambda ||\beta||^2 
    \end{equation}
    where we penalize according to the L2 norm of the coefficients. 
  \end{definition}

  \begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{img/Lambda_vs_RMS.png}
    \caption{Even with a slight increase in the regularization term $\lambda$, the RMS error on the testing set heavily decreases. }
    \label{fig:enter-label}
  \end{figure}

  \begin{theorem}[Least Squares Solution for Ridge Regression]
    The minimizer of the ridge loss is 
    \begin{equation}
      \hat{\beta} = (X^T X+ \lambda I)^{-1} X^T Y
    \end{equation}
  \end{theorem}
  \begin{proof}
    TBD
  \end{proof}

  \begin{theorem}[Bias Variance Decomposition of Ridge Regression]
    TBD 
  \end{theorem}

  From a computational point of view, we can see that by adding the $\lambda I$ term, it \textit{dampens} the matrix so that it does become invertible (or well conditioned), allowing us to find a solution. The higher the $\lambda$ term, the higher the damping effect. The next theorem compares the performance of the best ridge regression estimator to the best linear predictor. 

  \begin{theorem}[Hsu, Kakade, Zhang, 2014 \cite{hsu2014random}] 
    Suppose that $||X_i|| \leq r$ and let $\beta^T x$ be the best linear approximation to $m(x)$. Then, with probability at least $1 - 4 e^{-t}$, we have
    \begin{equation}
      r(\hat{\beta}) - r(\beta) \leq \bigg( 1 + O \bigg( \frac{1 + r^2 / \lambda}{n} \bigg) \bigg) \frac{\lambda ||\beta||^2}{2} + \frac{\sigma^2}{n} \frac{\Tr(\Sigma)}{2 \lambda}
    \end{equation}
  \end{theorem}

  We can see that the $\lambda$ term exists in the numerator on $\frac{\lambda ||\beta||^2}{2}$ and in the denominator on $\frac{\Tr(\Sigma)}{2 \lambda}$. This is the bias variance tradeoff. The first term is the bias term, which is the penalty for not being able to fit the data as well. The second term is the variance term, which is the penalty for having a more complex model. So our optimal $\lambda$ in the theoretical sense would be the one that minimizes the sum of these two terms. In practice, it's not this clean since we have unknown quantities in the formula, but just like how we did cross validation over the model complexity, we can also do cross validation over the $\lambda$. The decomposition above just gives you a theoretical feeling of how these things trade off. 

  \begin{code}[MWS of Ridge Regression in scikit-learn]
    \noindent\begin{minipage}{.6\textwidth}
    \begin{lstlisting}[]{Code}
      import numpy as np 
      from sklearn.linear_model import Ridge  

      X = np.random.randn(10, 5) 
      y = np.random.randn(10)
      # regularization parameter
      model = Ridge(alpha=1.0)  
      model.fit(X, y) 
      print(model.score(X, y))  
      print(model.intercept_)
      print(model.coef_) 
      print(model.predict(np.array([[1, 2, 3, 4, 5]]))) 
    \end{lstlisting}
    \end{minipage}
    \hfill
    \begin{minipage}{.39\textwidth}
    \begin{lstlisting}[]{Output}
      0.8605535024325397
      -0.28291076492665157
      [-0.10400521 -0.7587073  -0.05116735  1.16236649 -0.0401323 ]
      [2.39097184]
      .
      .
      .
      .
      .
      .
    \end{lstlisting}
    \end{minipage}
  \end{code}

  \begin{question}[To Do]
    Bayesian interpretation of ridge regression. 
  \end{question}


\section{Stepwise Regression} 

  Now we move to \textit{sparse} linear regression. 

  Now suppose that $d > n$, then the first problem is that we can no longer use least squares since $X^T X$ is no longer invertible and the same problem happens with maximum likelihood. This is known as the \textbf{high dimensional} or \textbf{large $p$, small $n$} problem. The most straightforward way is simply to reduce the covariates to a dimension smaller than $n$. This can be done with three ways. 
  \begin{enumerate}
    \item We perform PCA on the $X$ and use the first $k$ principal components where $k < n$. 
    \item We cluster the covariates based on their correlation. We can use one feature from each cluster or take the average of the covariates within each cluster. 
    \item We can screen the variables by choosing the $k$ features that have the largest correlation with $Y$. 
  \end{enumerate}
  Once this is done, we are back in the low dimensional regime and can use least squares. Essentially, this is a way to find a good subset of the covariates, which can be formalized by the following. Let $S$ be a subset of $[d]$ and let $X_S = (X_j \,:\, j \in S)$. If the size of $S$ is not too large, we can regress $Y$ on $X_S$ instead of $X$. 

\subsection{Best Subset Regression}

  \begin{definition}[Best Subset Regression]
    \textbf{Best subset regression} is a linear regression model that wants to solve the best subset $S$ that minimizes the constrained loss 
    \begin{equation}
      L(\beta, x, y) =  (y - \beta x)^2, \qquad \text{ subject to } \|\beta\|_0 \leq K
    \end{equation}
    where $\|\beta\|_0$ is the number of non-zero entries in $\beta$.\footnote{Note that $\| \cdot \|_0$ is not a norm.}
  \end{definition}

  There will be a bias variance tradeoff. As $k$ increases, the bias decreases but the variance increases. 

  The minimization of the empirical error is over all subset of size $k$, and from this fact we can expect bad news. 

  \begin{theorem}[Best Subset Regression is NP-Hard]
    Solving the best subset loss is NP-hard. 
  \end{theorem}
  \begin{proof}
    
  \end{proof}

  Even though best subset regression is infeasible, we can still approximate best subset regression in two different ways. 
  \begin{enumerate}
    \item A greedy approximation leads to \textit{forward stepwise regression}. 
    \item A convex relaxation of the problem leads to the \textit{Lasso} regression. 
  \end{enumerate}
  It turns out that the theoretical guarantees and computational time for both are the same, but the Lasso is much more popular due to having cleaner form and performing better in practice. 

\subsection{Forward Stepwise Regression} 

  Forward stepwise regression is a greedy algorithm that starts with an empty set of covariates and adds the covariate that most improves the fit. It avoids the NP-hardness of the best subset regression by adding covariates one by one. 

  \begin{definition}[Greedy Forward Stepwise Regression]
    Given your data $\mathcal{D}$, let's first standardize it to have mean $0$ and variance $1$.\footnote{This may or may not be a good idea, since the variance of each covariate can tell you a lot about the importance of the covariate.} You start off with a set $\mathcal{Q} = \{\}$ and choose the number of parameters $K$. 
    \begin{enumerate}
      \item With each covariate $X = (X_1, \ldots, X_n)$, we compute the correlation between it and the $Y$, which reduces to the inner product (since we standardized). 
      \begin{equation}
        \rho_j = \langle Y, X_{:, j} \rangle = \frac{1}{n} \sum_{i=1}^n Y_i X_{ji}
      \end{equation}

      \item Then, we take the covariate index that has the highest empirical correlation with $Y$, add it to $\mathcal{Q}$ and regress $Y$ only on this covariate. 
      \begin{equation}
        q_1 = \argmax_j \rho_j , \;\; \mathcal{Q} = \{q_1\}, \;\; \hat{\beta}_{q_1} = \argmin_{\beta} \frac{1}{n} ||Y - X_{:, q_1} \beta||^2 
      \end{equation}

      \item Then you repeat the process. You take the residual values $r = Y - X_{:, q_1} \hat{\beta}_{q_1} \in \mathbb{R}^n$ compute the correlation between $r$ and the remaining covariates, and pick our the maximum covariate index $q_2$. Then, you \textit{repeat the regression from start} with these two covariates 
      \begin{equation}
        q_2 = \argmax_j \langle r, X_{: ; j} \rangle , \;\; \mathcal{Q} = \{q_1, q_2\}, \;\; \hat{\beta}_{q_1, q_2} = \argmin_{\beta} \frac{1}{n} ||Y - X_{:,[q_1, q_2]} \beta||^2
      \end{equation}
      Note that you're not going to get the same coefficient for $\hat{\beta}_{q_1}$ as before since you're doing two variable regression. 

      \item You get out the residual values $r = Y - X_{:, [q_1, q_2]} \hat{\beta}_{q_1, q_2} \in \mathbb{R}^n$ and keep repeating this process until you have $K$ covariates in $\mathcal{Q}$. 
    \end{enumerate}
  \end{definition}

  Again, there is a bias variance tradeoff in choosing the number of covariates $K$, but through cross-validation, we can find the optimal $K$. It is also easy to add constraints, e.g. if we wanted to place a restriction that two adjacent covariates can't be chosen, we can easily add this to the algorithm. 

\subsection{Bias Variance Tradeoff}

\subsection{Concentration Bounds}

  \begin{theorem}[DeVore and Temlyakov, 1996]
    For all $f \in \mathcal{L}_1$, the residual $r_N$ after $N$ steps of OGA satisfies
    \begin{equation}
      \|r_N\| \leq \frac{\|f\|_{\mathcal{L}_1}}{\sqrt{N + 1}} \tag{4}
    \end{equation}
    for all $N \geq 1$.
  \end{theorem}
  \begin{proof}
    Note that $f_N$ is the best approximation to $f$ from $\text{Span}(V_N)$. On the other hand, the best approximation from the set $\{a g_N : a \in \mathbb{R}\}$ is $\langle f, g_N \rangle g_N$. The error of the former must be smaller than the error of the latter. In other words, $\|f - f_N\|^2 \leq \|f - f_{N-1} - \langle r_{N-1}, g_N \rangle g_N\|^2$. Thus,
    \begin{align}
      \|r_N\|^2 &\leq \|r_{N-1} - \langle r_{N-1}, g_N \rangle g_N\|^2 \\
      &= \|r_{N-1}\|^2 + |\langle r_{N-1}, g_N \rangle|^2 \|g_N\|^2 - 2|\langle r_{N-1}, g_N \rangle|^2 \\
      &= \|r_{N-1}\|^2 - |\langle r_{N-1}, g_N \rangle|^2. \tag{5}
    \end{align}

    Now, $f = f_{N-1} + r_{N-1}$ and $\langle f_{N-1}, r_{N-1} \rangle = 0$. So,
    \begin{align}
      \|r_{N-1}\|^2 &= \langle r_{N-1}, r_{N-1} \rangle = \langle r_{N-1}, f - f_{N-1} \rangle = \langle r_{N-1}, f \rangle - \langle r_{N-1}, f_{N-1} \rangle \\
      &= \langle r_{N-1}, f \rangle = \sum_j \beta_j \langle r_{N-1}, \psi_j \rangle \leq \sup_{\psi \in D} |\langle r_{N-1}, \psi \rangle| \sum_j |\beta_j| \\
      &= \sup_{\psi \in D} |\langle r_{N-1}, \psi \rangle| \|f\|_{\mathcal{L}_1} = |\langle r_{N-1}, g_N \rangle| \|f\|_{\mathcal{L}_1}.
    \end{align}

    Continuing from equation (5), we have
    \begin{align}
      \|r_N\|^2 &\leq \|r_{N-1}\|^2 - |\langle r_{N-1}, g_N \rangle|^2 = \|r_{N-1}\|^2 \left( 1 - \frac{\|r_{N-1}\|^2 |\langle r_{N-1}, g_N \rangle|^2}{\|r_{N-1}\|^4} \right) \\
      &\leq \|r_{N-1}\|^2 \left( 1 - \frac{\|r_{N-1}\|^2 |\langle r_{N-1}, g_N \rangle|^2}{|\langle r_{N-1}, g_N \rangle|^2 \|f\|_{\mathcal{L}_1}^2} \right) = \|r_{N-1}\|^2 \left( 1 - \frac{\|r_{N-1}\|^2}{\|f\|_{\mathcal{L}_1}^2} \right).
    \end{align}

    If $a_0 \geq a_1 \geq a_2 \geq \cdots$ are nonnegative numbers such that $a_0 \leq M$ and $a_N \leq a_{N-1}(1 - a_{N-1}/M)$ then it follows from induction that $a_N \leq M/(N+1)$. The result follows by setting $a_N = \|r_N\|^2$ and $M = \|f\|_{\mathcal{L}_1}^2$. $\square$
  \end{proof}

  If $f$ is not in $\mathcal{L}_1$, it is still possible to bound the error as follows.

  \begin{theorem}
    For all $f \in \mathcal{H}$ and $h \in \mathcal{L}_1$,
    \begin{equation}
      \|r_N\|^2 \leq \|f - h\|^2 + \frac{4\|h\|_{\mathcal{L}_1}^2}{N}. \tag{6}
    \end{equation}
  \end{theorem}
  \begin{proof}
    Choose any $h \in \mathcal{L}_1$ and write $h = \sum_j \beta_j \psi_j$ where $\|h\|_{\mathcal{L}_1} = \sum_j |\beta_j|$. Write $f = f_{N-1} + f - f_{N-1} = f_{N-1} + r_{N-1}$ and note that $r_{N-1}$ is orthogonal to $f_{N-1}$. Hence, $\|r_{N-1}\|^2 = \langle r_{N-1}, f \rangle$ and so
    \begin{align}
      \|r_{N-1}\|^2 &= \langle r_{N-1}, f \rangle = \langle r_{N-1}, h + f - h \rangle = \langle r_{N-1}, h \rangle + \langle r_{N-1}, f - h \rangle \\
      &\leq \langle r_{N-1}, h \rangle + \|r_{N-1}\| \|f - h\| \\
      &= \sum_j \beta_j \langle r_{N-1}, \psi_j \rangle + \|r_{N-1}\| \|f - h\| \\
      &\leq \sum_j |\beta_j| |\langle r_{N-1}, \psi_j \rangle| + \|r_{N-1}\| \|f - h\| \\
      &\leq \max_j |\langle r_{N-1}, \psi_j \rangle| \sum_j |\beta_j| + \|r_{N-1}\| \|f - h\| \\
      &= |\langle r_{N-1}, g_N \rangle| \|h\|_{\mathcal{L}_1} + \|r_{N-1}\| \|f - h\| \\
      &\leq |\langle r_{N-1}, g_N \rangle| \|h\|_{\mathcal{L}_1} + \frac{1}{2} (\|r_{N-1}\|^2 + \|f - h\|^2).
    \end{align}

    Hence,
    \begin{equation}
      |\langle r_{N-1}, g_N \rangle|^2 \geq \frac{(\|r_{N-1}\|^2 - \|f - h\|^2)^2}{4\|h\|_{\mathcal{L}_1}^2}.
    \end{equation}

    Thus,
    \begin{equation}
      a_N \leq a_{N-1} \left( 1 - \frac{a_{N-1}}{4\|h\|_{\mathcal{L}_1}^2} \right)
    \end{equation}
    where $a_N = \|r_N\|^2 - \|f - h\|^2$. By induction, the last displayed inequality implies that $a_N \leq 4\|h\|_{\mathcal{L}_1}^2/k$ and the result follows. $\square$
  \end{proof}

  \begin{corollary}
    For each $N$,
    \begin{equation}
      \|r_N\|^2 \leq \sigma_N^2 + \frac{4\theta_N^2}{N}
    \end{equation}
    where $\theta_N$ is the $\mathcal{L}_1$ norm of the best $N$-atom approximation.
  \end{corollary}

  By combining the previous results with concentration of measure arguments we get the following result, due to Barron, Cohen, Dahmen and DeVore (2008).

  \begin{theorem}
    Let $\hat{h}_n = \arg\min_{h \in \mathcal{F}_n} \|f_0 - h\|^2$. Suppose that $\limsup_{n \to \infty} \|\hat{h}_n\|_{\mathcal{L}_1,n} < \infty$. Let $N \sim \sqrt{n}$. Then, for every $\gamma > 0$, there exist $C > 0$ such that
    \begin{equation}
      \|f - \hat{f}_N\|^2 \leq 4\sigma_N^2 + \frac{C \log n}{n^{1/2}}
    \end{equation}
    except on a set of probability $n^{-\gamma}$.
  \end{theorem} 
  \begin{proof}
    
  \end{proof}

  If we square root it to compare the norm, this reduces to a rate of about $\frac{1}{\sqrt[4]{n}}$. This is the optimal rate. What this is saying that forward stepwise gets to within about $\frac{1}{\sqrt{n}}$ of what you would get if you did the perfect best subset regression. Another interesting property is that this bound is dimensionless, which makes sense since we are approximating the best $K$-term linear predictor (not the true regressor), which is a weaker claim. On the other hand, if we use nonparameteric estimators to estimate the true regressor, then we will get the curse of dimensionality. 

\subsection{Stagewise Regression} 

  Stagewise regression is a variant of forward stepwise regression where we add the covariate that most improves the fit, but we only take a small step in that direction. This is useful when we have a lot of covariates and we don't want to overfit. 


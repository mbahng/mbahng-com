\section{Activation Functions} 

  The choice of the activation function can have a significant impact on your training, and we will describe a few examples below. The first thing to note is that we must ensure that there is a nonzero gradient almost everywhere. If, for example, we had a piecewise constant activation function, the gradient is $0$ almost everywhere, and it would kill the gradient of the entire network. In the early days of deep learning, researchers used the probability-inspired sigmoid and tanh functions as the main source of nonlinearity. Let's go over them below. 

  \begin{definition}[Sigmoid]
    Sigmoid activations are historically popular since they have a nice interpretation as a saturating ``fire rate" of a neuron. However, there are 3 problems: 
    \begin{enumerate}
      \item The saturated neurons ``kill" the gradients, since if the input at any one point in the layers is too positive or negative, the gradient will vanish, making very small updates. This is known as the \textbf{vanishing gradient problem}. Therefore, the more layers a neural network has, the more likely we are to see this vanishing gradient problem. 
      \item Sigmoid functions are not zero centered (i.e. its graph doesn't cross the point $(0, 0)$ ). Consider what happens when the input $x$ to a neuron is always positive. Then, the sigmoid $f$ will have a gradient of 
      \begin{equation}
        f \bigg( \sum_i w_i x_i + b \bigg) \implies \frac{\partial f}{\partial w_i} = f^\prime \bigg( \sum_i w_i x_i + b \bigg)\, x_i
      \end{equation}
      which means that the gradients $\nabla_\mathbf{w} f$ will always have all positive elements or all negative elements, meaning that we will be restricted to moving in certain nonoptimal directions when updating our parameters. 
    \end{enumerate}
  \end{definition} 

  \begin{definition}[Hyperbolic Tangent]
    The hyperbolic tangent is zero centered, which is nice, but it still squashes numbers to range $[-1, 1]$ and therefore kills the gradients when saturated. 
  \end{definition}

  It turns out that these two activations were ineffective in deep learning due to saturation. A less probability inspired activation was the ReLU, which showed better generalization an speed of convergence. 

  \begin{definition}[Rectified Linear Unit]
    The ReLU function has the following properties: 
    \begin{enumerate}
        \item It does not saturate in the positive region. 
        \item It is very computationally efficient (and the fact that it is nondifferentiable at one point doesn't really affect computations). 
        \item It converges much faster than sigmoid/tanh in practice. 
        \item However, note that if the input is less than $0$, then the gradient of the ReLU is $0$. Therefore, if we input a vector that happens to have all negative values, then the gradient would vanish and we wouldn't make any updates. These ReLU ``dead zones" can be a problem since it will never activate and never update, which can happen if we have bad initialization. A more common case is when your learning rate is too high, and the weights will jump off the data manifold. 
    \end{enumerate}
  \end{definition}

  Unfortunately, the ReLU had some weaknesses, mainly being the \textit{dying ReLU}, which is when the ReLU is stuck in the negative region and never activates. This is a problem since the gradient is $0$ in the negative region, and so the weights will never update. Therefore, some researchers have proposed some modifications to the ReLU. 

  \begin{definition}[Leaky ReLU]
    The leaky ReLU 
    \begin{equation}
      \sigma(x) = \max\{0.01 x, x\}
    \end{equation}
    does not saturate (i.e. gradient will not die), is computationally efficient, and converges much faster than sigmoid/tanh in practice. We can also parameterize it with $\alpha$ and have the neural net optimize $\alpha$ along with the weights. 
    \begin{equation}
      \sigma(x) = \max\{\alpha x, x\}
    \end{equation}
  \end{definition}

  \begin{definition}[ELU]
    The exponential linear unit has all the benefits of ReLU, with closer to mean outputs. It has a negative saturation regime compared with leaky ReLU, but it adds some robustness to noise. 
    \begin{equation}
      \sigma(x) = \begin{cases} x & \text{ if } x > 0 \\ \alpha \big(\exp{x} - 1 \big) & \text{ if } x \leq 0 \end{cases}
    \end{equation}
  \end{definition}

  \begin{definition}[SELU]
    The scaled exponential linear unit is a self-normalizing activation function, which means that it preserves the mean and variance of the input. This is useful for deep networks, since the mean and variance of the input will be preserved through the layers. Its formula is 
    \begin{equation}
      \sigma(x) = \lambda \begin{cases} x & \text{ if } x > 0 \\ \alpha \big(\exp{x} - 1 \big) & \text{ if } x \leq 0 \end{cases}
    \end{equation}
    where $\lambda$ and $\alpha$ are constants.
  \end{definition}
  
  Later on, some further modifications were made, such as the \textbf{Swish} and the \textbf{Mish} \cite{mish} activation functions. These functions have a distinctive negative concavity, unlike ReLU, which accounts for preservation of small negative weights.  

  \begin{definition}[Swish]
    The Swish activation function is defined as 
    \begin{equation}
      \sigma(x) = x \cdot \sigma(\beta x) 
    \end{equation}
    where $\beta$ is a parameter that can be learned. 
  \end{definition}

  \begin{definition}[Mish]
    The Mish activation function is defined as 
    \begin{equation}
      \sigma(x) = x \cdot \tanh(\ln(1 + \exp(x))) 
    \end{equation}
  \end{definition}

  \begin{code}[Generating Graphs] 
    Code used to generate these graphs are \href{code/activation_functions.ipynb}{here}.
  \end{code} 


@misc{hsu2014random,
      title={Random design analysis of ridge regression}, 
      author={Daniel Hsu and Sham M. Kakade and Tong Zhang},
      year={2014},
      eprint={1106.2363},
      archivePrefix={arXiv},
      primaryClass={math.ST}
}

@book{gyorfi2002distribution,
  title={A Distribution-Free Theory of Nonparametric Regression},
  author={Gy{\"o}rfi, L. and Kohler, M. and Krzyzak, A. and Walk, H.},
  isbn={9780387954417},
  lccn={20021151},
  series={Springer Series in Statistics},
  url={https://books.google.com/books?id=Ovmb9oGBlo0C},
  year={2002},
  publisher={Springer New York}
}

@article{W92,
	abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
	author = {Williams, Ronald J. },
	date = {1992/05/01},
	date-added = {2025-02-07 14:29:20 -0500},
	date-modified = {2025-02-07 14:29:20 -0500},
	doi = {10.1007/BF00992696},
	id = {Williams1992},
	isbn = {1573-0565},
	journal = {Machine Learning},
	number = {3},
	pages = {229--256},
	title = {Simple statistical gradient-following algorithms for connectionist reinforcement learning},
	url = {https://doi.org/10.1007/BF00992696},
	volume = {8},
	year = {1992},
	bdsk-url-1 = {https://doi.org/10.1007/BF00992696}
}

@inproceedings{reduce,
  author = {Greensmith, Evan and Bartlett, Peter and Baxter, Jonathan},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {T. Dietterich and S. Becker and Z. Ghahramani},
  pages = {},
  publisher = {MIT Press},
  title = {Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning},
  url = {https://proceedings.neurips.cc/paper_files/paper/2001/file/584b98aac2dddf59ee2cf19ca4ccb75e-Paper.pdf},
  volume = {14},
  year = {2001}
}


@misc{reduce2,
  title={Variational Bayesian Inference with Stochastic Search}, 
  author={John Paisley and David Blei and Michael Jordan},
  year={2012},
  eprint={1206.6430},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/1206.6430}, 
}

@misc{vae1,
  title={Auto-Encoding Variational Bayes}, 
  author={Diederik P Kingma and Max Welling},
  year={2022},
  eprint={1312.6114},
  archivePrefix={arXiv},
  primaryClass={stat.ML},
  url={https://arxiv.org/abs/1312.6114}, 
}

@Article{ppca,
  author={Michael E. Tipping and Christopher M. Bishop},
  title={{Probabilistic Principal Component Analysis}},
  journal={Journal of the Royal Statistical Society Series B},
  year=1999,
  volume={61},
  number={3},
  pages={611-622},
  month={},
  keywords={},
  doi={10.1111/1467-9868.00196},
  abstract={Principal component analysis (PCA) is a ubiquitous technique for data analysis and processing, but one which is not based on a probability model. We demonstrate how the principal axes of a set of observed data vectors may be determined through maximum likelihood estimation of parameters in a latent variable model that is closely related to factor analysis. We consider the properties of the associated likelihood function, giving an EM algorithm for estimating the principal subspace iteratively, and discuss, with illustrative examples, the advantages conveyed by this probabilistic approach to PCA.},
  url={https://ideas.repec.org/a/bla/jorssb/v61y1999i3p611-622.html}
}


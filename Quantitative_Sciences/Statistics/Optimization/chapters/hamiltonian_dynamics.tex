\section{Hamiltonian Dynamics Inspired Samplers and Integrators}

    Let us have a system of $N$ point particles in $\mathbb{R}^3$, with the state of each particle fully characterized by its position and momentum vectors. Let us denote the masses of the particles as $m_i$, which will be commonly represented as the $3N \times 3N$ matrix  
    \begin{equation}
      \mathbf{M} = \mathrm{diag}(m_1, \ldots, m_N) \otimes I_3 = \begin{pmatrix}
      m_1\hspace{-2mm}& & & & & & & & \\ 
      & m_1\hspace{-2mm}& & & & & & & \\ 
      & & m_1\hspace{-2mm}& & & & & & \\ 
      & & & m_2 \hspace{-2mm}& & & & & \\[-1mm] 
      & & & & \ddots \hspace{-2mm} & & & & \\[-1mm] 
      & & & & & m_{N-1}\hspace{-4mm} & & & \\ 
      & & & & & & m_N \hspace{-2mm}& & \\ 
      & & & & & & & m_N\hspace{-2mm} & \\ 
      & & & & & & & & m_N\hspace{-2mm} \end{pmatrix},
    \end{equation}
    the position vector of all particles as $\mathbf{q} = (\mathbf{q}_1, \ldots, \mathbf{q}_N) \in \Omega_\mathbf{q} \subset \mathbb{R}^{3N}$, and the momentum vector of all particles as $\mathbf{p} = (\mathbf{p}_1, \ldots, \mathbf{p}_N) \in \Omega_\mathbf{p} \subset \mathbb{R}^{3N}$. The configuration space is therefore $\Omega_\mathbf{q} \times \Omega_\mathbf{p} = \Omega \subset \mathbb{R}^{3N} \times \mathbb{R}^{3N}$. The collective kinetic energy of the system is 
    \begin{equation}
      E (\mathbf{p}) = \frac{1}{2} \mathbf{p}^T \mathbf{M}^{-1} \mathbf{p}
    \end{equation}
    and hence the total energy/Hamiltonian of the particle system is 
    \begin{equation}
      H(\mathbf{q}, \mathbf{p}) = U(\mathbf{q}) + E(\mathbf{p})
    \end{equation}
    Note that the potential energy depends only on the position vector $\mathbf{q}$, while the kinetic energy depends on the momentum $\mathbf{p}$. The equations of motion for Hamiltonian flow states that the derivative of the position is the momentum, and the derivative of the momentum is the force, which is the gradient of the potential. Therefore, finding the time evolution of a system of particles boils down to solving the coupled equations below: 
    \begin{align*}
      \boldsymbol{\dot{q}} & = \mathbf{M}^{-1} \mathbf{p} \\
      \boldsymbol{\dot{p}} & = \mathbf{F}(q) = - \nabla_\mathbf{q} U(\mathbf{q})
    \end{align*}

    The gradient of $H: \Omega_\mathbf{q} \times \Omega_\mathbf{p} \longrightarrow \mathbb{R}$ can be represented as 
    \begin{equation}
      \nabla H(\mathbf{q}, \mathbf{p}) = 
      \begin{pmatrix} \nabla_\mathbf{q} H \\[1mm] \nabla_\mathbf{p} H \end{pmatrix} =
      \begin{pmatrix} \frac{\partial H}{\partial \mathbf{q}} \\[2mm] \frac{\partial H}{\partial \mathbf{p}} \end{pmatrix} = 
      \begin{pmatrix} \partial H / \partial q_1 \\ \vdots \\ \partial H / \partial q_{3N} \\ \partial H / \partial p_1 \\ \vdots \\ \partial H / \partial p_{3N} \end{pmatrix}
    \end{equation}
    But since $H(\mathbf{q}, \mathbf{p}) = U(\mathbf{q}) + E (\mathbf{p})$ is separable and since 
    \begin{align*}
      \nabla_\mathbf{p} E_\mathrm{kin}(p) & = \nabla_\mathbf{p} \frac{1}{2} \mathbf{p}^T \mathbf{M}^{-1} \mathbf{p} \\
      & = \nabla_\mathbf{p} \frac{1}{2} (m_1^{-1} p_{11}^2 + m_1^{-1} p_{12}^2 + m_1^{-1} p_{13}^2 + m_2^{-1} p_{21}^2 + \ldots + m_N^{-1} p_{N3}^2) \\
      & = \big(m_1^{-1} p_{11}, \;m_1^{-1} p_{12}, \;m_1^{-1} p_{13}, \;m_2^{-1} p_{21}, \ldots, m_N^{-1} p_{N3} \big)^T \\
      & = \mathbf{M}^{-1} \mathbf{p} 
    \end{align*}
    we have 
    \begin{equation}
      \nabla H(\mathbf{q}, \mathbf{p}) = \begin{pmatrix} \nabla_\mathbf{q} U (\mathbf{q})\\ \nabla_\mathbf{p} E_\mathrm{kin} (\mathbf{p}) \end{pmatrix} = \begin{pmatrix} \nabla_\mathbf{q} U (\mathbf{q})\\ \mathbf{M}^{-1} \mathbf{p} \end{pmatrix} 
    \end{equation}
    and therefore, the equations of motions can be rewritten as 
    \begin{equation}
      \begin{cases} \boldsymbol{\dot{q}} & = \mathbf{M}^{-1} \mathbf{p} \\
      \boldsymbol{\dot{p}} & = - \nabla_\mathbf{q} U(\mathbf{q}) \end{cases} \implies \begin{pmatrix} \boldsymbol{\dot{q}} \\ \boldsymbol{\dot{p}} \end{pmatrix} = \begin{pmatrix} \mathbf{0} & \mathbf{I}_{3N} \\ -\mathbf{I}_{3N} & \mathbf{0} \end{pmatrix} \begin{pmatrix} \nabla_\mathbf{q} U (\mathbf{q})\\ \mathbf{M}^{-1} \mathbf{p} \end{pmatrix} = \mathbf{J} \nabla H(\mathbf{q}, \mathbf{p})
    \end{equation}
    Given an initial point $(\mathbf{q}(0), \mathbf{p}(0)) \in \Omega_{\mathbf{q}} \times \Omega_{\mathbf{p}}$, the Hamiltonian flow map satisfies 
    \begin{equation}
      \Phi_t \big( \mathbf{q}(0), \mathbf{p}(0) \big) = \big(\mathbf{q}(t), \mathbf{p}(t)\big)
    \end{equation}

  \subsection{Properties of Hamiltonian Flow Maps}

    Hamiltonian flow maps $\Phi_t : \Omega \longrightarrow \Omega$ have important properties. 
    \begin{enumerate}
        \item The collection of flow maps form an algebraic group under the composition operator
        \begin{equation}
          \Phi_t \circ \Phi_s = \Phi_{t + s}
        \end{equation}
        with the identity element $\Phi_0 = \mathrm{Id}$ (the path map that doesn't go anywhere), and well-defined inverse 
        \begin{equation}
          \Phi_t^{-1} = \Phi_{-t}
        \end{equation}
        \item Symmetry holds in the sense that 
        \begin{equation}
          S \circ \Phi_t \circ S = \Phi_{-t}
        \end{equation}
        where the function $S: (\mathbf{q}, \mathbf{p}) \mapsto (\mathbf{q}, -\mathbf{p})$ flips the momentum. 
        \item Total energy is conserved under $\Phi_t$. 
        \begin{equation}
          H\big( \mathbf{q}(t), \mathbf{p}(t) \big) = H\big( \mathbf{q}(0), \mathbf{p}(0) \big)
        \end{equation}
        \item In the absence of an external force, the total momentum is conserved under $\Phi_t$. 
    \end{enumerate}

    \subsubsection{The Symplectic Property}

      The final property is less obvious. A fundamental property of solutions of Hamiltonian differential equations is that the collection $(\Phi_t)_{t \in \mathbb{R}}$ of associated flow maps has a symplectic group structure, which means that the symplectic 2-form is preserved under the action of each group element. 
      \begin{enumerate}
        \item A \textbf{1-form} $\alpha$ defined on $\mathbb{R}^{6N}$ is a family of linear mappings such that for every $\mathbf{x} \in \mathbb{R}^{6N}$, $\alpha(\mathbf{x})$ is a linear map from $\mathbb{R}^{6N}$ to $\mathbb{R}$. That is, given a linear map $\mathbf{a}: \mathbb{R}^{6N} \longrightarrow \mathbb{R}^{6N}$, we may define a one-form associated to this vector field $\mathbf{a} \mapsto \alpha$ by 
        \begin{equation}
          \alpha(\mathbf{x}) (\boldsymbol{\xi}) = \mathbf{a} (\mathbf{x})^T \boldsymbol{\xi}
        \end{equation}
        \item The \textbf{differential} of a function $g: \mathbb{R}^{6N} \longrightarrow \mathbb{R}$, denoted $\mathrm{d}g$, is a family of linear mappings from vectors $\boldsymbol{\xi} \in \mathbb{R}^{6N}$ into the reals defined by 
        \begin{equation}
          \mathrm{d}g (\mathbf{q}, \mathbf{p}) (\boldsymbol{\xi}) = \nabla g(\mathbf{q}, \mathbf{p})^T \boldsymbol{\xi}
        \end{equation}
        Therefore, we can see that the differential is an example of a 1-form. 
        \item The wedge product of 1-forms $\alpha, \beta$ is a \textbf{2-form}, which can be viewed as a quadratic form, i.e. a scalar-valued function of two vectors which is linear in each argument. It is written $\alpha \wedge \beta$ and is defined, for vectors $\boldsymbol{\xi}, \boldsymbol{\eta} \in \mathbb{R}^{6N}$ by 
        \begin{equation}
          (\alpha \wedge \beta)(\boldsymbol{\xi}, \boldsymbol{\eta}) \coloneqq \alpha(\boldsymbol{\xi}) \beta(\boldsymbol{\eta}) - \alpha(\boldsymbol{\eta}) \beta(\boldsymbol{\xi})
        \end{equation}
      \end{enumerate}
      Now, let $q_i, p_j: \mathbb{R}^{6N} \longrightarrow \mathbb{R}$ be the component functions mapping $(\mathbf{q}, \mathbf{p}) \mapsto q_i, p_j$, respectively, where $1 \leq i, j \leq 3N$. Then, $\mathrm{d} q_i, \mathrm{d} p_i$ are examples of differential 1-forms. The wedge product of the coordinate differentials $\mathrm{d}q_i, \mathrm{d} p_i$ can be written
      \begin{equation}
        (\mathrm{d}q_i \wedge \mathrm{d} p_i)(\boldsymbol{\xi}, \boldsymbol{\eta}) = \xi_i\eta_{i + 3N} - \xi_{i + 3N} \eta_i = \boldsymbol{\xi}^T \mathbf{J}^{(i)} \boldsymbol{\eta}
      \end{equation}
      where $\mathbf{J}$ is the matrix which has zeros everywhere except for $(\mathbf{J}^{(i)})_{i, 3N + i} = 1, (\mathbf{J}^{(i)})_{i + 3N, i} = -1$. Summing these terms results in the symplectic 2-form, denoted $\psi_S$: 
      \begin{equation}
        \psi_S \coloneqq \sum_{i=1}^{3N} \mathrm{d}q_i \wedge \mathrm{d} p_i (\boldsymbol{\xi}, \boldsymbol{\eta}) = \boldsymbol{\xi}^T \bigg( \sum_{i=1}^{3N} \mathbf{J}^{(i)} \bigg) \boldsymbol{\eta} = \boldsymbol{\xi}^T \mathbf{J} \boldsymbol{\eta}
      \end{equation}

      That is, since $\Phi_t: \mathbb{R}^{6N} \longrightarrow \mathbb{R}^{6N}$, then its Jacobian $\nabla \Phi_t$ is a $6N \times 6N$ matrix, and the condition above implies that 
      \begin{equation}
        \nabla \Phi_t^T \mathbf{J} \nabla \Phi_t = \mathbf{J} \text{ for all } t \in \mathbb{R}
      \end{equation}
      Denoting the Jacobian $\nabla \Phi_t$ as $\Phi_t^\prime$, we can take the determinant of both sides to find that 
      \begin{equation}
        \det\big(\nabla \Phi_t^T \mathbf{J} \nabla \Phi_t\big) = \det(\mathbf{J}) \implies \det(\nabla \Phi_t^T) \, \det(\mathbf{J})\, \det(\nabla \Phi_t) = \det(\mathbf{J})
      \end{equation}
      and so $\det{\Phi_t^\prime}^2 = 1$. In the case of a flow map, we know that for $t \rightarrow 0$, the flow map $\Phi_{t}$ would essentially reduce to the identity map $\mathrm{Id}$, and so
      \begin{equation}
        \lim_{t \rightarrow 0} \Phi_t^\prime = 1 \implies \det{\Phi_t^\prime} = + 1
      \end{equation}
      due to the determinant being a continuous function of $t$. Therefore a consequence of this is that $\Phi_t$ is volume preserving. 

  \subsection{Common Symplectic Integrators}

    We wish to solve for $\Phi_t$ numerically by constructing an approximation with acceptable error. 
    \begin{equation}
      (\hat{\mathbf{q}}_{n+1}, \hat{\mathbf{p}}_{n+1}) = \hat{\Phi}_{h} (\hat{\mathbf{q}}_{n}, \hat{\mathbf{p}}_{n})
    \end{equation}
    with $(\hat{\mathbf{q}_0}, \hat{\mathbf{p}_0}) = \big( \mathbf{q}(0), \mathbf{p}(0)\big)$. There is the obvious error stemming from the choice of a large $h$, but what is more important is that the geometric structure of the manifold $\big( \mathbf{q}(t), \mathbf{p}(t)\big)_{t > 0}$ corresponding to the trajectory of the exact solution is replicated by the discrete approximation $\big(\hat{\mathbf{q}}_n, \hat{\mathbf{p}}_n\big)_{n \in \mathbb{N}}$. The best way to do this is to construct such a structure preserving integration scheme by designing the integration map $\hat{\Phi}_{h}$ in such a way that the symplectic 2-form is preserved. That is, construct a \textbf{symplectic integration scheme} $\hat{\Phi}_{h}$ such that 
    \begin{equation}
      (\hat{\Phi}_{h}^\prime)^T \mathbf{J} \; \hat{\Phi}_{h}^\prime = \mathbf{J}
    \end{equation}

    \subsubsection{Euler and Symplectic Euler}

      The standard Euler integration scheme has many shortfalls, such as error growth and stability issues. Its algorithmic form reads 
      \begin{align*}
        \mathbf{q}_{k + 1} & = \mathbf{q}_k + h \, \mathbf{M}^{-1} \mathbf{p}_k \\
        \mathbf{p}_{k + 1} & = \mathbf{p}_k - h \, \nabla U(\mathbf{q}_k)
      \end{align*}
      Therefore, modified version of this scheme, called the \textbf{symplectic Euler integration scheme}, is used, which reads 
      \begin{align*}
        \mathbf{p}_{k+1} & = \mathbf{p}_k - h \, \nabla U(\mathbf{q}_k) \\
        \mathbf{q}_{k+1} & = \mathbf{q}_k + h \mathbf{M}^{-1} \mathbf{p}_{k+1} 
      \end{align*}
      which has the slight modification that to advance the timestep, we use the first equation to compute $\mathbf{p}_{k+1}$ and then insert this in the second. 

    \subsubsection{Verlet}

      One of the most commonly used symplectic numerical integrators is the \textbf{Stormer-Verlet method}, which in algorithmic form reads 
      \begin{align*}
        \mathbf{q}_{k + 1/2} & = \mathbf{q}_k + \frac{h}{2} \mathbf{M}^{-1} \mathbf{p}_k \\
        \mathbf{p}_{k+1} & = \mathbf{p}_k - h\, \nabla U(\mathbf{q}_{k+1/2}) \\
        \mathbf{q}_{k+1} & = \mathbf{q}_{k+1/2} + \frac{h}{2} \mathbf{M}^{-1} \mathbf{q}_{k+1} 
      \end{align*}
      We can see that this algorithm updates $\mathbf{q}_k \mapsto \mathbf{q}_{k+1/2}$ with the given $\mathbf{p}_k$ over half-time step, and then updates the force field vector with the new position vector $- \nabla U(\mathbf{q}_{k + 1/2})$. This new force is used to update the momentum $\mathbf{p}_k \mapsto \mathbf{p}_{k+1}$. Finally, the position is updated with the new momentum: $\mathbf{q}_{k + 1/2} \mapsto \mathbf{q}_{k+1}$. A closely related, alternative form is the \textbf{Velocity-Verlet method}, which updates the momentum first, then position, and finally momentum. 
      \begin{align*}
        \mathbf{p}_{k + 1/2} & = \mathbf{p}_k - \frac{h}{2} \nabla U (\mathbf{q}_k) \\
        \mathbf{q}_{k+1} & = \mathbf{q}_k + h \, \mathbf{M}^{-1} \mathbf{p}_{k + 1/2} \\
        \mathbf{p}_{k + 1} & = \mathbf{p}_{k + 1/2} - \frac{h}{2} \nabla U (\mathbf{q}_{k+1}) 
      \end{align*}
      The Velocity Verlet method is 2nd order (globally). While the algorithm may not look like a second order, we can see that with simple substitution, we have a second order evaluation of $\mathbf{q}_{k+1}$ (up to $h^2$ term) followed by an evaluation of $\mathbf{p}_{k+1}$. 
      \begin{align*}
        \mathbf{q}_{k+1} & = \mathbf{q}_k + h \mathbf{M}^{-1} \mathbf{p}_{k + 1/2} \\
        & = \mathbf{q}_k + h \mathbf{M}^{-1} \bigg( \mathbf{p}_k - \frac{h}{2} \nabla U (\mathbf{q}_k) \bigg) \\
        & = \mathbf{q}_k + h \mathbf{M}^{-1} \mathbf{p}_k - \frac{1}{2} h^2 \nabla U(\mathbf{q}_k) \\
        \mathbf{p}_{k+1} & = \mathbf{p}_{k + 1/2} - \frac{h}{2} \nabla U (\mathbf{q}_{k+1}) \\
        & = \bigg(\mathbf{p}_k - \frac{h}{2} \nabla U(\mathbf{q}_k)\bigg) - \frac{h}{2} \nabla U(\mathbf{q}_{k+1}) \\
        & = \mathbf{p}_k - \frac{h}{2} \big[ \nabla U(\mathbf{q}_k) + \nabla U(\mathbf{q}_{k+1}) \big]
      \end{align*}
      Setting $\mathbf{M} = I$, $\mathbf{F} = - \nabla_\mathbf{q} U$ and expanding the $\mathbf{q}_{k+1}$ in the inner term, we get 
      \begin{align*}
        \mathbf{q}_{k+1} & = \mathbf{q}_k + h \mathbf{p}_k + \frac{1}{2} h^2 \mathbf{F} (\mathbf{q}_k) \\
        \mathbf{p}_{k+1} & = \mathbf{p}_k + \frac{h}{2} \bigg[ \mathbf{F} (\mathbf{q}_k) + \mathbf{F} \Big( \mathbf{q}_{k} + h \mathbf{p}_k + \frac{1}{2} h^2  \mathbf{F}(\mathbf{q}_k) \Big) \bigg]
      \end{align*}
      The first equation is already a polynomial, i.e. it is in the form of a series expansion in powers of $h$ where the coefficients are functions of the starting point $(\mathbf{q}_k, \mathbf{p}_k)$. The second equation may be written as a series expansion in powers of $h$ as well. 
      \begin{align*}
        \mathbf{p}_{k+1} & = \mathbf{p}_k + \frac{h}{2} \mathbf{F}(\mathbf{q}_k) + \frac{h}{2} \bigg[ \mathbf{F} (\mathbf{q}_k) + h \mathbf{F}^\prime (\mathbf{q}_k) \big( \mathbf{p}_k + \frac{h}{2} \mathbf{F}(\mathbf{q}_k) \big) + \frac{h^2}{2} \mathbf{F}^{\prime\prime} (\mathbf{q}_k) \big( \mathbf{p}_k + \frac{h}{2} \mathbf{F}(\mathbf{q}_k)\big)^2 + \ldots \bigg]
      \end{align*}
      which we will neglect terms involving 4th and higher powers of $h$. Combining terms of like powers of $h$, we have 
      \begin{equation}
        \mathbf{p}_{k+1} = \mathbf{p}_k + h \mathbf{F}(\mathbf{q}_k) + \frac{h^2}{2} \mathbf{p}_k \mathbf{F}^\prime (\mathbf{q}_k) + \frac{h^3}{4} \big[ \mathbf{F}^\prime (\mathbf{q}_k) \mathbf{F}(\mathbf{q}_k) + \mathbf{p}_k^2 \mathbf{F}^{\prime\prime} (\mathbf{q}_k)\big] + \mathcal{O}(h^4)
      \end{equation}
      We compare this against the Taylor expansion of the exact solution $\mathbf{z}(t) \coloneqq \big( \mathbf{q}(t), \mathbf{p}(t) \big)$. We evaluate 
      \begin{align*}
        \mathbf{q}(t + h) & = \mathbf{q}(t) + h \mathbf{q}^\prime (t) + \frac{h^2}{2} \mathbf{q}^{\prime\prime} (t) + \frac{h^3}{6} \mathbf{q}^{\prime\prime\prime} (t) + \mathcal{O}(h^4) \\
        & = \mathbf{q}(t) + h \mathbf{p} (t) + \frac{h^2}{2} \mathbf{F}(\mathbf{q}(t)) + \frac{h^3}{6} \mathbf{F}^\prime (\mathbf{q}(t)) \, \mathbf{p}(t) + \mathcal{O}(h^4) 
      \end{align*}
      where the calculations followed from the fact that $\mathbf{q}^\prime = \mathbf{p}$, which means that $\mathbf{q}^{\prime\prime} = \mathbf{p}^\prime = \mathbf{F}(\mathbf{q})$, which means that $\mathbf{q}^{\prime\prime\prime} = \frac{d}{dt} \mathbf{F}(\mathbf{q}) = \mathbf{F}^\prime (\mathbf{q}) \, \mathbf{q}^\prime = \mathbf{F}^\prime (\mathbf{q}) \, \mathbf{p}$. Then, we have 
      \begin{align*}
        \mathbf{p}(t + h) & = \mathbf{p}(t) + h \mathbf{p}^\prime (t) + \frac{h^2}{2} \mathbf{p}^{\prime\prime} (t) + \frac{h^3}{6} \mathbf{p}^{\prime\prime\prime} (t) + \mathcal{O}(h^4) \\
        & = \mathbf{p}(t) + h \mathbf{F} (\mathbf{q}(t)) + \frac{h^2}{2} \mathbf{p}(t) \, \mathbf{F}^\prime (\mathbf{q}(t)) + \frac{h^3}{6} \big[ \mathbf{p}(t)^2 \, \mathbf{F}^{\prime\prime} (\mathbf{q}(t)) + \mathbf{F}^\prime (\mathbf{q}(t)) \, \mathbf{F} (\mathbf{q}(t)) \big] + \mathcal{O}(h^4) 
      \end{align*}
      which follows from the fact that $\mathbf{p}^{\prime\prime\prime} = (\mathbf{p} \, \mathbf{F}^\prime)^\prime = \mathbf{p}^2 \, \mathbf{F}^{\prime\prime} + \mathbf{F}^\prime \mathbf{F}$. 
      \\
      Now, let's compare them. Let us have initial point $\mathbf{z}_k = \mathbf{z}(t) = \big( \mathbf{q}(t), \mathbf{p}(t)\big) = \big( \mathbf{q}_k, \mathbf{p}_k\big)$ at time $t$. The actual flow and the integrator takes in $\mathbf{z}(t)$ and $\mathbf{z}_k$, respectively, but they are the same initial point, so we will label them with $\mathbf{z} = (\mathbf{q}, \mathbf{p})$. We can use the flow map $\Phi_h\big( \mathbf{z}(t) \big)$ to evaluate the exact position $\mathbf{z}(t + h)$ after time $h$. That is, $\Phi_h \big(\mathbf{z}(t)\big) \coloneqq \mathbf{z}\big( h, \mathbf{z}(t)\big) = \mathbf{z} (t + h)$. The Taylor expansion of this flow map is 
      \begin{equation}
        \Phi_h \big(\mathbf{z}(t)\big) = \mathbf{z}(t + h) = \begin{cases} 
          \mathbf{q}(t + h) = \mathbf{q} + h \mathbf{p} + \frac{h^2}{2} \mathbf{F} + \frac{h^3}{6} \mathbf{F}^\prime \, \mathbf{p} + \mathcal{O}(h^4) \\
          \mathbf{p}(t + h) = \mathbf{p} + h \mathbf{F} + \frac{h^2}{2} \mathbf{p} \, \mathbf{F}^\prime + \frac{h^3}{6} \big[ \mathbf{p}^2 \, \mathbf{F}^{\prime\prime} + \mathbf{F}^\prime\, \mathbf{F} \big] + \mathcal{O}(h^4)
        \end{cases}
      \end{equation}
      The numerical integrator would calculate something slightly different. That is, given the initial point $\mathbf{z}(t) = \mathbf{z}_k$, $\hat{\Phi}_h \big(\mathbf{z}(t)\big) = \mathbf{z}_{k+1}$ is the numerical approximation after time $h$. The Taylor expansion of this integrator is 
      \begin{equation}
        \hat{\Phi}_h \big( \mathbf{z}(t)\big) = \mathbf{z}_{k+1} = \begin{cases}
          \mathbf{q}_{k+1} = \mathbf{q} + h \mathbf{p} + \frac{h^2}{2} \mathbf{F} \\
          \mathbf{p}_{k+1} = \mathbf{p} + h \mathbf{F} + \frac{h^2}{2} \mathbf{p} \mathbf{F}^\prime + \frac{h^3}{4} \big[ \mathbf{F}^\prime \mathbf{F} + \mathbf{p}^2 \mathbf{F}^{\prime\prime} \big] + \mathcal{O}(h^4)
        \end{cases}
      \end{equation}
      We should get $\mathbf{z}_{k+1} \approx \mathbf{z}(t + h)$, by looking at the differences, we find that these differ in the third (and higher) order terms. 
      \begin{align*}
        \mathbf{q}_{k+1} - \mathbf{q}(t + h) & = - \frac{h^3}{6} \mathbf{F}^\prime \mathbf{p} + \mathcal{O}(h^4) \\
        \mathbf{p}_{k+1} - \mathbf{p}(t + h) & = \frac{h^3}{12} \big[ \mathbf{p}^2 \mathbf{F}^{\prime\prime} + \mathbf{F}^\prime \mathbf{F} \big] + \mathcal{O}(h^4)
      \end{align*}
      We can, in summary, state that the \textit{local} error is third order
      \begin{equation}
        ||\hat{\Phi}_h (\mathbf{z}) - \Phi_h (\mathbf{z})|| = \kappa(\mathbf{z}) h^3 + \mathcal{O}(h^4)
      \end{equation}
      where $\kappa(\mathbf{z}) \coloneqq \kappa(\mathbf{q}, \mathbf{p})$ is a function of the position and momentum. We may then define the maximum local error as 
      \begin{equation}
        \bar{K} \coloneqq \max_{t \in [0, \tau]} \kappa \big(\mathbf{z}(t)\big)
      \end{equation}
      and summing this all up leads to the total error being bounded by a constant multiple of $h^2$, achieving consistency of order 2. 

    \subsubsection{Yoshida 4th-Order}

      The Yoshida Fourth Order Scheme is overall three iterations of velocity Verlet (making it also a symplectic integrator), using stepsizes $\tau_0 h, \tau_1 h, \tau_0 h$, respectively. We write this with subindices $\alpha, \beta$ to indicate the intermediate stages, and abuse our notation to be similar to those in computer science. 
      \begin{align*}
        \mathbf{p}_\alpha & = \mathbf{p} - (\tau_0\, h/2) \nabla U(\mathbf{q}) \\
        \mathbf{q}_\alpha & = \mathbf{q} + (\tau_0 \, h) \mathbf{M}^{-1} \mathbf{p}_\alpha \\
        \mathbf{p}_\alpha & = \mathbf{p}_\alpha - (\tau_0 \, h / 2) \nabla U(\mathbf{q}_\alpha) \\
        \mathbf{p}_\beta & = \mathbf{p}_\alpha - (\tau_1 \, h/2) \nabla U(\mathbf{q}_\alpha) \\
        \mathbf{q}_\beta & = \mathbf{q}_\alpha + (\tau_1 \, h) \mathbf{M}^{-1} \mathbf{p}_\beta \\
        \mathbf{p}_\beta & = \mathbf{p}_\beta - (\tau_1 \, h/2) \nabla U(\mathbf{q}_\beta) \\
        \mathbf{p} & = \mathbf{p}_\beta - (\tau_0 \, h/2) \nabla U(\mathbf{q}_\beta) \\
        \mathbf{q} & = \mathbf{q}_\beta + (\tau_0 \, h) \mathbf{M}^{-1} \mathbf{p} \\
        \mathbf{p} & = \mathbf{p} - (\tau_0 \, h/2) \nabla U(\mathbf{q}) 
      \end{align*}
      The equations can be written in a simplified form, combining several of the steps. This scheme requires three new evaluations of the force $\nabla U$ per iteration, making it significantly more expensive than the vanilla second-order Verlet method. However, this method is of 4th order. 

  \subsection{Adjoint Method}

    For the true flow map $\Phi_t: \Omega \longrightarrow \Omega$, we know that due to time-reversibility, the inverse map is the same as the same map with a backward timestep: 
    \begin{equation}
      \Phi_t^{-1} = \Phi_{-t}
    \end{equation}
    For a discretized integrator $\hat{\Phi}$, this may not always be the case (even though symplectic forms might be preserved). Therefore, we can define the \textbf{adjoint} of the numerical scheme to be 
    \begin{equation}
      (\hat{\Phi}_{h})^\dagger \coloneqq \hat{\Phi}_{-h}^{-1}
    \end{equation}
    Clearly, the adjoint of the true flow map is the same as the original, i.e. $\Phi_t$ is self-adjoint. Furthermore, the adjoint of the adjoint of any flow map is the original flow map. 

    \subsubsection{Adjoint of Euler's Method}

      Consider Euler's method $\hat{\Phi}_{h}$ in fully general form, with $\mathbf{z} = (\mathbf{q}, \mathbf{p})^T$. Then, we have 
      \begin{align*}
        \hat{\Phi}_{h}: \mathbf{z}_k \mapsto \mathbf{z}_{k+1} & \text{ such that } \mathbf{z}_{k + 1} = \mathbf{z}_k + h \,\mathbf{f}( \mathbf{z}_k) \\
        \hat{\Phi}_{h}^{-1}: \mathbf{z}_k \mapsto \mathbf{z}_{k+1} & \text{ such that } \mathbf{z}_{k} = \mathbf{z}_{k+1} + h \,\mathbf{f}( \mathbf{z}_{k+1}) \\
        \hat{\Phi}_h^\dagger = \hat{\Phi}_{-h}^{-1}: \mathbf{z}_k \mapsto \mathbf{z}_{k+1} & \text{ such that } \mathbf{z}_{k} = \mathbf{z}_{k+1} - h \,\mathbf{f}( \mathbf{z}_{k+1}) \iff \mathbf{z}_{k+1} = \mathbf{z}_k + h\, \mathbf{f}(\mathbf{z}_{k+1}) 
      \end{align*}
      and so 
      and clearly, $\hat{\Phi}_{-h}^{-1}$ defines $\mathbf{z}_{k+1}$ implicitly. 

    \subsubsection{Adjoint of Symplectic Euler's Method}

      To construct the adjoint method of the symplectic Euler scheme, we see that that $\hat{\Phi}_{h}^{-1}$ maps $(\mathbf{q}_k, \mathbf{p}_k) \mapsto (\mathbf{q}_{k+1}, \mathbf{p}_{k+1})$ such that
      \begin{align*}
        \mathbf{p}_k & = \mathbf{p}_{k+1} - h \, \nabla U(\mathbf{q}_{k+1}) \\
        \mathbf{q}_k & = \mathbf{q}_{k+1} + h \, \mathbf{M}^{-1} \mathbf{p}_{k} 
      \end{align*}
      and therefore $\hat{\Phi}_{- h}^{-1}$ maps $(\mathbf{q}_k, \mathbf{p}_k) \mapsto (\mathbf{q}_{k+1}, \mathbf{p}_{k+1})$ such that
      \begin{align*}
        \mathbf{q}_{k+1} & = \mathbf{q}_k + h \, \mathbf{M}^{-1} \mathbf{p}_k \\
        \mathbf{p}_{k+1} & = \mathbf{p}_k - h \, \nabla U (\mathbf{q}_{k+1})
      \end{align*}
      We find that the adjoint of the symplectic Euler scheme is explicitly defined. 

  \subsection{Building Symplectic Integrators: Splitting Methods}

    Let $H(\mathbf{q}, \mathbf{p}) = H_1 (\mathbf{q}, \mathbf{p}) + H_2 (\mathbf{q}, \mathbf{p})$ have flow map $\Phi_t$, and let $\Phi_t^1, \Phi_t^2$ be the flow maps for the systems with Hamiltonians $H_1, H_2$ respectively. We propose that the map 
    \begin{equation}
      \Psi_t \coloneqq \Phi_t^1 \circ \Phi_t^2
    \end{equation}
    is an approximation of $\Phi_t$. Notice that the order of composition can be arbitrary due to commutativity of addition. For $\Psi_t$ to be a first order approximation of $\Phi_t$, we need at least 
    \begin{equation}
      ||\Psi_t (\mathbf{z}) - \Phi_t (\mathbf{z}) || \leq C(\mathbf{z}) t^2
    \end{equation}
    That is, the local error must be 2nd order. Let $\mathbf{z}_0 = (\mathbf{q}_0, \mathbf{p}_0) \in \Omega$ be some arbitrary initial point, and let us flow it across time $t$ to the new point $\Phi_t (\mathbf{z}_0)$. We do a first-order Taylor expansion the flow with respect to the time $t$, 
    \begin{align*}
      \Phi_t (\mathbf{z}_0) & = \mathbf{z}_0 + t \big[\Phi_t (\mathbf{z}_0)\big]^\prime + \mathcal{O}(t^2) \\
      & = \mathbf{z}_0 + t \mathbf{J} \nabla_\mathbf{z} H(\mathbf{z}_0) + \mathcal{O}(t^2) \\
      & = \mathbf{z}_0 + t (\mathbf{J} \nabla_\mathbf{z} H_1 + \mathbf{J} \nabla_\mathbf{z} H_2) (\mathbf{z}_0) + \mathcal{O}(t^2) \tag{3.1} \label{3.1}
    \end{align*}
    On the other hand, we have
    \begin{align*}
      \Phi_t^1 (\mathbf{z}_0) & = \mathbf{z}_0 + t \mathbf{J} \nabla_\mathbf{z} H_1 (\mathbf{z}_0) + \mathcal{O}(t^2) \\
      \Phi_t^2 (\mathbf{z}_0) & = \mathbf{z}_0 + t \mathbf{J} \nabla_\mathbf{z} H_2 (\mathbf{z}_0) + \mathcal{O}(t^2) 
    \end{align*}
    and composing them gives 
    \begin{align*}
      \Phi_t^1 \circ \Phi_t^2 & = \mathbf{z} + t \mathbf{J} \nabla H_2 (\mathbf{z}) + t\mathbf{J} \nabla H_1 \big( \mathbf{z} + t \mathbf{J} \nabla H_2 (\mathbf{z}) \big) + \mathcal{O}(t^2) \\
      & = \mathbf{z} + t \mathbf{J} \nabla H_2 (\mathbf{z}) + t \mathbf{J} \nabla H_1 (\mathbf{z}) + \underbrace{t^2 (\mathbf{J} \nabla H_1) \circ (\mathbf{J} \nabla H_2)  (\mathbf{z})\big)}_{\mathcal{O}(t^2)} + \mathcal{O}(t^2) \\
      & = \mathbf{z} + t (\mathbf{J} \nabla H_2 + \mathbf{J} \nabla H_1) (\mathbf{z}) + \mathcal{O}(t^2)
    \end{align*}
    which agrees with the terms of \eqref{3.1} up to second order, and therefore the local error is indeed second order. 

    \subsubsection{Symplectic Euler Constructed from Splitting Schemes}

      Let $H_1 (\mathbf{q}, \mathbf{p}) = \mathbf{p}^T \mathbf{M}^{-1} \mathbf{p}/2$ and $H_2 (\mathbf{q}, \mathbf{p}) = U(\mathbf{q})$, then the splitting method for $H = H_1 + H_2$ can be obtained by determining the flow maps for each of the two parts. For $H_1$ and $H_2$ we have the differential equations 
      \begin{equation}
        \begin{cases} \boldsymbol{\dot{q}} = \mathbf{M}^{-1} \mathbf{p} \\ \boldsymbol{\dot{p}} = - \nabla_\mathbf{q} U (\mathbf{q}) \end{cases} \implies H_1 \begin{cases} \boldsymbol{\dot{q}} = \mathbf{M}^{-1} \mathbf{p} \\ \boldsymbol{\dot{p}} = \mathbf{0} \end{cases} \text{ and } H_2 \begin{cases} \boldsymbol{\dot{q}} = \mathbf{0} \\ \boldsymbol{\dot{p}} = - \nabla_\mathbf{q} U (\mathbf{q}) \end{cases}
      \end{equation}
      The fact that $\boldsymbol{\dot{p}} = 0$ for $H_1$ tells us that the momentum is constant and therefore the trajectory $\mathbf{q}$ is linear, and hence the discrete flow map $\hat{\Phi}_h^1$ is 
      \begin{equation}
        \hat{\Phi}_h^1 \begin{pmatrix} \mathbf{q}_k \\ \mathbf{p}_k \end{pmatrix} = \begin{pmatrix} \mathbf{q}_{k+1} \\ \mathbf{p}_{k+1} \end{pmatrix} = \begin{pmatrix} \mathbf{q}_k + h \mathbf{M}^{-1} \mathbf{p}_k \\ \mathbf{p}_k \end{pmatrix}
      \end{equation}
      The flow map $\hat{\Phi}_h^2$ is 
      \begin{equation}
        \hat{\Phi}_h^2 \begin{pmatrix} \mathbf{q}_k \\ \mathbf{p}_k \end{pmatrix} = \begin{pmatrix} \mathbf{q}_{k+1} \\ \mathbf{p}_{k+1} \end{pmatrix} = \begin{pmatrix} \mathbf{q}_k \\ \mathbf{p}_k - h \nabla U (\mathbf{q}_k) \end{pmatrix}
      \end{equation}
      The composition of these maps is 
      \begin{equation}
        \hat{\Phi}_h^1 \circ \hat{\Phi}_h^2 = \begin{cases} 
        \mathbf{q}_{k+1} = \mathbf{q}_k + h \mathbf{M}^{-1} \mathbf{p}_{k+1} \\
        \mathbf{p}_{k+1} = \mathbf{p}_k - h \nabla U (\mathbf{q}_k) 
        \end{cases}
      \end{equation}
      which is precisely the symplectic Euler method. Composing the same two maps in the opposite order gives the adjoint symplectic Euler method. 
      \begin{equation}
        \big(\hat{\Phi}_h^1 \circ \hat{\Phi}_h^2 \big)^\dagger = \hat{\Phi}_h^2 \circ \hat{\Phi}_h^1 = \begin{cases} 
        \mathbf{q}_{k+1} = \mathbf{q}_k + h \mathbf{M}^{-1} \mathbf{p}_k \\
        \mathbf{p}_{k+1} = \mathbf{p}_k - h \nabla U(\mathbf{q}_{k+1}) 
        \end{cases}
      \end{equation}

    \subsubsection{Symplectic Verlet Method from Splitting Schemes}

      For the symplectic Euler method $\hat{\Phi}_h$ and its adjoint method $\hat{\Phi}_h^\dagger$, consider the composition 
      \begin{equation}
        \mathcal{K}_h \coloneqq \hat{\Phi}_{h/2}^\dagger \circ \hat{\Phi}_{h/2}
      \end{equation}
      Computing this, we have $\hat{\Phi}_{h/2} (\mathbf{q}_k, \mathbf{p}_k) = (\mathbf{q}_{k + 1/2}, \mathbf{p}_{k + 1/2})$, and $\hat{\Phi}_{h/2}^\dagger (\mathbf{q}_{k + 1/2}, \mathbf{p}_{k + 1/2}) = (\mathbf{q}_{k+1}, \mathbf{p}_{k+1})$ defined 
      \begin{align*}
        \hat{\Phi}_{h/2} \begin{pmatrix} \mathbf{q}_k \\ \mathbf{p}_k \end{pmatrix} & = \begin{cases} 
        \mathbf{q}_{k + 1/2} = \mathbf{q}_k + \frac{h}{2} \mathbf{M}^{-1} \mathbf{p}_{k + 1/2} \\
        \mathbf{p}_{k + 1/2} = \mathbf{p}_k - \frac{h}{2} \nabla U(\mathbf{q}_k)
        \end{cases} \\
        \hat{\Phi}_{h/2}^\dagger \begin{pmatrix} \mathbf{q}_{k+ 1/2} \\ \mathbf{p}_{k+ 1/2} \end{pmatrix} & = \begin{cases} 
        \mathbf{q}_{k+1} = \mathbf{q}_{k + 1/2} + \frac{h}{2} \mathbf{M}^{-1} \mathbf{p}_{k + 1/2} \\
        \mathbf{p}_{k+1} = \mathbf{p}_{k + 1/2} - \frac{h}{2} \nabla U (\mathbf{q}_{k+1})
        \end{cases} 
      \end{align*}
      This composition simplifies to 
      \begin{equation}
        \mathcal{K}_h \coloneqq \hat{\Phi}_{h/2}^\dagger \circ \hat{\Phi}_{h/2} = \begin{cases} 
        \mathbf{p}_{k + 1/2} & = \mathbf{p}_k - \frac{h}{2} \nabla U(\mathbf{q}_k) \\
        \mathbf{q}_{k + 1} & = \mathbf{q}_k + h \mathbf{M}^{-1} \mathbf{p}_{k + 1/2} \\
        \mathbf{p}_{k + 1} & = \mathbf{q}_{k + 1/2} - \frac{h}{2} \nabla U (\mathbf{q}_{k+1})
        \end{cases}
      \end{equation}
      which is precisely the velocity Verlet method in Hamiltonian form. Since we have obtained this method as the composition of two symplectic maps, and the symplectic maps form a group, we know that this method will also be symplectic. Similarly, we can construct the adjoint map of $\mathcal{K}_h$ by simply taking the composition in the other direction, which we see to be the same (i.e. $\mathcal{K}_h$ is symmetric/self-adjoint). 
      \begin{equation}
        \mathcal{K}_h^\dagger \coloneqq \big(\hat{\Phi}_{h/2}^\dagger \circ \hat{\Phi}_{h/2}\big)^\dagger = \hat{\Phi}_{h/2}^\dagger \circ \hat{\Phi}_{h/2} = \mathcal{K}_h
      \end{equation}

    \subsubsection{General Composition Methods}

      In general, if we have any two symplectic numerical methods, say $\hat{\Phi}_h^1$ and $\hat{\Phi}_h^2$, then the composition 
      \begin{equation}
        \hat{\Phi}_h \coloneqq \hat{\Phi}_h^1 \circ \hat{\Phi}_h^2
      \end{equation}
      is another symplectic numerical method. The order of this new method is typically the minimum of the orders of the two methods involved, but it can be higher, as the example of the Verlet method (constructed by composing the Euler and its adjoint, both of order 1). 

  \subsection{Modified, Shadow Hamiltonians}

    We already know that our symplectic discretized schemes successfully conserves the 2-form, but these schemes do not actually conserve the Hamiltonian. Let us take the 1-dimensional harmonic oscillator with frequency $\omega$, which has the Hamiltonian $H(q, p) = p^2/2 + \omega^2 q^2/2$. Let us discretize it with the adjoint symplectic Euler method (regarding the mass $m = 1$): 
    \begin{align*}
      q_{k+1} & = q_k + h p_k \\ 
      p_{k+1} & = q_{k+1} - h \omega^2 q_{k+1}
    \end{align*}
    Then, we can do simple algebra to see that $H(q_k, p_k) \neq H(q_{k+1}, p_{k+1})$. 
    \begin{align*}
      H(q_{k+1}, p_{k+1}) & = \frac{1}{2} (p - h \omega^2 q_{k+1})^2 + \frac{1}{2} \omega^2 (q + h p)^2 \\
      & = \frac{1}{2} (p_k^2 - 2p_kh \omega^2 q_{k+1} + h^2 \omega^4 q_k^2)^2 + \frac{1}{2} \omega^2 (q_k^2 + 2hq_k p_k + h^2 p_k^2)^2 \\
      &  = \ldots \neq H(q_k, p_k) 
    \end{align*}
    However, if we modify the Hamiltonian from $H(q, p) = p^2 /2 + \omega^2 q^2 / 2$ to 
    \begin{equation}
      \Tilde{H} (q, p) = \frac{1}{2} \big( p^2 + h \omega^2 p q + \omega^2 q^2 \big)
    \end{equation}
    Then it turns out that $\Tilde{H}(q_k, p_k) = \Tilde{H}(q_{k+1}, p_{k+1})$, and so this new modified Hamiltonian is conserved. This is significant, since now we have some other invariant property of the numerical method. The phase space of this 1-dimensional oscillator is simply $\Omega_q \times \Omega_p \subset \mathbb{R} \times \mathbb{R}$. If we were to visualize the level sets of the Hamiltonian, then we can imagine the level sets of the modified Hamiltonian to be a "perturbed" version of the original ones. The fact that there even exists a modified Hamiltonian invariant is another special property of symplectic integrators. If we used Euler's method to solve the harmonic oscillator, we would find that energy grows without bound. 

    \subsubsection{Lie Derivatives and Poisson Brackets}

      Let us have $\mathbf{z} \in \mathbb{R}^m$ in our phase space. Furthermore, let us assume that at any point $\boldsymbol{\zeta} \in \mathbb{R}^m$ there is a unique solution $\mathbf{z}(t, \boldsymbol{\zeta})$ such that $\mathbf{\dot{z}}(t) = \mathbf{f}\big(\mathbf{z}(t)\big), \; \mathbf{z}(0) = \boldsymbol{\zeta}$ is globally defined for all $t$. This also means that $\mathbf{f}: \mathbb{R}^m \longrightarrow \mathbb{R}^m$ is a vector field defining the phase flow on $\mathbb{R}^m$. On this phase space let us define a scalar field $\phi: \mathbb{R}^m \longrightarrow \mathbb{R}$. Letting $\hat{\phi} = \phi \circ \mathbf{z}: \mathbb{R} \longrightarrow \mathbb{R}$ be the function outputting the value of $\phi$ across the path $\phi$, we can take its derivative using chain rule
      \begin{align*}
        \frac{d}{dt} \hat{\phi} \bigg|_{t=0} & = \begin{pmatrix}
        \frac{\partial \phi}{\partial z_1} (\zeta) \\[0.5em]
        \frac{\partial \phi}{\partial z_2} (\zeta) \\
        \ldots \\ 
        \frac{\partial \phi}{\partial z_m} (\zeta) \end{pmatrix} 
        \begin{pmatrix}
        \frac{d z_1}{dt} (0) &\hspace{-2mm} \frac{d z_2}{dt} (0) &\hspace{-2mm} \ldots &\hspace{-2mm} \frac{d z_m}{dt} (0) \end{pmatrix} \\
        & = \nabla \phi(\boldsymbol{\zeta}) \cdot \mathbf{\dot{z}} (0) \\ 
        & = \mathbf{f}(\boldsymbol{\zeta}) \cdot \nabla \phi(\boldsymbol{\zeta}) 
      \end{align*}
      That is, the derivative of $\hat{\phi}$ at $t = 0$ is the dot product of the derivative vector at $\boldsymbol{\zeta}$ and the gradient of the scalar potential at $\boldsymbol{\zeta}$. From this, we can define the \textbf{Lie derivative} $\mathcal{L}_\mathbf{f}$ as 
      \begin{equation}
        \mathcal{L}_\mathbf{f} \phi \coloneqq \mathbf{f} \cdot \nabla \phi
      \end{equation}
      This is similar to the directional derivative of $\phi$ in direction $\mathbf{f}$. Therefore, the equation for the evolution of $\phi$ can be written as follows. Note also that the origin of time is irrelevant since we can always just shift the time frame by a constant, so we can really focus on the point on the path in $\mathbb{R}^m$ rather than the associated time. Therefore, at a certain time $t$ with associated point $\mathbf{z}(t)$, this Lie derivative at $\mathbf{z}(t)$ in direction $\mathbf{f}$ under scalar field $\phi$ is 
      \begin{equation}
        \frac{d}{dt} \phi\big(\mathbf{z}(t)\big) = ( \mathcal{L}_\mathbf{f} \phi) \big(\mathbf{z}(t) \big)
      \end{equation}
      Similarly, the second derivative at $\mathbf{z}(t)$ in direction $\mathbf{f}$ under $\phi$ will be denoted 
      \begin{equation}
        \frac{d^2}{dt^2} \phi\big( \mathbf{z}(t) \big) = (\mathcal{L}_f^2 \phi) \big( \mathbf{z}(t) \big)
      \end{equation}
      and so on for higher derivatives. The Taylor series expansion of $\phi(\mathbf{z}(t))$, centered at $0$, along a solution of the differential equation can therefore be written as 
      \begin{align*}
        \phi \big(\mathbf{z}(t) \big) & = \phi \big(\mathbf{z}(0) \big) + t \, \bigg( \frac{d}{dt} \phi \big( \mathbf{z}(t)\big) \bigg|_{t = 0} \bigg) + \frac{t^2}{2} \, \bigg( \frac{d^2}{dt^2} \phi\big( \mathbf{z}(t)\big) \bigg|_{t = 0} \bigg) + \ldots \\
        & = \phi \big(\mathbf{z}(0) \big) + t (\mathcal{L}_\mathbf{f} \phi) \big( \mathbf{z}(0) \big) + \frac{t^2}{2} ( \mathcal{L}_f^2 \phi) \big( \mathbf{z}(0)\big) + \ldots \\
        & = \big( e^{t \mathcal{L}_\mathbf{f}} \phi) \big( \mathbf{z}(0)\big) 
      \end{align*}
      In summary, given the initial value problem $\mathbf{\dot{z}}(t) = \mathbf{f}\big( \mathbf{z}(t)\big), \; \mathbf{z}(0) = \boldsymbol{\zeta}$ and any scalar field $\phi$ defined on $\mathbb{R}^m$, we have $\phi( \mathbf{z}(t)) = ( e^{t \mathcal{L}_\mathbf{f}} \phi)(\mathbf{z}(0))$. By setting $\phi$ to simply be the component functions $z_i$ of $\mathbf{z}$, this fully defines $\mathbf{z}(t)$ given $\mathbf{z}(0) = \boldsymbol{\zeta}$. We also don't need to fix the initial point, since the flow map $\Phi_t$ is a collection of all the flows for every single possible initial point. Concisely, by abuse of notation, the flow map $\Phi_t$ can be represented by 
      \begin{equation}
        \Phi_t = \exp(t \mathcal{L}_\mathbf{f})
      \end{equation}
      More strictly speaking, what is really meant by the equality above is that the individual components satisfy 
      \begin{equation}
        z_i (t, \boldsymbol{\zeta}) = \big[ \Phi_t (\boldsymbol{\zeta}) \big]_i = \big(\exp(t \mathcal{L}_\mathbf{f}) z_i \big) (\boldsymbol{\zeta})
      \end{equation}
      Ignoring the initial term in the Taylor series allows us write 
      \begin{equation}
        e^{t \mathcal{L}_\mathbf{f}} \phi = \phi + t \mathcal{L}_\mathbf{f} \phi + \frac{t^2}{2} \mathcal{L}_\mathbf{f}^2 \phi + \ldots
      \end{equation}
      which may or may not be bounded with respect to functions $\phi$. Though significant, we will ignore this problem for now. We now introduce the \textbf{Poisson bracket}, which is defined for two smooth scalar-valued functions $g_1$ and $g_2$ of phase variables $(\mathbf{q}, \mathbf{p}) \in \mathbb{R}^m$ by 
      \begin{equation}
        \{g_1, g_2\} \coloneqq \nabla g_1^T \mathbf{J} \nabla g_2 = \sum_{i=1}^m \bigg( \frac{\partial g_1}{\partial q_i} \frac{\partial g_2}{\partial p_i} - \frac{ \partial g_2}{\partial q_i} \frac{\partial g_1}{\partial p_i} \bigg)
      \end{equation}
      where $\mathbf{J}$ is the skew symmetric symplectic structure matrix.
      \begin{equation}
        \mathbf{J} = \begin{pmatrix} \mathbf{0} & \mathbf{I} \\ -\mathbf{I} & \mathbf{0} \end{pmatrix}
      \end{equation}
      As the name suggests, the Poisson bracket has the bracket structure: it is bilinear, skew-symmetric, and satisfied the Jacobi identity defined 
      \begin{equation}
        \{ g_1, \{g_2, g_3\} \} + \{g_3, \{ g_1, g_2\}\} + \{g_2, \{g_3, g_1\}\} = 0
      \end{equation}
      We can in fact write differential equations in terms of Poisson brackets. For example, the simple component DEQ $\dot{q}_i = p_i$ (of vector DEQ $\mathbf{\dot{q}} = \mathbf{p}$) can be written in terms of brackets as the first line, with the derivation shown in the following lines. 
      \begin{align*}
        \dot{q}_i & = \{q_i, H\} \\
        & = \nabla q_i^T \mathbf{J} \nabla H \\
        & = \begin{pmatrix}
        \mathbf{e}_i & 0 
        \end{pmatrix} \begin{pmatrix}
        \mathbf{0} & \mathbf{I} \\ -\mathbf{I} & \mathbf{0} 
        \end{pmatrix} \begin{pmatrix}
        \nabla_\mathbf{q} H \\ \nabla_\mathbf{p} H 
        \end{pmatrix} \\
        & = \nabla_{p_i} H = \frac{\partial}{\partial p_i} H = \frac{\partial}{\partial p_i} \frac{1}{2} ||\mathbf{p}||^2 + U(\mathbf{q}) = p_i
      \end{align*}
      More generally, if $F(\mathbf{q}, \mathbf{p})$ is any smooth, scalar-valued function of the phase variables, we may write 
      \begin{equation}
        \dot{F} = \frac{d}{dt} F \big( \mathbf{q}(t), \mathbf{p}(t)\big) = \{F, H\}
      \end{equation}
      We can see this because 
      \begin{align*}
        \{F, H\} & = \nabla F^T \mathbf{J} \nabla H \\
        & = \begin{pmatrix}
        \nabla_\mathbf{q} F & \nabla_\mathbf{p} F 
        \end{pmatrix} \begin{pmatrix}
         \mathbf{0} & \mathbf{I} \\ -\mathbf{I} & \mathbf{0} 
        \end{pmatrix} \begin{pmatrix}
        \nabla_\mathbf{q} H \\ \nabla_\mathbf{p} H
        \end{pmatrix} \\
        & = \nabla_\mathbf{q} F \cdot \nabla_\mathbf{p} H - \nabla_\mathbf{q} H \cdot \nabla_\mathbf{p} F \\
        & = \nabla_\mathbf{q} F \cdot \nabla_\mathbf{p} \bigg( \frac{1}{2} ||\mathbf{p}||^2 + U(\mathbf{q}) \bigg) - \nabla_\mathbf{q} \bigg( \frac{1}{2} ||\mathbf{p}||^2 + U(\mathbf{q}) \bigg)  \cdot \nabla_\mathbf{p} F \\
        & = \nabla_\mathbf{q} F \cdot \mathbf{p} - \nabla_\mathbf{q} F \cdot \nabla_\mathbf{q} U(\mathbf{q}) \\
        & = \frac{\partial F}{\partial \mathbf{q}} \frac{ d \mathbf{q}}{d t} - \frac{\partial F}{\partial \mathbf{p}} \frac{d \mathbf{p}}{d t} \\
        & = \frac{d}{d t} F \big( \mathbf{q}(t), \mathbf{p}(t) \big) 
      \end{align*}
      Therefore, we have the following relation between the Lie derivative and the Poisson bracket. 
      \begin{equation}
        \mathcal{L}_{\mathbf{J} \nabla H} F = \mathbf{J} \nabla H \cdot \nabla F = \nabla F^T \mathbf{J} \nabla H = \{F, H\}
      \end{equation}
      What this says is that given the coupled Hamiltonian equations
      \begin{equation}
        \begin{cases} \boldsymbol{\dot{q}} & = \mathbf{M}^{-1} \mathbf{p} \\
        \boldsymbol{\dot{p}} & = - \nabla_\mathbf{q} U(\mathbf{q}) \end{cases} \implies \begin{pmatrix} \boldsymbol{\dot{q}} \\ \boldsymbol{\dot{p}} \end{pmatrix} = \begin{pmatrix} \mathbf{0} & \mathbf{I}_{3N} \\ -\mathbf{I}_{3N} & \mathbf{0} \end{pmatrix} \begin{pmatrix} \nabla_\mathbf{q} U (\mathbf{q})\\ \mathbf{M}^{-1} \mathbf{p} \end{pmatrix} = \mathbf{J} \nabla H(\mathbf{q}, \mathbf{p})
      \end{equation}
      the Lie derivative $\mathcal{L}_{\mathbf{J} \nabla H} F$ under scalar field $F$ can be represented in terms of the Lie bracket $\{F, H\}$, and therefore $\exp(t \mathcal{L}_{\mathbf{J} \nabla H})$ can be regarded as the Hamiltonian flow of the system. Following the convention, we will simplify the expression $\mathcal{L}_{\mathbf{J} \nabla H}$ to simply $\mathcal{L}_H$. Note that $\mathcal{L}_H$ takes in a smooth scalar field $F$ and outputs another scalar field that tells the directional derivative in direction $H$. 
      \begin{equation}
        \mathcal{L}_H: C^\infty (\mathbb{R}^m) \longrightarrow C^\infty (\mathbb{R}^m)
      \end{equation}

    \subsubsection{Backward Error Analysis for Hamiltonian Splitting Methods}

      Note that the relation $\mathcal{L}_H \phi = \{\phi, H\}$ is linear in $H$ (due to bilinearity). Let us have a system with Hamiltonian $H = H_1 + H_2$. Then, $\mathcal{L}_H = \mathcal{L}_{H_1 + H_2} = \mathcal{L}_{H_1} + \mathcal{L}_{H_2}$, and thus the flow map of the system is 
      \begin{equation}
        \Phi_t = e^{ t(\mathcal{L}_{H_1} + \mathcal{L}_{H_2})}
      \end{equation}
      The splitting method based on a composition of flows on $H_1$ and $H_2$ is $e^{t \mathcal{L}_{H_1}} e^{t \mathcal{L}_{H_2}}$. It is well known that given noncommuting operators $A, B$, $e^{A + B}$ does not necessarily equal $e^A e^B$. Expanding and subtracting gives us the difference to be (where $[A, B] = AB - BA$ is the commutator): 
      \begin{equation}
        e^{h \mathcal{L}_{H_1}} e^{h \mathcal{L}_{H_2}} - e^{h \mathcal{L}_H} = \frac{h^2}{2} [\mathcal{L}_{H_1}, \mathcal{L}_{H_2}] + \mathcal{O}(h^3)
      \end{equation}
      We can see that since $\mathcal{L}_{H} f = \{f, H\}$, $\mathcal{L}_{H_1} \mathcal{L}_{H_2} f = \{ \{f, H_2\}, H_1 \}$ and thus the commutator reduces to 
      \begin{align*}
        [\mathcal{L}_{H_1}, \mathcal{L}_{H_2}] f & = \{ \{ f, H_2\}, H_1\} - \{ \{ f, H_1\}, H_2\} & \\
        & = \{ \{ f, H_2\}, H_1\} - \{ \{ H_1, f\}, H_2\} & \text{(skew symmetry)}\\
        & = - \{ \{H_2, H_1\}, f \} & \text{(Jacobi identity)} \\
        & = \{f, \{H_2, H_1\}\} & \text{(skew symmetry)} \\
        & = \mathcal{L}_{\{H_1, H_2\}} f 
      \end{align*}
      This means that it is possible to relate the commutator of Lie derivatives of Hamiltonian fields $H_1, H_2$ to the Lie derivative of the Poisson bracket of the corresponding Hamiltonians. Ignoring the $\mathcal{O}(h^3)$ term, we can interpret the error $[\mathcal{L}_{H_1}, \mathcal{L}_{H_2}] = \mathcal{L}_{\{H_1, H_2\}}$ as itself being derived from another Hamiltonian. Let us expand $e^{h \mathcal{L}_{H_1}} e^{h \mathcal{L}_{H_2}} = e^{h \mathcal{L}_{\Tilde{H}}}$ using the Baker-Campbell-Hausdorff formula: 
      \begin{equation}
        e^{h \mathcal{L}_{H_1}} e^{h \mathcal{L}_{H_2}} = \exp \bigg( h\big(\mathcal{L}_{H_1} + \mathcal{L}_{H_2}\big) + \frac{h^2}{2} [\mathcal{L}_{H_1}, \mathcal{L}_{H_1}] + \frac{h^3}{12} \big( [\mathcal{L}_{H_1}, [\mathcal{L}_{H_1}, \mathcal{L}_{H_2}]] - [\mathcal{L}_{H_2}, [\mathcal{L}_{H_1}, \mathcal{L}_{H_2}]] \big) + \ldots \bigg)
      \end{equation}
      Then, $h \mathcal{L}_{\Tilde{H}}$ would be the term in the exponent. Dividing by $h$ and substituting $\mathcal{L}_H = \mathcal{L}_{H_1} + \mathcal{L}_{H_2}$ gives 
      \begin{align*}
        \mathcal{L}_{\Tilde{H}} & = \mathcal{L}_H + \frac{h}{2} [\mathcal{L}_{H_1}, \mathcal{L}_{H_2}] + \frac{h^2}{12} \big( [\mathcal{L}_{H_1}, [\mathcal{L}_{H_1}, \mathcal{L}_{H_2}]] - [\mathcal{L}_{H_2}, [\mathcal{L}_{H_1}, \mathcal{L}_{H_2}]] \big) + \ldots \\
        & = \mathcal{L}_H + \frac{h}{2} \mathcal{L}_{\{H_1, H_2\}} + \frac{h^2}{12} \big( \mathcal{L}_{\{H_1, \{H_1, H_2\}\}} - \mathcal{L}_{\{H_2, \{H_1, H_2\}\}} \big) + \ldots \\
        & = \mathcal{L}_{H + \frac{h}{2} \{H_1, H_2\} + \frac{h^2}{12} (\{ H_1, \{H_1, H_2\}\} - \{H_2, \{H_1, H_2\}\}) + \ldots} 
      \end{align*}
      which implies that the Hamiltonian $\Tilde{H}$ of the splitting approximation deviates from the true Hamiltonian $H$ through the BCH formula. 
      \begin{equation}
        \Tilde{H} = H + \underbrace{\frac{h}{2} \{H_1, H_2\} + \frac{h^2}{12} (\{ H_1, \{H_1, H_2\}\} - \{H_2, \{H_1, H_2\}\}) + \ldots}_{\text{error term}}
      \end{equation}
      This series $\Tilde{H}$ is referred to as the \textbf{shadow Hamiltonian} corresponding to the splitting method. The numerical method may be viewed as being equivalent to the exact solution of a nearby Hamiltonian system, rather than the true one. We can visualize the isocontour lines for a double-well model along with its modified Verlet Hamiltonian below. 
      \begin{center}
        \includegraphics[scale=0.5]{img/Shadow_Hamiltonian.png}
      \end{center}
      Note that we still haven't addressed the convergence of this series, but we simply assume that the error term is bounded (which may not always be justified). Furthermore if $H_1$ and $H_2$ commute, i.e. $\{H_1, H_2\} = 0$, then there is no error in splitting. There are few special splitting cases where this would happen. An alternative approach to numerically solving the SDE is to find a scheme with Hamiltonian that has \textit{its} shadow Hamiltonian to be our target one. That is, we use a perturbed version of the original SDE and discretize it, which should lead to a higher order scheme. 

    \subsubsection{Symplectic Euler}

      Recall that splitting our Hamiltonian using 
      \begin{equation}
        H_1 = \frac{1}{2} \mathbf{p}^T \mathbf{M}^{-1} \mathbf{p}, \;\;\; H_2 = U(\mathbf{q})
      \end{equation}
      gives us the symplectic Euler method. The BCH expansion gives us the following perturbed Hamiltonian, which we can see has a leading error term of power $1$, making it a first-order scheme. 
      \begin{align*}
        \Tilde{H}_h & = H + \frac{h}{2} \{H_1, H_2\} + \frac{h^2}{12} \big( \{H_1, \{H_1, H_2\}\} - \{ H_2, \{ H_1, H_2\}\} \big) + \ldots \\
        & = H + \frac{H}{2} \nabla H_1^T \mathbf{J} \nabla H_2 + \ldots \\
        & = H + \frac{h}{2} \bigg[ \begin{pmatrix} \mathbf{0} & \mathbf{p}^T \mathbf{M}^{-1} \end{pmatrix} \begin{pmatrix} \mathbf{0} & \mathbf{I} \\ -\mathbf{I} & \mathbf{0} \end{pmatrix} \begin{pmatrix} \nabla_\mathbf{q} U(\mathbf{q}) \\ \mathbf{0} \end{pmatrix} \bigg] + \ldots \\ 
        & = H - \frac{h}{2} \mathbf{p}^T \mathbf{M}^{-1} \nabla U(\mathbf{q}) + \frac{h^2}{12} \big[ \mathbf{p}^T \mathbf{M}^{-1} U^{\prime\prime} \mathbf{M}^{-1} \mathbf{p} + \nabla U(\mathbf{q})^T \mathbf{M}^{-1} \nabla U(\mathbf{q}) \big] \\
        & - \frac{h^3}{12} \nabla U (\mathbf{q})^T \mathbf{M}^{-1} U^{\prime\prime} (\mathbf{q}) \mathbf{M}^{-1} \mathbf{p} + \mathcal{O}(h^4)
      \end{align*}

    \subsubsection{Velocity Verlet}

      Given a Hamiltonian symmetrically split into three parts
      \begin{equation}
        H(\mathbf{q}, \mathbf{p}) =  H_1 + H_2 + H_3 = \frac{1}{2} U(\mathbf{q}) + \frac{1}{2} T(\mathbf{p}) + \frac{1}{2} U(\mathbf{q})
      \end{equation}
      calculating the estimate $\exp{(h \mathcal{L}_H)} \approx \exp(\frac{h}{2} \mathcal{L}_{H_1})\,\exp(\frac{h}{2} \mathcal{L}_{H_2})\,\exp(\frac{h}{2} \mathcal{L}_{H_3})$ using the BCH lemma gives the following. Notice that the symmetricity of the splitting scheme allows us to cancel out the odd powered terms. 
      \begin{equation}
        \Tilde{H}_h = T + U + \frac{h^2}{12} \Big( \{ T, \{T, U\}\} - \frac{1}{2} \{U, \{U, T\}\} \Big) + \ldots
      \end{equation}
      The shadow Hamiltonian of the Velocity Verlet scheme applied to a single degree of freedom system of the form $H(q, p) = U(q) + \frac{1}{2} p^2$ then gives 
      \begin{align*}
        \Tilde{H}(q, p) & = H(q, p) + \frac{h^2}{24} \big(2 p U'' (q) p - (U^\prime (q))^2 \big) + h^4 \Big( \frac{1}{720} p^4 U''''(q) - \frac{1}{120} p^2 U'(q) U'''(q) \\
        & - \frac{1}{240} (U'(q))^2 U''(q) - \frac{1}{60} p^2 (U''(q))^2 + U'(q) U'''(q) \big) + \mathcal{O}(h^6) 
      \end{align*}
      Higher order symplectic integrators give higher order error terms (e.g. Yoshida-4, Imada-4). 
  
  \subsection{Hamiltonian Monte Carlo (HMC)}

    Hamiltonian Monte Carlo is one type of MCMC Metropolis-Hastings algorithms, with a Hamiltonian dynamics evolution simulated using a time-reversible, symplectic integrator (usually Velocity-Verlet). We first initialize our chain $\mathbf{X}_0 = (\mathbf{q}^0, \mathbf{p}^0)$ and compute the Hamiltonian $H(\mathbf{q}, \mathbf{p}) = U(\mathbf{q}) + \frac{1}{2} \mathbf{p}^T \mathbf{M}^{-1} \mathbf{p}$. Given that we have $\mathbf{X}_k = (\mathbf{q}_k, \mathbf{p}_k)$ at the end of the $k$th step, we then repeat the following steps: 
    \begin{enumerate}
        \item Fix $\mathbf{q}$ but pick $\mathbf{p}_{k+1} \sim \mathcal{N}(\mathbf{p}_k, \mathbf{\Sigma})$. 
        \item We run Velocity Verlet (or some other symplectic scheme) for some fixed number of steps $L$ of stepsize $h$, which models Hamiltonian flow to some new position $(\mathbf{q}_k^\prime, \mathbf{p}_k^\prime)$. This is our transition proposal. Note that for every step in Velocity Verlet, we must compute the gradient of the potential. In order to simulate Hamiltonian flow, this gradient must be exactly computed; our batch approximation will lead to discretized steps that is not deterministic anymore and do not fulfill our symplectic properties and energy preservation. 
        \item We accept this proposal with probability 
        \[\alpha = \min \bigg( 1, \frac{\exp \big[ -H(\mathbf{q}_k^\prime, \mathbf{p}_k^\prime) \big]}{\exp \big[ -H(\mathbf{q}_k, \mathbf{p}_k)\big]} \bigg)\]
        and assign $\mathbf{X}_{k+1} = (\mathbf{q}_{k+1}, \mathbf{p}_{k+1}) = (\mathbf{q}_k^\prime, \mathbf{p}_k^\prime)$ upon acceptance and $\mathbf{X}_{k+1} = \mathbf{X}_k$ if not. Note that in this step, we require the exact evaluations of our Hamiltonian. 
    \end{enumerate}
    Hamiltonian Monte Carlo is very useful if we could efficiently calculate the true log-posterior, but otherwise, the batch approximation will not model a Hamiltonian flow (and thus will not preserve the symplectic, time-reversibility, etc. properties), rendering HMC useless. 

    HMC is able to draw samples in high dimensions with greater efficiency than classical MCMC. Its key advantage is its ability to draw samples that are large distances apart by evolving them via Hamiltonian dynamics. The acceptance rate depends on the error accumulated along the sample trajectory (i.e. the error of the shadow Hamiltonian), and remains large even in high dimensions. However, a large step size (leading to greater error of shadow Hamiltonian), a large system, or a poorly behaved target density leads to greater numerical error and thus to lower sample acceptance, which induces heavy autocorrelation, necessitating a larger sample size and thus higher computational costs. 

    One approach to ease this burden is to exploit the structure of the numerical integrator error and instead target the density corresponding to a modified, \textit{shadow Hamiltonian}. This leads to higher sample acceptance rate, at the cost of some induced bias. This bias is usually well-quantified, and we can compensate for this induced bias. 

  \subsection{No U-Turn Sampler (NUTS)}


\section{Random Walk Metropolis}

    Given that we have computed a scalar multiple of a high dimensional posterior $\pi = \frac{f}{c}$ defined in $\mathbb{R}^n$ for $n >> 1$, we would like to either optimize $f$ or sample from $f$ to find its true normalizing factor $c$. There are some overlaps in the methods used to achieve these goals. Let us denote our (parameter) state as $\theta \in \mathbb{R}^n$, with a discrete time step denoted by $t$ and step size $h$. 

    Markov Chain Monte Carlo algorithms are extremely simple and computationally efficient, since they only require to compute $f(\theta)$, without any gradient information. They generate a sequence of correlated samples which on the long run converge to a sequence of independent samples. The degree of correlation of nearby samples is called the \textit{autocorrelation} of the MCMC sampler. We first generate a proposal step according to some kernel and then decide whether to accept or reject that proposal. Usually, we have a series of "burn-in" steps that allow the chain to first converge to a local maximum, which we can then throw away. The simplest version of this is with an isotropic Gaussian kernel. 

    \begin{algorithm}
      \caption{Random Walk Metropolis Hastings w/ Isotropic Gaussian Kernel}\label{alg:metropolis_gaussian}
      \begin{algorithmic}

      \Require Initial $\boldsymbol{\theta}_0$, Stepsize $h$, Burn-in steps $\mathcal{B}$

      \For{$t = 0$ to $T$}
          \State $\epsilon_t \sim \mathcal{N}(0, I)$ 
          \State $P_{t+1} \gets \theta_t + \epsilon_t$
          \If{$f(P_{t+1}) \geq f(\theta_t)$}
              \State $\theta_{t+1} \gets P_{t+1}$ 
          \Else
              \State $\delta \sim \mathrm{Uniform}[0, 1]$
              \If{$\delta < f(P_{t+1}) / f(\theta_t)$}
                  \State $\theta_{t+1} \gets P_{t+1}$ 
              \Else 
                  \State $\theta_{t+1} \gets \theta_t$
              \EndIf
          \EndIf
      \EndFor

      \State Delete first $\mathcal{B}$ states of $\boldsymbol{\theta} = [\theta_0, \theta_1, \ldots, \theta_T]$

      \end{algorithmic}
    \end{algorithm}

    Note that the step size is very important here: If $h$ is too small, then this chain would behave like a random walk. If $h$ it too big, then this chain would mainly stay at one state. Ideally, the acceptance probability should be between $0.2$ and $0.7$. 

    This isotropic MH is not robust, since it would not work well if some parameters of $\theta$ are correlated and the estimated covariance of $f$ at some local maximum is more "diagonal." Therefore, some adaptive mechanism is needed, which we can implement by estimating the covariance matrix of the proposal kernel using the empirical covariance of the proposal steps. To reduce memory allocation, we should use a recursive algorithm to compute the mean and covariance, rather than having to store all the $\theta_t$'s. To maintain stability, we may start adapting after a certain number of steps $B$ and compute covariance estimates every $U$ steps. 

    \begin{algorithm}
      \caption{Adaptive Random Walk Metropolis}\label{alg:adaptive_metro}
      \begin{algorithmic}

      \Require Initial $\boldsymbol{\theta}_0$, Stepsize $h$, Burn-in steps $\mathcal{B}$, Adaptation burn-in $B$, Adaptation frequency $U$
      \State $\mu_0^\mathrm{emp} \gets 0$ 
      \State $\Sigma_0 \gets I$
      \State $\Sigma^\mathrm{emp}_0 \gets I$

      \For{$t = 0$ to $T$}
          \State $\epsilon_t \sim \mathcal{N}(0, \Sigma_t)$ 
          \State $P_{t+1} \gets \theta_t + \epsilon_t$
          \If{$f(P_{t+1}) \geq f(\theta_t)$}
              \State $\theta_{t+1} \gets P_{t+1}$ 
          \Else
              \State $\delta \sim \mathrm{Uniform}[0, 1]$
              \If{$\delta < f(P_{t+1}) / f(\theta_t)$}
                  \State $\theta_{t+1} \gets P_{t+1}$ 
              \Else 
                  \State $\theta_{t+1} \gets \theta_t$
              \EndIf
          \EndIf
          
          \State $\Sigma^\mathrm{emp}_{t+1} \gets \frac{1}{t+1} \big[(\theta^{t+1} - \mu_t) (\theta^{t+1} - \mu_t)^T - \Sigma^\mathrm{emp}_t \big]$
          \State $\mu_{t+1}^{\mathrm{emp}} \gets \mu_t + \frac{1}{t+1} [ \theta_{t+1} - \mu_t ]$
          
          \If{$t > B$ and $t$ is divisible by $U$} 
              \State $\Sigma_{t+1} \gets \Sigma^\mathrm{emp}_{t+1}$
          \EndIf
      \EndFor

      \State Delete first $\mathcal{B}$ states of $\boldsymbol{\theta} = [\theta_0, \theta_1, \ldots, \theta_T]$

      \end{algorithmic}
    \end{algorithm}

    On top of this even, we can precondition the initial $\Sigma_0$ to be some other estimate of the posterior and weight it accordingly so that our proposal covariance is some "balance" of this computed estimate and the empirical estimate, using a damping parameter $\alpha$. 
    \begin{equation}
      \Sigma_t = \alpha \Sigma_0 + (1 - \alpha) \Sigma^{\mathrm{emp}}, \;\;\;\;\; 0 \leq \alpha \leq 1
    \end{equation}

    The lower the $\alpha$, the more the precomputed estimate is "washed away" by the empirical covariance. We can also treat the $\alpha$ as a variable function $\alpha(t)$ and adapt its value as the chain runs. For example, if we would like the precomputed covariance to have more weight in the beginning (for stability), but eventually completely overpowered by the empirical covariance, we can choose it such that $\alpha(0) = 1$ and $\alpha \rightarrow 0$ as $t \rightarrow +\infty$, with the specific behavior customized to the problem. 

    \begin{algorithm}
      \caption{Adaptively Preconditioned Random Walk Metropolis}\label{alg:adaptive_precon_metro}
      \begin{algorithmic}

      \Require Initial $\boldsymbol{\theta}_0$, Stepsize $h$, Burn-in steps $\mathcal{B}$, Adaptation burn-in $B$, Adaptation frequency $U$, Damping function $\alpha$, Precomputed covariance estimate $\Sigma^{\mathrm{pre}}$
      \State $\mu_0^\mathrm{emp} \gets 0$ 
      \State $\Sigma^\mathrm{emp}_0 \gets I$
      \State $\Sigma_0 \gets I$

      \For{$t = 0$ to $T$}
          \State $\epsilon_t \sim \mathcal{N}(0, \Sigma_t)$ 
          \State $P_{t+1} \gets \theta_t + \epsilon_t$
          \If{$f(P_{t+1}) \geq f(\theta_t)$}
              \State $\theta_{t+1} \gets P_{t+1}$ 
          \Else
              \State $\delta \sim \mathrm{Uniform}[0, 1]$
              \If{$\delta < f(P_{t+1}) / f(\theta_t)$}
                  \State $\theta_{t+1} \gets P_{t+1}$ 
              \Else 
                  \State $\theta_{t+1} \gets \theta_t$
              \EndIf
          \EndIf
          
          \State $\Sigma^\mathrm{emp}_{t+1} \gets \frac{1}{t+1} \big[(\theta^{t+1} - \mu_t) (\theta^{t+1} - \mu_t)^T - \Sigma^\mathrm{emp}_t \big]$
          \State $\mu_{t+1}^\mathrm{emp} \gets \mu_t + \frac{1}{t+1} [ \theta_{t+1} - \mu_t ]$
          
          \If{$t > B$ and $t$ is divisible by $U$} 
              \State $\Sigma_{t+1} \gets \alpha(t) \cdot \Sigma^\mathrm{pre} + (1 - \alpha(t)) \cdot \Sigma^\mathrm{emp}_{t+1}$
          \EndIf
      \EndFor
      \State Delete first $\mathcal{B}$ states of $\boldsymbol{\theta} = [\theta_0, \theta_1, \ldots, \theta_T]$

      \end{algorithmic}
    \end{algorithm}

  \subsection{Ensemble Methods}


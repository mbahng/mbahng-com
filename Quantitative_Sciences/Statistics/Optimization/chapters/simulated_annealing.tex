\section{Simulated Annealing}

  Unlike the previous optimizers, \textit{simulated annealing} is useful in finding \textit{global} optima in the presence of multimodal functions within a usually very large discrete space $\mathcal{S}$. Given some function $f$ defined on $\mathcal{S}$, we would like to find its global maximum. Rather than picking the "best move" using gradient information (like SGD), we propose a random move. Let us start at a state $\theta_k$ and propose a random $P_{k+1}$. We denote $\Delta f = f(P_{k+1}) - f(\theta_k)$. 
  \begin{enumerate}
      \item If the selected move improves the solution (i.e. $\Delta f \geq 0$, then it is always accepted and we set $\theta_{k+1} \gets P_{k+1}$. 
      \item Otherwise, when $\Delta f < 0$ it makes the move with the following acceptance probability 
      \[p(\theta_{k+1} \gets P_{k+1} \mid \Delta f < 0) = e^{\Delta f / T(t)}\]
  \end{enumerate}
  We can see that if $\Delta f$ is very negative (the move is very bad), then this probability of acceptance decreases as well. Furthermore, $T(t)$ represents some sort of "temperature" that we anneal as a function of time, called the \textit{annealing schedule}. $T$ starts off at a high value, increasing the rate at which bad moves are accepted, which promotes exploration of $\mathcal{S}$ and allows the algorithm to travel to suboptimal areas. As $T$ decreases, the vast majority of steps move uphill, promoting exploitation, which means that once the algorithm is in the right search space, there is no need to search other sections of the search space. 

  \begin{algorithm}
  \caption{Simulated Annealing}\label{alg:sim_anneal}
  \begin{algorithmic}

  \Require Initial $\theta_0$, Transition kernel $\pi(\theta_{k+1} \mid \theta_k)$, Annealing schedule $T(t)$

  \For{$t = 0$ to $T$ until convergence}
      \State $P_{t+1} \sim \pi( \cdot \mid \theta_t)$
      
      \If{$f(P_{t+1}) - f(\theta_t) \geq 0$} 
          \State $\theta_{t+1} \gets P_{t+1}$ 
      \Else 
          \State $\delta \sim \mathrm{Uniform}[0, 1]$
          \If{$\delta < \exp[(f(P_{t+1}) - f(\theta_t))/T(t)]$}
              \State $\theta_{t+1} \gets P_{t+1}$ 
          \Else 
              \State $\theta_{t+1} \gets \theta_t$ 
          \EndIf
      \EndIf
  \EndFor

  \end{algorithmic}
  \end{algorithm}

  This algorithm is very easy to implement and provides optimal solutions to a wide range of problems (e.g. TSP and nonlinear optimization), but it can take a long time to run if the annealing schedule is very long. We can stop either if $T$ reaches a certain threshold or if we have determined convergence. 


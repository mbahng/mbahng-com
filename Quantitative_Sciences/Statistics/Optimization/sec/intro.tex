Optimization is such an important tool that it deserves a set of notes in itself. All problems in model training essentially stems from non-ideal optimization. Knowing the strengths and weaknesses of each optimizer allows you to diagnose which ones to use. 

Generally, we (non-exclusively) categorize optimization algorithms as such: 
\begin{enumerate}
  \item \textit{Convex}? Convex optimization is pretty easy to solve and has been studied extensively. For nonconvex optimization, none of the algorithms can guarantee that we will find the global minima, and this is one of the hardest problems in statistics.\footnote{In practice, when we are doing high-dimensional nonconvex optimization, the best we can do is play around with some properties. In these cases, 0th order approximations are hopeless since the dimensions are too high, and second order approximations are hopeless either since computing the Hessian is too expensive for one run. Therefore, we must resort to some first order methods. } 
  \item \textit{Constrained}? Is the parameter space constrained to a certain manifold? 
  \item \textit{Order}. Do we use derivatives at all? First-order derivatives (gradient)? Second-order (Hessian)? 
\end{enumerate}

These algorithms try to solve the following potential problems. 
\begin{enumerate}
  \item \textit{Convergence}. Do we converge to some point? 
  \item \textit{Optimality}. Is this point close to the true global minima?  
  \item \textit{Efficiency}. Can we iterate efficiently? 
\end{enumerate} 

As a benchmark test, the following function will be used a lot. 

\begin{definition}[Rosenbrock Function]
  The \textbf{Rosenbrock function} is defined 
  \begin{equation}
    f(x, y) = (a - x)^2 + b (y - x^2)^2
  \end{equation}
  which has a global minimum at $(a, b)$. 

  \begin{figure}[H]
    \centering 
    \includegraphics[scale=0.4]{img/rosenbrock.png}
    \caption{Typically, we set $a = 1, b = 100$. } 
    \label{fig:rosenbrock}
  \end{figure}
\end{definition}


\section{Geodesic Regression}

  In regression, note that we are finding a function $f: \mathcal{X} \to \mathcal{Y}$. In usual linear regression, both $\mathcal{X}, \mathcal{Y}$ are Euclidean space. However, there are cases where it may not be realistic that one or more of them should be modeled as a vector space. Rather, they may be part of a lower-dimensional manifold. For instance, if we want to use linear regression to predict the top $k$ principal components of a dataset, they must be orthogonal, i.e. must be in a \textit{Stiefl manifold}. 

  There are way to model this. For instance, we could have a projection operator that maps from $\mathbb{R}^m \to \mathcal{Y}$. This has its issues too, for one not being very efficient since perhaps the dimension of $\mathcal{Y}$ may be much less than $m$. Therefore, it might be better to directly regress onto a manifold. There were many attempts at this, but the first model to generalize OLS to manifolds was created by Fletcher in 2011 \cite{2011fletcher} and expanded shortly in \cite{2012fletcher}. 

  We start with the case where there is one covariate (i.e. $\mathcal{X} = \mathbb{R}$) and $\mathcal{Y} = (M, d)$ is a smooth Riemannian manifold with a metric. Recall that for a smooth manifold $M$, for any $p \in M$ and $v \in T_p M$, the tangent space at $p$, there is a unique geodesic curve $\gamma: [0, 1] \to M$ satisfying $\gamma(0) = p$, $\gamma^\prime (0) = v$. This geodesic is guaranteed to exist locally, and with this,we can define the exponential map at $p$ in the direction of $v$ as 
  \begin{equation}
    \exp_p (v) = \exp(p, v) = \gamma(1)
  \end{equation}
  In other words, the exponential map takes a position and velocity as input and returns the point at time 1 along the geodesic with these initial conditions. With this motivation, we use slightly different notation than regular linear regression, referring $p$ as the bias and $v$ as the coefficient. 

  \begin{definition}[Geodesic Regression] 
    The \textbf{geodesic regression} model is a probabilistic model that predicts the conditional distribution of $y \in (M, d)$ given $x \in \mathbb{R}$ as 
    \begin{equation}
      y = \exp(\exp(p, vx), \epsilon)
    \end{equation} 
    where the parameters are $\theta = \{p, v\}$, and $\epsilon$ is a random variable defined over the tangent space at $\exp(p, vx)$. 
  \end{definition} 

  Note that if we set $\mathcal{Y} = \mathbb{R}^m$, then we get the ordinary linear regression model back. 

  \begin{definition}[Least Squares Geodesic Regression]
    The least squares geodesic regression aims to minimize the MSE loss 
    \begin{equation}
      L(\theta, (x, y)) = L(p, v, x, y) = d(\exp(p, vx), y)^2
    \end{equation}
  \end{definition}

  \begin{lemma}[Risk]
    The risk is 
    \begin{equation}
      R(f) = \mathbb{E}_{x, y} \left[ d(\exp(p, vx), y)^2 \right]
    \end{equation} 
    and the empirical risk for a dataset $\mathcal{D} = \{(x^{(i)}, y^{(i)})\}_{i=1}^n$ is 
    \begin{equation}
      \hat{R}(f) = \frac{1}{n} \sum_{i=1}^n d(\exp(p, v x^{(i)}), y^{(i)})^2
    \end{equation}
  \end{lemma}

  Unfortunately, minimizing this does not yield an analytic solution. 

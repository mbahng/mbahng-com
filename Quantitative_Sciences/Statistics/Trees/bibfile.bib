@article{1976hyafil,
  title = {Constructing optimal binary decision trees is NP-complete},
  journal = {Information Processing Letters},
  volume = {5},
  number = {1},
  pages = {15-17},
  year = {1976},
  issn = {0020-0190},
  doi = {https://doi.org/10.1016/0020-0190(76)90095-8},
  url = {https://www.sciencedirect.com/science/article/pii/0020019076900958},
  author = {Laurent Hyafil and Ronald L. Rivest},
  keywords = {Binary decision trees, computational complexity, NP-complete}
}

@book{1984breiman,
  title={Classification and Regression Trees},
  author={Breiman, L. and Friedman, J. and Stone, C.J. and Olshen, R.A.},
  isbn={9780412048418},
  lccn={83019708},
  url={https://books.google.com/books?id=JwQx-WOmSyQC},
  year={1984},
  publisher={Taylor \& Francis}
}

@article{1986quinlan,
  added-at = {2008-02-26T11:58:58.000+0100},
  author = {Quinlan, J. R.},
  biburl = {https://www.bibsonomy.org/bibtex/24b1ef1c16c39d56f0f132c191870d776/schaul},
  citeulike-article-id = {2378698},
  description = {idsia},
  interhash = {3fe7356363a918dd24aeba82ba71d75a},
  intrahash = {4b1ef1c16c39d56f0f132c191870d776},
  journal = {Machine Learning},
  keywords = {nn},
  pages = {81--106},
  priority = {2},
  timestamp = {2008-02-26T12:02:56.000+0100},
  title = {Induction of Decision Trees},
  volume = 1,
  year = 1986
}

@book{1993quinlan,
  author = {Quinlan, J. Ross},
  title = {C4.5: Programs for Machine Learning},
  year = {1993},
  isbn = {1558602402},
  publisher = {Morgan Kaufmann Publishers Inc.},
  address = {San Francisco, CA, USA},
  abstract = {From the Publisher:Classifier systems play a major role in machine learning and knowledge-based systems, and Ross Quinlan's work on ID3 and C4.5 is widely acknowledged to have made some of the most significant contributions to their development. This book is a complete guide to the C4.5 system as implemented in C for the UNIX environment. It contains a comprehensive guide to the system's use , the source code (about 8,800 lines), and implementation notes. The source code and sample datasets are also available on a 3.5-inch floppy diskette for a Sun workstation. C4.5 starts with large sets of cases belonging to known classes. The cases, described by any mixture of nominal and numeric properties, are scrutinized for patterns that allow the classes to be reliably discriminated. These patterns are then expressed as models, in the form of decision trees or sets of if-then rules, that can be used to classify new cases, with emphasis on making the models understandable as well as accurate. The system has been applied successfully to tasks involving tens of thousands of cases described by hundreds of properties. The book starts from simple core learning methods and shows how they can be elaborated and extended to deal with typical problems such as missing data and over hitting. Advantages and disadvantages of the C4.5 approach are discussed and illustrated with several case studies.This book and software should be of interest to developers of classification-based intelligent systems and to students in machine learning and expert systems courses.}
}

@article{2020lin,
  author       = {Jimmy Lin and
                  Chudi Zhong and
                  Diane Hu and
                  Cynthia Rudin and
                  Margo I. Seltzer},
  title        = {Generalized Optimal Sparse Decision Trees},
  journal      = {CoRR},
  volume       = {abs/2006.08690},
  year         = {2020},
  url          = {https://arxiv.org/abs/2006.08690},
  eprinttype    = {arXiv},
  eprint       = {2006.08690},
  timestamp    = {Thu, 10 Sep 2020 09:33:05 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2006-08690.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{2025mctavish,
  title={Leveraging Predictive Equivalence in Decision Trees}, 
  author={Hayden McTavish and Zachery Boner and Jon Donnelly and Margo Seltzer and Cynthia Rudin},
  year={2025},
  eprint={2506.14143},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2506.14143}, 
}

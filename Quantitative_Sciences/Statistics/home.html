<!DOCTYPE html>
<html lang="en">
<head>
  <title>Muchang Bahng | Duke Math</title>
  <link rel="apple-touch-icon" sizes="180x180" href="/assets/Images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/Images/favicon.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/Images/favicon.ico">

  <link rel="stylesheet" href="/assets/CSS/Header_Footer.css">
  <link rel="stylesheet" href="/assets/CSS/personal_studies.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  </script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pdf.js/2.9.359/pdf.min.js"></script>
  
  <style>
    .intro_text {
      font-size: 14px;
    }
    
    .intro_text p {
      margin-bottom: 18px;
      line-height: 1.6;
    }
    
    #intro-preview {
      position: relative;
      max-height: 120px;
      overflow: hidden;
      -webkit-mask-image: linear-gradient(to bottom, black 0%, black 40%, transparent 100%);
      mask-image: linear-gradient(to bottom, black 0%, black 40%, transparent 100%);
    }
    
    #fade-overlay {
      display: none;
    }
    
    #read-more-btn {
      background: none;
      border: none;
      color: #666;
      cursor: pointer;
      text-decoration: none;
      padding: 12px 20px;
      font: inherit;
      font-size: 14px;
      transition: all 0.3s ease;
      z-index: 10;
      position: relative;
      display: block;
      margin: 10px auto 0 auto;
      width: fit-content;
    }
    
    #read-more-btn::before {
      content: "↓";
      margin-right: 8px;
      transition: transform 0.3s ease;
      display: inline-block;
    }
    
    #read-more-btn::after {
      content: "↓";
      margin-left: 8px;
      transition: transform 0.3s ease;
      display: inline-block;
    }
    
    #read-more-btn:hover {
      color: #333;
    }
    
    #read-more-btn:hover::before,
    #read-more-btn:hover::after {
      transform: translateY(3px);
      animation: bounce 0.6s ease-in-out infinite;
    }
    
    @keyframes bounce {
      0%, 100% { transform: translateY(3px); }
      50% { transform: translateY(6px); }
    }
    
    #read-less-btn {
      background: none;
      border: none;
      color: #666;
      cursor: pointer;
      text-decoration: none;
      padding: 12px 20px;
      font: inherit;
      font-size: 14px;
      transition: all 0.3s ease;
      z-index: 10;
      position: relative;
      display: block;
      margin: 15px auto 0 auto;
      width: fit-content;
    }
    
    #read-less-btn::before {
      content: "↑";
      margin-right: 8px;
      transition: transform 0.3s ease;
      display: inline-block;
    }
    
    #read-less-btn::after {
      content: "↑";
      margin-left: 8px;
      transition: transform 0.3s ease;
      display: inline-block;
    }
    
    #read-less-btn:hover {
      color: #333;
    }
    
    #read-less-btn:hover::before,
    #read-less-btn:hover::after {
      transform: translateY(-3px);
      animation: bounceUp 0.6s ease-in-out infinite;
    }
    
    @keyframes bounceUp {
      0%, 100% { transform: translateY(-3px); }
      50% { transform: translateY(-6px); }
    }
  </style>
</head>

<body>
<div class="header">
  <div id="menu_button" onclick="myFunction(this)">
      <div id="bar1"></div>
      <div id="bar2"></div>
      <div id="bar3"></div>
  </div>
  <a id="HeaderName" href="/index.html" style="text-decoration: none;">Muchang Bahng</a>
</div>

<div class=space></div>

<p class="title">Statistics</p>
<div class="intro_text"> 
<div id="intro-preview">

  <p>In any machine learning problem, we are looking for a best function to fit some data. This function may be a regressor, a classifier (supervised learning), a projection operator, a random variable (unsupervised learning), or anything in between. But when we talk about "best," we must define two things. First, what set of functions it is best over (since otherwise we could just choose an abtrarily complex function that will interpolate the data)? Second, what do we even mean by "best"?</p>  

  <p>Let's deal with the first question. In many theoretical statistics, the three most natural <b>function spaces</b> to work with are Holder spaces, Soblov spaces, and RKHS. Many of these properties will be covered in a functional analysis course, but we go into more detail specifically for learning theory. Another essential tool that we will use constantly is <b>concentration of measure</b>, which will give us theoretical guarantees of how models learn. </p>

  <p>The next step is to define what "best" means. This is done through a loss function, and there are generally two schools of thought on how we can construct a loss function. The first is through statistical inference, which attempts to take data and use it to <i>infer</i> what the underlying distribution is. This is usually an extremely difficult problem, and there are again two schools of thought on it. In <b>frequentist inference</b>, rather than trying to describe the entire distribution directly, we try to talk about certain parameters about the distribution (e.g. what is the mean, variance?). In <b>Bayesian inference</b>, we take a prior distribution and update it using Bayes rule to get our posterior. Even when the models become moderately complex, these problems are not analytically solvable, which is why we need numerical optimization or sampling methods.</p>  
  
  <p>The second school of thought on constructing loss functions is statistical <b>decision theory</b>. Rather than use inference to estimate our true distribution, we attempt to create a loss function from scratch by quantifying the cost-benefits of each term. This is useful for adjusting canonical standards (e.g. why p=0.05?) that may sometimes be arbitrary in practice.</p> 

  <p>A very popular interpretation of the loss function is the bias-variance tradeoff. Note that the tradeoff is a property of a loss function, not a model or a function space! Now that we have a loss function, we can define our risk as the expected loss over the true data generating distribution. Unfortunately, we don't know this value since we don't know the true distribution, so we can take the empirical risk and minimize that. This is the core idea of empirical risk minimization in <b>learning theory</b>. Ideally, we would want the true risk and the empirical risk to be pretty close together most of the time, and in statistical learning theory, we use concentration of measure to bound these. Usually, you would see that the bound is decaying with $n$ (which means that we get better approximates with more samples) and increasing with $d$ (curse of dimensionality). The difference between our estimator and the true best function (or the bounds themselves) can often be decomposed into three terms which are analogues of the bias, variance, and noise. This gives a slightly more general interpretation of the tradeoff. </p>

  <p>Once the foundations of these two theories are established, the main applications lie in <b>machine learning</b>, which uses computer science to design algorithmic approaches to learning. This field can be seen as an integration of statistics and computer science, since we heavily use the theory of algorithms to optimize objective functions that are determined by statistics (e.g. greedy algorithms to fit decision trees, L1 regularization as a convex approximation of best subset regression). I've thought for a while on how to best organize these notes, since you can divide them according to different properties. I've realized that there's no right answer, but generally when I do research, there is usually a specific model or a set of similar models I have in mind (e.g. trees). Therefore, I like to divide them by the architecture of the model, and keep all the theory, properties, and algorithms in one place for reference. </p>

  <p>With the universal approximation theorem, better engineering, and exponentially-increasing computatonal power, <b>deep neural networks</b> have become extremely powerful models for complex and high-dimensional data. They start out with simple multilayer perceptrons but recent research has pushed the architectures to CNNs, RNNs, LSTMs, energy models, encoder-decoders, flow models, attention layers, and most recently diffusion models. While the models are inherent black-box in nature, several heuristics and architectures have been developed to push their applications to the field of <b>computer vision</b> (CV) and <b>natural language processing</b> (NLP). The most noticeable success of these applications come in autonomous driving and large language models (LLMs).</p>

  <p>Finally, another subfield of machine learning is called <b>reinforcement learning</b>, which teaches agents to make decisions through simulations involving trial and error. These models are widely used in robotics and simulations, and this field of optimizing rewards and penalties heavily relies on <b>decision theory</b>.</p>

  <p>All of my personal notes are free to download, use, and distrbute under the Creative Commons "Attribution- NonCommercial-ShareAlike 4.0 International" license. Please contact me if you find any errors in my notes or have any further questions.</p>

<div id="fade-overlay"></div>
</div>
<button id="read-more-btn" onclick="toggleReadMore()">Read more</button>
<div id="intro-full" style="display: none;">

  <p>In any machine learning problem, we are looking for a best function to fit some data. This function may be a regressor, a classifier (supervised learning), a projection operator, a random variable (unsupervised learning), or anything in between. But when we talk about "best," we must define two things. First, what set of functions it is best over (since otherwise we could just choose an abtrarily complex function that will interpolate the data)? Second, what do we even mean by "best"?</p>  

  <p>Let's deal with the first question. In many theoretical statistics, the three most natural <b>function spaces</b> to work with are Holder spaces, Soblov spaces, and RKHS. Many of these properties will be covered in a functional analysis course, but we go into more detail specifically for learning theory. Another essential tool that we will use constantly is <b>concentration of measure</b>, which will give us theoretical guarantees of how models learn. </p>

  <p>The next step is to define what "best" means. This is done through a loss function, and there are generally two schools of thought on how we can construct a loss function. The first is through statistical inference, which attempts to take data and use it to <i>infer</i> what the underlying distribution is. This is usually an extremely difficult problem, and there are again two schools of thought on it. In <b>frequentist inference</b>, rather than trying to describe the entire distribution directly, we try to talk about certain parameters about the distribution (e.g. what is the mean, variance?). In <b>Bayesian inference</b>, we take a prior distribution and update it using Bayes rule to get our posterior. Even when the models become moderately complex, these problems are not analytically solvable, which is why we need numerical optimization or sampling methods.</p>  
  
  <p>The second school of thought on constructing loss functions is statistical <b>decision theory</b>. Rather than use inference to estimate our true distribution, we attempt to create a loss function from scratch by quantifying the cost-benefits of each term. This is useful for adjusting canonical standards (e.g. why p=0.05?) that may sometimes be arbitrary in practice.</p> 

  <p>A very popular interpretation of the loss function is the bias-variance tradeoff. Note that the tradeoff is a property of a loss function, not a model or a function space! Now that we have a loss function, we can define our risk as the expected loss over the true data generating distribution. Unfortunately, we don't know this value since we don't know the true distribution, so we can take the empirical risk and minimize that. This is the core idea of empirical risk minimization in <b>learning theory</b>. Ideally, we would want the true risk and the empirical risk to be pretty close together most of the time, and in statistical learning theory, we use concentration of measure to bound these. Usually, you would see that the bound is decaying with $n$ (which means that we get better approximates with more samples) and increasing with $d$ (curse of dimensionality). The difference between our estimator and the true best function (or the bounds themselves) can often be decomposed into three terms which are analogues of the bias, variance, and noise. This gives a slightly more general interpretation of the tradeoff. </p>

  <p>Once the foundations of these two theories are established, the main applications lie in <b>machine learning</b>, which uses computer science to design algorithmic approaches to learning. This field can be seen as an integration of statistics and computer science, since we heavily use the theory of algorithms to optimize objective functions that are determined by statistics (e.g. greedy algorithms to fit decision trees, L1 regularization as a convex approximation of best subset regression). I've thought for a while on how to best organize these notes, since you can divide them according to different properties. I've realized that there's no right answer, but generally when I do research, there is usually a specific model or a set of similar models I have in mind (e.g. trees). Therefore, I like to divide them by the architecture of the model, and keep all the theory, properties, and algorithms in one place for reference. </p>

  <p>With the universal approximation theorem, better engineering, and exponentially-increasing computatonal power, <b>deep neural networks</b> have become extremely powerful models for complex and high-dimensional data. They start out with simple multilayer perceptrons but recent research has pushed the architectures to CNNs, RNNs, LSTMs, energy models, encoder-decoders, flow models, attention layers, and most recently diffusion models. While the models are inherent black-box in nature, several heuristics and architectures have been developed to push their applications to the field of <b>computer vision</b> (CV) and <b>natural language processing</b> (NLP). The most noticeable success of these applications come in autonomous driving and large language models (LLMs).</p>

  <p>Finally, another subfield of machine learning is called <b>reinforcement learning</b>, which teaches agents to make decisions through simulations involving trial and error. These models are widely used in robotics and simulations, and this field of optimizing rewards and penalties heavily relies on <b>decision theory</b>.</p>

  <p>All of my personal notes are free to download, use, and distrbute under the Creative Commons "Attribution- NonCommercial-ShareAlike 4.0 International" license. Please contact me if you find any errors in my notes or have any further questions.</p>

<button id="read-less-btn" onclick="toggleReadMore()">Read less</button>
</div>
</div>

<div class="html_row">
  <div class="html_note_section" style="margin-left: 0.2%;">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Information_Theory/paper.pdf" target="_blank"><b>Information Theory</b></a></p>
      <ul class="html_note_description">
        <li>Entropy: <i>KL-Divergence</i> </li>
        <li>Compression: <i>Symbol Codes, Huffman</i> </li>
      </ul>
    </div>
  </div>

  <div class="html_note_section" style="margin-left: 5%;">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Signal_Processing/paper.pdf" target="_blank"><b>Signal Processing</b></a></p>
      <ul class="html_note_description">
      </ul>
    </div>
  </div>
</div>

<div class="html_row">
  <div class="html_note_section" style="margin-left: 0.2%;">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Function_Spaces/paper.pdf" target="_blank"><b>Function Spaces</b></a></p>
      <ul class="html_note_description">
        <li>Sobelov Spaces: <i></i> </li>
        <li>Holder Spaces: <i></i> </li>
        <li>Reproducing Kernel Hilbert Spaces: <i></i> </li>
      </ul>
    </div>
  </div>

  <div class="html_note_section" style="margin-left: 5%;">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Concentration_of_Measure/paper.pdf" target="_blank"><b>Concentration of Measure</b></a></p>
      <ul class="html_note_description">
        <li>Hoeffding: <i>Chernoff Bounds, Subgaussian variables</i> </li> 
        <li>Talagrand, Bernstein</li>
      </ul>
    </div>
  </div>
</div>

<div class="html_row">
  <div class="html_note_section" style="margin-left: 0.2%;">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Frequentist_Inference/paper.pdf" target="_blank"><b>Frequentist Inference</b></a></p>
      <ul class="html_note_description">
        <li>Goals: <i>Point Estimation, Confidence Sets, Hypothesis Tests</i> </li>
        <li>Point Estimators: <i>Method of Moments, Maximum Likelihood (MLE)</i></li>
        <li>Bias Variance Tradeoff: <i>MSE, MAE, Regularization</i></li> 
        <li>Asymptotic Analysis</li>
      </ul>
    </div>
  </div>

  <div class="html_note_section" style="margin-left:5%">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Bayesian_Inference/paper.pdf"><b>Bayesian Inference</b></a></p>
      <ul class="html_note_description">
        <li>Goals: <i>Point Estimation, Credible Intervals, Posterior Inference</i></li>
        <li>Notation: <i>Priors, Posteriors, Conjugate Distributions</i></li>
        <li>Point Estimators: <i>Posterior Means, Maximum-a-Posteriori (MAP)</i></li>
      </ul>
    </div>
  </div>
</div>

<div class="html_row">
  <div class="html_note_section" style="margin-left: 0.2%;">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Optimization/paper.pdf" target="_blank"><b>Optimization</b></a></p>
      <ul class="html_note_description">
      </ul>
    </div>
  </div>

  <div class="html_note_section" style="margin-left: 5%;">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Sampling/paper.pdf" target="_blank"><b>Sampling and Integration</b></a></p>
      <ul class="html_note_description"> 
        <li>MCMC: <i>Metropolis-Hastings, Hamiltonian Monte Carlo, Gibbs</i></li> 
        <li>Langevin: <i>SGLD, MALA</i></li> 
        <li>Practical: <i>Adaptive Methods, Preconditioning</i></li>
      </ul>
    </div>
  </div>
</div>

<div class="html_row">
  <div class="html_note_section" style="margin-left: 0.2%;">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Decision_Theory/paper.pdf" target="_blank"><b>Decision Theory</b></a></p>
      <ul class="html_note_description">
      </ul>
    </div>
  </div>

  <div class="html_note_section" style="margin-left:5%">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Learning_Theory/paper.pdf"><b>Learning Theory</b></a></p>
      <ul class="html_note_description">
        <li>Empirical Risk Minimization: <i></i></li>
      </ul>
    </div>
  </div>
</div>

<div class="html_row">
  <div class="html_note_section" style="margin-left: 0.2%;">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Linear_Regression/paper.pdf"><b>Linear Regression</b></a></p>
      <ul class="html_note_description">
        <li>Ordinary Least Squares: <i>MSE Loss, Bias-Variance Decomposition, Least-Squares Solution, Gauss-Markov Theorem, Likelihood Estimation with Gaussian residuals</i></li> 
        <li>Significance Tests, Confidence Sets</li> 
        <li>Regularization: <i>Ridge, Best Subset, Greedy Stepwise, Lasso</i></li> 
        <li>Robust: <i>Mean Sb</i></li> 
        <li>Bayesian: <i>Regularization with Priors</i></li>
        <li>GLMs: <i>Link Functions, Canonical Link Functions (To be moved)</i></li>
      </ul>
    </div>
  </div>

  <div class="html_note_section" style="margin-left: 5%;">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Linear_Classification/paper.pdf" target="_blank"><b>Linear Classification</b></a></p>
      <ul class="html_note_description">
        <li>Perceptron: <i></i> </li>
        <li>Logistic/Softmax: <i></i> </li>
        <li>Support Vector Machines: <i></i> </li>
        <li>LDA: <i></i> </li>
      </ul>
    </div>
  </div>
</div>

<div class="html_row">
  <div class="html_note_section" style="margin-left: 0.2%;">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Time_Series/paper.pdf" target="_blank"><b>Time Series</b></a></p>
      <ul class="html_note_description">
        <li>Stationarity: <i></i> </li>
        <li>Autoregressive: <i></i> </li>
      </ul>
    </div>
  </div>

  <div class="html_note_section" style="margin-left: 5%;">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Kernels_Smoothers/paper.pdf" target="_blank"><b>Kernels and Smoothers</b></a></p>
      <ul class="html_note_description">
      </ul>
    </div>
  </div>
</div>

<div class="html_row">
  <div class="html_note_section" style="margin-left: 0.2%;">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Trees/paper.pdf" target="_blank"><b>Trees</b></a></p>
      <ul class="html_note_description">
        <li>Decision Trees: <i>Classification/Regression Trees, Model Space</i></li> 
        <li>Greedy Optimization: <i>ID3 with Information Gain, CART with Gini Reduction, c4.5</i></li> 
        <li>Improved Optimization: <i>GOSDT</i></li> 
        <li>Soft Decision Trees: <i>Soft Splitting, Neural Trees</i></li> 
      </ul>
    </div>
  </div>

  <div class="html_note_section" style="margin-left: 5%;">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Ensembles/paper.pdf" target="_blank"><b>Ensembles</b></a></p>
      <ul class="html_note_description">
        <li>Resampling: <i>Jackknife, Bootstrap</i></li>
        <li>Boosting: <i>Adaboost, Gradient Boosting</i></li>
        <li>Bagging: <i>Random Forests</i></li>
      </ul>
    </div>
  </div>
</div>

<div class="html_row">
  <div class="html_note_section" style="margin-left: 0.2%;">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Factors_Components/paper.pdf" target="_blank"><b>Factors and Components</b></a></p>
      <ul class="html_note_description">
        <li>Factor Analysis: <i></i> </li>
        <li>Principal Componenet Analysis: <i>Static, Dynamic, Functional, Robust, Sparse</i> </li> 
        <li>Independent Component Analysis: <i></i></li>
      </ul>
    </div>
  </div>

  <div class="html_note_section" style="margin-left: 5%;">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Clustering/paper.pdf" target="_blank"><b>Clustering</b></a></p>
      <ul class="html_note_description"> 
        <li>K Means: <i></i></li> 
        <li>Isomap:</li>
        <li>Multidimensional Scaling (MDS)</li>
        <li>tSNE: <i></i></li> 
        <li>Uniform Manifold Approximation and Projection (UMAP): <i></i></li> 
      </ul>
    </div>
  </div>
</div>

<div class="html_row">
  <div class="html_note_section" style="margin-left: 0.2%;">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Latent_Variable/paper.pdf" target="_blank"><b>Latent Variable Models</b></a></p>
      <ul class="html_note_description">
        <li>Gaussian Mixture Models<i></i> </li> 
        <li>Variational Bayes, EM Algorithm</li>
      </ul>
    </div>
  </div>

  <div class="html_note_section" style="margin-left: 5%;">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Graphical/paper.pdf" target="_blank"><b>Graphical Models</b></a></p>
      <ul class="html_note_description"> 
        <li>Graphical Distributions: <i>Bayesian Networks (Directed), Markov Random Fields (Undirected)</i></li>
      </ul>
    </div>
  </div>
</div>

<div class="html_row">
  <div class="html_note_section" style="margin-left: 0.2%">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Deep_Learning/paper.pdf" target="_blank"><b>Deep Learning</b></a></p>
      <ul class="html_note_description">
        <li>Multilayer Perceptron: <i>Activation Functions, Preprocessing, Weight Initialization, Weight Space Symmetries</i></li>
        <li>Network Training: <i>Automatic Differentiation, Forward/Back Propagation, Numpy Implementation, PyTorch, </i></li>
        <li>Regularization and Stability: <i>Early Stopping, Dropout, L1/L2 Penalty Terms, Max Norm Regularization, Normalization Layers, Data Augmentation, Sharpness Awareness Maximization, Network Pruning, Guidelines</i></li>
        <li>Convolutional Neural Nets: <i>Kernels, Convolutional Layers, Pooling Layers, Architectures</i></li>
        <li>Recurrent Neural Nets: <i>Uni/Bi-directional RNNs, Stacked RNNs, Loss Functions, LSTMs, GRUs</i></li>
        <li>Autoencoders: <i></i></li>
        <li>Transformers: <i></i></li>
      </ul>
    </div>
  </div>

  <div class="html_note_section" style="margin-left:5%">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Reinforcement_Learning/paper.pdf"><b>Reinforcement Learning</b></a></p>
      <ul class="html_note_description">
      </ul>
    </div>
  </div>
</div>

<div class="html_row">
  <div class="html_note_section" style="margin-left: 0.2%;">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Computer_Vision/paper.pdf" target="_blank"><b>Computer Vision</b></a></p>
      <ul class="html_note_description">
        <li>Image Processing: <i>OpenCV Functionality, Transforming, Drawing, Masking, Kernels, Color Channels</i></li>
        <li>Convolutional Neural Nets: <i>Convolution Layers, Pooling Layers, Architectures</i></li>
        <li>Network Training: <i>Backpropagation, Implementation from Scratch</i></li>
      </ul>
    </div>
  </div>

  <div class="html_note_section" style="margin-left:5%">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Natural_Language_Processing/paper.pdf"><b>Natural Language Processing</b></a></p>
      <ul class="html_note_description">
        <li>Basics: <i>Regular Expressions, Tokenization, Lemmization, Stemming, NLTK</i></li>
        <li>Classical Learning: <i>N-Gram Model, Naive Bayes, Logistic Regression, Sentiment Analysis</i></li>
        <li>Embeddings: <i>Frequency Semantics, Word2Vec, Doc2Vec, gensim, </i></li>
        <li>Recurrent Neural Nets: <i></i></li>
      </ul>
    </div>
  </div>
</div>

<div class="html_row">
  <div class="html_note_section" style="margin-left: 0.2%;">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Other/paper.pdf" target="_blank"><b>Others</b></a></p>
      <ul class="html_note_description">
        <li>Topological Data Analysis</li> 
        <li>Geodesic Regression: <i></i></li> 
        <li>Frechet Regression: <i></i></li>
      </ul>
    </div>
  </div>

  <div class="html_note_section" style="margin-left:5%">
    <div class="html_note_text">
      <p class="html_note_title"><a href="./paper.pdf"><b></b></a></p>
      <ul class="html_note_description">
      </ul>
    </div>
  </div>
</div>

<script src="../../assets/JS/page_line_count.js"></script>

<script>
// Make PDF links open in same tab
document.addEventListener('DOMContentLoaded', function() {
    const pdfLinks = document.querySelectorAll('a[href$=".pdf"]');
    pdfLinks.forEach(link => {
        link.setAttribute('target', '_self');
    });
});

// Toggle read more functionality
function toggleReadMore() {
    const previewSection = document.getElementById('intro-preview');
    const fullSection = document.getElementById('intro-full');
    const readMoreBtn = document.getElementById('read-more-btn');
    
    if (fullSection.style.display === 'none') {
        // Show full text
        previewSection.style.display = 'none';
        fullSection.style.display = 'block';
        readMoreBtn.style.display = 'none';
    } else {
        // Show preview with fade
        previewSection.style.display = 'block';
        fullSection.style.display = 'none';
        readMoreBtn.style.display = 'block';
    }
}
</script>

</body>

<!DOCTYPE html>
<html lang="en">
<head>
  <title>Muchang Bahng | Duke Math</title>
  <link rel="apple-touch-icon" sizes="180x180" href="/assets/Images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/Images/favicon.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/Images/favicon.ico">

  <link rel="stylesheet" href="/assets/CSS/Header_Footer.css">
  <link rel="stylesheet" href="/assets/CSS/personal_studies.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  </script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pdf.js/2.9.359/pdf.min.js"></script>
  
  <style>
    .intro_text {
      font-size: 14px;
    }
    
    .intro_text p {
      margin-bottom: 18px;
      line-height: 1.6;
    }
    
    #intro-preview {
      position: relative;
      max-height: 120px;
      overflow: hidden;
      -webkit-mask-image: linear-gradient(to bottom, black 0%, black 40%, transparent 100%);
      mask-image: linear-gradient(to bottom, black 0%, black 40%, transparent 100%);
    }
    
    #fade-overlay {
      display: none;
    }
    
    #read-more-btn {
      background: none;
      border: none;
      color: #666;
      cursor: pointer;
      text-decoration: none;
      padding: 12px 20px;
      font: inherit;
      font-size: 14px;
      transition: all 0.3s ease;
      z-index: 10;
      position: relative;
      display: block;
      margin: 10px auto 0 auto;
      width: fit-content;
    }
    
    #read-more-btn::before {
      content: "↓";
      margin-right: 8px;
      transition: transform 0.3s ease;
      display: inline-block;
    }
    
    #read-more-btn::after {
      content: "↓";
      margin-left: 8px;
      transition: transform 0.3s ease;
      display: inline-block;
    }
    
    #read-more-btn:hover {
      color: #333;
    }
    
    #read-more-btn:hover::before,
    #read-more-btn:hover::after {
      transform: translateY(3px);
      animation: bounce 0.6s ease-in-out infinite;
    }
    
    @keyframes bounce {
      0%, 100% { transform: translateY(3px); }
      50% { transform: translateY(6px); }
    }
    
    #read-less-btn {
      background: none;
      border: none;
      color: #666;
      cursor: pointer;
      text-decoration: none;
      padding: 12px 20px;
      font: inherit;
      font-size: 14px;
      transition: all 0.3s ease;
      z-index: 10;
      position: relative;
      display: block;
      margin: 15px auto 0 auto;
      width: fit-content;
    }
    
    #read-less-btn::before {
      content: "↑";
      margin-right: 8px;
      transition: transform 0.3s ease;
      display: inline-block;
    }
    
    #read-less-btn::after {
      content: "↑";
      margin-left: 8px;
      transition: transform 0.3s ease;
      display: inline-block;
    }
    
    #read-less-btn:hover {
      color: #333;
    }
    
    #read-less-btn:hover::before,
    #read-less-btn:hover::after {
      transform: translateY(-3px);
      animation: bounceUp 0.6s ease-in-out infinite;
    }
    
    @keyframes bounceUp {
      0%, 100% { transform: translateY(-3px); }
      50% { transform: translateY(-6px); }
    }
  </style>
</head>

<body>
<div class="header">
  <div id="menu_button" onclick="myFunction(this)">
      <div id="bar1"></div>
      <div id="bar2"></div>
      <div id="bar3"></div>
  </div>
  <a id="HeaderName" href="/index.html" style="text-decoration: none;">Muchang Bahng</a>
</div>

<div class=space></div>

<p class="title">Statistics</p>
<div class="intro_text"> 
<div id="intro-preview">
<p>If you were to invent statistics from scratch, how would I do it? Statistics can be seen as a "converse" of probability, and it is essentially a field that branches out from math. In probability, one takes a distribution and attempts to describe what the samples look like. In statistics, we are given the samples first and then try to <i>infer</i> what the distribution is. This is usually an extremely difficult problem, and so rather than trying to describe the entire distribution, we try to talk about certain parameters about the distribution (e.g. what is the mean, variance?).</p>

<p>At the heart of statistics is the seemingly unrelated <b>information theory</b>, which talks about how much information (e.g. bits) can be transmitted through a noisy channel. The concepts of entropy and KL-divergence provide a good transition between probability and statistics. At this point, we can branch off into two paradigms of inference. First is the <b>frequentist</b> approach, which attempts to model the parameter as a random variable that realizes through a sampling distribution. The second is the <b>Bayesian</b> approach, which assumes a prior distribution on the data, which with the data and Bayes rule gets updated to our posterior.</p>

<p>Once the foundations of these two theories are established, the main applications lie in <b>machine learning</b>, which uses computer science to design algorithmic approaches to statistical inference. This field can be seen as an integration of statistics and computer science, since we heavily use the theory of algorithms to optimize objective functions that are determined by statistics (e.g. greedy algorithms to fit decision trees, L1 regularization as a convex approximation of best subset regression). In fact, <b>optimization</b> is such an important subset of machine learning that it deserves its own set of notes. In here, I go through the main concepts in <i>convex optimization</i>, along with an index of other non-convex methods. It turns out that the fields of optimization and <b>sampling</b> (which is also used for numerical integration) are heavily related, as slight modifications of optimizers lead to samplers (e.g. SGD vs SGLD).</p>

<div id="fade-overlay"></div>
</div>
<button id="read-more-btn" onclick="toggleReadMore()">Read more</button>

<div id="intro-full" style="display: none;">
<p>If you were to invent statistics from scratch, how would I do it? Statistics can be seen as a "converse" of probability, and it is essentially a field that branches out from math. In probability, one takes a distribution and attempts to describe what the samples look like. In statistics, we are given the samples first and then try to <i>infer</i> what the distribution is. This is usually an extremely difficult problem, and so rather than trying to describe the entire distribution, we try to talk about certain parameters about the distribution (e.g. what is the mean, variance?).</p>

<p>At the heart of statistics is the seemingly unrelated <b>information theory</b>, which talks about how much information (e.g. bits) can be transmitted through a noisy channel. The concepts of entropy and KL-divergence provide a good transition between probability and statistics. At this point, we can branch off into two paradigms of inference. First is the <b>frequentist</b> approach, which attempts to model the parameter as a random variable that realizes through a sampling distribution. The second is the <b>Bayesian</b> approach, which assumes a prior distribution on the data, which with the data and Bayes rule gets updated to our posterior.</p>

<p>Once the foundations of these two theories are established, the main applications lie in <b>machine learning</b>, which uses computer science to design algorithmic approaches to statistical inference. This field can be seen as an integration of statistics and computer science, since we heavily use the theory of algorithms to optimize objective functions that are determined by statistics (e.g. greedy algorithms to fit decision trees, L1 regularization as a convex approximation of best subset regression). In fact, <b>optimization</b> is such an important subset of machine learning that it deserves its own set of notes. In here, I go through the main concepts in <i>convex optimization</i>, along with an index of other non-convex methods. It turns out that the fields of optimization and <b>sampling</b> (which is also used for numerical integration) are heavily related, as slight modifications of optimizers lead to samplers (e.g. SGD vs SGLD).</p>

<p>With the universal approximation theorem, better engineering, and exponentially-increasing computatonal power, <b>deep neural networks</b> have become extremely powerful models for complex and high-dimensional data. They start out with simple multilayer perceptrons but recent research has pushed the architectures to CNNs, RNNs, LSTMs, energy models, encoder-decoders, flow models, attention layers, and most recently diffusion models. While the models are inherent black-box in nature, several heuristics and architectures have been developed to push their applications to the field of <b>computer vision</b> (CV) and <b>natural language processing</b> (NLP). The most noticeable success of these applications come in autonomous driving and large language models (LLMs).</p>

<p>Finally, another subfield of machine learning is called <b>reinforcement learning</b>, which teaches agents to make decisions through simulations involving trial and error. These models are widely used in robotics and simulations, and this field of optimizing rewards and penalties heavily relies on <b>decision theory</b>.</p>

<p>All of my personal notes are free to download, use, and distrbute under the Creative Commons "Attribution- NonCommercial-ShareAlike 4.0 International" license. Please contact me if you find any errors in my notes or have any further questions.</p>

<button id="read-less-btn" onclick="toggleReadMore()">Read less</button>
</div>
</div>

<div class="html_row">
  <div class="html_note_section" style="margin-left: 0.2%;">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Information_Theory/paper.pdf" target="_blank"><b>Information Theory</b></a></p>
      <ul class="html_note_description">
        <li><i></i> </li>
      </ul>
    </div>
  </div>

  <div class="html_note_section" style="margin-left: 5%;">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Sampling_and_Optimization/paper.pdf" target="_blank"><b>Sampling, Optimization, and Integration</b></a></p>
      <ul class="html_note_description">
        <li>This is at the heart of all things statistics, so it's worth making a set of notes that outlines the methods of all the sampling and optimization algorithms and the theories behind them. I focus on convex optimization. </li>
        <li><i>Random Walk Metropolis w/ Preconditioning & Adaptation, Automatic Differentiation, Gradient Descent, SGLD, MALA</i></li>
        <li><i>Phase Flows, Hamiltonian Integration, Langevin Integration, Leapfrog Integrator, Splitting Methods</i></li>
        <li><i>Hamiltonian Monte Carlo, NUTS</i></li>
        <li><i>Netwon's Optimization Method, BFGS, Simulated Annealing, Adam</i></li>
      </ul>
    </div>
  </div>
</div>

<div class="html_row">
  <div class="html_note_section" style="margin-left: 0.2%;">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Frequentist_Statistics/paper.pdf" target="_blank"><b>Frequentist Statistics</b></a></p>
      <ul class="html_note_description">
        <li>Sampling Distributions: <i>Confidence Intervals, Hypothesis Testing, Central Limit Theorem</i> </li>
      </ul>
    </div>
  </div>

  <div class="html_note_section" style="margin-left:5%">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Bayesian_Statistics/paper.pdf"><b>Bayesian Statistics</b></a></p>
      <ul class="html_note_description">
        <li>Bayes Rule: <i>Prior & Posterior Distributions, Likelihood, Marginalization, Bayes Box, Common Distributions, Beta Family, Multivariate Gaussians</i></li>
        <li>Bayesian Inference: <i>Parameter Estimation, Beta-Binomial Distribution, Conjugate Distributions & Priors, Credible Intervals, Point Estimates, Exponential Family</i></li>
        <li>Linear Regression: <i>Bayesian & Frequentist Regression, Basis Functions, Hyperparameters & Hierarchical Priors, Parameter Distributions & Predictive Functions, Bayesian Model Selection & Averaging, Gaussian Error/OLS & Laplace Error/LAV, L1 & L2 Regularization w/ Laplace & Gaussian Priors, Sparse Models, Equivalent Kernel</i></li>
        <li>Markov Chain Monte Carlo: <i>Metropolis-Hastings, Detailed Balance, Monte Carlo Integration, Gibbs Sampling</i></li>
      </ul>
    </div>
  </div>
</div>

<div class="html_row">
  <div class="html_note_section" style="margin-left: 0.2%;">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Machine_Learning/paper.pdf"><b>Classical Machine Learning</b></a></p>
      <ul class="html_note_description">
        <li>Statistical Learning Theory</li>
        <li>Low and High Dimensional Linear Regression and Classification</li>
        <li>Low and High Dimensional Nonparametric Regression and Classification</li>
        <li>Cross Validation</li>
        <li>Decision Theory</li>
        <li>Generalized Linear Models</li>
        <li>Boosting and Bagging</li>
        <li>Density Estimation and Clustering</li>
        <li>Graphical Models</li>
        <li>Factor Analysis</li>
        <li>Dimensionality Reduction</li>
      </ul>
    </div>
  </div>

  <div class="html_note_section" style="margin-left:5%">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Deep_Learning/paper.pdf" target="_blank"><b>Deep Learning</b></a></p>
      <ul class="html_note_description">
        <li>Multilayer Perceptron: <i>Activation Functions, Preprocessing, Weight Initialization, Weight Space Symmetries</i></li>
        <li>Network Training: <i>Automatic Differentiation, Forward/Back Propagation, Numpy Implementation, PyTorch, </i></li>
        <li>Regularization and Stability: <i>Early Stopping, Dropout, L1/L2 Penalty Terms, Max Norm Regularization, Normalization Layers, Data Augmentation, Sharpness Awareness Maximization, Network Pruning, Guidelines</i></li>
        <li>Convolutional Neural Nets: <i>Kernels, Convolutional Layers, Pooling Layers, Architectures</i></li>
        <li>Recurrent Neural Nets: <i>Uni/Bi-directional RNNs, Stacked RNNs, Loss Functions, LSTMs, GRUs</i></li>
        <li>Autoencoders: <i></i></li>
        <li>Transformers: <i></i></li>
      </ul>
    </div>
  </div>
</div>

<div class="html_row">
  <div class="html_note_section" style="margin-left: 0.2%;">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Computer_Vision/paper.pdf" target="_blank"><b>Computer Vision</b></a></p>
      <ul class="html_note_description">
        <li>Image Processing: <i>OpenCV Functionality, Transforming, Drawing, Masking, Kernels, Color Channels</i></li>
        <li>Convolutional Neural Nets: <i>Convolution Layers, Pooling Layers, Architectures</i></li>
        <li>Network Training: <i>Backpropagation, Implementation from Scratch</i></li>
      </ul>
    </div>
  </div>

  <div class="html_note_section" style="margin-left:5%">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Natural_Language_Processing/paper.pdf"><b>Natural Language Processing</b></a></p>
      <ul class="html_note_description">
        <li>Basics: <i>Regular Expressions, Tokenization, Lemmization, Stemming, NLTK</i></li>
        <li>Classical Learning: <i>N-Gram Model, Naive Bayes, Logistic Regression, Sentiment Analysis</i></li>
        <li>Embeddings: <i>Frequency Semantics, Word2Vec, Doc2Vec, gensim, </i></li>
        <li>Recurrent Neural Nets: <i></i></li>
      </ul>
    </div>
  </div>
</div>

<div class="html_row">
  <div class="html_note_section" style="margin-left: 0.2%">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Decision_Theory/paper.pdf"><b>Decision Theory</b></a></p>
      <ul class="html_note_description">
      </ul>
    </div>
  </div>

  <div class="html_note_section" style="margin-left:5%">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Reinforcement_Learning/paper.pdf"><b>Reinforcement Learning</b></a></p>
      <ul class="html_note_description">
      </ul>
    </div>
  </div>
</div>

<div class="html_row">
  <div class="html_note_section" style="margin-left: 0.2%;">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Intro_Machine_Learning/paper.html" target="_blank"><b>Introduction to Machine Learning</b></a></p>
      <ul class="html_note_description">
        <li>Regression: <i>Least-Squares, Normal Equations, Batch/Stochastic Gradient Descent, Polynomial Regression</i></li>
        <li>Classification: <i>K-Nearest Neighbors, Perceptron, Logistic Regression</i></li>
        <li>GLMs: <i>Exponential Family, Link Functions, GLM Construction, Softmax Regression, Poisson Regression</i></li>
        <li>Generative Learning Algorithms: <i>Gaussian Discriminant Analysis, Naive Bayes, Laplace Smoothing</i></li>
        <li>Kernel Methods: <i>Feature Maps, Kernel Trick</i></li>
        <li>SVM: <i>Functional, Function & Geometric Margins, Optimal Margin Classifiers, Lagrange Duality, Primal vs Dual Optimization</i></li>
        <li>Deep Learning: <i>Nonlinear Regression, Mini-batch SGD, Activation Functions (ReLU), 2-Layer & Multilayered Neural Networks, Vectorization, Backpropagation, Convolutional Neural Networks, Graph Neural Networks</i></li>
        <li>Decision Trees: <i>Recursive Binary Splitting (Greedy Algorithms), Classification Error, Discrete/Continuous Features, Overfitting, Pruning Trees, Random Forest</i></li>
        <li>Unsupervised Learning: <i>K-Means, Mixture of Gaussians, EM-Algorithm, Convexity, Evidence Lower Bound </i></li>
        <li>PCA: <i>Factor Analysis, EM Algorithm, Component Eignvectors, SVD, Eigenfaces</i></li>
      </ul>
    </div>
  </div>

  <div class="html_note_section" style="margin-left:5%">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Image_Processing/paper.pdf"><b>Image and Video Processing [ECE 588]</b></a></p>
      <ul class="html_note_description">
      </ul>
    </div>
  </div>
</div>

<script src="../../assets/JS/page_line_count.js"></script>

<script>
// Make PDF links open in same tab
document.addEventListener('DOMContentLoaded', function() {
    const pdfLinks = document.querySelectorAll('a[href$=".pdf"]');
    pdfLinks.forEach(link => {
        link.setAttribute('target', '_self');
    });
});

// Toggle read more functionality
function toggleReadMore() {
    const previewSection = document.getElementById('intro-preview');
    const fullSection = document.getElementById('intro-full');
    const readMoreBtn = document.getElementById('read-more-btn');
    
    if (fullSection.style.display === 'none') {
        // Show full text
        previewSection.style.display = 'none';
        fullSection.style.display = 'block';
        readMoreBtn.style.display = 'none';
    } else {
        // Show preview with fade
        previewSection.style.display = 'block';
        fullSection.style.display = 'none';
        readMoreBtn.style.display = 'block';
    }
}
</script>

</body>

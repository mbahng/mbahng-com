<!DOCTYPE html>
<html lang="en">
<head>
  <title>Muchang Bahng | Duke Math</title>
  <link rel="apple-touch-icon" sizes="180x180" href="/assets/Images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/Images/favicon.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/Images/favicon.ico">

  <link rel="stylesheet" href="/assets/CSS/Header_Footer.css">
  <link rel="stylesheet" href="/assets/CSS/personal_studies.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  </script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pdf.js/2.9.359/pdf.min.js"></script>
  
  <style>
    .intro_text {
      font-size: 14px;
    }
    
    .intro_text p {
      margin-bottom: 18px;
      line-height: 1.6;
    }
    
    #intro-preview {
      position: relative;
      max-height: 120px;
      overflow: hidden;
      -webkit-mask-image: linear-gradient(to bottom, black 0%, black 40%, transparent 100%);
      mask-image: linear-gradient(to bottom, black 0%, black 40%, transparent 100%);
    }
    
    #fade-overlay {
      display: none;
    }
    
    #read-more-btn {
      background: none;
      border: none;
      color: #666;
      cursor: pointer;
      text-decoration: none;
      padding: 12px 20px;
      font: inherit;
      font-size: 14px;
      transition: all 0.3s ease;
      z-index: 10;
      position: relative;
      display: block;
      margin: 10px auto 0 auto;
      width: fit-content;
    }
    
    #read-more-btn::before {
      content: "↓";
      margin-right: 8px;
      transition: transform 0.3s ease;
      display: inline-block;
    }
    
    #read-more-btn::after {
      content: "↓";
      margin-left: 8px;
      transition: transform 0.3s ease;
      display: inline-block;
    }
    
    #read-more-btn:hover {
      color: #333;
    }
    
    #read-more-btn:hover::before,
    #read-more-btn:hover::after {
      transform: translateY(3px);
      animation: bounce 0.6s ease-in-out infinite;
    }
    
    @keyframes bounce {
      0%, 100% { transform: translateY(3px); }
      50% { transform: translateY(6px); }
    }
    
    #read-less-btn {
      background: none;
      border: none;
      color: #666;
      cursor: pointer;
      text-decoration: none;
      padding: 12px 20px;
      font: inherit;
      font-size: 14px;
      transition: all 0.3s ease;
      z-index: 10;
      position: relative;
      display: block;
      margin: 15px auto 0 auto;
      width: fit-content;
    }
    
    #read-less-btn::before {
      content: "↑";
      margin-right: 8px;
      transition: transform 0.3s ease;
      display: inline-block;
    }
    
    #read-less-btn::after {
      content: "↑";
      margin-left: 8px;
      transition: transform 0.3s ease;
      display: inline-block;
    }
    
    #read-less-btn:hover {
      color: #333;
    }
    
    #read-less-btn:hover::before,
    #read-less-btn:hover::after {
      transform: translateY(-3px);
      animation: bounceUp 0.6s ease-in-out infinite;
    }
    
    @keyframes bounceUp {
      0%, 100% { transform: translateY(-3px); }
      50% { transform: translateY(-6px); }
    }
  </style>
</head>

<body>
<div class="header">
  <div id="menu_button" onclick="myFunction(this)">
      <div id="bar1"></div>
      <div id="bar2"></div>
      <div id="bar3"></div>
  </div>
  <a id="HeaderName" href="/index.html" style="text-decoration: none;">Muchang Bahng</a>
</div>

<div class=space></div>

<p class="title">Statistics</p>
<div class="intro_text"> 
<div id="intro-preview">

  <p>If you were to invent statistics from scratch, how would I do it? Statistics can be seen as a "converse" of probability, and it is essentially a field that branches out from math. In probability, one takes a distribution and attempts to describe what the samples look like. In statistics, we are given the samples first and then try to <i>infer</i> what the distribution is. This is usually an extremely difficult problem, and so rather than trying to describe the entire distribution, we try to talk about certain parameters about the distribution (e.g. what is the mean, variance?).</p>

  <p>At the heart of statistics is the seemingly unrelated <b>information theory</b>, which talks about how much information (e.g. bits) can be transmitted through a noisy channel. The concepts of entropy and KL-divergence provide a good transition between probability and statistics. At this point, we can branch off into two paradigms of inference. First is the <b>frequentist</b> approach, which attempts to model the parameter as a random variable that realizes through a sampling distribution. The second is the <b>Bayesian</b> approach, which assumes a prior distribution on the data, which with the data and Bayes rule gets updated to our posterior.</p>

  <p>Once the foundations of these two theories are established, the main applications lie in <b>machine learning</b>, which uses computer science to design algorithmic approaches to statistical inference. This field can be seen as an integration of statistics and computer science, since we heavily use the theory of algorithms to optimize objective functions that are determined by statistics (e.g. greedy algorithms to fit decision trees, L1 regularization as a convex approximation of best subset regression). In fact, <b>optimization</b> is such an important subset of machine learning that it deserves its own set of notes. In here, I go through the main concepts in <i>convex optimization</i>, along with an index of other non-convex methods. It turns out that the fields of optimization and <b>sampling</b> (which is also used for numerical integration) are heavily related, as slight modifications of optimizers lead to samplers (e.g. SGD vs SGLD).</p>

  <div id="fade-overlay"></div>
  </div>
  <button id="read-more-btn" onclick="toggleReadMore()">Read more</button>

  <div id="intro-full" style="display: none;">
  <p>If you were to invent statistics from scratch, how would I do it? Statistics can be seen as a "converse" of probability, and it is essentially a field that branches out from math. In probability, one takes a distribution and attempts to describe what the samples look like. In statistics, we are given the samples first and then try to <i>infer</i> what the distribution is. This is usually an extremely difficult problem, and so rather than trying to describe the entire distribution, we try to talk about certain parameters about the distribution (e.g. what is the mean, variance?).</p>

  <p>At the heart of statistics is the seemingly unrelated <b>information theory</b>, which talks about how much information (e.g. bits) can be transmitted through a noisy channel. The concepts of entropy and KL-divergence provide a good transition between probability and statistics. At this point, we can branch off into two paradigms of inference. First is the <b>frequentist</b> approach, which attempts to model the parameter as a random variable that realizes through a sampling distribution. The second is the <b>Bayesian</b> approach, which assumes a prior distribution on the data, which with the data and Bayes rule gets updated to our posterior.</p>

  <p>Once the foundations of these two theories are established, the main applications lie in <b>machine learning</b>, which uses computer science to design algorithmic approaches to statistical inference. This field can be seen as an integration of statistics and computer science, since we heavily use the theory of algorithms to optimize objective functions that are determined by statistics (e.g. greedy algorithms to fit decision trees, L1 regularization as a convex approximation of best subset regression). In fact, <b>optimization</b> is such an important subset of machine learning that it deserves its own set of notes. In here, I go through the main concepts in <i>convex optimization</i>, along with an index of other non-convex methods. It turns out that the fields of optimization and <b>sampling</b> (which is also used for numerical integration) are heavily related, as slight modifications of optimizers lead to samplers (e.g. SGD vs SGLD).</p>

  <p>With the universal approximation theorem, better engineering, and exponentially-increasing computatonal power, <b>deep neural networks</b> have become extremely powerful models for complex and high-dimensional data. They start out with simple multilayer perceptrons but recent research has pushed the architectures to CNNs, RNNs, LSTMs, energy models, encoder-decoders, flow models, attention layers, and most recently diffusion models. While the models are inherent black-box in nature, several heuristics and architectures have been developed to push their applications to the field of <b>computer vision</b> (CV) and <b>natural language processing</b> (NLP). The most noticeable success of these applications come in autonomous driving and large language models (LLMs).</p>

  <p>Finally, another subfield of machine learning is called <b>reinforcement learning</b>, which teaches agents to make decisions through simulations involving trial and error. These models are widely used in robotics and simulations, and this field of optimizing rewards and penalties heavily relies on <b>decision theory</b>.</p>

  <p>All of my personal notes are free to download, use, and distrbute under the Creative Commons "Attribution- NonCommercial-ShareAlike 4.0 International" license. Please contact me if you find any errors in my notes or have any further questions.</p>

<button id="read-less-btn" onclick="toggleReadMore()">Read less</button>
</div>
</div>

<div class="html_row">
  <div class="html_note_section" style="margin-left: 0.2%;">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Information_Theory/paper.pdf" target="_blank"><b>Information Theory</b></a></p>
      <ul class="html_note_description">
        <li><i></i> </li>
      </ul>
    </div>
  </div>

  <div class="html_note_section" style="margin-left: 5%;">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Signal_Processing/paper.pdf" target="_blank"><b>Signal Processing</b></a></p>
      <ul class="html_note_description">
      </ul>
    </div>
  </div>
</div>

<div class="html_row">
  <div class="html_note_section" style="margin-left: 0.2%;">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Frequentist_Inference/paper.pdf" target="_blank"><b>Frequentist Inference</b></a></p>
      <ul class="html_note_description">
        <li>Goals: <i>Point Estimation, Confidence Sets, Hypothesis Tests</i> </li>
        <li>Point Estimators: <i>Method of Moments, Maximum Likelihood (MLE)</i></li>
        <li>Bootstrap, Jackknife</li> 
        <li>Bias Variance Tradeoff: <i>Regularization</i></li> 
        <li>Asymptotic Analysis</li>
      </ul>
    </div>
  </div>

  <div class="html_note_section" style="margin-left:5%">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Bayesian_Inference/paper.pdf"><b>Bayesian Inference</b></a></p>
      <ul class="html_note_description">
        <li>Goals: <i>Point Estimation, Credible Intervals, Posterior Inference</i></li>
        <li>Notation: <i>Priors, Posteriors, Conjugate Distributions</i></li>
        <li>Point Estimators: <i>Posterior Means, Maximum-a-Posteriori (MAP)</i></li>
      </ul>
    </div>
  </div>
</div>

<div class="html_row">
  <div class="html_note_section" style="margin-left: 0.2%;">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Learning_Theory/paper.pdf" target="_blank"><b>Statistical Learning and Decision Theory</b></a></p>
      <ul class="html_note_description">
        <li>Decision Theory: <i>Choosing the best estimator</i></li> 
        <li>Complexity: <i>Rademacher Complexity, VC dimension</i></li>
      </ul>
    </div>
  </div>

  <div class="html_note_section" style="margin-left: 5%;">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Sampling_and_Optimization/paper.pdf" target="_blank"><b>Sampling, Optimization, and Integration</b></a></p>
      <ul class="html_note_description"> 
        <li>Gradient Optimizers: <i>SGD, Adam</i></li> 
        <li>Hessian and pseudo-Hessian Optimizers: <i>Newton, BFGS, LBFGS</i></li> 
        <li>Constrained Optimization: <i>Lagrangians</i></li>
        <li>Samplers: <i>Metropolis-Hastings, Gibbs, SGLD, OU-methods</i></li>
      </ul>
    </div>
  </div>
</div>

<div class="html_row">
  <div class="html_note_section" style="margin-left: 0.2%;">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Linear_Regression/paper.pdf"><b>Linear Regression</b></a></p>
      <ul class="html_note_description">
        <li>Ordinary Least Squares: <i></i></li> 
        <li>Low and High Dimensional Linear Regression and Classification</li>
        <li>Low and High Dimensional Nonparametric Regression and Classification</li>
        <li>Generalized Linear Models</li>
      </ul>
    </div>
  </div>

  <div class="html_note_section" style="margin-left: 5%;">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Linear_Classification/paper.pdf" target="_blank"><b>Linear Classification</b></a></p>
      <ul class="html_note_description">
        <li><i></i> </li>
      </ul>
    </div>
  </div>
</div>

<div class="html_row">
  <div class="html_note_section" style="margin-left: 0.2%;">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Time_Series/paper.pdf" target="_blank"><b>Time Series</b></a></p>
      <ul class="html_note_description">
      </ul>
    </div>
  </div>

  <div class="html_note_section" style="margin-left: 5%;">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Kernels_Smoothers/paper.pdf" target="_blank"><b>Kernels and Smoothers</b></a></p>
      <ul class="html_note_description">
      </ul>
    </div>
  </div>
</div>

<div class="html_row">
  <div class="html_note_section" style="margin-left: 0.2%;">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Trees/paper.pdf" target="_blank"><b>Trees</b></a></p>
      <ul class="html_note_description">
        <li><i></i> </li>
      </ul>
    </div>
  </div>

  <div class="html_note_section" style="margin-left: 5%;">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Ensemble/paper.pdf" target="_blank"><b>Ensembles</b></a></p>
      <ul class="html_note_description">
        <li>Boosting and Bagging: <i>Adaboost, Gradient Boosting</i></li>
      </ul>
    </div>
  </div>
</div>

<div class="html_row">
  <div class="html_note_section" style="margin-left: 0.2%;">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Factors_Components/paper.pdf" target="_blank"><b>Factors and Components</b></a></p>
      <ul class="html_note_description">
        <li>Factor Analysis: <i></i> </li>
        <li>Principal Componenet Analysis: <i>Static, Dynamic, Functional, Robust, Sparse</i> </li> 
        <li>Independent Component Analysis: <i></i></li>
      </ul>
    </div>
  </div>

  <div class="html_note_section" style="margin-left: 5%;">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Clustering/paper.pdf" target="_blank"><b>Clustering</b></a></p>
      <ul class="html_note_description"> 
        <li>K Means: <i></i></li> 
        <li>Multidimensional Scaling (MDS)</li>
        <li>tSNE: <i></i></li> 
        <li>Uniform Manifold Approximation and Projection (UMAP): <i></i></li> 
      </ul>
    </div>
  </div>
</div>

<div class="html_row">
  <div class="html_note_section" style="margin-left: 0.2%;">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Latent_Variable/paper.pdf" target="_blank"><b>Latent Variable Models</b></a></p>
      <ul class="html_note_description">
        <li>Gaussian Mixture Models<i></i> </li> 
        <li>Variational Bayes, EM Algorithm</li>
      </ul>
    </div>
  </div>

  <div class="html_note_section" style="margin-left: 5%;">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Graphical/paper.pdf" target="_blank"><b>Graphical Models</b></a></p>
      <ul class="html_note_description"> 
        <li>Graphical Distributions: <i>Bayesian Networks (Directed), Markov Random Fields (Undirected)</i></li>
      </ul>
    </div>
  </div>
</div>

<div class="html_row">
  <div class="html_note_section" style="margin-left: 0.2%">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Deep_Learning/paper.pdf" target="_blank"><b>Deep Learning</b></a></p>
      <ul class="html_note_description">
        <li>Multilayer Perceptron: <i>Activation Functions, Preprocessing, Weight Initialization, Weight Space Symmetries</i></li>
        <li>Network Training: <i>Automatic Differentiation, Forward/Back Propagation, Numpy Implementation, PyTorch, </i></li>
        <li>Regularization and Stability: <i>Early Stopping, Dropout, L1/L2 Penalty Terms, Max Norm Regularization, Normalization Layers, Data Augmentation, Sharpness Awareness Maximization, Network Pruning, Guidelines</i></li>
        <li>Convolutional Neural Nets: <i>Kernels, Convolutional Layers, Pooling Layers, Architectures</i></li>
        <li>Recurrent Neural Nets: <i>Uni/Bi-directional RNNs, Stacked RNNs, Loss Functions, LSTMs, GRUs</i></li>
        <li>Autoencoders: <i></i></li>
        <li>Transformers: <i></i></li>
      </ul>
    </div>
  </div>

  <div class="html_note_section" style="margin-left:5%">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Reinforcement_Learning/paper.pdf"><b>Reinforcement Learning</b></a></p>
      <ul class="html_note_description">
      </ul>
    </div>
  </div>
</div>

<div class="html_row">
  <div class="html_note_section" style="margin-left: 0.2%;">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Computer_Vision/paper.pdf" target="_blank"><b>Computer Vision</b></a></p>
      <ul class="html_note_description">
        <li>Image Processing: <i>OpenCV Functionality, Transforming, Drawing, Masking, Kernels, Color Channels</i></li>
        <li>Convolutional Neural Nets: <i>Convolution Layers, Pooling Layers, Architectures</i></li>
        <li>Network Training: <i>Backpropagation, Implementation from Scratch</i></li>
      </ul>
    </div>
  </div>

  <div class="html_note_section" style="margin-left:5%">
    <div class="html_note_text">
      <p class="html_note_title"><a href="Natural_Language_Processing/paper.pdf"><b>Natural Language Processing</b></a></p>
      <ul class="html_note_description">
        <li>Basics: <i>Regular Expressions, Tokenization, Lemmization, Stemming, NLTK</i></li>
        <li>Classical Learning: <i>N-Gram Model, Naive Bayes, Logistic Regression, Sentiment Analysis</i></li>
        <li>Embeddings: <i>Frequency Semantics, Word2Vec, Doc2Vec, gensim, </i></li>
        <li>Recurrent Neural Nets: <i></i></li>
      </ul>
    </div>
  </div>
</div>

<script src="../../assets/JS/page_line_count.js"></script>

<script>
// Make PDF links open in same tab
document.addEventListener('DOMContentLoaded', function() {
    const pdfLinks = document.querySelectorAll('a[href$=".pdf"]');
    pdfLinks.forEach(link => {
        link.setAttribute('target', '_self');
    });
});

// Toggle read more functionality
function toggleReadMore() {
    const previewSection = document.getElementById('intro-preview');
    const fullSection = document.getElementById('intro-full');
    const readMoreBtn = document.getElementById('read-more-btn');
    
    if (fullSection.style.display === 'none') {
        // Show full text
        previewSection.style.display = 'none';
        fullSection.style.display = 'block';
        readMoreBtn.style.display = 'none';
    } else {
        // Show preview with fade
        previewSection.style.display = 'block';
        fullSection.style.display = 'none';
        readMoreBtn.style.display = 'block';
    }
}
</script>

</body>

A course on the study of data structures and algorithms. We can start off by defining what these two terms mean. 

\begin{definition}[Data Structure]
  A \textbf{data structure} $\mathcal{D}$ is an object that can store some amount of data. The \textbf{size} $|\mathcal{D}|$ of the data structure is a finite collection of nonnegative integers used to characterize the number of values that it stores.\footnote{For example, the size of an array is $N$, but the size of a graph $G = (V, E)$ is characterized by two numbers $(N, M)$ representing the number of vertices and edges in the graph.} 
\end{definition}

\begin{definition}[Algorithm]
  Given a data structure $\mathcal{D}$, an \textbf{algorithm} $\mathcal{A}(\mathcal{D})$ is a procedure on the data structure for correctly solving a mathematical problem in a finite number of steps. There are generally two paradigms on how algorithms work: 
  \begin{enumerate}
    \item \textit{Iterative}. Where $\mathcal{A}(\mathcal{D})$ is called that processes $\mathcal{D}$. 
    \item \textit{Recursive}. $\mathcal{A}(\mathcal{D})$ calls the same algorithm on a finite number of processed (usually smaller) data structures $\mathcal{A}(\mathcal{D}_i)$. After these recursive calls, the base algorithm may then postprocess the outputs to generate a new output. The general way to do this is to consider the base case of $\mathcal{D}$ when $\mathcal{A}(\mathcal{D})$ should not make any more recursive calls. Then assuming that the algorithm runs correctly for some input, determine what we should do with the recursive call.\footnote{We must ensure that every recursive call gets closer to the base case, or this will never end in a finite number of steps and is thus not an algorithm. }
  \end{enumerate}
  The \textbf{runtime} of an algorithm is a function $T(|\mathcal{D}|)$ with respect to the size of the dataset that outputs the number of steps it takes to complete this algorithm.\footnote{The reason I don't put time as the output is that time can refer to many things, such as physical time or the number of flops, which may differ depending on what computer you run an algorithm on. The number of steps is constant, and informally these refer to \textit{atomic} steps that cannot be decomposed further into substeps.}\footnote{There are two notions of runtime here. We can compare $f$ and $g$ with respect to the \textit{value} $N$ of the input, or we can compare them with respect to the \textit{number of bits} $n$ in the input. While we mostly use the complexity w.r.t. the value in these notes, we should be aware for certain (especially low-level operations), the bit complexity is also important, which will be focused on in my advanced algorithms notes. }
\end{definition}

\begin{figure}[H]
  \centering 
  \begin{tikzpicture}[
    scale=0.7, transform shape,
    box/.style={draw, minimum width=2cm, minimum height=1.5cm, align=center},
    arrow/.style={-{Stealth[length=3mm]}, thick, blue},
    curved arrow/.style={-{Stealth[length=3mm]}, thick, red, bend left=30},
    level distance=2cm,
    sibling distance=1.8cm,
    smalltext/.style={font=\scriptsize} % Added small text style
  ]
  % Left Figure - Linear Recursion
  \begin{scope}[local bounding box=left]
    \node[box] (A) {$\mathcal{A}(\mathcal{D}_1)$};
    \node[box, below=1.5cm of A] (B) {$\mathcal{A}(\mathcal{D}_2)$};
    \node[box, below=1.5cm of B] (C) {$\mathcal{A}(\mathcal{D}_3)$};
    
    \draw[arrow] (A) -- (B) node[smalltext, midway, right=1pt] {calls};
    \draw[arrow] (B) -- (C) node[smalltext, midway, right=1pt] {calls};
    
    % Properly positioned output labels with text on the left side
    \draw[curved arrow] (B) to node[smalltext, pos=0.5, left=2pt] {outputs} (A);
    \draw[curved arrow] (C) to node[smalltext, pos=0.5, left=2pt] {outputs} (B);
  \end{scope}
  % Right Figure - Binary Tree Recursion
  \begin{scope}[local bounding box=right, xshift=8cm]
    \node[box] (Root) {$\mathcal{A}(\mathcal{D})$};
    
    \node[box, below left=1.5cm and 0.8cm of Root] (L1) {$\mathcal{A}(\mathcal{D}_1)$};
    \node[box, below right=1.5cm and 0.8cm of Root] (R1) {$\mathcal{A}(\mathcal{D}_2)$};
    
    \node[box, below left=1.5cm and 0.8cm of L1] (L2) {$\mathcal{A}(\mathcal{D}_{11})$};
    \node[box, below right=1.5cm and 0.8cm of L1] (L3) {$\mathcal{A}(\mathcal{D}_{12})$};
    
    \node[box, below left=1.5cm and 0.8cm of R1] (R2) {$\mathcal{A}(\mathcal{D}_{21})$};
    \node[box, below right=1.5cm and 0.8cm of R1] (R3) {$\mathcal{A}(\mathcal{D}_{22})$};
    
    % Downward arrows from parent to child (blue) with "calls" labels
    \draw[arrow] (Root) -- (L1) node[smalltext, midway, above left=-1pt] {calls};
    \draw[arrow] (Root) -- (R1) node[smalltext, midway, above right=-1pt] {calls};
    \draw[arrow] (L1) -- (L2) node[smalltext, midway, above left=-1pt] {calls};
    \draw[arrow] (L1) -- (L3) node[smalltext, midway, above right=-1pt] {calls};
    \draw[arrow] (R1) -- (R2) node[smalltext, midway, above left=-1pt] {calls};
    \draw[arrow] (R1) -- (R3) node[smalltext, midway, above right=-1pt] {calls};
    
    % Properly placed output labels for the binary tree
    \draw[curved arrow] (L1) to[bend right=30] (Root);
    \draw[curved arrow] (R1) to[bend left=30] node[smalltext, auto] {outputs} (Root);
    \draw[curved arrow] (L2) to[bend right=30] (L1);
    \draw[curved arrow] (L3) to[bend left=30] node[smalltext, auto] {outputs} (L1);
    \draw[curved arrow] (R2) to[bend right=30] (R1);
    \draw[curved arrow] (R3) to[bend left=30] node[smalltext, auto] {outputs} (R1);
  \end{scope}
  \end{tikzpicture}
  \caption{Linear recursion on the left. Binary tree recursion on the right. } 
  \label{fig:recursion_fig}
\end{figure}

This is a pretty loose definition of data structure and algorithms, but the point I want to make is that data structures are a fundamental object of study, and to interact with these data structures, we can use algorithms (hence I put algorithms as taking a data structure as an input). The whole purpose of having different data structures is that we can construct efficient algorithms that can interact with them in different ways, e.g. getter/setter algorithms. Therefore, this is sort of like the chicken and egg situation. Algorithms act on data structures, but data structures are inspired by algorithms.  

Clearly, the runtime is extremely important when comparing the efficiency of different algorithms. To formalize this comparison, we need the power of mathematical analysis to analyze the asymptotic behavior between two functions.  

\begin{definition}[Complexity]
  Given two positive functions $f, g$, 
  \begin{enumerate}
    \item $f = O(g)$ if $f/g$ is bounded.\footnote{Note that it is more accurate to write $f \in O(g)$, since we consider $O(g)$ a class of functions for which the property holds.} 
    \item $f = \Omega(g)$ if $g/f$ is bounded, i.e. $g = O(f)$. 
    \item $f = \Theta (g)$ if $f = O(g)$ and $g = O(f)$. 
  \end{enumerate}
  Recalling analysis, if we want to figure out the complexity of two positive functions $f, g$,\footnote{These will be positive since the runtime must be positive.} we can simply take the limit. 
  \begin{equation}
    \lim_{x \rightarrow +\infty} \frac{f(x)}{g(x)} = \begin{cases} 
      0 & \implies f = O(g) \\
      0 < x < +\infty & \implies f = \Theta(g) \\
      +\infty & \implies f = \Omega(g)
    \end{cases}
  \end{equation} 
\end{definition}

Most of the time, we will have to use L'Hopital's rule to derive these actual limits, but the general trend is $\log n$ is small, $\mathrm{poly}(n)$ grows faster, $\exp(n)$ grows even faster, $n!$ even faster, and $n^n$ even faster. Some nice properties follow immediately. 

\begin{theorem}[Properties]
  Some basic properties, which shows very similar properties to a vector space. 
  \begin{enumerate}
    \item Transitivity. 
    \begin{align}
      f = O(g), g = O(h) & \implies f = O(h) \\
      f = \Omega(g), g = \Omega(h) & \implies f = \Omega(h) \\
      f = \Theta(g), g = \Theta(h) & \implies f = \Theta(h) \\
    \end{align}

    \item Linearity.  
      \begin{align}
        f = O(h), g = O(h) & \implies f + g = O(h) \\
        f = \Omega(h), g = \Omega(h) & \implies f + g = \Omega(h) \\
        f = \Theta(h), g = \Theta(h) & \implies f + g = \Theta(h) \\
      \end{align}
  \end{enumerate}
\end{theorem}

\begin{example}[Comparing Runtimes]
  Compare the following functions. 
  \begin{enumerate}
    \item $f(n) = \log_{10} (n), g(n) = \log_2 (n)$. Since they are different bases, we can write $f(n) = \log (n) / \log (10)$ and $g(n) = \log(n) / \log(2)$. They differ by a constant factor, so $f = \Theta(g)$. 

    \item $f(n) = (\log n)^{20}, g(n) = n$. We have 
    \begin{equation}
      \lim_{n \rightarrow \infty} \frac{(\log n)^20}{n} = \lim_{n \rightarrow \infty} \frac{20 \cdot (\log n)^{19} \cdot \frac{1}{n}}{1} = \ldots = \lim_{n \rightarrow \infty} \frac{20!}{n} = 0 \implies f = O(g)
    \end{equation}

    \item $f(n) = n^{100}, g(n) = 1.01^n$. We have 
      \begin{equation}
        \lim_{n \rightarrow \infty} \frac{n^{100}}{1.01^n} = \lim_{n \rightarrow \infty} \frac{100 n^{99}}{1.01^n \cdot \log (1.01)} =  \ldots = \lim_{n \rightarrow \infty} \frac{100!}{1.01^n \cdot (\log 1.01)^100} = 0 \implies f = O(g)
      \end{equation}
  \end{enumerate}
\end{example}

Let's do a slightly more nontrivial example. 

\begin{example}[Complexity of a Recursive Function]
  Given the following algorithm, what is the runtime? 
  \begin{lstlisting}
    for i in range(1, n+1): 
      j = 1 
      while j <= i: 
        j = 2 * j
  \end{lstlisting}
  Now we can see that for each $i$, we will double up to $\log_2 (i)$ times. Therefore summing this all over $i$ is 
  \begin{equation}
    \sum_{i = 1}^n \log_2 (i) = \log_2 (n!) \leq \log_2 (n^n) = n \log_2 (n)
  \end{equation}
  and so we can see that the runtime is $O(n \log n)$. Other ways to do this is to just replace the summation with an integral.\footnote{Need more justification on why this is the case. Mentioned in lecture.} 
  \begin{equation}
    \int_1^n \log_2 (x) \,dx = x \log(x) - x \big|_1^n = n \log(n) - n + 1 = O(n \log n)
  \end{equation}
\end{example}


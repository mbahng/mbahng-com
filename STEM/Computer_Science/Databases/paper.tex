\documentclass{article}

% packages
  % basic stuff for rendering math
  \usepackage[letterpaper, top=1in, bottom=1in, left=1in, right=1in]{geometry}
  \usepackage[utf8]{inputenc}
  \usepackage[english]{babel}
  \usepackage{amsmath} 
  \usepackage{amssymb}
  % \usepackage{amsthm}

  % extra math symbols and utilities
  \usepackage{mathtools}        % for extra stuff like \coloneqq
  \usepackage{mathrsfs}         % for extra stuff like \mathsrc{}
  \usepackage{centernot}        % for the centernot arrow 
  \usepackage{bm}               % for better boldsymbol/mathbf 
  \usepackage{enumitem}         % better control over enumerate, itemize
  \usepackage{hyperref}         % for hypertext linking
  \usepackage{fancyvrb}          % for better verbatim environments
  \usepackage{newverbs}         % for texttt{}
  \usepackage{xcolor}           % for colored text 
  \usepackage{listings}         % to include code
  \usepackage{lstautogobble}    % helper package for code
  \usepackage{parcolumns}       % for side by side columns for two column code
  \usepackage{array} 
  \usepackage{algorithm}
  \usepackage{algpseudocode}

  % page layout
  \usepackage{fancyhdr}         % for headers and footers 
  \usepackage{lastpage}         % to include last page number in footer 
  \usepackage{parskip}          % for no indentation and space between paragraphs    
  \usepackage[T1]{fontenc}      % to include \textbackslash
  \usepackage{footnote}
  \usepackage{etoolbox}
  \usepackage[table]{xcolor}

  % for custom environments
  \usepackage{tcolorbox}        % for better colored boxes in custom environments
  \tcbuselibrary{breakable}     % to allow tcolorboxes to break across pages

  % figures
  \usepackage{pgfplots}
  \pgfplotsset{compat=1.18}
  \usepackage{float}            % for [H] figure placement
  \usepackage{tikz}
  \usepackage{tikz-cd}
  \usepackage{circuitikz}
  \usetikzlibrary{shapes,arrows,positioning}
  \usetikzlibrary{arrows}
  \usetikzlibrary{positioning}
  \usetikzlibrary{calc}
  \usepackage{graphicx}
  \usepackage{caption} 
  \usepackage{subcaption}
  \captionsetup{font=small}

  % for tabular stuff 
  \usepackage{dcolumn}

  \usepackage[nottoc]{tocbibind}
  \pdfsuppresswarningpagegroup=1
  \hfuzz=5.002pt                % ignore overfull hbox badness warnings below this limit

% New and replaced operators
  \DeclareMathOperator{\Tr}{Tr}
  \DeclareMathOperator{\Sym}{Sym}
  \DeclareMathOperator{\Span}{span}
  \DeclareMathOperator{\std}{std}
  \DeclareMathOperator{\Cov}{Cov}
  \DeclareMathOperator{\Var}{Var}
  \DeclareMathOperator{\Corr}{Corr}
  \DeclareMathOperator{\pos}{pos}
  \DeclareMathOperator*{\argmin}{\arg\!\min}
  \DeclareMathOperator*{\argmax}{\arg\!\max}
  \newcommand{\ket}[1]{\ensuremath{\left|#1\right\rangle}}
  \newcommand{\bra}[1]{\ensuremath{\left\langle#1\right|}}
  \newcommand{\braket}[2]{\langle #1 | #2 \rangle}
  \newcommand{\qed}{\hfill$\blacksquare$}     % I like QED squares to be black
  \def\ojoin{\setbox0=\hbox{$\bowtie$}%
    \rule[-.02ex]{.25em}{.4pt}\llap{\rule[\ht0]{.25em}{.4pt}}}
  \def\lojoin{\mathbin{\ojoin\mkern-5.8mu\bowtie}}
  \def\rojoin{\mathbin{\bowtie\mkern-5.8mu\ojoin}}
  \def\fojoin{\mathbin{\ojoin\mkern-5.8mu\bowtie\mkern-5.8mu\ojoin}}
% Custom Environments
  \newtcolorbox[auto counter, number within=section]{question}[1][]
  {
    colframe = orange!25,
    colback  = orange!10,
    coltitle = orange!20!black,  
    breakable, 
    title = \textbf{Question \thetcbcounter ~(#1)}
  }

  \newtcolorbox[auto counter, number within=section]{exercise}[1][]
  {
    colframe = teal!25,
    colback  = teal!10,
    coltitle = teal!20!black,  
    breakable, 
    title = \textbf{Exercise \thetcbcounter ~(#1)}
  }
  \newtcolorbox[auto counter, number within=section]{solution}[1][]
  {
    colframe = violet!25,
    colback  = violet!10,
    coltitle = violet!20!black,  
    breakable, 
    title = \textbf{Solution \thetcbcounter}
  }
  \newtcolorbox[auto counter, number within=section]{lemma}[1][]
  {
    colframe = red!25,
    colback  = red!10,
    coltitle = red!20!black,  
    breakable, 
    title = \textbf{Lemma \thetcbcounter ~(#1)}
  }
  \newtcolorbox[auto counter, number within=section]{theorem}[1][]
  {
    colframe = red!25,
    colback  = red!10,
    coltitle = red!20!black,  
    breakable, 
    title = \textbf{Theorem \thetcbcounter ~(#1)}
  } 
  \newtcolorbox[auto counter, number within=section]{proposition}[1][]
  {
    colframe = red!25,
    colback  = red!10,
    coltitle = red!20!black,  
    breakable, 
    title = \textbf{Proposition \thetcbcounter ~(#1)}
  } 
  \newtcolorbox[auto counter, number within=section]{corollary}[1][]
  {
    colframe = red!25,
    colback  = red!10,
    coltitle = red!20!black,  
    breakable, 
    title = \textbf{Corollary \thetcbcounter ~(#1)}
  } 
  \newtcolorbox[auto counter, number within=section]{proof}[1][]
  {
    colframe = orange!25,
    colback  = orange!10,
    coltitle = orange!20!black,  
    breakable, 
    title = \textbf{Proof. }
  } 
  \newtcolorbox[auto counter, number within=section]{definition}[1][]
  {
    colframe = yellow!25,
    colback  = yellow!10,
    coltitle = yellow!20!black,  
    breakable, 
    title = \textbf{Definition \thetcbcounter ~(#1)}
  } 
  \newtcolorbox[auto counter, number within=section]{example}[1][]
  {
    colframe = blue!25,
    colback  = blue!10,
    coltitle = blue!20!black,  
    breakable, 
    title = \textbf{Example \thetcbcounter ~(#1)}
  } 
  \newtcolorbox[auto counter, number within=section]{algo}[1][]
  {
    colframe = green!25,
    colback  = green!10,
    coltitle = green!20!black,  
    breakable, 
    title = \textbf{Algorithm \thetcbcounter ~(#1)}
  } 

  \BeforeBeginEnvironment{example}{\savenotes}
  \AfterEndEnvironment{example}{\spewnotes}
  \BeforeBeginEnvironment{lemma}{\savenotes}
  \AfterEndEnvironment{lemma}{\spewnotes}
  \BeforeBeginEnvironment{theorem}{\savenotes}
  \AfterEndEnvironment{theorem}{\spewnotes}
  \BeforeBeginEnvironment{corollary}{\savenotes}
  \AfterEndEnvironment{corollary}{\spewnotes}
  \BeforeBeginEnvironment{proposition}{\savenotes}
  \AfterEndEnvironment{proposition}{\spewnotes}
  \BeforeBeginEnvironment{definition}{\savenotes}
  \AfterEndEnvironment{definition}{\spewnotes}
  \BeforeBeginEnvironment{exercise}{\savenotes}
  \AfterEndEnvironment{exercise}{\spewnotes}
  \BeforeBeginEnvironment{proof}{\savenotes}
  \AfterEndEnvironment{proof}{\spewnotes}
  \BeforeBeginEnvironment{solution}{\savenotes}
  \AfterEndEnvironment{solution}{\spewnotes}
  \BeforeBeginEnvironment{question}{\savenotes}
  \AfterEndEnvironment{question}{\spewnotes}
  \BeforeBeginEnvironment{code}{\savenotes}
  \AfterEndEnvironment{code}{\spewnotes}

  \definecolor{dkgreen}{rgb}{0,0.6,0}
  \definecolor{gray}{rgb}{0.5,0.5,0.5}
  \definecolor{mauve}{rgb}{0.58,0,0.82}
  \definecolor{lightgray}{gray}{0.93}

  % default options for listings (for code)
  \lstset{
    autogobble,
    frame=ltbr,
    language=SQL,                           % the language of the code
    aboveskip=3mm,
    belowskip=3mm,
    showstringspaces=false,
    columns=fullflexible,
    keepspaces=true,
    basicstyle={\small\ttfamily},
    numbers=left,
    firstnumber=1,                        % start line number at 1
    numberstyle=\tiny\color{gray},
    keywordstyle=\color{blue},
    commentstyle=\color{dkgreen},
    stringstyle=\color{mauve},
    backgroundcolor=\color{lightgray}, 
    breaklines=true,                      % break lines
    breakatwhitespace=true,
    tabsize=3, 
    xleftmargin=2em, 
    framexleftmargin=1.5em, 
    stepnumber=1
  }

% Page style
  \pagestyle{fancy}
  \fancyhead[L]{Databases}
  \fancyhead[C]{Muchang Bahng}
  \fancyhead[R]{Fall 2024} 
  \fancyfoot[C]{\thepage / \pageref{LastPage}}
  \renewcommand{\footrulewidth}{0.4pt}          % the footer line should be 0.4pt wide
  \renewcommand{\thispagestyle}[1]{}  % needed to include headers in title page

\begin{document}

\title{Databases}
\author{Muchang Bahng}
\date{Fall 2024}

\maketitle
\tableofcontents
\pagebreak 

  This is a course on database languages (SQL, XML, JSON) and database management systems (Postgres, MongoDB). 

  \begin{definition}[Data Model]
    A \textbf{data model} is a notation for describing data or information, consisting of 3 parts. 
    \begin{enumerate}
      \item \textit{Structure of the data}. The physical structure (e.g. arrays are contiguous bytes of memory or hashmaps use hashing). This is higher level than simple data structures. 
      \item \textit{Operations on the data}. Usually anything that can be programmed, such as \textbf{querying} (operations that retrieve information), \textbf{modifying} (changing the database), or \textbf{adding/deleting}. 
      \item \textit{Constraints on the data}. Describing what the limitations on the data can be. 
    \end{enumerate}
  \end{definition}

  There are two general types: relational databases, which are like tables, and semi-structured data models, which follow more of a tree or graph structure (e.g. JSON, XML). We'll cover in the following order: 
  \begin{enumerate}
    \item The theory of relational algebra. 
    \item Practical applications with SQL. 
    \item Theory and practice of XML.  
    \item Theory and practice of JSON. 
  \end{enumerate}

\section{Theory of Relations}

    The most intuitive way to store data is with a \textit{table}, which is called a relational data model, which is the norm since the 1990s. We will first talk about the structure, then the operations, and finally some constraints. 

  \subsection{Structure}

    \begin{definition}[Relation]
      A \textbf{relation} is a set or multiset $R$ with the following data structure. 
      \begin{enumerate}
        \item Each element $r \in R$ is called a \textbf{tuple}/\textbf{row}, with the form $r = (a_1 \in T_1, \ldots, a_n \in T_n)$. Despite its name and our use of indices, the tuple itself is \textit{not ordered}. 
        \item Each $T_i$ is a primitive\footnote{loosely defined here} type (e.g. int float, string, but not a list or set).\footnote{Note that this is also a form of constraint on our data, but the types are usually defined under structure.}
        \item Each $T_i$ is given an alphanumeric name $A_i$ as a variable, called the \textbf{attribute}. We denote $\mathbf{A}$ as the set of the attribute of relation $R$. 
        \item The \textbf{schema} of $R$ tells us a summary of its structure: $R(A_1 \; T_1, A_2 \; T_2, \ldots, A_n \; T_n)$. 
        \item Given a time parameter $t$, $R_t$ represents the \textbf{instance} of the relation $R$ at time $t$. 
      \end{enumerate}
      Given these names, we can visualize relations as tables, tuples as rows, and attributes as columns. 
    \end{definition} 

    We introduce this in this abstract way because there are many implementation differences between DBMSs. For example, the set of available primitive types may differ, or we may be able to define attribute names with special characters. Even in \textit{temporal databases}, we may also keep track of the history of its instances rather than just the current instance. 

    \begin{example}[Schemas]
      Here are a few schemas, which has the name of the relation followed by the attributes and its types. 
      \begin{lstlisting}
        Beer (name string, brewer string)
        Serves (bar string, price float)
      \end{lstlisting}
      Note that this is analogous to a function declaration in C++. 
    \end{example}  

    \begin{definition}[Compatiblity]
      Two relations $R$ and $S$ are \textbf{compatible}, denoted $R \simeq S$, if they have the same schema (same set of attribute types and names). 
    \end{definition}

    While we have not talked about any types yet, there is a common one that we should mention now. 

    \begin{definition}[Null]
      A \textbf{null} value, denoted with $\omega$ indicates an unknown but not empty value. The specific behavior of this value will be covered when we get into SQL implementations. 
    \end{definition}

  \subsection{Operations and Relational Algebra}

      We have just defined sets, and the natural thing to do is to construct functions on sets, i.e. what operations are legal. We introduce this with \textit{relational algebra}, which gives a powerful way to construct new relations from given relations. Simply put, an algebra is an algebraic structure with a set of operands (elements) and operators.\footnote{Not the technical definition.}

      \begin{definition}[Relational Algebra] 
        A \textbf{relational algebra} consists of a set of operations $O$ grouped into 4 broad categories. 
        \begin{enumerate}
          \item \textit{Set Operations}. Union, intersection, and difference. 
          \item \textit{Removing}. Selection removes tuples and projection removes attributes. 
          \item \textit{Combining}. Cartesian products, join operations. 
          \item \textit{Renaming}. Doesn't affect the tuples, but changes the name of the attributes or the relation itself. 
        \end{enumerate}
      \end{definition}

      Let's take a look at each of these operations more carefully, using the following relation. 

      \begin{figure}[H]
        \centering
        \begin{tabular}{|l|l|r|}
        \hline
        \rowcolor[HTML]{E26B0A} 
        \textcolor{white}{\textbf{bar}} & \textcolor{white}{\textbf{beer}} & \textcolor{white}{\textbf{price}} \\ \hline
        \rowcolor[HTML]{FBCEB1}
        The Edge & Budweiser & 2.50 \\ \hline
        \rowcolor[HTML]{FBCEB1}
        The Edge & Corona & 3.00 \\ \hline
        \rowcolor[HTML]{FBCEB1}
        Satisfaction & Budweiser & 2.25 \\ \hline
        \end{tabular}
        \caption{The example relation, which we will denote \texttt{serves}, which we will use to demonstrate the following operations.} 
        \label{fig:serves}
      \end{figure} 

    \subsubsection{Set Operations}

      \begin{definition}[Union]
        Given $R \simeq S$, $R \cup S$ is the set/multiset of tuples in either $R$ or $S$. 
      \end{definition}

      \begin{definition}[Intersection]
        Given $R \simeq S$, $R \cap S$ is the set/multiset of tuples in both $R$ or $S$. 
      \end{definition}

      \begin{definition}[Difference]
        Given $R \simeq S$, $R - S$ is the set of tuples in $R$ but not $S$, or the multiset that keeps track of counts in $R$ and $S$. 
      \end{definition}

      Note that this set is not minimal. 

      \begin{lemma} 
        Given $R \simeq S$, 
        \begin{align}
          R \cap S  & = R - (R - S) \\ 
                    & = S - (S - R) \\ 
                    & = R \bowtie S
        \end{align}
      \end{lemma}
      \begin{proof}
        The natural join will check for all attributes in each schema, but sine we assumed that they had the same schema, it must check for equality over all attributes.
      \end{proof}

      \begin{example}[Symmetric Difference of Relations]
        Let $R(A,B,C) = \{(1,2,3), (4,2,3), (4,5,6), (2,5,3), (1,2,6)\}$ and\\
        $S(A,B,C) = \{(2,5,3), (2,5,4), (4,5,6), (1,2,3)\}$\\
        Compute $(R - S) \cup (S - R)$ (symmetric difference). Identify one tuple in the result.

        \begin{enumerate}
          \item $(4,5,6)$ This tuple appears in both R and S, so it's not in the symmetric difference.
          
          \item $(2,5,3)$ This tuple appears in both R and S, so it's not in the symmetric difference.
          
          \item $\mathbf{(1,2,6)}$ This tuple appears only in R, so it's in R - S and thus in the symmetric difference.
          
          \item $(1,2,3)$ This tuple appears in both R and S, so it's not in the symmetric difference.
        \end{enumerate}
      \end{example}

    \subsubsection{Projection and Selection}

      \begin{definition}[Selection]
        The \textbf{selection} operator $\sigma_p$ filters the tuples of a relation $R$ by some condition $p$. 
        \begin{equation}
          \sigma_p (R)
        \end{equation}
        It must be the case that $p$ is deducible by looking only at that row, but it may not be (e.g. the condition where the count of an attribute in the row passes a threshold). 
      \end{definition}

      \begin{definition}[Projection]
        Given that $L \subset \mathbf{A}$ is a a subset of $R$'s attributes, the \textbf{projection} operator $\pi_L$ filters the attributes of a relation $R$. 
        \begin{equation}
          \pi_L (R)
        \end{equation}
      \end{definition}

      \begin{example}[Projection Operation]
        Given $R(A,B,C) = \{(1,2,3), (4,2,3), (4,5,6), (2,5,3), (1,2,6)\}$\\
        Compute $\pi_{C,B}(R)$ and identify one tuple.

        \begin{enumerate}
          \item $(4,2)$ 
          \item $\mathbf{(3,2)}$ This is correct. 
          \item $(4,2,3)$ Projection onto C,B should only have two attributes, not three.
          \item $(2,3)$ This reverses the specified projection order of C,B.
        \end{enumerate}
      \end{example}

    \subsubsection{Product and Join}

      \begin{definition}[Cartesian Product]
        The \textbf{cartesian product} of two not-necessarily compatible relations $R$ and $S$ is the relation 
        \begin{equation}
          R \times S = \{r \in S, s \in S\} 
        \end{equation}
        which has a length of $|R| \times |S|$. It is commutative (again, tuples are not ordered despite its name), and if $S$ and $R$ have the same attribute name $n$, then we usually prefix it by the relation to distinguish it: $S.n, R.n$. 
      \end{definition}

      \begin{definition}[Theta-Join]
        The \textbf{theta-join} with \textbf{join condition/predicate} $p$ gives 
        \begin{equation}
          R \bowtie_p S = \sigma_p (R \times S)
        \end{equation}
        If $p$ consists of only equality conditions, then it is called an \textbf{equi-join}. 
      \end{definition}

      \begin{example}[Conditional Join Query]
        Given $R(A,B) = \{(1,2), (3,4), (5,6)\}$ and\\
        $S(B,C,D) = \{(2,4,6), (4,6,8), (4,7,9)\}$\\
        Compute the join with conditions $R.A < S.C$ AND $R.B < S.D$.

        \begin{lstlisting}[basicstyle=\ttfamily]
          SELECT A, R.B, S.B, C, D
          FROM R, S
          WHERE R.A < S.C AND R.B < S.D
        \end{lstlisting}

        \begin{enumerate}
          \item $(5,6,2,4,6)$ This doesn't satisfy R.A < S.C as 5 > 4.
          
          \item $(1,2,4,4,6)$ The values don't properly satisfy both conditions.
          
          \item $(5,6,4,6,9)$ Doesn't satisfy conditions as 5 < 6.
          
          \item $\mathbf{(1,2,4,7,9)}$ Valid result as 1 < 7 and 2 < 9, satisfying both conditions.
        \end{enumerate}
      \end{example}

      \begin{definition}[Natural/Inner Join]
        In a $\theta$-join, if $p$ is not specified ($R \bowtie S$), then this is called a \textbf{natural join}. The $p$ is automatically implied to be $R.A = S.A$ for all $A \in R.\mathbf{A} \cap S.\mathbf{A}$, and if $R.\mathbf{A} \cap S.\mathbf{A} = \emptyset$, then this is reduced to the cartesian product. 
      \end{definition}

      \begin{example}[Natural Join Operation]
        Given $R(A,B) = \{(1,2), (3,4), (5,6)\}$ and\\
        $S(B,C,D) = \{(2,4,6), (4,6,8), (4,7,9)\}$\\
        Find $R \bowtie S$ and identify one resulting tuple.

        \begin{enumerate}
          \item $(1,2,6,8)$ The values don't match on the joining attribute B.
          
          \item $(5,6,7,8)$ Not a valid join result as these values don't align on B.
          
          \item $(1,2,4,8)$ The values don't match properly in the join condition.
          
          \item $\mathbf{(3,4,6,8)}$ Valid join result as B=4 matches between R and S, and remaining values align correctly.
        \end{enumerate}
      \end{example}

      \begin{lemma}[Deletion of Duplicate Attribute in Natural Join]
        There is a difference between a natural join and a theta-join with its explicitly written join predicate counterpart. Say that $R$ and $S$ both have attribute $A$. 
        \begin{enumerate}
          \item In theta-join, $R \bowtie_p S$ will contain $R.A$ and $S.A$. 
          \item In natural join, $R \bowtie S$ will contain $A$ only. 
        \end{enumerate}
      \end{lemma}

      \begin{example}[Simple Filter]
        Find all the addresses of the bars that Ben goes to. 
        \begin{table}[H]
          \centering
          \begin{tabular}{|>{\columncolor[HTML]{92AFDC}}l|>{\columncolor[HTML]{92AFDC}}l|}
          \hline
          \textbf{name} & \textbf{address} \\ \hline
          \rowcolor[HTML]{DCE6F2}
          The Edge & 108 Morris Street \\ \hline
          \rowcolor[HTML]{DCE6F2}
          Satisfaction & 905 W. Main Street \\ \hline
          \end{tabular}
          \caption{Bar Information}
          \label{tab:bar-info}
          \end{table}

          % Frequents table
          \begin{table}[H]
          \centering
          \begin{tabular}{|>{\columncolor[HTML]{4472C4}}l|>{\columncolor[HTML]{4472C4}}l|>{\columncolor[HTML]{4472C4}}c|}
          \hline
          \textbf{drinker} & \textbf{bar} & \textbf{times\_a\_week} \\ \hline
          \rowcolor[HTML]{DCE6F2}
          Ben & Satisfaction & 2 \\ \hline
          \rowcolor[HTML]{DCE6F2}
          Dan & The Edge & 1 \\ \hline
          \rowcolor[HTML]{DCE6F2}
          Dan & Satisfaction & 2 \\ \hline
          \end{tabular}
          \caption{Frequents Information}
          \label{tab:frequents-info}
        \end{table} 
        We do the following. 
        \begin{equation}
          \pi_{\mathrm{address}} \big( \mathrm{Bar} \bowtie_{\mathrm{name = bar}} \sigma_{\mathrm{drinker = Dan}} (\mathrm{Frequents} ) \big)
        \end{equation}
      \end{example}

      There is an extension of relational algebra that uses more join operators. We introduce them now. While natural/inner joins result in a relation of matching tuples in the two operands, and outer join contains these tuples and additionally some tuples formed by extending an unmatched tuple in one of the operands. 

      \begin{definition}[Left Outer Join]
        A \textbf{left outer join} $R \lojoin S$ results in the set of all tuples in $R$, plus those in $S$ that matches (equal in shared attributes) with $R$. If a $r$ doesn't match with any $S$, then we fill the $S$ attributes with $\omega$. 
        \begin{equation}
          R \lojoin S = (R \bowtie S) \cup ( (R - \pi_{R.\mathbf{A}} (R \bowtie S)) \times \{(\omega, \ldots, \omega)\})
        \end{equation}
      \end{definition} 

      \begin{definition}[Right Outer Join]
        A \textbf{right outer join} $R \rojoin S$ results in the set of all tuples in $S$, plus those in $r \in R$ that matches (equal in shared attributes) with some $s \in S$. If a $s \in S$ doesn't match with any $r \in R$, then we fill the $R$ attributes with $\omega$. 
        \begin{equation}
          R \rojoin S = (R \bowtie S) \cup ( \{(\omega, \ldots, \omega)\} \times (R - \pi_{R.\mathbf{A}} (R \bowtie S)))
        \end{equation}
      \end{definition}

      \begin{definition}[Full Outer Join]
        A \textbf{full outer join} $R \fojoin S$ results in the set of all tuples where their is a match in either left or right tables. If a $r \in R$ doesn't match with a $s$, or a $s$ doesn't match with a $r$, we fill those values in with null. 
        \begin{equation}
          R \fojoin S = (R \lojoin S) \cup (R \rojoin S)
        \end{equation}
      \end{definition}

      \begin{figure}[H]
        \centering 
        \includegraphics[scale=0.4]{img/outerjoin.png}
        \caption{Nice diagram from W3Schools.} 
        \label{fig:outerjoin}
      \end{figure} 

      \begin{example}[Outer Join]
        Consider the following relations. 

        \begin{center}
          \begin{minipage}{0.5\textwidth}
            \centering
            \begin{tabular}{|l|l|l|}
            \hline
            Name & EmpId & DeptName \\
            \hline
            Harry & 3415 & Finance \\
            Sally & 2241 & Sales \\
            George & 3401 & Finance \\
            Harriet & 2202 & Sales \\
            Tim & 1123 & Executive \\
            \hline
            \end{tabular}
          \end{minipage}%
          \begin{minipage}{0.5\textwidth}
            \centering
            \begin{tabular}{|l|l|}
            \hline
            DeptName & Manager \\
            \hline
            Sales & Harriet \\
            Production & Charles \\
            \hline
            \end{tabular}
          \end{minipage}
        \end{center}

        We demonstrate the join operations. 
        \begin{enumerate}
          \item Left outer join. 
          \begin{center}
            \begin{tabular}{|l|l|l|l|}
            \hline
            Name & EmpId & DeptName & Manager \\
            \hline
            Harry & 3415 & Finance & $\omega$ \\
            Sally & 2241 & Sales & Harriet \\
            George & 3401 & Finance & $\omega$ \\
            Harriet & 2202 & Sales & Harriet \\
            Tim & 1123 & Executive & $\omega$ \\
            \hline
            \end{tabular}
          \end{center}

          \item Right outer join. 

          \begin{center}
            \begin{tabular}{|l|l|l|l|}
            \hline
            Name & EmpId & DeptName & Manager \\
            \hline
            Sally & 2241 & Sales & Harriet \\
            Harriet & 2202 & Sales & Harriet \\
            $\omega$ & $\omega$ & Production & Charles \\
            \hline
            \end{tabular}
          \end{center}

          \item Full outer join. 
          \begin{center}
            \begin{tabular}{|l|l|l|l|}
            \hline
            Name & EmpId & DeptName & Manager \\
            \hline
            Harry & 3415 & Finance & $\omega$ \\
            Sally & 2241 & Sales & Harriet \\
            George & 3401 & Finance & $\omega$ \\
            Harriet & 2202 & Sales & Harriet \\
            Tim & 1123 & Executive & $\omega$ \\
            $\omega$ & $\omega$ & Production & Charles \\
            \hline
            \end{tabular}
          \end{center}
        \end{enumerate}
      \end{example}

    \subsubsection{Renaming}

      \begin{definition}[Renaming]
        Given a relation $R$, 
        \begin{enumerate}
          \item $\rho_S (R)$ means that you are chaning the relation name to $S$. 
          \item $\rho_{(A_1, \ldots, A_n)} (R)$ renames the attribute names to $(A_1, \ldots, A_n)$. 
          \item $\rho_{S(A_1, \ldots, A_n)} (R)$ renames the relation name to $S$ and the attribute names to $(A_1, \ldots, A_n)$. 
        \end{enumerate}
        It does not really adding any processing power. It is only used for convenience. 
      \end{definition}

    \subsubsection{Monotonicity}
      
      \begin{definition}[Monotone Operators]
        An operator $O(R)$ is monotone with respect to input $R$ if increasing the size (number of rows/tuples) of $R$ does not decrease the output relation $O$.  
        \begin{equation}
          R \subset R^\prime \implies O(R) \subset O(R^\prime)
        \end{equation}
      \end{definition}

      \begin{example}[Monotone Operators]
        Let's go through to see if each operator is monotone. 
        \begin{enumerate}
          \item Selection is monotone. 
          \item Projection is monotone. 
          \item Cross Product is monotone. 
          \item Join is monotone. 
          \item All joins are monotone w.r.t. all parameters. 
          \item Union is monotone. 
          \item Intersection is monotone. 
          \item Difference $R - S$ is monotone w.r.t. $R$ but not monotone w.r.t. $S$.
        \end{enumerate}
      \end{example}

      \begin{example}[Getting maximum of an attribute]
        Given a schema $R(a, b, c)$, how do we find the maximum of $a$? This is hard to come up in the first time since we are not allowed to compare across lines. However, we can do the following: 
        \begin{enumerate}
          \item Take the cross product of $R_1 (a_1, b_1, c_1) \times R_2 (a_2, b_2, c_2)$. 
          \item Select all tuples that are not maxes by selecting all rows where $a_1 < a_2$. The resulting rows will contain only those that are not maximums. 
        \end{enumerate}
        Therefore, we do the following 
        \begin{equation}
          \max_a (R) = [\pi_a (R) \times \pi_a (R)] - \sigma_{a_1 < a_2} [\pi_{a \mapsto a_1} (R) \times \pi_{a \mapsto a_2} (R)]
        \end{equation}
        The minimum can be solved analogously. 
      \end{example}

      Notice that the $\max_{att}$ operator is \textit{not} monotone, since the old answer is overwritten. 
      \begin{equation}
        \{\mathrm{old max}\} \not\subset \{\mathrm{new max}\}
      \end{equation}
      Generally, whenever we want to construct a non-monotone operator, we want to use the set difference since the composition of monotones is monotone. 

      You should determine when to project, before or after the difference. 

  \subsection{Constraints} 

    Like mathematical structures, relational databases would not be very useful if they didn't have any structure on them. Our final database property after structure and operations are \textit{constraints}, which can also be written in relational algebra. Through it may not seem obvious, all constraints can be written with what we call \textit{set constraints}. 

    \begin{definition}[Set Constraints]
      There are two ways in which we can use relational algebra to express constraints. If $R$ and $S$ are relations, then 
      \begin{enumerate}
        \item $R = \emptyset$ constrains $R$ to be empty.
        \item $R \subset S$ constrains $R$ to be a subset of $S$.\footnote{Note that this is technically unnecessary, since we can write $R - S = \emptyset$. We can also write $R = \emptyset \iff R \subset \emptyset$.}
      \end{enumerate}
    \end{definition} 

    There are three main constraints we will look at: 
    \begin{enumerate}
      \item domain constraints 
      \item key constraints 
      \item referential integrity
    \end{enumerate}

    \subsubsection{Domain Constraints}

      There is not much to say here. We have already looked at this constraint when defining the type $T$ of an attribute $A$. That is, an instance of an attribute $A$ must be of type $T$: $a \in T$. 

      \begin{definition}[Domain Constraints]
        We can also constrain the domain of a certain attribute $r$ of relation $R$. Let $C(r)$ be the constraint. Then, 
        \begin{equation}
          \sigma_{\text{not } C(r)} (R) = \emptyset
        \end{equation}
      \end{definition}

    \subsubsection{Key Constraints}

      \begin{definition}[Key]
        A set of attributes $\mathcal{K} \subset \mathbf{A}$ form a \textbf{key} for a relation 
        \begin{enumerate}
          \item if we do not allow two tuples in any relation instance to have the same values in \textit{all} attributes of the key (i.e. in general). 
          \item no proper subset of $\mathcal{K}$ can also be a key for \textit{any} relation instance, that is, $\mathcal{K}$ is \textit{minimal}.\footnote{By minimal we do not mean that the number of attributes in $K$ is minimal. It is minimal in the sense that no proper subset of $\mathcal{K}$ can be a key.}
        \end{enumerate}
        A relation may have multiple keys, but we typically pick one as the \textbf{primary key} and underline all its attributes in the schema, e.g. \texttt{Address(\underline{street}, city, state, \underline{zip})}. 
      \end{definition}

      While we can make a key with a set of attributes, many databases use artificial keys such as unique ID numbers for safety. 

      \begin{example}[Keys of User Relation]
        Given the schema \texttt{User(uid, name, age)}, 
        \begin{enumerate}
          \item \texttt{uid} is a key of \texttt{User} 
          \item \texttt{age} is not a key (not an identifier) even if the relation at the current moment all have different ages. 
          \item \{\texttt{uid, name}\} is not a key (not minimal)
        \end{enumerate}
      \end{example}

      \begin{definition}[Key Constraints]
        If we have the key $\mathcal{K} = (k_1, \ldots, k_m) \subset \mathbf{A}$ of a relation $R$, we can express this constraint as such. We rename $R$ to copies $R_1, R_2$, denote $\mathcal{K}^\prime = \mathbf{A} - \mathcal{K}$, and write 
        \begin{equation}
          \sigma_{R_1.\mathcal{K} = R_2.\mathcal{K} \text{ and } R_1.\mathcal{K}^\prime \neq R_2.\mathcal{K}^\prime} (R_1 \times R_2) = \emptyset
        \end{equation}
        That is, if we found a $r_1 \in R_1, r_2 \in R_2$ that matched in the key attributes, then it must be the case that $r_1 = r_2$. But if they are equal in the rest of the attributes, they will be projected out. 
      \end{definition}

    \subsubsection{Referential Integrity}

      \begin{definition}[Referential Integrity Constraints]
        One way that we can use this is through \textit{referential integrity} constraints, which asserts that a value appearing as an attribute $r$ in relation $R$ also should appear in a value of an attribute $s$ in relation $S$. That is, 
        \begin{equation}
          \pi_r (R) \subset \pi_s (S)
        \end{equation}
      \end{definition}

  \subsection{Functional Dependencies} 

      A generalization of keys is functional dependencies. Since it can be viewed as another form of key constraint, it is included here, but it is also a method of describing relationships within data, which will be useful in effective designing of databases.  

      \begin{definition}[Functional Dependency]
        Given a relation $R$ with attributes $\mathbf{A}$, let $\mathbf{a} = (a_1, \ldots, a_n), \mathbf{b} = (b_1, \ldots, b_m) \subset \mathbf{A}$. Then, the constraint 
        \begin{equation}
          \mathbf{a} \mapsto \mathbf{b}
        \end{equation}
        also called ``$\mathbf{a}$ \textbf{functionally determines} $\mathbf{b}$,'' means that if two tuples agree on $\mathbf{a}$, then they must agree on $\mathbf{b}$. We say that $R$ satisfies a FD $f: \mathbf{a} \mapsto \mathbf{b}$ or a set of FDs $F = \{f\}$ if this constraint is satisfied. 
      \end{definition}

      From this, we can see that the term ``functional'' comes from a literal function being defined on the input $\mathbf{a}$. 

      \begin{lemma}[FDs as Key Constraints]
        Note that the functional dependency $\mathbf{a} \mapsto \mathbf{b}$ also implies the key constraint 
        \begin{equation}
          \sigma_{R_1.\mathbf{a} = R_2.\mathbf{a} \text{ and } R_1.\mathbf{b} \neq R_2.\mathbf{b}^\prime} (R_1 \times R_2) = \emptyset
        \end{equation}
      \end{lemma}

      \begin{definition}[Superkey]
        A set of attributes $\mathbf{k}$ of a relation $R$ is called a \textbf{superkey} if 
        \begin{equation}
          \mathbf{k} \mapsto \mathbf{r} - \mathbf{k}
        \end{equation}
        If no $\mathbf{k}^\prime \subset \mathbf{k}$ functionally determines $\mathbf{r}$, then it is a key. 
      \end{definition}
      
      \begin{example}[Clarification of Minimal Meaning]
        Given a relation $R(A, B, C, D, E, F)$ with functional dependencies 
        \begin{equation}
          AEF \mapsto C, BF \mapsto C, EF \mapsto D, ACDE \mapsto F
        \end{equation}
        We can see that every attribute not on the right hand side ($C, D, F$) must be a key. If we do a bit of testing out, we can see that 
        \begin{enumerate}
          \item $ABEF$ is a key. 
          \item $ABCDE$ is also a key since even though it has 5 attributes, it is minimal in the sense that no proper subset functionally determines every other attribute.   
        \end{enumerate}
      \end{example}

    \subsubsection{Structure on Spaces of Functional Dependencies}

      To introduce additional structure, we will introduce two spaces. 
      \begin{enumerate}
        \item Given a relation $R$, let us consider the set of all FDs $F = F(R)$ on $R$. This is clearly a large set, which increases exponentially w.r.t. the number of attributes in $R$. 
        \item Let us denote the set of all relations $R$ satisfying $F$ as $R_F$, which is an infinite set. 
      \end{enumerate}

      \begin{theorem}[Armstrong Axioms]
        Let's prove a few properties of FDs, which have nice structure. 
        \begin{enumerate}
          \item \textit{Splitting and Combining}. The two sets of FDs are equal. 
            \begin{equation}
              \{\mathbf{a} \mapsto \mathbf{b}\} \iff \{ \mathbf{a} \mapsto b_i \mid i = 1, \ldots, m\}
            \end{equation}

          \item \textit{Trivial FDs}. Clearly elements of $\mathbf{a}$ uniquely determines its own attributes. 
            \begin{equation}
              \mathbf{a} \mapsto \mathbf{b} \implies \mathbf{a} \mapsto \mathbf{b} - \mathbf{a}
            \end{equation}
            or can also be written as 
            \begin{equation}
              \mathbf{b} \subset \mathbf{a} \implies \mathbf{a} \mapsto \mathbf{b}
            \end{equation}

          \item \textit{Augmentation}. 
            \begin{equation}
              \mathbf{a} \mapsto \mathbf{b} \implies \mathbf{a}, \mathbf{c} \mapsto \mathbf{b}, \mathbf{c}
            \end{equation}

          \item \textit{Transitivity}. If $\mathbf{a} \mapsto \mathbf{b}, \mathbf{b} \mapsto \mathbf{c}$, then 
            \begin{equation}
              \mathbf{a} \mapsto \mathbf{c}
            \end{equation}
        \end{enumerate}
      \end{theorem}
      \begin{proof}
        Trivial. 
      \end{proof}

      It is also possible to put a partial order on $F$. 

      \begin{definition}[Partial Order]
        Given two FDs $f$ and $g$, consider the set of all relations $R$ satisfying $f$ and $g$, denoted as $R_f$ and $R_g$. 
        \begin{enumerate}
          \item Then $f \implies g$ iff $R_f \subset R_g$. 
          \item $f \iff g$ iff $R_f = R_g$. 
        \end{enumerate}
      \end{definition}

      Moreover, we can use this structure on $F$ to induce structure on the set of attributes $\mathbf{r}$. 

      \begin{definition}[Closure of Attributes]
        The \textbf{closure} of $\mathbf{r}$ under a set of FDs $F$ is the set of attributes $\mathbf{b}$ s.t. 
        \begin{equation}
          R_F = R_\mathbf{b}
        \end{equation}
        We denote this as $\mathbf{b} = \mathbf{r}^+$. To actually compute the closure, we take a greedy approach by starting with $\mathbf{r}$ and incrementally adding attributes satisfying $F$ until we cannot add any more. 
      \end{definition}

      \begin{theorem}[Implication of Functional Dependencies]
        If we want to know where one FD $f: \mathbf{a} \mapsto \mathbf{b}$ follows from a set $F$ of functional dependencies, 
        \begin{enumerate}
          \item We compute the closure $\mathbf{a}^+$ w.r.t. $F$. 
          \item If $\mathbf{b} \subset \mathbf{a}^+$, then $f$ follows from $F$. 
        \end{enumerate}
        Alternatively, we can also use the \textit{Armstrong axioms} above to derive all implications. 
      \end{theorem}
    
    \subsubsection{Projections of Functional Dependencies} 

      If we have a relation $R$ with a set of FDs $F$, and we project $R^\prime = \pi_{\mathbf{r}^\prime} (R)$, then the set of FDs $F^\prime$ that hold for $R^\prime$ consists of 
      \begin{enumerate}
        \item The FDs that follow from $F$, and 
        \item involve only attributes of $R$. 
      \end{enumerate} 

      Calculating the FDs that hold is not trivial, and the calculations of the FDs for $R^\prime$ is exponential in the number of attributes of $R^\prime$. 

      \begin{theorem}[Calculating Projections of Functional Dependencies]
        Given a relation $R$ and its set of functional dependencies $F$, let $R^\prime = \pi(R)$ be a projection. 
        \begin{enumerate}
          \item Let $G$ be the set of FDs that we construct that hold for $R^\prime$. Initially it is empty. 
          \item For each set of attributes $\mathbf{r}$ that is a subset of the attributes of $R_1$, compute $\mathbf{r}^+$. This computation is performed w.r.t. the set of FDs in $S$, and may involve attributes in the schema of $R$ but not in $R^\prime$. Add to $G$ all nontrivial FDs $\mathbf{r} \rightarrow A$ such that $A$ is both in $\mathbf{r}^+$ and an attribute in $R^\prime$. 
        \end{enumerate}
        This gives our projection, but it may not be minimal. To make it minimal, repeat the two steps, independently. 
        \begin{enumerate}
          \item If there is an FD $g$ in $G$ that follows from the other FDs in $G$, remove $g$. 
          \item Let $g = \mathbf{a} \mapsto \mathbf{b} $ be an FD in $G$, with at least 2 attributes in $\mathbf{a}$, and let $\mathbf{a}^\prime$ be $\mathbf{a}$ with one of its attributes removed. If $g^\prime = \mathbf{a}^\prime \mapsto \mathbf{b}$ follows from the FDs in $G$ (including $g$), then replace $g$ with $g^\prime$. 
        \end{enumerate}
        Once neither step can be done, we have a minimal set. 
      \end{theorem}

      \begin{example}[Computing FDs for Projections]
        Suppose $R(A,B,C,D)$ has FD's $A \rightarrow B$, $B \rightarrow C$, and $C \rightarrow D$. Suppose also that we wish to project out the attribute $B$, leaving a relation $R_1(A,C,D)$. In principle, to find the FD's for $R_1$, we need to take the closure of all eight subsets of $\{A, C, D\}$, using the full set of FD's, including those involving $B$. However, there are some obvious simplifications we can make.

        \begin{itemize}
          \item Closing the empty set and the set of all attributes cannot yield a nontrivial FD.
          \item If we already know that the closure of some set $X$ is all attributes, then we cannot discover any new FD's by closing supersets of $X$.
        \end{itemize}

        Thus, we may start with the closures of the singleton sets, and then move on to the doubleton sets if necessary. For each closure of a set $X$, we add the FD $X \rightarrow E$ for each attribute $E$ that is in $X^+$ and in the schema of $R_1$, but not in $X$.

        First, $\{A\}^+ = \{A,B,C,D\}$. Thus, $A \rightarrow C$ and $A \rightarrow D$ hold in $R_1$. Note that $A \rightarrow B$ is true in $R$, but makes no sense in $R_1$, because $B$ is not an attribute of $R_1$.

        Next, we consider $\{C\}^+ = \{C,D\}$, from which we get the additional FD $C \rightarrow D$ for $R_1$. Since $\{D\}^+ = \{D\}$, we can add no more FD's, and are done with the singletons.

        Since $\{A\}^+$ includes all attributes of $R_1$, there is no point in considering any superset of $\{A\}$. The reason is that whatever FD we could discover, for instance $AC \rightarrow D$, follows from an FD with only $A$ on the left side: $A \rightarrow D$ in this case. Thus, the only doubleton whose closure we need to take is $\{C, D\}^+ = \{C,D\}$. This observation allows us to add nothing. We are done with the closures, and the FD's we have discovered are $A \rightarrow C$, $A \rightarrow D$, and $C \rightarrow D$.

        If we wish, we can observe that $A \rightarrow D$ follows from the other two by transitivity. Therefore a simpler, equivalent set of FD's for $R_1$ is $A \rightarrow C$ and $C \rightarrow D$. This set is, in fact, a minimal basis for the FD's of $R_1$. 
      \end{example}

\section{Design Theory for Relational Databases}
    
    Okay, we know all that is needed to define a database, but not how to work practically well with it yet. Given some set of data or some structure, we must find an optimal way to store it, within either one or multiple relations. How should we do this? The first step is to categorize the potential problems. 

  \subsection{Anomalies and Decomposition}
    
    \begin{definition}[Anomaly]
      Beginners often try to cram too much into a relation, resulting in \textbf{anomalies} of three forms. 
      \begin{enumerate}
        \item \textit{Redundancies}. Information repeated unnecessarily in several tuples. 
        \item \textit{Updates}. Updating information in one tuple can leave the same information unchanged in another (which violates referential integrity). 
        \item \textit{Deletion}. If a set of values becomes empty, we may lose other information as a side effect (another violation of referential integrity). 
      \end{enumerate}
    \end{definition}

    \begin{example}[Redundancies]
      Let's try to see where these anomalies came from. Consider a non-trivial FD $\mathbf{a} \mapsto \mathbf{b}$ where $\mathbf{a}$ is not a superkey. Since $\mathbf{a}$ is not a superkey, there are attributes, say $\mathbf{c}$ that are not functionally determined by $\mathbf{a}$. Therefore, there are multiple combinations of $\mathbf{a, b, c}$ which have the same $\mathbf{a, b}$ but not $\mathbf{c}$, leading to redundancy. 

      \begin{table}[H]
        \centering
        \begin{tabular}{|c|c|c|}
          \hline
          \textbf{a} & \textbf{b} & \textbf{c} \\
          \hline
          $x$ & $y$ & $z_1$ \\ 
          $\vdots$ & $\vdots$ & $\vdots$ \\ 
          $x$ & $y$ & $z_{100}$ \\
          \hline
        \end{tabular}
        \caption{Redundant information in \textbf{a} and \textbf{b} attributes. }
        \label{tab:redundant}
      \end{table}
      If $T_{\mathbf{a}}, T_{\mathbf{b}}$ are both 4-byte integers, we have just wasted 400 bytes of space. It seems that most of the redundancy comes from using $a$ and $b$ too many times. If we had two relations $R$ and $S$. 
    \end{example}

    To eliminate these anomalies, we want to \textbf{decompose} relations, which involve splitting $R$ into two new relations $R_1, R_2$. 

    \begin{definition}[Decomposition]
      Given relation with schema $R(\mathbf{A})$, we can decompose $R$ into two relations $R_1 (\mathbf{A}_1)$ and $R_2 (\mathbf{A}_2)$ such that 
      \begin{enumerate}
        \item $\mathbf{A} = \mathbf{A}_1 \cup \mathbf{A}_2$ (not necessarily disjoint)
        \item $R_1 = \pi_{\mathbf{A}_1} (R)$
        \item $R_2 = \pi_{\mathbf{A}_2} (R)$
      \end{enumerate}
      There are two types of decomposition: 
      \begin{enumerate}
        \item \textbf{lossy} decomposition of $R$ to $R_1, R_2$ means that joining $R_1, R_2$ does not give us $R$. 
        \item \textbf{lossless} decomposition indeed gives us back $R$. 
      \end{enumerate}
    \end{definition}

    \begin{theorem}[Decomposition]
      Any decomposition $R_1, R_2$ of $R$ satisfies 
      \begin{equation}
        R \subset R_1 \bowtie R_2
      \end{equation}
    \end{theorem} 
    \begin{proof}
      If $R_1.\mathbf{A} \cap R_2.\mathbf{A} = \emptyset$, then $R \subset R_1 \times R_2$ for sure. If there is some overlap, then given some $r \in R$, we will be able to find $r_1 \in R_1$ and $r_2 \in R_2$, and they will definitely join in their overlapping attribute, giving $R$. 
    \end{proof}

    Therefore, we should be worried about if our decomposition will create \textit{new} tuples since it will never delete relevant ones.

    \begin{example}[Lossy and Lossless Decomposition]
      Consider the relation 
      \begin{table}[H]
        \centering
        \begin{tabular}{|c|c|c|}
          \hline
          \textbf{X} & \textbf{Y} & \textbf{Z} \\
          \hline
          $a$ & $b$ & $c_1$ \\
          $a$ & $b$ & $c_2$ \\
          $a_1$ & $b$ & $c_2$ \\
          \hline
        \end{tabular}
        \label{tab:ex5}
      \end{table}
      Projecting to $(X, Y)$ and $(X, Z)$ gives us a lossless decomposition. 
      \begin{table}[H]
        \centering
        \begin{minipage}{.32\textwidth}
          \centering
          \begin{tabular}{|c|c|}
            \hline
            \textbf{X} & \textbf{Y} \\
            \hline
            $a$ & $b$ \\
            $a_1$ & $b$ \\
            \hline
          \end{tabular}
          \label{tab:ex6}
        \end{minipage}
        \begin{minipage}{.32\textwidth}
          \centering
          \begin{tabular}{|c|c|}
            \hline
            \textbf{X} & \textbf{Z} \\
            \hline
            $a$ & $c_1$ \\
            $a$ & $c_2$ \\
            $a_1$ & $c_2$ \\
            \hline
          \end{tabular}
          \label{tab:ex7}
        \end{minipage}
        \begin{minipage}{.32\textwidth}
          \centering
          \begin{tabular}{|c|c|c|}
            \hline
            \textbf{X} & \textbf{Y} & \textbf{Z}\\
            \hline
            $a$ & $b$ & $c_1$ \\
            $a$ & $b$ & $c_2$ \\
            $a_1$ & $b$ & $c_2$ \\
            \hline
          \end{tabular}
          \label{tab:ex71}
        \end{minipage}
      \end{table}
      While projecting to $(X, Y)$ and $(Y, Z)$ gives us a lossy one since we get $(a_1, b, c_1)$ when joining the decompositions, which is not in the original relation. 
      \begin{table}[H]
        \centering
        \begin{minipage}{.32\textwidth}
          \centering
          \begin{tabular}{|c|c|}
            \hline
            \textbf{X} & \textbf{Y} \\
            \hline
            $a$ & $b$ \\
            $a_1$ & $b$ \\
            \hline
          \end{tabular}
          \label{tab:ex8}
        \end{minipage}
        \begin{minipage}{.32\textwidth}
          \centering
          \begin{tabular}{|c|c|}
            \hline
            \textbf{Y} & \textbf{Z} \\
            \hline
            $b$ & $c_1$ \\
            $b$ & $c_2$ \\
            \hline
          \end{tabular}
          \label{tab:ex9}
        \end{minipage}
        \begin{minipage}{.32\textwidth}
          \centering
          \begin{tabular}{|c|c|c|}
            \hline
            \textbf{X} & \textbf{Y} & \textbf{Z}\\
            \hline
            $a$ & $b$ & $c_1$ \\
            $a$ & $b$ & $c_2$ \\
            $a_1$ & $b$ & $c_1$ \\
            $a_1$ & $b$ & $c_2$ \\
            \hline
          \end{tabular}
          \label{tab:ex10}
        \end{minipage}
      \end{table}
      Generally, the intuition behind trying to find a lossless decomposition is this: For a value of a certain attribute $X$ that is in both decompositions $R_1$ and $R_2$, we want only one instance of a value on one side. For example, 
      \begin{enumerate}
        \item in the lossless decomposition, notice how for overlapping attribute \textbf{X}, for each instance $a, a_1$, we had 1 of each on the left relation, and however many on the right. 
        \item in the lossy decomposition, notice how we have two $b$'s on the left and two $b$'s on the right for the lossy decomposition. Rather, we want something like 1 $b$ vs multiple $b$'s.
      \end{enumerate}
    \end{example}

  \subsubsection{Boyce-Codd Normal Form}

    This decomposition eliminated the redundancy anomaly, and for attributes \textbf{X} and \textbf{Z}, it eliminated the deletion and update anomalies. Let's formalize the conditions needed to decompose such a relation, and how we should actually decompose it. 

    \begin{definition}[BNCF]
      BCNF is defined in two equivalent ways: 
      \begin{enumerate}
        \item A relation $R$ is in \textbf{BNCF} iff whenever there is a nontrivial FD $\mathbf{a} \mapsto \mathbf{b}$, it is the case that $\mathbf{a}$ is a superkey for $R$. 
        \item Given a relation $R$ with a set of nontrivial functional dependencies $F = \{\mathbf{a} \mapsto \mathbf{b}\}$, it is in BCNF if for every $\mathbf{a}$, $\mathbf{a}^+$ is the set of all attributes of $R$. 
      \end{enumerate}
      Note that since there are multiple keys, $\mathbf{a}$ does not always have to include the same key. 
    \end{definition}

    \begin{example}[Non-BNCF Form]
      The table below is not in BCNF form since 
      \begin{equation}
        (\texttt{title}, \texttt{year}) \mapsto (\texttt{length}, \texttt{genre}, \texttt{studioName}) 
      \end{equation}
      Is a functional dependency where the LHS is not a superkey (the key is \texttt{title}, \texttt{year}). 
      \begin{table}[H]
        \centering
        \begin{tabular}{|l|c|c|l|l|l|}
        \hline
        \textbf{title} & \textbf{year} & \textbf{length} & \textbf{genre} & \textbf{studioName} & \textbf{starName} \\
        \hline
        Star Wars & 1977 & 124 & SciFi & Fox & Carrie Fisher \\
        Star Wars & 1977 & 124 & SciFi & Fox & Mark Hamill \\
        Star Wars & 1977 & 124 & SciFi & Fox & Harrison Ford \\
        Gone With the Wind & 1939 & 231 & drama & MGM & Vivien Leigh \\
        Wayne's World & 1992 & 95 & comedy & Paramount & Dana Carvey \\
        Wayne's World & 1992 & 95 & comedy & Paramount & Mike Meyers \\
        \hline
        \end{tabular}
        \caption{Movie Data}
        \label{tab:moviedata}
      \end{table} 
    \end{example}

    \begin{example}[BNCF Form]
      However, if we decompose this into the following tables, both satisfy BCNF. 

      \begin{table}[H]
        \centering
        \begin{tabular}{|l|c|c|l|l|}
        \hline
        \textbf{title} & \textbf{year} & \textbf{length} & \textbf{genre} & \textbf{studioName} \\
        \hline
        Star Wars & 1977 & 124 & sciFi & Fox \\
        Gone With the Wind & 1939 & 231 & drama & MGM \\
        Wayne's World & 1992 & 95 & comedy & Paramount \\
        \hline
        \end{tabular}
        \caption{Simplified Movie Data}
        \label{tab:simplemoviedata}
      \end{table}

      \begin{table}[H]
        \centering
        \begin{tabular}{|l|c|l|}
        \hline
        \textbf{title} & \textbf{year} & \textbf{starName} \\
        \hline
        Star Wars & 1977 & Carrie Fisher \\
        Star Wars & 1977 & Mark Hamill \\
        Star Wars & 1977 & Harrison Ford \\
        Gone With the Wind & 1939 & Vivien Leigh \\
        Wayne's World & 1992 & Dana Carvey \\
        Wayne's World & 1992 & Mike Meyers \\
        \hline
        \end{tabular}
        \caption{Movie Titles, Years, and Stars}
        \label{tab:moviestars}
      \end{table}
    \end{example}

    \begin{algo}[Constructing BCNF of a Relation]
      To actually construct this, we act on BCNF violations. Given that we have found a FD $\mathbf{a} \mapsto \mathbf{b}$ that doesn't satisfy BCNF (i.e. $\mathbf{a}$ is not a superkey) of relation $R$, we decompose it into the following $R_1$ and $R_2$. 
      \begin{enumerate}
        \item We want $\mathbf{a}$ to be a superkey for one of the subrelations, say $R_1$. Therefore, we have it satisfy $\mathbf{a} \mapsto \mathbf{a}^+$, which is satisfied by definition, and set 
          \begin{equation}
            R_1 = \pi_{\mathbf{a}, \mathbf{a}^+} (R)
          \end{equation}
        \item We don't want any loss in data, so we take the rest of the attributes not in the closure and define  
          \begin{equation}
            R_2 = \pi_{\mathbf{a}, \mathbf{r} - \mathbf{a}^+} (R)
          \end{equation}
      \end{enumerate}
      We keep doing this until every subrelation satisfies BCNF. This is guaranteed to terminate since we are decreasing the size of the relations until all attributes are superkeys.  
    \end{algo}

    \begin{theorem}[Lossless Guarantee for BCNF]
      If we decompose on a BCNF violation as stated above, then our decomposition is guaranteed to be a lossless join decomposition. 
    \end{theorem}
    \begin{proof}
      An outline of the proof means that given a BCNF violation $\mathbf{a} \mapsto \mathbf{b}$, we must show that anything we project always comes back in the join
      \begin{equation}
        R \subset \pi_{\mathbf{a}^+} (R) \bowtie \pi_{\mathbf{a}, \mathbf{r} - \mathbf{a}} (R)
      \end{equation}
      which is proved trivially, and anything that comes back in the join must be in the original relation
      \begin{equation}
        \pi_{\mathbf{a}^+} (R) \bowtie \pi_{\mathbf{a}, \mathbf{r} - \mathbf{a}} (R) \subset R
      \end{equation}
      We can use the fact that $\mathbf{a} \mapsto \mathbf{b}$. 
    \end{proof}

    The lossless guarantee makes BCNF very attractive, but it comes with a tradeoff. 

    \begin{theorem}[BCNF Does Not Preserve Functional Dependencies]
      BCNF may not enforce equivalent functional dependencies over its decomposed relations compared to its original relation. 
    \end{theorem}
    \begin{proof}
      We consider a counterexample. Given a schema $R(A, B, C)$ with FDs 
      \begin{equation}
        f = AB \mapsto C, \;\; g = C \mapsto B
      \end{equation} 
      When we decompose based on $C \mapsto B$ into $R_1 (C, B), R_2(C, A)$, the join is still lossless because we kept $C$ in both relations, and when we join the two on $C$, each $C$ value will match exactly one $B$ value (due to $C \rightarrow B$ in $R_2$). However, we cannot enforce $AB \rightarrow C$. 
    \end{proof}

    It is initially confusing since one may ask that if functional dependencies aren't preserved, then doesn't this mean lossy decomposition? Not exactly, since the lossless join property is independent of dependency preservation. The lossless join property ensures we can reconstruct the exact original relation through natural joins, without getting any spurious tuples. Dependency preservation simply means we can't enforce a FD when inserting/updating. In fact, this is analogous to some functional dependencies being removed when projecting a relation. 

  \subsubsection{Recovery and Chase Test} 

    Now let's talk about properties of general decompositions. 

    \begin{theorem}[Any 2-Attribute Relation Satisfies BNCF]
      Any 2-attribute relations is in BCNF. Let's label the attributes $a, b$ and go through the cases. 
      \begin{enumerate}
        \item There are no nontrivial FDs, meaning that $\{A, B\}$ is the only key. Then BCNF must hold since only a nontrivial FD can violate this condition. 
        \item $a \mapsto b$ holds but not $b \mapsto a$, meaning that $a$ is the only key. Thus there is no violation since $a$ is a superkey. 
        \item $b \mapsto a$ holds but not $a \mapsto b$. This is symmetric as before. 
        \item Both hold, meaning that both $a$ and $b$ are keys. Since any FD has at least one of $a, b$ on the left, this is satisfied.\footnote{Note that BCNF only requires \textit{some} key to be contained on the left side, not that all keys are. } 
      \end{enumerate}
    \end{theorem}

    From this theorem stating that any 2-attribute relation satisfies BCNF, we can just trivially decompose all relations into relations $R_1, \ldots, R_n$, and we are done, right? Not exactly, since to ensure lossless join, we must start with the original relation and \textit{split only on the BCNF violations}. This is what makes it lossless, and simply splitting arbitrarily will lead to a lossy decomposition. Furthermore, even if it was lossless, it will destroy most functional dependencies too. 

    This motivates the need for a general test to see if a decomposition of $R$ is lossless. That is, is it true that $\pi_{S_1}(R) \bowtie \pi_{S_2}(R) \bowtie \cdots \bowtie \pi_{S_k}(R) = R$? Three important things to remember are:

    \begin{itemize}
      \item The natural join is associative and commutative. It does not matter in what order we join the projections; we shall get the same relation as a result. In particular, the result is the set of tuples $t$ such that for all $i = 1, 2, \ldots, k$, $t$ projected onto the set of attributes $S_i$ is a tuple in $\pi_{S_i}(R)$.
      
      \item Any tuple $t$ in $R$ is surely in $\pi_{S_1}(R) \bowtie \pi_{S_2}(R) \bowtie \cdots \bowtie \pi_{S_k}(R)$. The reason is that the projection of $t$ onto $S_i$ is surely in $\pi_{S_i}(R)$ for each $i$, and therefore by our first point above, $t$ is in the result of the join.
      
      \item As a consequence, $\pi_{S_1}(R) \bowtie \pi_{S_2}(R) \bowtie \cdots \bowtie \pi_{S_k}(R) = R$ when the FD's in $F$ hold for $R$ if and only if every tuple in the join is also in $R$. That is, the membership test is all we need to verify that the decomposition has a lossless join.
    \end{itemize}

    \begin{theorem}[Chase Test for Lossless Join]
      The \textbf{chase test} for a lossless join is just an organized way to see whether a tuple $t$ in $\pi_{S_1}(R) \bowtie \pi_{S_2}(R) \bowtie \cdots \bowtie \pi_{S_k}(R)$ can be proved, using the FD's in $F$, also to be a tuple in $R$. If $t$ is in the join, then there must be tuples in $R$, say $t_1, t_2, \ldots, t_k$, such that $t$ is the join of the projections of each $t_i$ onto the set of attributes $S_i$, for $i = 1,2,\ldots,k$. We therefore know that $t_i$ agrees with $t$ on the attributes of $S_i$, but $t_i$ has unknown values in its components not in $S_i$.
    \end{theorem}

    We draw a picture of what we know, called a \emph{tableau}. Assuming $R$ has attributes $A,B,\ldots$ we use $a,b,\ldots$ for the components of $t$. For $t_i$, we use the same letter as $t$ in the components that are in $S_i$, but we subscript the letter with $i$ if the component is not in $S_i$. In that way, $t_i$ will agree with $t$ for the attributes of $S_i$, but have a unique value — one that can appear nowhere else in the tableau — for other attributes.

    \begin{example}
      Suppose we have relation $R(A,B,C,D)$, which we have decomposed into relations with sets of attributes $S_1 = \{A,D\}$, $S_2 = \{A,C\}$, and $S_3 = \{B,C,D\}$. Then the tableau for this decomposition is:

      \[
      \begin{array}{|c|c|c|c|}
      \hline
      A & B & C & D \\
      \hline
      a & b_1 & c_1 & d \\
      a & b_2 & c & d_2 \\
      a_3 & b & c & d \\
      \hline
      \end{array}
      \]

      The first row corresponds to set of attributes $A$ and $D$. Notice that the components for attributes $A$ and $D$ are the unsubscripted letters $a$ and $d$. However, for the other attributes, $b$ and $c$, we add the subscript 1 to indicate that they are arbitrary values. This choice makes sense, since the tuple $(a,b_1,c_1,d)$ represents a tuple of $R$ that contributes to $t = (a,b,c,d)$ by being projected onto $\{A,D\}$ and then joined with other tuples. Since the $B$- and $C$-components of this tuple are projected out, we know nothing yet about what values the tuple had for those attributes.

      Similarly, the second row has the unsubscripted letters in attributes $A$ and $C$, while the subscript 2 is used for the other attributes. The last row has the unsubscripted letters in components for $\{B,C,D\}$ and subscript 3 on $a$. Since each row uses its own number as a subscript, the only symbols that can appear more than once are the unsubscripted letters.
    \end{example}

    Remember that our goal is to use the given set of FD's $F$ to prove that $t$ is really in $R$. In order to do so, we "chase" the tableau by applying the FD's in $F$ to equate symbols in the tableau whenever we can. If we discover that one of the rows is actually the same as $t$ (that is, the row becomes all unsubscripted symbols), then we have proved that any tuple $t$ in the join of the projections was actually a tuple of $R$.

    To avoid confusion, when equating two symbols, if one of them is unsubscripted, make the other be the same. However, if we equate two symbols, both with their own subscript, then you can change either to be the other. However, remember that when equating symbols, you must change all occurrences of one to be the other, not just some of the occurrences.

    \begin{example}
      Let us continue with the decomposition of the previous example, and suppose the given FD's are $A \rightarrow B$, $B \rightarrow C$, and $CD \rightarrow A$. Start with the tableau:

      \[
      \begin{array}{|c|c|c|c|}
      \hline
      A & B & C & D \\
      \hline
      a & b_1 & c_1 & d \\
      a & b_1 & c & d_2 \\
      a_3 & b & c & d \\
      \hline
      \end{array}
      \]

      Since the first two rows agree in their $A$-components, the FD $A \rightarrow B$ tells us they must also agree in their $B$-components. That is, $b_1 = b_2$. We can replace either one with the other, since they are both subscripted. Let us replace $b_2$ by $b_1$. Then after applying $B \rightarrow C$:

      \[
      \begin{array}{|c|c|c|c|}
      \hline
      A & B & C & D \\
      \hline
      a & b_1 & c & d \\
      a & b_1 & c & d_2 \\
      a_3 & b & c & d \\
      \hline
      \end{array}
      \]

      Next, we observe that the first and third rows agree in both columns $C$ and $D$. Thus, we may apply the FD $CD \rightarrow A$ to deduce that these rows also have the same $A$-value; that is, $a = a_3$. We replace $a_3$ by $a$, giving us:

      \[
      \begin{array}{|c|c|c|c|}
      \hline
      A & B & C & D \\
      \hline
      a & b_1 & c & d \\
      a & b_1 & c & d_2 \\
      a & b & c & d \\
      \hline
      \end{array}
      \]

      At this point, we see that the last row has become equal to $t$, that is, $(a,b,c,d)$. We have proved that if $R$ satisfies the FD's $A \rightarrow B$, $B \rightarrow C$, and $CD \rightarrow A$, then whenever we project onto $\{A,D\}$, $\{A,C\}$, and $\{B,C,D\}$ and rejoin, what we get must have been in $R$. In particular, what we get is the same as the tuple of $R$ that we projected onto $\{B,C,D\}$.
    \end{example}

    \begin{example}[Chase Test for Lossless-join Decomposition]
      Consider relation $R(A,B,C,D)$ with functional dependencies:
      \[ A \rightarrow B, \quad C \rightarrow D, \quad AD \rightarrow C, \quad BC \rightarrow A \]

      We will perform the chase test on decomposition $\{AB, ACD, BC, BD\}$ to prove it is lossless.

      \begin{enumerate}
      \item Initial chase table with subscript 'a' for known values and variables for unknown:
          \[
          \begin{array}{|c|c|c|c|l|}
          \hline
          A & B & C & D & \text{Source} \\
          \hline
          a & b & c & d & \text{Original} \\
          a & b & b_1 & b_2 & \text{From }AB \\
          b_3 & b_4 & c & d & \text{From }ACD \\
          b_5 & b_4 & c & b_6 & \text{From }BC \\
          b_7 & b & b_8 & d & \text{From }BD \\
          \hline
          \end{array}
          \]

      \item Apply functional dependencies sequentially:

          \begin{itemize}
          \item Apply $A \rightarrow B$:
              \begin{itemize}
              \item Where $A$ values match, $B$ values must match
              \item Rows 1 and 2: since $A=a$, $B$ must be same
              \item No change needed as $B=b$ in both
              \end{itemize}

          \item Apply $C \rightarrow D$:
              \begin{itemize}
              \item Where $C$ values match, $D$ values must match
              \item Rows 1, 3, 4: $C=c$, so $D$ must be same
              \item $b_6$ becomes $d$
              \end{itemize}

          Updated table after $C \rightarrow D$:
          \[
          \begin{array}{|c|c|c|c|}
          \hline
          A & B & C & D \\
          \hline
          a & b & c & d \\
          a & b & b_1 & b_2 \\
          b_3 & b_4 & c & d \\
          b_5 & b_4 & c & d \\
          b_7 & b & b_8 & d \\
          \hline
          \end{array}
          \]

          \item Apply $AD \rightarrow C$:
              \begin{itemize}
              \item Where $A$ and $D$ match, $C$ must match
              \item Row 1 and 2: $A=a$ but different $D$, no change
              \end{itemize}

          \item Apply $BC \rightarrow A$:
              \begin{itemize}
              \item Where $B$ and $C$ match, $A$ must match
              \item Row 3 and 4: same $B$ ($b_4$) and $C$ ($c$), so $A$ must match
              \item $b_3$ becomes $b_5$
              \end{itemize}
      \end{itemize}

      \item Final chase table:
          \[
          \begin{array}{|c|c|c|c|}
          \hline
          A & B & C & D \\
          \hline
          a & b & c & d \\
          a & b & b_1 & b_2 \\
          b_5 & b_4 & c & d \\
          b_5 & b_4 & c & d \\
          b_7 & b & b_8 & d \\
          \hline
          \end{array}
          \]

      \item Conclusion:
          \begin{itemize}
          \item The chase process shows variable equating ($b_3 = b_5$)
          \item The decomposition is lossless because:
              \begin{itemize}
              \item $AB$ and $BC$ share $B$, enforcing $BC \rightarrow A$
              \item $ACD$ preserves both $C \rightarrow D$ and $AD \rightarrow C$
              \item Common attributes ensure lossless reconstruction
              \item Each subrelation preserves relevant FDs
              \end{itemize}
          \item The natural join: $(AB \bowtie BC) \bowtie ACD \bowtie BD$ recovers $R$ without loss
          \end{itemize}
      \end{enumerate}

      Therefore, $\{AB, ACD, BC, BD\}$ is a valid lossless-join decomposition in BCNF.
    \end{example}

  \subsection{The Entity-Relationship Model} 

      We have now learned to decompose relations effectively without anomalies, but it is not clear what these relations may represent. When designing a database, it isn't really ideal to just dump all data into a relation and then try to iteratively decompose it until you get a bunch of relations in BCNF. It's better to have a starting plan of what relations you should need and try and construct interpretable relationships between a pair or set of relations. To do this, \textit{Entity-Relationship diagrams} are of great help. 

      Let's briefly talk about keys again. The most obvious application of keys is allowing lookup of a row by its key value. A more practical application of keys are its way to link key IDs for one relation to another key ID of a different relation. For example, we may have two schemas \texttt{Member(uid, gid)} and \texttt{Group(gid)}, and we can join these two using the condition \texttt{Member.gid = Group.gid}. 

      \begin{definition}[Entity-Relationship Model] 
        We can think of every relation as modeling some \textit{data} or a \textit{relationships between data}.\footnote{Sometimes, the line may be blurred, but the designer will have to make the choice.} This is shown in an \textbf{E/R diagram}. 
        \begin{enumerate}
          \item An \textbf{entity set} is a relation that contains data, represented as a \textit{rectangle}. Its tuples are called \textit{entities}.\footnote{This is analogous to an entity set being a class and entities as objects.} 
          \item A \textbf{relationship set} is a relation that contains relationships (sometimes stored as the key pairs of two entity sets), represented as a \textit{diamond}. Its tuples are called \textbf{relationships}.\footnote{A minor detail is that relationships aren't really relations since the tuples in relations connect two entities, rather than the keys themselves, so some care must be taken to convert the entities into a set of attributes. }
          \item \textbf{Attributes} are properties of entities or relationships, like attributes of tuples or objects, represented as \textit{ovals}. Key attributes are underlined. 
        \end{enumerate}
        Don't confuse entity sets with entities and relationship sets with relationships. 
      \end{definition}

      \begin{example}[E/R Diagram]
        Let us model a social media database with the relations 
        \begin{enumerate}
          \item \texttt{Users(\underline{uid}, name, age, popularity)} recording information of a user. 
          \item \texttt{Member(\underline{uid}, \underline{gid}, from\_date)} recording whether a user is in a group and when they first joined. 
          \item \texttt{Groups(\underline{gid}, name)} recording information of group. 
        \end{enumerate}
        The ER diagram is shown below, where we can see that the Member relation shows a relationship between the two entities Users and Groups. 

        \begin{figure}[H]
          \centering 
          \includegraphics[scale=0.4]{img/social_media.png}
          \caption{Social media database ER diagram.} 
          \label{fig:social_media}
        \end{figure}

        Note that the \texttt{from\_date} attribute must be a part of the Member relation since it isn't uniquely associated with a user (a user can join multiple groups on different dates) or a group (two users can join a group on different dates). If there is an instance that someone joins, leaves, and rejoins a group, then we can modify our design by either: 
        \begin{enumerate}
          \item overwriting the first date joined 
          \item making another relation \texttt{MembershipRecords} which has a date also part of the key, which will capture historical membership.  
        \end{enumerate}
      \end{example}

      Therefore, we must determine if a relation models an entity or a relationship. There could also be multiple relationship sets between the same entity sets, e.g. if \texttt{Member} and \texttt{Likes} associates between \texttt{Users} and \texttt{Groups}. 

  \subsection{Relationships and Multiplicity} 
  
      A relationship really stores minimal information in the sense that if you have you know the entities it connects, that determines everything about the relation. 

      \begin{theorem}[Relationships are Functionally Dependent on Connecting Entities]
        In a relationship set, each relationship is uniquely identified by the entities it connects. More formally, if a relationship $R$ connects entities $e_1 \in E_1, \ldots, e_n \in E_n$ with keys $\mathbf{k} = (k_1, \ldots, k_n)$, then $\mathbf{k}$ is a superkey of $R$. 
      \end{theorem}

    \subsubsection{Multiplicity of Binary Relationships}

      The 4 categories representing the multiplicity just further categorizes how this functional dependency is realized. 

      \begin{definition}[Multiplicity of 2-Way Relationships]
        Given that $E$ and $F$ are entity sets, 
        \begin{enumerate}
          \item \textit{Many-many}: Each entity in E is related to $0$ or more entities in $F$ and vice versa. There are no restrictions, and we have \texttt{IsMemberOf(\underline{uid}, \underline{gid})}.\footnote{Note that this is just a restating of the theorem before.} 

          \begin{figure}[H]
            \centering 
            \includegraphics[scale=0.3]{img/ismember.png}
            \caption{} 
            \label{fig:ismember}
          \end{figure}

          \item \textit{Many-One}: Each entity in $E$ is related to $0$ or $1$ entities in $F$, but each entity in $F$ is related to $0$ or more in $E$. If $E$ points to $F$ with a straight arrow, then you can just think that this is a function, and we have \texttt{IsOwnedBy(\underline{gid}, uid)}. If we have a rounded arrow, this means that for each group, its owner \texttt{must} exist in \texttt{Users} (so no $0$). 

          \begin{figure}[H]
            \centering 
            \includegraphics[scale=0.3]{img/isowned.png}
            \caption{} 
            \label{fig:isowned}
          \end{figure}

        \item \textit{One-One}: Each entity in $E$ is related to $0$ or $1$ entity in $F$ and vice versa. We can have either \texttt{IsLinkedTo(\underline{uid}, twitter\_uid)} or \texttt{IsLinkedTo(uid, \underline{twitter\_uid})} and must choose a primary key from these two possible keys.  

        \begin{figure}[H]
          \centering 
          \includegraphics[scale=0.3]{img/islinked.png}
          \caption{} 
          \label{fig:islinked}
        \end{figure}
        \end{enumerate}
      \end{definition}

      \begin{theorem}[Multiplicity with Functional Dependencies]
        You may notice that multiplicity and functional dependence are very similar that is. If we have two relations $R, S$ and have a relationship pointing from $R$ to $S$, then this states the FD $\mathbf{r} \mapsto \mathbf{s}$! Say that the keys are $\mathbf{k}_R, \mathbf{k}_S$, respectively. Then, we have 
        \begin{equation}
          \mathbf{k}_R \mapsto \mathbf{r} \mapsto \mathbf{s} \mapsto \mathbf{k}_S
        \end{equation} 
      \end{theorem}

      \begin{example}[Movie Stars]
        Given the relations 
        \begin{enumerate}
          \item \texttt{Movies(\underline{title, year}, length, name)}
          \item \texttt{Stars(\underline{name}, address)} of a movie star and their address. 
          \item \texttt{Studios(\underline{name}, address)} 
          \item \texttt{StarsIn(\underline{star\_name}, \underline{movie\_name}, \underline{movie\_year})} 
          \item \texttt{Owns(studio\_name, \underline{movie\_name}, \underline{movie\_year})}
        \end{enumerate}
        We have the following ER diagram 
        \begin{figure}[H]
          \centering 
          \includegraphics[scale=0.3]{img/movie_stars.png}
          \caption{Movie stars. } 
          \label{fig:movie_stars}
        \end{figure}
      \end{example}

      \begin{example}[Relationship within Itself]
        Sometimes, there is a relationship of an entity set with itself. This gives the relations 
        \begin{enumerate}
          \item \texttt{Users(\underline{uid}, ...)} 
          \item \texttt{IsFriendOf(\underline{uid1}, \underline{uid2})} 
          \item \texttt{IsChildOf(\underline{child\_uid}, parent\_uid)}
        \end{enumerate}
        This can be modeled by the following. Note that 
        \begin{enumerate}
          \item users have no limitations on who is their friend. 
          \item assuming that all parents are single, a person can have at most one parent, so we have an arrow.  
        \end{enumerate}
        \begin{figure}[H]
          \centering 
          \includegraphics[scale=0.3]{img/within_itself.png}
          \caption{} 
          \label{fig:within_itself}
        \end{figure}
      \end{example}

    \subsubsection{Multiplicity of Multiway Relationships}

      Sometimes, it is necessary to have a relationship between 3 or more entity sets. It can be confusing to contruct the relations with the necessary keys. A general rule of thumb for constructing the relation of a relationship is 
      \begin{enumerate}
        \item Everything that the arrows point into are not keys.   
        \item Everything else are keys. So the arrow stumps are keys. 
      \end{enumerate}

      \begin{example}[Movie Stars]
        Suppose that we wanted to model \textit{Contract} relationship involving a studio, a star, and a movie. This relationships represents that a studio had contracted with a particular star to act in a particular movie. We want a contract to be owned by one studio, but one studio can have multiple contracts for different combinations of stars and movies. This gives the relations 
        \begin{enumerate}
          \item \texttt{Stars(\underline{name}, address)} 
          \item \texttt{Movies(\underline{title, year}, length, name)} 
          \item \texttt{Studios(\underline{name}, address)} 
          \item \texttt{Contracts(\underline{star\_name}, \underline{movie\_name}, studio\_name)}
        \end{enumerate}
        \begin{figure}[H]
          \centering 
          \includegraphics[scale=0.4]{img/contracts.png}
          \caption{} 
          \label{fig:contracts}
        \end{figure}
        We can make this even more complex by modifying contracts to have a studio of the star and the producing studio. 
        \begin{enumerate}
          \item \texttt{Contracts(\underline{star\_name}, \underline{movie\_name}, produce\_studio\_name, star\_studio\_name)}
        \end{enumerate}
        \begin{figure}[H]
          \centering 
          \includegraphics[scale=0.4]{img/four_ary.png}
          \caption{} 
          \label{fig:four_ary}
        \end{figure}
        Note that contracts can also have attributes, e.g. salary or time period. 
      \end{example}

      \begin{example}[Social Media]
        In a 3-ary relationship a user must have an initiator in order to join a group. In here, the \texttt{isMemberOf} relation has an initiator, which must be unique for each initiated member, for a given group. 
        \begin{enumerate}
          \item \texttt{User(\underline{uid}, ...)} 
          \item \texttt{Group(\underline{gid}, ...)} 
          \item \texttt{IsMemberOf(\underline{member}, initiator, \underline{gid})} since a member must have a unique pair of initiator/group that they are in. 
        \end{enumerate}
        \begin{figure}[H]
          \centering 
          \includegraphics[scale=0.3]{img/three_ary.png}
          \caption{} 
          \label{fig:three_ary}
        \end{figure}
      \end{example}

      But can we model n-ary relationships with only binary relationships? Our intuition says we can't, for the same reasons that we get lossy decomposition into 2-attribute schemas when we try to satisfy BCNF. 

      \begin{example}[N-ary Relationships vs Multiple Binary Relationships]
        N-ary relationships in general cannot be decomposed into multiple binary relationships. Consider the following diagram. 
        \begin{figure}[H]
          \centering 
          \includegraphics[scale=0.4]{img/nary_vs_binary.png}
          \caption{Attempt at reducing nary to binary ER relationships. } 
          \label{fig:}
        \end{figure}
        \begin{enumerate}
          \item u1 is in both g1 and g2, so \texttt{IsMemberOf} contains both (u1, g1) and (u2, g2)
          \item u2 served as both an initiator in both g1 and g2, so \texttt{InitiatesFor} contains both (g1, u2) and (g2, u2). 
          \item But in reality, u1 was initiated by u2 for g1 but not u2 for g2. This contradicts the information that you would get when joining the \texttt{IsMemberOf} and \texttt{InitiatesFor} relations. 
        \end{enumerate}
        Therefore, combining binary relations may generate something spurious that isn't included in the n-ary relationship. 
      \end{example}

  \subsection{Subclasses of Entity Sets}

    Sometimes, an entity set contains certain entities that have special properties not associated with all members of the set. We model this by using a \textbf{isa} relationship with a triangle. 
    
    \begin{figure}[H]
      \centering 
      \includegraphics[scale=0.4]{img/isa.png}
      \caption{There are two types of movies: cartoons and murder-mysteries, which can have their own sub-attributes and their own relationships.} 
      \label{fig:isa}
    \end{figure}

    Suppose we have a tree of entity sets, connected by \textit{isa} relationships. A single entity consists of \textit{components} from one or more of these entity sets, and each component is inherited from its parent. 

  \subsection{Weak Entity Sets}

    It is possible for an entity set's key to be composed of attributes, some or all of which belong to another entity set. There are two reasons why we need weak entity sets. 
    \begin{enumerate}
      \item Sometimes, entity sets fall into a hierarchy based on classifications unrelated to the \textit{isa} hierarchy. If entities of set $R$ are subunits of entities in set $F$, it is possible that the names of $R$-entities are not unique until we take into account the name of its $S$-entity.\footnote{Think of university rooms in different buildings.}
      \item The second reason is that we want to eliminate multiway relationships, which are not common in practice anyways. These weak entity sets have no attributes and have keys purely from its supporting sets. 
    \end{enumerate}

    \begin{definition}[Weak Entity Set]
      A \textbf{weak entity set} $R$ (double rectangles) depends on other sets. It is an entity set that 
      \begin{enumerate}
        \item has a key consisting of 0 or more of its own attributes, and 
        \item has key attributes from \textbf{supporting entity sets} that are reached by many-one \textbf{supporting relationships} (double diamonds) from it to other sets $S$. 
      \end{enumerate}
      It must satisfy the following. 
      \begin{enumerate}
        \item The relationship $T$ must be binary and many-one from $R$ to $S$. 
        \item $T$ must have referential integrity from $R$ to $S$ (since these are keys and therefore must exist in supporting sets), which is why we have a rounded arrow. 
        \item The attributes that $S$ supplies for the key of $R$ must be key attributes of $S$, unless $S$ is also weak, and it will get keys from its supporting entity set. 
        \item If there are several different supporting relationships from $R$ to the same $S$, then each relationship is used to supply a copy of the key attributes of $S$ to help form the key of $R$. 
      \end{enumerate}
      If an entity set supplies any attributes for its own key, then those attributes will be underlined. 
    \end{definition}

    \begin{example}
      To specify a location, it is not enough to specify just the seat number. The room number, and the building name must be also specified to provide the exact location. There are no extra attributes needed for this subclass, which is why a \textit{isa} relationship doesn't fit into this. 
      \begin{figure}[H]
        \centering 
        \includegraphics[scale=0.4]{img/weak_entity.png}
        \caption{Specifying a seat is not enough to determine the exact location in a university. We must know the room number and the building to fully identify it. Note that we must keep linking until we get to a regular, non-weak entity. } 
        \label{fig:weak_entity}
      \end{figure}
    \end{example}

    We generally want to use a weak entity set if an entity does not have attributes to define itself. 
    
    \begin{example}
      Say that we want to make a database with the constraints. 
      \begin{enumerate}
        \item For states, record the name and capital city. 
        \item For counties, record the name, area, and location (state) 
        \item For cities, record the name, population, and location (county and state) 
        \item Names of states should be unique. 
        \item Names of counties are unique within a state. 
        \item Names of cities are unique within a county. 
        \item A city is always located in a single county. 
        \item A county is always located in a single state. 
      \end{enumerate}
      Then, our ER diagram may look like 
      \begin{figure}[H]
        \centering 
        \includegraphics[scale=0.3]{img/city1.png}
        \caption{A weakness is that this doesn't prevent a city in state $X$ from being the capital of another state $Y$.} 
        \label{fig:city1}
      \end{figure}
    \end{example}

    \begin{example}
      Design a database with the following. 
      \begin{enumerate}
        \item A station has a unique name and address, and is either an express station or a local station. 
        \item A train has a unique number and engineer, and is either an express or local train. 
        \item A local train can stop at any station. 
        \item An express train only stops at express stations. 
        \item A train can stop at a station for any number of times during a train. 
        \item Train schedules are the same every day. 
      \end{enumerate}
      Then, our ER diagram may look like 
      \begin{figure}[H]
        \centering 
        \includegraphics[scale=0.4]{img/train1.png}
        \caption{} 
        \label{fig:train1}
      \end{figure}
    \end{example}

  \subsection{Translating ER Diagrams to Relational Designs}

    One a simple level, converting an ER diagram to a relational database schema is straightforward. Here are some rules we list. 

    \begin{theorem}[Converting Entity Sets]
      Turn each entity set into a relation with the same set of attributes. 
    \end{theorem}

    \begin{theorem}[Converting Relationships]
      Replace a relationship by a relation whose attributes are the keys for the connected entity sets along with its own attributes. If an entity set is involved several times in a relationship, then its key attributes are repeated, so you must rename them to avoid duplication.
    \end{theorem}

    \begin{theorem}[Reduce Repetition for Many-One Relationships]
      We can actually reduce repetition for many-one relationships. For example, if there is a many-one relationship $T$ from relation $R$ to relation $S$, then $\mathbf{r}$ functionally determines $\mathbf{s}$, so we can combine them into one relation consisting of 
      \begin{enumerate}
        \item all attributes of $R$. 
        \item key attributes of $S$. 
        \item Any attributes belonging to relationship $T$. 
      \end{enumerate}
    \end{theorem}
    
    \begin{theorem}[Handling Weak Entity Sets]
      To build weak entity sets, we must do three things. 
      \begin{enumerate}
        \item The relation for weak entity set $W$ must include its own attributes, all key (but not non-key) attributes of supporting entity sets, and all attributes for supporting relationships for $W$. 
        \item The relation for any relationship where $W$ appears must use the entire set of keys gotten from $W$ and its supporting entity sets. 
        \item Supporting relationships should not be converted since they are many-one, so we can use the reduce repetition for many-one relationships rule above. 
      \end{enumerate}
    \end{theorem}

    \begin{example}
      To translate the seat, rooms, and buildings diagram, 
      \begin{figure}[H]
        \centering 
        \includegraphics[scale=0.4]{img/seat.png}
        \caption{} 
        \label{fig:seat}
      \end{figure}
      we have 
      \begin{enumerate}
        \item \texttt{Building(\underline{name}, year)} 
        \item \texttt{Room(\underline{building\_name}, \underline{room\_num}, capacity)}
        \item \texttt{Seat(\underline{building\_name}, \underline{room\_num}, \underline{seat\_num}, left\_or\_right)}
      \end{enumerate}
      Note that we do not need to convert the relationships since they are contained within the entity set relations. So ignore double diamonds. 
    \end{example}

    \begin{figure}[H]
      \centering 
      \includegraphics[scale=0.45]{img/movie_hierarchy.png}
      \caption{A figure of the movie hierarchy for convenience. } 
      \label{fig:movie_hierarchy}
    \end{figure}

    \begin{theorem}[Converting Subclass Structures]
      To convert subclass structure with a \textit{isa} hierarchy, there are multiple ways we can convert them. 
      \begin{enumerate}
        \item \textit{E/R Standard}. An entity is in all superclasses and only contains the attributes its own subclass. For each entity set $R$ in the hierarchy, create a relation that includes the key attributes from the root and any attributes belonging to $R$. This gives us 
          \begin{enumerate}
            \item \texttt{Movies(title, year, length, genre)}
            \item \texttt{MurderMysteries(title, year, weapon)}
            \item \texttt{Cartoons(title, year)}
          \end{enumerate}
        \item \textit{Object Oriented}. For each possible subtree that includes the root, create one relation whose schema includes all the attributes of all entity sets in the subtree.
          \begin{enumerate}
            \item \texttt{Movies(title, year, length, genre)} 
            \item \texttt{MoviesC(title, year, length, genre)} 
            \item \texttt{MoviesMM(title, year, length, genre, weapon)}
            \item \texttt{MoviesCMM(title, year, length, genre, weapon)}
          \end{enumerate}
          Additionally, the relationship would be \texttt{Voices(title, year, starName)}. 
        \item \textit{Null Values}. Create one relation for the entire hierarchy containing all attributes of all entity sets. Each entity is one tuple, and the tuple has null values for attributes the entity does not have. We would in here always have a single schema. 
          \begin{enumerate}
            \item \texttt{Movie(title, year, length, genre, weapon)}
          \end{enumerate}
      \end{enumerate}
    \end{theorem}

    Note that the difference between the first two is that in ER, \texttt{MurderMysteries} does not contain the attributes of its superclass, while in OO, it does. 

    As you probably notice, each standard has pros and cons. The nulls approach uses only one relation, which is simple and nice. To filter out over all movies, E/R is nice since we only filter through \texttt{Movies}, whilst in OO we have to go through all relations. However, when we want to filter movies that are both Cartoons and Murder Mysteries, then OO is better since we can only select from \texttt{MoviesCMM} rather than having to go through multiple relations for ER or filter out with further selections in Null. Also, OO uses the least memory, since it doesn't waste space on null values on attributes.  


    \begin{example}
      Let's put this all together to revisit the train station example. 
      \begin{figure}[H]
        \centering 
        \includegraphics[scale=0.4]{img/final_station.png}
        \caption{For convenience} 
        \label{fig:final_station}
      \end{figure}
      We can use the ER standard to define the first 6 regular relations in single rectangles.  
      \begin{enumerate}
        \item \texttt{Train(\underline{number}, engineer)}
        \item \texttt{LocalTrain(\underline{number}}
        \item \texttt{ExpressTrain(\underline{number}}
        \item \texttt{Station(\underline{name}, address)}
        \item \texttt{LocalStation(\underline{name})}
        \item \texttt{ExpressStation(\underline{name})}
      \end{enumerate}
      Then we can construct the weak entity sets. 
      \begin{enumerate}
        \item \texttt{LocalTrainStops(\underline{local\_train\_num}, \underline{time})}
        \item \texttt{ExpressTrainStops(\underline{express\_train\_num}, \underline{time})}
      \end{enumerate}
      Then we can construct the relationships (marked with the arrows).  
      \begin{enumerate}
        \item \texttt{LocalTrainStopsAtStation(\underline{local\_train\_number}, \underline{time}, station\_name)}
        \item \texttt{ExpressTrainStopsAtStation(\underline{express\_train\_number}, \underline{time}, express\_station\_name)}
      \end{enumerate}
      Note that we can simplify these 10 relations to 8. For example, the \texttt{LocalTrain} and \texttt{LocalStation} relations are redundant since it can be computed as 
      \begin{align}
        LocalTrain & = \pi_{number} (Train) - ExpressTrain \\
        LocalStation & = \pi_{number} (Station) - ExpressStation
      \end{align}
      There is a tradeoff since it's an extra computation when checking. However, if we had used the Null Value strategy, this would be a lot simpler, and we can use value constraints on the train and station type, which can be implemented in the DBMS (though not directly in the ER diagram). 
    \end{example}

\section{SQL} 

    SQL (Structured Query Language) is the standard query language supported by most DBMS and is a syntactically sugared form of relational algebra. It is \textbf{declarative}, where the programmer specifies what answers a query should return,but not how the query should be executed. The DBMS picks the best execution strategy based on availability of indices, data/workload characteristics, etc. (i.e. provides physical data independence). It contrasts to a \textbf{procedural} or an \textbf{operational} language like C++ or Python. The reason we need this specific query language dependent on relational algebra is that it is \textit{less} powerful than general purpose languages like C or Python. These things can all be stored in structs or arrays, but the simplicity allows the compiler to make huge efficiency improvements. 

    Just like how we approached relational algebra with structure, operations, and constraints, we will do the same with SQL, although we will introduce structure with constraints first (it's natural to talk about constraints with structure, since you define the constraints when you create the SQL relations). While this wasn't talked about much before, SQL actually uses \textit{multisets}, or \textit{bags}, by default. It's operations support both set and bag operations, and we will cover both of them here. Working with sets or bags is really context dependent, but it does have a few advantages. 
    \begin{enumerate}
      \item To take the union of two bags, we can just add everything into the other without going through to check for duplicates. 
      \item When we project relations as bags, we also don't need to search through all pairs to find duplicates. 
    \end{enumerate}
    This allows for efficiency at the cost of memory. 

  \subsection{Structure and Constraints}

      \begin{definition}[Primitive Types]
        The primitive types are listed.\footnote{One thing to note is that keywords are usually written in uppercase by convention.}
        \begin{enumerate}
          \item \textit{Characters}. \texttt{CHAR(n)} represents a string of fixed length $n$, where shorter strings are padded, and \texttt{VARCHAR(n)} is a string of variable length up to $n$, where an endmarker or string-length is used. 
          \item \textit{Bit Strings}. \texttt{BIT(n)} represents bit strings of length $n$. \texttt{BIT VARYING(n)} represents variable length bit strings up to length $n$. 
          \item \textit{Booleans}. \texttt{BOOLEAN} represents a boolean, which can be \texttt{TRUE}, \texttt{FALSE}, or \texttt{UNKNOWN}. 
          \item \textit{Integers}. \texttt{INT} or \texttt{INTEGER} represents an integer. 
          \item \textit{Floating points}. \texttt{FLOAT} or \texttt{REAL} represents a floating point number, with a higher precision obtained by \texttt{DOUBLE PRECISION}. 
          \item \textit{Datetimes}. \texttt{DATE} types are of form \texttt{'YYYY-MM-DD'}, and \texttt{TIME} types are of form \texttt{'HH:MM:SS.AAAA'} on a 24-hour clock. 
          \item \textit{Null}. \texttt{NULL} represents a null value. 
        \end{enumerate}
      \end{definition}
      
      Before we can even query or modify relations, we should know how to make or delete one. 

      \begin{theorem}[\texttt{CREATE TABLE}, \texttt{DROP TABLE}]
        We can create and delete a relation using \texttt{CREATE TABLE} and \texttt{DROP TABLE} keywords and inputting the schema. 
        \begin{lstlisting}
          CREATE TABLE Movies(
            name CHAR(30), 
            year INT, 
            director VARCHAR(50), 
            seen DATE
          ); 

          DROP TABLE Movies; 
        \end{lstlisting}
      \end{theorem}

      \begin{theorem}[\texttt{DEFAULT}]
        We can also determine default values of each attribute with the \texttt{DEFAULT KEYWORD}. 
        \begin{lstlisting}
          ALTER TABLE Movies ADD rating INT 0; 
          ...
          CREATE TABLE Movies(
            name CHAR(30) DEFAULT 'UNKNOWN', 
            year INT DEFAULT 0, 
            director VARCHAR(50), 
            seen DATE DEFAULT '0000-00-00'
          ); 
        \end{lstlisting}
      \end{theorem}

    \subsubsection{Nulls}

      We are not guaranteed that we will have all data. What if there are some null values? We need some way to handle unknown or missing attribute values. One way is to use a default value (like $-1$ for age), but this can mess with other operations, such as getting average values of certain groups of users, or can make computations harder since we have to first filter out users with \texttt{age=-1} before querying. Another way is to use a valid bit for every attribute. For example, \texttt{User(uid, name, age)} could map to \texttt{User(uid, name, name\_valid, age, age\_valid)}, but this is not efficient as well. 

      A better solution is to decompose the table into multiple relations such that a missing value indicates a missing row in one of the subrelations. For example, we can decompose \texttt{User(uid, name, age, pop)} to 
      \begin{enumerate}
        \item \texttt{UserID(uid)}
        \item \texttt{UserName(uid, name)}
        \item \texttt{UserAge(uid, age)}
        \item \texttt{UserPop(uid, pop)}
      \end{enumerate}
      This is conceptually the cleanest solution but also complicates things. Firstly, the natural join wouldn't work, since compared to a single table with null values, the natural join of these tables would exclude all tuples that have at least one null value in them. 

      \begin{definition}[\texttt{NULL}]
        SQL's solution is to have a special value \texttt{NULL} indicating an unknown but not empty value. It has the following properties. 
        \begin{enumerate}
          \item It holds for every domain (null is the same for booleans, strings, ints, etc.). 
          \item Any operations like $+, -, \times, >$... leads to a \texttt{NULL} value. 
          \item All aggregate functions except \texttt{COUNT} return a \texttt{NULL}. \texttt{COUNT} also counts null values. 
        \end{enumerate}
      \end{definition}

      \begin{theorem}[Three-Valued Logic]
        Here is another way to implement the unknown logic with \textit{three-valued logic}. Suppose we set True=1, False=0. Then we can see that given statements $x, y$ which evaluate to $0, 1$, 
        \begin{enumerate}
          \item $x$ and $y$ is equivalent to $\min(x, y)$
          \item $x$ or $y$ is equivalent to $\max(x, y)$ 
          \item not $x$ is equivalent to $1 - x$
        \end{enumerate}
        It turns out that if we set unknown=0.5, then this logic also works out very nicely. Check it yourself. 
      \end{theorem}

      The way operations treat null values is extremely important, and we will add a sentence on how it treats nulls for each operation. 

      \begin{example}[Warnings]
        Note that null breaks a lot of equivalences, leading to unintended consequences. 
        \begin{enumerate}
          \item The two are not equivalent since if we have nulls, the average ignores all nulls, while the second query will sum up all non-nulls and divide by the count including the nulls. 
          \begin{lstlisting}
            SELECT AVG(pop) FROM User; 
            SELECT SUM(pop) / COUNT(*) FROM User; 
          \end{lstlisting}

          \item The two are also not equivalent since \texttt{pop = pop} is not True, but Unknown, for nulls, so it would not return nulls. The first query would return all tuples, even nulls. 
          \begin{lstlisting}
            SELECT * from User; 
            SELECT * from User WHERE pop = pop; 
          \end{lstlisting}

          \item Never compare equality with null, since this never outputs True. Rather, you should use the special keywords \texttt{IS NULL} and \texttt{IS NOT NULL}. 
            \begin{lstlisting}
              SELECT * FROM User WHERE pop = NULL; // never returns anything 
              SELECT * FROM User WHERE pop IS NULL; // correct 
            \end{lstlisting}
        \end{enumerate}
      \end{example}

    \subsubsection{Modifying Tables}

      This was not an operation we defined in relational algebra, but it is so essential that we should state it now. What if we want to add or delete another attribute? This is quite a major change. 

      \begin{theorem}[\texttt{ALTER TABLE}]
        We can add or drop attributes by using the \texttt{ALTER TABLE} keyword followed by 
        \begin{enumerate}
          \item \texttt{ADD} and then the attribute name and then its type. 
          \item \texttt{DROP} and then the attribute name. 
        \end{enumerate}
        \begin{lstlisting}
          ALTER TABLE Movies ADD rating INT; 
          ALTER TABLE Movies DROP director; 
        \end{lstlisting}
      \end{theorem} 

      We have briefly saw how to create and drop tables. To update a table, we can do the following. 

      \begin{definition}[\texttt{INSERT}]
        You can either 
        \begin{enumerate}
          \item insert one row  
            \begin{lstlisting}
              INSERT INTO Member VALUES (789, "Muchang")
            \end{lstlisting} 
          \item or you can insert the output of a query. 
            \begin{lstlisting}
              INSERT INTO Member 
              (SELECT uid, name FROM User); 
            \end{lstlisting}
        \end{enumerate}
      \end{definition}

      \begin{definition}[\texttt{DELETE}]
        You can either 
        \begin{enumerate}
          \item delete everything from a table (but not the schema, unlike \texttt{DROP TABLE}). 
            \begin{lstlisting}
              DELETE FROM Member; 
            \end{lstlisting}

          \item Delete according to a \texttt{WHERE} condition 
            \begin{lstlisting}
              DELETE FROM Member 
              WHERE age < 18; 
            \end{lstlisting}

          \item Delete according to a \texttt{WHERE} condition extracted from another query. 
            \begin{lstlisting}
              DELETE FROM Member 
              WHERE uid IN (SELECT uid FROM User WHERE age < 18); 
            \end{lstlisting}
            
        \end{enumerate}
      \end{definition}

      \begin{definition}[\texttt{UPDATE}]
        You can either 
        \begin{enumerate}
          \item Update a value of an attribute for all tuples. 
            \begin{lstlisting}
              UPDATE User 
              SET pop = (SELECT AVG(pop) from User); 
            \end{lstlisting}

          \item Update a value of an attribute for all tuples satisfying a \texttt{WHERE} condition.\footnote{Note that this does not incrementally update the values. It updates all at once from the average of the old table from the subquery.}
            \begin{lstlisting}
              UPDATE User 
              SET name = 'Barney' 
              WHERE uid = 182; 
            \end{lstlisting}
        \end{enumerate}
      \end{definition}

    \subsubsection{Constraints}

      We mainly use constraints to protect the data integrity and relieve the coder from responsibility. It also tells the DBMS about the data so it can optimize better. 

      \begin{definition}[\texttt{NOT NULL} Constraints]
        This tells that an attribute cannot be entered as null (this is already enforced for keys). 
        \begin{lstlisting}
          CREATE TABLE User 
          (uid INTEGER NOT NULL, 
          name VARCHAR(30) NOT NULL, 
          twitterid VARCHAR(15) NOT NULL, 
          age INTEGER, 
          pop FLOAT); 
        \end{lstlisting}
      \end{definition}

      \begin{definition}[\texttt{PRIMARY KEY}, \texttt{UNIQUE} Key Constraints]
        There are multiple ways to identify keys. 
        \begin{enumerate}
          \item Use the \texttt{PRIMARY KEY} keyword to make \texttt{name} the key. It can be substituted with \texttt{UNIQUE}. You can include at most one primary key, but any number of \texttt{UNIQUE}. This means that either name or id can be used as a key, but we must choose one primary key, so we are restricted to at most one. 
          \begin{lstlisting}
            CREATE TABLE Movies(
              name CHAR(30) NOT NULL PRIMARY KEY,
              id CHAR(30) NOT NULL UNIQUE,
              year INT NOT NULL, 
              director VARCHAR(50), 
              seen DATE
            ); 
          \end{lstlisting}

          \item Use the \texttt{PRIMARY KEY} keyword, which allows you to choose a combination of attributes as the key. It can be substituted with \texttt{UNIQUE}. 
          \begin{lstlisting}
            CREATE TABLE Movies(
              name CHAR(30),
              year INT, 
              director VARCHAR(50), 
              seen DATE, 
              PRIMARY KEY (name, year)
            ); 
          \end{lstlisting}
        \end{enumerate}
      \end{definition}

      \begin{definition}[Referential Integrity]
        Like we said before, a referential integrity means that if an attribute $a$ appears in $R$, then $a$ must appear in some other $T$. That is, there are no \textbf{dangling pointers}, where $R.a$ does exists but $T.a$ does not. There are names for these: 
        \begin{enumerate}
          \item \textbf{Foreign keys} is like $R.a$, where it must point to a valid primary key, e.g. \texttt{User.uid} or \texttt{Group.gid}, which are like entity sets. 
          \item \textbf{Primary keys} are like $T.a$, where it must exist if $R.a$ exists, e.g. \texttt{Member.uid} or \texttt{Member.gid}, which are relationships. 
        \end{enumerate}
        In SQl, we must make sure that the referenced columns must be the primary key and the referencing columns form a foreign key. There are two ways to do it. 
        \begin{lstlisting}
          CREATE TABLE Member (
          uid INT NOT NULL 
            REFERNECES User(uid),  // 1. put the references as you define it
          gid CHAR(10) NOT NULL,   // 2. define the attribute first
          PRIMARY KEY(uid, gid), 
          FOREIGN KEY (gid) REFERENCES Group(gid)); // 2. then reference it
        \end{lstlisting}
        If you have multi-attribute referential integrity, the second method is better. 
        \begin{lstlisting}
          ...
          FOREIGN KEY (gid, time) REFERENCES Group(gid, time)); 
        \end{lstlisting}
      \end{definition}

      \begin{example}[Handling Referential Integrity Violations]
        Say that you have a referential integrity constraint as above. Then there are two scenarios. If we insert or update a Member row so it refers to a non-existent User.uid, the DBMS will not allow this. If we delete or update a User row whose uid is referenced by some Member row, then there are certain scenarios. 
        \begin{enumerate}
          \item \textit{Reject}. The DBMS will reject this. 
          \item \textit{Cascade}. It will ripple changes (on an update) to all referring rows. 
          \item \textit{Set null}. It will set all references to NULL. 
        \end{enumerate}
        These options can be specified in SQL. 
      \end{example}

      \begin{definition}[Tuple/Attribute-Based Checks]
        These checks are only associated with a single table and are only checked when a tuple/attribute is inserted/updated. It rejects if the condition evaluates to False, but \textit{True/Unknown are fine} (unlike only True in \texttt{WHERE} conditions!). There are two ways to write this in SQL. 
        \begin{lstlisting}
          CREATE TABLE User(
            ... // 1. Directly put the check constraint in definition
            age INTEGER CHECK(age IS NULL OR age > 0), 
            ...
          ); 

          CREATE TABLE Member(
            uid INTEGER NOT NULL,           // 2. First define attribute 
            CHECK(uid IN (SELECT uid FROM User)),  // 2. Then check it 
          ...
          )
        \end{lstlisting}
        Note that in the second example, this is sort of like a referential integrity constraint. However, this is weaker since it only checks for changes in the Member relation, while the referential integrity constraint is checked for every change in both the Member and User relations. If a check evaluates to False, then the DBMS rejects the insertions/updates. 
      \end{definition}

  \subsection{Basic Relational Algebra Operations} 

    We won't repeat the definitions of the operations as they are mentioned in the first chapter. We will cover SQL syntax for both set and bag operations. Note that the way set operations work in SQL is that they remove duplicates in the operands and then remove duplicates in the result. In bag operations, we can think of each row $a$ having an implicit count of times $c_a$ it appears in the table.  

    \subsubsection{Renaming}
      
      Counterintuitively, the first thing we should know how to do is rename. This is because when we do the rest of these operations, due to naming conflicts, the need to copy a relation, or reduce confusion, we often rename relations and attributes within these queries. 

      \begin{definition}[Renaming]
        There are two ways you can rename an attribute or a relation. 
        \begin{enumerate}
          \item Explicit, using the \texttt{AS} keyword. 
            \begin{lstlisting}
              old_name AS new_name
            \end{lstlisting}
          \item Implicit, using a space. 
            \begin{lstlisting}
              old_name new_name
            \end{lstlisting}
        \end{enumerate}
      \end{definition}

      \begin{example}
        Here's a more informative example, using syntax that we will learn later. Note that by renaming the relation, all attributes are renamed as well. 
        \begin{lstlisting}
          SELECT
              s.id, 
              s.name AS student_name,
              c.name AS course_name
          FROM 
              Students s         -- rename relation
              JOIN Courses c     -- rename relation
              ON s.course_id = c.id; 
        \end{lstlisting}
      \end{example}

    \subsubsection{Set/Bag Operations}

      \begin{definition}[Union]
        The \textbf{UNION} and \texttt{UNION ALL} keywords calculate the set and bag union, respectively. Bag union sums up the counts from the 2 tables. 

        \noindent\begin{minipage}{.5\textwidth}
          \begin{lstlisting}[]{Code}
            (SELECT * FROM Relation1)  // {1, 2, 3}
            UNION 
            (SELECT * FROM Relation2); // {2, 3, 4}
            // {1, 2, 3, 4}
          \end{lstlisting}
          \end{minipage}
          \hfill
          \begin{minipage}{.49\textwidth}
          \begin{lstlisting}[]{Output}
            (SELECT * FROM Relation1)  // {1, 1, 2}
            UNION ALL 
            (SELECT * FROM Relation2); // {1, 2, 2}
            // {1, 1, 1, 2, 2, 2}
          \end{lstlisting}
        \end{minipage} 
        \texttt{UNION} and \texttt{UNION ALL} both treat NULLs as equal to each other. 
      \end{definition} 

      \begin{definition}[Intersection]
        The \textbf{INTERSECT} and \texttt{INTERSECT ALL} keywords calculate the set and bag intersection, respectively. Bag intersection does a proper-subtract\footnote{Subtracts the counts and truncates counts to $0$ if negative. So $\{a, a\} - \{a, a, a\} = \{\}$.} 

        \noindent\begin{minipage}{.5\textwidth}
          \begin{lstlisting}[]{Code}
            (SELECT * FROM Relation1)  // {1, 2, 3}
            INTERSECT 
            (SELECT * FROM Relation2); // {2, 3, 4}
          \end{lstlisting}
          \end{minipage}
          \hfill
          \begin{minipage}{.49\textwidth}
          \begin{lstlisting}[]{Output}
            (SELECT * FROM Relation1)  // {1, 1, 2}
            INTERSECT ALL 
            (SELECT * FROM Relation2); // {1, 2, 2}
            // {1, 2}
          \end{lstlisting}
        \end{minipage}
        Both treat nulls as equal to each other and will return NULL if it appears on both sides. 
      \end{definition}

      \begin{definition}[Difference]
        The \textbf{EXCEPT} and \texttt{EXCEPT ALL} keywords calculate the set and bag difference, respectively. Bag difference takes the minimum of the two counts. 

        \noindent\begin{minipage}{.5\textwidth}
          \begin{lstlisting}[]{Code}
            (SELECT * FROM Relation1)  // {1, 2, 3}
            EXCEPT 
            (SELECT * FROM Relation2); // {2, 3, 4}
          \end{lstlisting}
          \end{minipage}
          \hfill
          \begin{minipage}{.49\textwidth}
          \begin{lstlisting}[]{Output}
            (SELECT * FROM Relation1)  // {1, 1, 2}
            EXCEPT ALL 
            (SELECT * FROM Relation2); // {1, 2, 2}
            // {1}
          \end{lstlisting}
        \end{minipage}
        Both treat NULLs as equal to each other and will return NULL if it appears more times in the first table than the second. 
      \end{definition}

      \begin{example}[Interpretation of Except All]
        Look at these two operations on the schema \texttt{Poke(uid1, uid2, timestamp)}. 

        \noindent\begin{minipage}{.5\textwidth}
        \begin{lstlisting}[]{Code}
          (SELECT uid1 FROM Poke) 
          EXCEPT 
          (SELECT uid2 FROM Poke); 
        \end{lstlisting}
        \end{minipage}
        \hfill
        \begin{minipage}{.49\textwidth}
        \begin{lstlisting}[]{Output}
          (SELECT uid1 FROM Poke) 
          EXCEPT ALL
          (SELECT uid2 FROM Poke); 
        \end{lstlisting}
        \end{minipage}
        The first operation returns all users who poked others but were never poked, while the second returns all users who poked others more than they were poked. 
      \end{example}

    \subsubsection{Predicates, Projection, Selection}

      \begin{definition}[Condition/Predicate]
        The condition $p$ used in in selection and joins is implemented with the \texttt{WHERE} keyword. The operations used in predicates are listed below. 
        \begin{enumerate}
          \item Equality is denoted with \texttt{=}, and not equals is denoted with \texttt{<>}. Equality evaluates to true if both attribute values are not NULL and match.  
          \item Comparisons is done with \texttt{>}, \texttt{<}, \texttt{>=}, \texttt{<=}. 
          \item The \texttt{IN} and \texttt{NOT IN} gives you filters that restrict the domain of a certain attribute. 
          \begin{lstlisting}
            SELECT * 
            FROM relation 
            WHERE sex in ['male', 'female']; 
          \end{lstlisting}
        \end{enumerate} 
        \texttt{WHERE} will only select rows for which the condition is True (not False or unknown). 
      \end{definition}

      Both selection and projection, known as \textbf{queries}, is done with the same keyword \texttt{SELECT}, and we can do both of them at the same time (the DBMS will determine which operation to do first). Unlike set/bag operations before, \texttt{SELECT} is by default a bag operator. 

      \begin{definition}[Selection]
        The \texttt{SELECT \*} and \texttt{SELECT DISTINCT \*} keywords apply the selection bag and set operator, respectively. The $\ast$ species all attributes in the relation. 
        \begin{lstlisting}
          SELECT * 
          FROM relation 
          WHERE 
            p_1 AND p_2 AND ... ; 
        \end{lstlisting}
      \end{definition} 

      \begin{definition}[Projection]
        The \texttt{SELECT} and \texttt{SELECT DISTINCT} keywords apply the projection bag and set operator, respectively. Rather than putting an $\ast$, we should now specify our desired attributes. 
        \begin{lstlisting}
          SELECT date, age, uid
          FROM relation 
        \end{lstlisting}
      \end{definition} 

      \begin{example}[Comparing Null values always returns Null]
        The two are also not equivalent since \texttt{pop = pop} is not True, but Unknown, for nulls, so it would not return nulls. The first query would return all tuples, even nulls. 
        \begin{lstlisting}
          SELECT * from User; 
          SELECT * from User WHERE pop = pop; 
        \end{lstlisting}
      \end{example}

      \begin{example}[Never Compare Equality with Null]
        Never compare equality with null, since this never outputs True. Rather, you should use the special keywords \texttt{IS NULL} and \texttt{IS NOT NULL}. 
        \begin{lstlisting}
          SELECT * FROM User WHERE pop = NULL; // never returns anything 
          SELECT * FROM User WHERE pop IS NULL; // correct 
        \end{lstlisting}
      \end{example}

    \subsubsection{Products and Joins} 

      All products and joins use bag semantics and to use set semantics just use a \texttt{SELECT DISTINCT}. 

      \begin{definition}[Cartesian Product]
        The \texttt{CROSS JOIN} keyword or the \texttt{,} operator is used to calculate the cartesian product of two relations using bag semantics. 
        \begin{lstlisting}
          SELECT *
          FROM table1
          CROSS JOIN table2; 

          SELECT *
          FROM table1, table2; 
        \end{lstlisting} 
      \end{definition}

      \begin{definition}[Theta Join]
        A theta join is performed by simply adding a \texttt{WHERE} clause after a cross join, using bag semantics. 
        \begin{lstlisting}
          SELECT *
          FROM table1, table2 
          WHERE table1.A = table2.B;
        \end{lstlisting}
      \end{definition} 

      \begin{definition}[Natural/Inner Join]
        The \texttt{JOIN} keyword returns records that have matching values in both tables using bag semantics. 
        \begin{lstlisting}[language=SQL]
          SELECT * FROM Students s
          JOIN Courses c ON s.id = c.id;
        \end{lstlisting}
        For inner join, note that NULL attributes do not match each other (as stated in equality predicate) so they are not included. 
      \end{definition}

      \begin{definition}[Left Outer Join]
        \texttt{LEFT (OUTER) JOIN}: Returns all records from the left table $R$, and the matched records from the right table $S$. For a given $r \in R$, if there are no $s \in S$ that matches it, then the $S$ attributes will all be NULL. 
        \begin{lstlisting}[language=SQL]
          SELECT * FROM Students s
          LEFT JOIN Courses c ON s.id = c.id;
        \end{lstlisting} 
        This keeps all NULLs on the left table. 
      \end{definition}

      \begin{definition}[Right Outer Join]
        \texttt{RIGHT (OUTER) JOIN}: Returns all records from the right table $S$, and the matched records from the left table $R$ with potential NULLs. For a given $s \in S$, if there are no $r \in R$ that matches it, then the $R$ attributes will all be NULL. 
        \begin{lstlisting}[language=SQL]
          SELECT * FROM Students s
          RIGHT JOIN Courses c ON s.id = c.id;
        \end{lstlisting}
        This keeps all NULLs on the right table. 
      \end{definition}

      \begin{definition}[Full Outer Join]
        \texttt{FULL (OUTER) JOIN}: Returns all records when there is a match in either left or right table with potential NULLs.
        \begin{lstlisting}[language=SQL]
          SELECT * FROM Students s
          FULL OUTER JOIN Courses c ON s.id = c.id;
        \end{lstlisting}
        This keeps all NULLs on the both tables. 
      \end{definition}

      \begin{example}[Motivation for Outer Join]
        Take a look at the following motivating example. Suppose we want to find all members and their respective groups from \texttt{Group(gid, name)}, \texttt{Member(uid, gid)}, \texttt{User(uid, name)}. Then we can write the query 
        \begin{lstlisting}
          SELECT g.gid, g.name AS gname, 
              u.uid, u.name AS uname 
          FROM Group g, Member m, User u 
          WHERE g.gid = m.gid AND m.uid = u.uid; 
        \end{lstlisting}
        This looks fine, but what happens if \textit{Group} is empty? That is, there is a group in the \textit{Group} relation but does not appear in the \textit{Member} relation. Then, \texttt{m.gid} will evaluate to False and would not appear in the joined table, which is fine, but what if we wanted to make sure all groups appeared in this master membership table? If a group is empty, we may want it to just have null values for \texttt{uid} and \texttt{uname}. In this case, we want to use outer join. 
      \end{example}

      \begin{example}[Complex Example with Beers]
        Given the schemas 
        \begin{enumerate}
          \item \texttt{Frequents(drinker, bar, times\_a\_week)}, 
          \item \texttt{Serves(bar, beer, price)}, 
          \item \texttt{Likes(drinker, beer)}, 
        \end{enumerate}
        say that we want to select drinkers and bars that visit the bars at least 2 times a week and the bars serves at least 2 beers liked by the drinker and count the number of beers served by the bars that are liked by the drinker. The query is shown below. 

        \begin{lstlisting}
          SELECT F.drinker, F.bar, COUNT(L.beer) 
          FROM Frequents F, Serves S, Likes L 
          WHERE F.drinker = L.drinker
            AND F.bar = S.bar 
            AND L.beer = S.beer 
            AND F.times_a_week >= 2 
          GROUP BY F.drinker, F.bar 
          HAVING COUNT(L.beer) >= 2 
        \end{lstlisting}
      \end{example}

  \subsection{Arithmetic and Sorting} 

    We can also do arithmetic on columns. 

    \begin{definition}[Arithmetic Operations]
      SQL supports 
      \begin{enumerate}
        \item \textit{Addition} using \texttt{+}. It returns NULL if at least 1 of the operands are NULL. 
        \item \textit{Subtraction} using \texttt{-}. It returns NULL if at least 1 of the operands are NULL. 
        \item \textit{Multiplication} using \texttt{\*}. It returns NULL if at least 1 of the operands are NULL. 
        \item \textit{Division} using \texttt{/}. It returns NULL if at least 1 of the operands are NULL or there is division by $0$ (or may result in error depending on DBMS). For integers it will truncate. 
        \item \textit{Modulo} using \texttt{\%}. It returns NULL if at least 1 of the operands are NULL. 
      \end{enumerate}
      You can use parentheses to compose operations together. 
    \end{definition}

    \begin{example}[Arithmetic]
      Here is a comphrensive example. 
      \begin{lstlisting}
        SELECT 
          price + tax as total_price,
          salary - deductions as net_salary,
          price * quantity as total_value,
          total / count as average,
          id % 2 as is_odd
        FROM Orders; 
      \end{lstlisting}
    \end{example}

    \begin{definition}[Sorting]
      The \texttt{ORDER BY} keyword sorts the relation. In fact, it is more of a display operation since the tuples in the relation are unsorted by definition. 
      \begin{enumerate}
        \item \texttt{ORDER BY attribute ASC} orders in ascending order. This the default. 
        \item \texttt{ORDER BY attribute DESC} orders in descending order.
      \end{enumerate}
    \end{definition}

  \subsection{Aggregate Functions} 

      \begin{definition}[Standard SQL Aggregate Functions] 
        \textbf{Aggregate functions} compute something from a collection of tuples, rather than a single row. 
        \begin{enumerate}
          \item \texttt{COUNT} counts the number of rows, including NULLS, in a query. \texttt{COUNT(DISTINCT att)} counts the distinct count of an attribute in a query and ignores NULLs. 
          \item \texttt{SUM} counts the sum of the values of an attribute in a query. It ignores NULL values. 
          \item \texttt{AVG} is the average. It ignores NULL values. 
          \item \texttt{MIN} is the minimum of an attribute. It ignores NULL values and only returns NULL if all values are NULL. 
          \item \texttt{MAX} is the maximum of an attribute. It ignores NULL values and only returns NULL if all values are NULL. 
        \end{enumerate}
      \end{definition}

      \begin{example}[Calculate Popularity within Age Group]
        If we want to find the number of users under 18 and their average popularity, then we can write 
        \begin{lstlisting}
          SELECT COUNT(*), AVG(pop) 
          FROM User 
          WHERE age < 18; 
        \end{lstlisting}
      \end{example}

      \begin{example}[Average is Not Sum/Count]
        The two are not equivalent since if we have nulls, the average ignores all nulls, while the second query will sum up all non-nulls and divide by the count including the nulls. 
        \begin{lstlisting}
          SELECT AVG(pop) FROM User; 
          SELECT SUM(pop) / COUNT(*) FROM User; 
        \end{lstlisting}
      \end{example}

    \subsubsection{Group By} 

      By default, the aggregate function operates on the entire relation. If we want more fine grained control by looking at a subset of tuples, then we can use the \texttt{GROUP BY} clause. 

      \begin{definition}[\texttt{GROUP BY}]
        \texttt{GROUP BY} is used when you want to group the query by equal values of the attributes. The syntax is 
        \begin{lstlisting}
          SELECT ... FROM ... WHERE ... 
          GROUP BY age; 
        \end{lstlisting}
        To parse this, first form the groups based on the same values of all attributes in the group by clause. Then, output only one row in the select clause per group. We can look at the following order
        \begin{enumerate}
          \item \texttt{FROM}. Look at where we are querying from. 
          \item \texttt{WHERE}. Find out if this condition is satisfied to filter the main query. 
          \item \texttt{GROUP BY}. Group rows according to the values of the GROUP BY columns. 
          \item \texttt{SELECT}. Compute the select query for each group. The number of groups should be equal to the number of rows in the final output. 
        \end{enumerate}
        Note that if a query uses aggregation/group by, every column referenced in select must be either aggregated or a group by column. 
      \end{definition}

      \begin{example}
        If we want to find the number of users in a certain age and their average popularity, for all ages, then we can write 
        \begin{lstlisting}
          SELECT age, AVG(pop) 
          FROM Users
          GROUP BY age; 
        \end{lstlisting}
        You don't necessarily have to report the group by attribute in the select. The two following examples are perfectly fine, though in the right query, \texttt{age} may not functionally determine \texttt{AVG(pop)}. 

        \noindent\begin{minipage}{.5\textwidth}
        \begin{lstlisting}[]{Code}
          SELECT AVG(pop) 
          FROM User
          GROUP BY age; 
          
        \end{lstlisting}
        \end{minipage}
        \hfill
        \begin{minipage}{.49\textwidth}
        \begin{lstlisting}[]{Output}
          SELECT age, AVG(pop)  
          FROM User 
          GROUP BY age, name; 
        \end{lstlisting}
        \end{minipage}

        In the example below, this left query is not syntactically correct since \texttt{name} is not in the group by clause or aggregated. This is true even if \texttt{age} functionally determines \texttt{name}. Neither is the right since the lack of a group by clause means that the aggregate query is over the entire relation, which has multiple uid values. 

        \noindent\begin{minipage}{.5\textwidth}
        \begin{lstlisting}[]{Code}
          SELECT age, name, AVG(pop) 
          FROM User
          GROUP BY age; 
        \end{lstlisting}
        \end{minipage}
        \hfill
        \begin{minipage}{.49\textwidth}
        \begin{lstlisting}[]{Output}
          SELECT uid, MAX(pop) 
          FROM User; 
          .
        \end{lstlisting}
        \end{minipage}
      \end{example}

      As you can see, this is great to use for aggregate functions. If there is no group by clause, this is equivalent to grouping everything together.  

      \begin{example}
        Given the relation 

        \begin{table}[H]
          \centering
          \begin{tabular}{|c|c|c|}
            \hline
            \textbf{A} & \textbf{B} & \textbf{C} \\
            \hline
            1 & 1 & 10 \\ 
            2 & 1 & 8 \\ 
            1 & 1 & 10 \\ 
            2 & 3 & 8 \\ 
            2 & 1 & 6 \\ 
            2 & 2 & 2 \\ 
            \hline
          \end{tabular}
          \caption{Original relation. }
          \label{tab:groupby}
        \end{table}
      
        Running the query 
        \begin{lstlisting}
          SELECT A, B, SUM(C) AS S 
          FROM R 
          GROUP BY A, B; 
        \end{lstlisting}
        gives 
        
        \begin{table}[H]
          \centering
          \begin{tabular}{|c|c|c|}
            \hline
            \textbf{A} & \textbf{B} & \textbf{S} \\
            \hline
            1 & 1 & 20 \\ 
            2 & 1 & 14 \\ 
            2 & 3 & 8 \\ 
            2 & 2 & 2 \\ 
            \hline
          \end{tabular}
          \caption{Our query. }
          \label{tab:groupby_output}
        \end{table}
      \end{example}

    \subsubsection{Having} 

      \begin{definition}[\texttt{HAVING}]
        If you want to filter out groups having certain conditions, you must use the \texttt{HAVING} keyword rather than \texttt{WHERE}. The syntax is 
        \begin{lstlisting}
          SELECT A, B, SUM(C) FROM ... WHERE ... 
          GROUP BY ... 
          HAVING SUM(C) < 10; 
        \end{lstlisting}
        You should look at the \texttt{HAVING} clause after you look at the \texttt{GROUP BY} but before \texttt{SELECT}. \texttt{HAVING} will only select rows for which the condition is True (not False or null). 
      \end{definition}

      \begin{example}
        Given the relation 

        \begin{table}[H]
          \centering
          \begin{tabular}{|c|c|c|}
            \hline
            \textbf{A} & \textbf{B} & \textbf{C} \\
            \hline
            1 & 1 & 10 \\ 
            2 & 1 & 8 \\ 
            1 & 1 & 10 \\ 
            2 & 3 & 8 \\ 
            2 & 1 & 6 \\ 
            2 & 2 & 2 \\ 
            \hline
          \end{tabular}
          \caption{Original relation. }
          \label{tab:groupby2}
        \end{table}
      
        Running the query 
        \begin{lstlisting}
          SELECT A, B, SUM(C) AS S 
          FROM R 
          GROUP BY A, B 
          HAVING SUM(C) > 8; 
        \end{lstlisting}
        gives 
        
        \begin{table}[H]
          \centering
          \begin{tabular}{|c|c|c|}
            \hline
            \textbf{A} & \textbf{B} & \textbf{S} \\
            \hline
            1 & 1 & 20 \\ 
            2 & 1 & 14 \\ 
            \hline
          \end{tabular}
          \caption{Our query. }
          \label{tab:groupby2_output}
        \end{table}
      \end{example}

      \begin{example}
        Given the schema \texttt{Sailor(sid, name, age, rating)}, to find the age of the youngest sailor with age at least 18, for each rating with at least 2 sailors, we can run the query. 
        \begin{lstlisting}
          SELECT S.rating, MIN(S.age) AS minage 
          FROM Sailors S 
          WHERE S.age >= 18
          GROUPBY S.rating 
          HAVING COUNT(*) > 1; 
        \end{lstlisting}

        \begin{figure}[H]
          \centering 
          \includegraphics[scale=0.4]{img/sailors.png}
          \caption{The thought process.} 
          \label{fig:thought}
        \end{figure}
      \end{example}

  \subsection{Nested Queries and Views} 

    \subsubsection{Subqueries}

      We have so far worked with a single query consisting of a single select statement. However, we can extend this by including queries that have other queries in them. Conceptually, this is easy to understand since we are just composing more operators in relational algebra. 

      \begin{definition}[Subquery]
        A \textbf{subquery} is a query contained inside another query. 
      \end{definition}

      \begin{definition}[Correlated Subqueries]
        A \textbf{correlated subquery} is a subquery that is an input to a parent query. It is a query that needs a parameter from the main query and are generally slower to run. They are hard to parse, but generally you should look in this order (though this is not how the DBMS actually implements it). 
        \begin{enumerate}
          \item \texttt{FROM}. Look at where we are querying from and for loop over each row. 
          \item \texttt{SELECT}. For each row in the outer query, take whatever values/parameters are needed for this row to input into the subquery. 
          \item \texttt{WHERE}. Find out if the condition resulting from the subquery is satisfied. If so, keep this. 
        \end{enumerate}
      \end{definition}

      \begin{definition}[\texttt{EXISTS}]
        The \texttt{EXISTS}(subquery) keyword checks if a subquery is empty or not, and \texttt{NOT EXISTS} checks the negation. 
      \end{definition}

      \begin{example}[Age]
        Given \texttt{User(name, age)}, say that we want to get all users whose age is equal to a person named Bart. Then, we want to select all users from the relation. For each user, we perform the subquery where this original tuple age coincides the others age and the others name is Bart. The outer query only returns those rows for which the \texttt{EXISTS} subquery returned true. Then we can write the two equivalent queries. 

        \noindent\begin{minipage}{.5\textwidth}
        \begin{lstlisting}[]{Code}
          SELECT * 
          FROM User as u
          WHERE EXISTS(SELECT * FROM User
                        WHERE name = "Bart" 
                        AND age = u.age); 
        \end{lstlisting}
        \end{minipage}
        \hfill
        \begin{minipage}{.49\textwidth}
        \begin{lstlisting}[]{Output}
          SELECT * 
          FROM User 
          WHERE age IN(SELECT age 
                        FROM User 
                        WHERE name = 'Bart'); 
        \end{lstlisting}
        \end{minipage}

        To understand this, we use the rules from above. 
        \begin{enumerate}
          \item \texttt{FROM}. We take each row in the outer query. 
          \item \texttt{SELECT}. For each row, we take the age from that row.  
          \item \texttt{WHERE}. The subquery looks for any user named \texttt{Bart} with that same age. If such a user exists, the row is kept. 
        \end{enumerate}
      \end{example}

      Here is a very useful keyword that simplifies complex nested queries, one example of a \textbf{common table expression (CTEs)}. 
      
      \begin{definition}[\texttt{WITH}]
        The \texttt{WITH} clause aliases many relations returned from queries. 
        \begin{lstlisting}
          WITH Temp1 AS (SELECT ...), 
               Temp2 AS (SELECT ...) 
          SELECT X, Y 
          FROM Temp1, Temp2 
          WHERE ...
        \end{lstlisting}
      \end{definition}

      \begin{example}
        To extend the Bart age example, we can think of temporarily storing a query of only Bart's ages, and then comparing it when doing the main query over \texttt{User}. 
        \begin{lstlisting}
          WITH BartAge AS 
            (SELECT age 
            FROM User 
            WHERE name = 'Bart') 
          SELECT U.uid, U.name, U.pop, 
          FROM User U, BartAge B 
          WHERE U.age = B.age; 
        \end{lstlisting}
      \end{example}

    \subsubsection{Scalar Subqueries}
      
      \begin{definition}[Scalar Subqueries]
        Sometimes, if a query returns 1 scalar, then you can use it in your \texttt{WHERE} clause. You must be sure that a query will return exactly 1 scalar (not $0$ and not more than $1$), or there will be a runtime error.  
      \end{definition}

      \begin{example}
        If we want to compute users with the same age as Bart, we can write 
        \begin{lstlisting}
          SELECT * FROM User 
          WHERE age = (SELECT age from User WHERE name = 'Bart'); 
        \end{lstlisting}
        However, we may not know if Bart functionally determines age, so we must be careful. 
      \end{example}

    \subsubsection{Quantified Subqueries}

      \begin{definition}[\texttt{ALL}, \texttt{ANY}]
        We have the following \textbf{quantified subqueries}, which performs a broadcasting condition. 
        \begin{enumerate}
          \item \texttt{ALL} checks if all conditions are true. If any are NULL, then the whole result is NULL. 
          \item \texttt{ANY} checks if any condition is true. If any are true, then the whole result is true. If all are either NULL or false, then \texttt{ANY} will return NULL. 
        \end{enumerate}
      \end{definition}

      \begin{example}[Popular Users]
        Which users are the most popular? We can write this in two ways. 

        \noindent\begin{minipage}{.5\textwidth}
        \begin{lstlisting}[]{Code}
          SELECT * 
          FROM User 
          WHERE pop >= ALL(SELECT pop from User); 
          .
        \end{lstlisting}
        \end{minipage}
        \hfill
        \begin{minipage}{.49\textwidth}
        \begin{lstlisting}[]{Output}
          SELECT * 
          FROM User 
          WHERE NOT 
            (pop < ANY(SELECT pop from User)); 
        \end{lstlisting}
        \end{minipage}

        To review, here are more ways you can do the same query. 

        \noindent\begin{minipage}{.5\textwidth}
        \begin{lstlisting}[]{Code}
          SELECT * 
          FROM User AS u 
          WHERE NOT EXISTS  
            (SELECT * FROM User
            WHERE pop > u.pop); 
        \end{lstlisting}
        \end{minipage}
        \hfill
        \begin{minipage}{.49\textwidth}
        \begin{lstlisting}[]{Output}
          SELECT * FROM User 
          WHERE uid NOT IN 
            (SELECT u1.uid
            FROM User as u1, User as u2 
            WHERE u1.pop < u2.pop); 
        \end{lstlisting}
        \end{minipage}
      \end{example}
    
    \subsubsection{Views}

      \begin{definition}[View]
        A \textbf{view} is a virtual table that can be used across other queries. Tables used in defining a view are called \textbf{base tables}. 
      \end{definition}

      \begin{example}[Jessica's Circle]
        You can create a temporary table that can be used for future queries. 
        \begin{lstlisting}
          CREATE VIEW JessicaCircle AS 
          SELECT * FROM User 
          WHERE uid in (SELECT uid FROM Member WHERE gid = 'jes'); 
        \end{lstlisting}
        Once you are done, you can drop this view with 
        \begin{lstlisting}
          DROP VIEW JessicaCircle; 
        \end{lstlisting}
      \end{example}

  \subsection{Recursion} 

    Recursion seems like a foreign concept in relational databases and was not supported until SQL3 , but we can consider the following motivating example. 

    \begin{example}[Ancestors]
      Given a schema \texttt{Parent(parent, child)}, we can find Bart's grandparents with the following query. 
        \begin{lstlisting}
          SELECT p1.parent as grandparent 
          FROM Parent p1, Parent p2 
          WHERE p1.child = p2.parent 
          AND p2.child = 'Bart';
        \end{lstlisting}
        We can do the same for parents, great grandparents, and so forth, but there is no clean way to get all of Bart's ancestors with a single query.  
    \end{example} 

    \begin{definition}[Recursion]
      You can implement a recursive query using the \texttt{WITH RECURSIVE} keyword. 
    \end{definition}

    \begin{example}[Recursive Query for Ancestors]
      Lines 2-7 defines the relation recursively, and then lines 8-10 is the query using the relation defined in the WITH clause. 
      \begin{lstlisting}
        WITH RECURSIVE 
        Ancestor(anc, desc) AS 
        ((SELECT parent, child FROM Parent) -- base case
        UNION 
        (SELECT a1.anc, a2.desc             -- recursion step
        FROM Ancestors a1, Ancestors a2     -- recursion step
        WHERE a1.desc = a2.anc))            -- recursion step
        SELECT anc 
        FROM Ancestor 
        WHERE desc = 'Bart'; 
      \end{lstlisting}
    \end{example}

\section{Index and B+ Trees} 

    So far, we've abstracted away the inner workings of the DBMS and was just told to trust it. From here, we'll delve into the inner workings of modern DBMS systems, starting with hardware.   

  \subsection{Hardware and Memory Layout}

    Recall the memory hierarchy, which starts with CPU registers, followed by caches, memory, and a disk. The speed of read/write, called I/O, determines how fast you can retrieve this data. 

    \begin{definition}[Hard Disk]
      In SQL queries, the (disk) I/O dominates the execution time, so this must be analyzed first. A typical hard drive consists of a bunch of \textbf{disks/platters} that are spun by a \textbf{spindle} and read by a \textbf{disk head} on a \textbf{disk arm}. Each disk is pizza sliced into \textbf{sectors} and donut sliced into \textbf{tracks} (and a collection of tracks over all disks is a \textbf{cylinder}), and the arm will read one \textbf{block} of information, which is a logical unit of transfer consisting of one or more blocks (i.e. like a word, which is usually 4 bytes).\footnote{You cannot just write 1 byte. You must rewrite the entire block with the 1 byte updated. If we want to write 2 bytes which are on separate blocks, we must do 2 block writes.} 
    \end{definition}

    \begin{definition}[Access Time]
      The access time is the sum of 
      \begin{enumerate}
        \item the \textit{seek time}: time for disk heads to move to the correct cylinder 
        \item the \textit{rotational delay}: time for the desired block to rotate under the disk head 
        \item the \textit{transfer time}: time to read/write data on the block
      \end{enumerate}
      This spindle rotation and moving arm is slow, so the times are dominated by the first two. 
    \end{definition}

    Note that this is heavily dependent on the data being accessed. Sequential data is extremely fast while random access is slow. Therefore, we should try to store data that should be accessed together next to each other in the disk.  

    \begin{definition}[Memory, Buffer Pool]
      As we expect, the DBMS stores a cache of blocks, called the \textbf{memory/buffer pool} containing some fixed size of $M$ maximum blocks. We read/write to this pool of blocks (which costs some time per blocks) and then these dirty (which are written/updated) are flushed back to the disk. It essentially acts as a middleman between the disk and the programmers, and every piece of data that goes between the two must go through the memory. 

      \begin{figure}[H]
        \centering 
        \includegraphics[scale=0.4]{img/disk_memory.png}
        \caption{We store the memory address of the block (blue) and a bit indicating whether it is dirty or not (red). } 
        \label{fig:disk_memory}
      \end{figure}
    \end{definition}

    Therefore, if we want to read $N > M$ blocks, then the first $M$ blocks must be loaded into memory, outputted into stdout, and then the memory must be refilled with the rest of the blocks. If we have updated a block in memory, then we should flush it before overwriting this block in memory.\footnote{idk perhaps we can have a overhead bit indicating whether a block is updated, along with the memory address of this block so it can find it back on the disk.} Replacement strategies won't be covered here. Note that unlike algorithms, which focus on the cost of the algorithm after it is read into memory, we focus on the cost of loading data from the disk into memory. 

    So how should we increase performance? 
    \begin{enumerate}
      \item \textit{Disk Layout}: Keep related things close together, e.g. in the same sector/block, or same track, or same cylinder, or adjacent cylinders. 
      \item \textit{Prefetching}: When fetching a block from the disk, fetch the next block as well since it's pretty likely to access data from the next block. This is basically locality. 
      \item \textit{Parallel I/O}: We can have more heads working at the same time. 
      \item \textit{Disk Scheduling Algorithm}: e.g. the elevator algorithm sorts the cylinders so that you don't go back and forth between cylinders when fetching.  
      \item \textit{Track Buffer}: Read/write one entire track at a time. 
    \end{enumerate}

    Now let's talk about how the actual bytes are stored in memory. 

    \begin{definition}[Row Major Order, NSM]
      We group rows together contiguously in the disk block.\footnote{This is the most standard storage policy.} If we have a relation with schema \texttt{R(INT(4), CHAR(24), INT(4), DOUBLE(8))}, we first store the rows together, with extra space in between since we might append new attributes, and have a tuple of pointers to the start of each row (orange lines). 

      \begin{figure}[H]
        \centering 
        \includegraphics[scale=0.6]{img/row_major.png}
        \caption{} 
        \label{fig:row_major.png}
      \end{figure}
    \end{definition}

    Note that \texttt{VARCHAR} still allocates the same number of bytes, but adds padding. 

    \begin{definition}[Column Major, PAX]
      We group columns together contiguously in the disk block, which allows for optimization since all types are the same and we can just use pointer arithmetic to get every attribute in $O(1)$ time. We split the block into chunks and have metadata of pointers that point to the start of the array for each attribute. 

      \begin{figure}[H]
        \centering 
        \includegraphics[scale=0.6]{img/column_major.png}
        \caption{} 
        \label{fig:column_major}
      \end{figure}
    \end{definition}

  \subsection{Search Keys and Index} 

    Now that we've seen how the data is actually laid out in memory, we can look under the hood to see what happens when we make a query as such. Consider the relation schema \texttt{User(\underline{uid}, age)}, along with the two SQL queries, specifying on the key attribute and a non-key attribute. 
    
    \noindent\begin{minipage}{.5\textwidth}
      \begin{lstlisting}[]{Code}
        SELECT * 
        FROM User 
        WHERE uid = 112;
      \end{lstlisting}
      \end{minipage}
      \hfill
      \begin{minipage}{.49\textwidth}
      \begin{lstlisting}[]{Output}
        SELECT * 
        FROM User 
        WHERE age = 12;
      \end{lstlisting}
    \end{minipage} 

    The DBMS will have to go to disk and scan it to find the tuples satisfying the predicate. However, it is a bit more sophisticated than a simple complete scan. 

    \begin{definition}[Search Key]
      The \textbf{search key} is simply the attribute on which our query is defined (e.g. \texttt{uid} and \texttt{age} in the example above). The values (e.g. \texttt{112} and \texttt{12}) are called the \textbf{search key values}. 
    \end{definition} 

    This is distinct from a closely-related concept called the index, which also refers to attributes. The tuples may take on some structure depending on the attribute. It may be sorted in one attribute but random in another. 

    \begin{definition}[Index]
      This set of attribute values, which really act as pointers, that we use to scan more efficiently on disk is called the \textbf{index}. 

      \begin{figure}[H]
        \centering 
        \includegraphics[scale=0.4]{img/dense_sparse.png}
        \caption{The dense index has 10 values, but there are two Jessicas, so it is indeed dense. The sparse index is on the uid, and since the relation is sorted (a more specific form of \textit{clustering}) in the disk, we can use sparse keys.} 
        \label{fig:dense_sparse}
      \end{figure}

      Sparse and dense just refers to how much an index ``covers'' a disk block. 
      \begin{enumerate}
        \item A \textbf{dense index} means that there is one index entry for each search key value. One entry may point to multiple records
        \item A \textbf{sparse index} means that there is one index entry for each block. Records must be clustered according to the search key as shown below, and this can optimize searching. 
      \end{enumerate}

      Note that 
      \begin{enumerate}
        \item The sparse index must contain at least the number of blocks, while the dense index must contain at least the number of unique search key values. Since the sparse index is much smaller, we may be able to fit it into main memory and not have to pay the I/O cost. 
        \item A dense index does not require anything on the records, while the sparse requires the data to be clustered. 
        \item Lookup is easy on dense since we can directly see if it exists. For sparse, we must first follow the pointer and scan the block. (e.g. if we wanted to look for 279, we want to look at the address pointed to by 123, and scan down until we hit it or reach a number greater than 279). 
        \item Update is usually easier on sparse indices since we don't need to update the index unless we add a new block. For dense, if we added a new person Muchang, then we would have to add \texttt{Muchang} to the dense index. 
      \end{enumerate}
    \end{definition}

    \begin{definition}[Primary vs Secondary Index]
      Primary and secondary refers to what the index is over. 
      \begin{enumerate}
        \item A \textbf{primary index} is created for the primary key of the relation. Records are usually clustered by the primary key, so this can be sparse. 
        \item A \textbf{secondary index} is any index that is not over a primary key and is usually dense. 
      \end{enumerate}
      In SQL, the \texttt{PRIMARY KEY} declaration automatically creates a primary index, and the \texttt{UNIQUE} declaration creates a secondary index. 
    \end{definition}

    \begin{example}[Additional Secondary Indices]
      You can also create an additional secondary index on non-key attributes. For example, if you think that you will query based on popularity often, you can do 
      \begin{lstlisting}
        CREATE INDEX UserPopIndex ON User(pop); 
      \end{lstlisting}
      which will create a dense index to speed up lookup at the cost of memory. 
    \end{example}

  \subsection{Tree Index} 

    \subsubsection{B-Trees}

      So really, an index is really a set of pointers, but we must be able to store this set of pointers. This leads to the problem of the index being too big to fit into a block. In this case, we can just create a sparse index on top of the dense index. This allows us to store a large index across blocks. 
      
      \begin{figure}[H]
        \centering 
        \includegraphics[scale=0.4]{img/too_big.png}
        \caption{We store a sparse index in memory which points to a dense index on disk, which then points to the tuples of the relation, also on disk. } 
        \label{fig:too_big}
      \end{figure}

      If the index is still too big, we store another index on top of that, and we have pretty much a tree. This is called the Index Sequential Access Method (ISAM). 
      
      \begin{example}[Index Traversal in Tree]
        If we want to look up 197 in this tree, we traverse down. Note that we have the root node in memory already, but we would require an IO operation to traverse the pointer to the first block, and then another IO cost to go to the second. 
        \begin{figure}[H]
          \centering 
          \includegraphics[scale=0.4]{img/isam_lookup.png}
          \caption{An index tree of depth 3.} 
          \label{fig:isam_lookup}
        \end{figure}
      \end{example}

      A problem with this method is also a problem with BSTs. If we want to delete 123 and add 107 ten times, then we have an unbalanced binary search tree and in extreme cases, this reduces to a linear search. 

      \begin{figure}[H]
        \centering 
        \includegraphics[scale=0.4]{img/isam_problem.png}
        \caption{If this block overflows, then we want to expand this, leading to an unbalanced BST and reducing our search to linear time.} 
        \label{fig:isam_problem}
      \end{figure}
      

      \begin{definition}[B Tree]
        Therefore, this static data structure is not good, and we must use a more flexible one, called a \textbf{B-tree}.
        \begin{figure}[H]
          \centering 
          \includegraphics[scale=0.4]{img/b_tree.png}
          \caption{B-tree. Each node can now hold multiple entries in sorted order. It is height balanced, though we will not show it here.} 
          \label{fig:b_tree}
        \end{figure}
      \end{definition}

    \subsubsection{B+ Trees}

      The actual structure that modern DBMS uses is the slightly more sophisticated B+ tree. 

      \begin{definition}[B+ Tree]
        While in B trees, the non-leaf nodes are also data, \textbf{B+ trees} divide the nodes into leaf nodes, which represent the data in this data structure, and the non-leaf nodes, which do not represent data but index nodes containing index entries. 

        \begin{figure}[H]
          \centering 
          \includegraphics[scale=0.4]{img/bp_tree.png} 
          \caption{Note that the 59, which represents both the index and the data value, are repeated. In here, we assume a block size of 2. The leaf nodes are the indices that point to the tuples in the disk/memory. Sometimes, we may store the tuple directly in the nodes, which saves us another level of indirection, but this may cause memory problems if the tuples have too many attributes.} 
          \label{fig:bp_tree} 
        \end{figure}

        It has the following constraints. 
        \begin{enumerate}
          \item The \textbf{fanout} refers the to maximum number of pointers that can come out of each node. 
          \begin{figure}[H]
            \centering 
            \includegraphics[scale=0.3]{img/fanout.png}
            \caption{With fanout constraint 4, for index nodes, we have 3 values with 4 pointers representing each range. For the leaves, we have 3 pointers to the disk addresses plus 1 pointer to the next node. } 
            \label{fig:fanout}
          \end{figure}

          \item The root is the only node that can store one value. It is special in this way. 
        \end{enumerate}
      \end{definition}

      Therefore, if we have a fanout of $f$, the table shows the constraints. 

      \begin{figure}[H]
        \centering 
        \includegraphics[scale=0.3]{img/chart.png}
        \caption{Note that there is a minimum constraint to ensure that the tree is balanced. } 
        \label{fig:chart}
      \end{figure}

      Now let's describe the implementation behind its supported operations. 

      \begin{algo}[Lookup]
        If we query \texttt{SELECT * FROM R WHERE k = 179;}, we go through the B+ tree's indices and reach the leaf node. From here, we can use the pointer to go to the memory address holding this tuple in memory/disk. If we query \texttt{k = 32}, then we will not find it after reaching \texttt{35}. If we want to query a range \texttt{32 < k < 179}, we start at 32 and use the leaf pointers to go to the next leaf until we hit 179.  

        \begin{figure}[H]
          \centering 
          \includegraphics[scale=0.4]{img/fanout_ex.png}
          \caption{} 
          \label{fig:fanout_ex}
        \end{figure}

        In practice, there are much more pointers, so we could start at 179 and go back to 32 if we had backwards pointers. 
      \end{algo}

      \begin{algo}[Insertion]
        Insertion onto a leaf node having space is easy. 
        
        \begin{figure}[H]
          \centering 
          \includegraphics[scale=0.4]{img/insertion_easy.png}
          \caption{Note that to traverse this tree, we had to access 3 blocks of memory since for each block, we had to go to the disk, look up its contents, and retrieve the index of the next block (retrieve the blocks containing (100), (30), and (30, 35)) Then we have to update this block of (30, 35) and flush it. }
          \label{fig:insertion_easy}
        \end{figure}

        When we have a full node, it is more complicated. 
        \begin{enumerate}
          \item Say that we have a full node. We can try to push the rest of the leaf nodes to next node having space, but we are not guaranteed that the neighbors will have space. 

          \begin{figure}[H]
            \centering 
            \includegraphics[scale=0.4]{img/insertion_1.png}
            \caption{} 
            \label{fig:insertion_1}
          \end{figure}

          \item We can split the node, called a \textbf{copy-up}. However, this new node now has no pointer. If the parent node is not full, we can just add the pointer and update its value. 

          \begin{figure}[H]
            \centering 
            \includegraphics[scale=0.4]{img/insertion_2.png}
            \caption{} 
            \label{fig:insertion_2}
          \end{figure}

          \item If it is full, then we should split the parent node as well. This is called a \textbf{push-up}. 

          \begin{figure}[H]
            \centering 
            \includegraphics[scale=0.4]{img/insertion_3.png}
            \caption{} 
            \label{fig:insertion_3.png}
          \end{figure}

          \item This means that we have to update the parent of the parent. If the parent is not full, then we simply add it, and if it is full, then we split the parent of the parent, and so on. If we reach the root node, then we just split the root and increase the height of the tree.\footnote{This is rare in practice since the fanout is much greater than 3.}
        \end{enumerate}
      \end{algo}

      \begin{algo}[Deletion]
        Deleting a value is simple if after deletion, the leaf node has at least $f/2$ pointers ($f/2 - 1$ values). 
        
        \begin{enumerate}
          \item If the node has less than $f/2$ pointers, then it is too empty. We can adjust by taking adjacent nodes and moving them from the full nodes to the empty nodes, but this may steal too much from siblings. 

          \begin{figure}[H]
            \centering 
            \includegraphics[scale=0.4]{img/deletion_1.png}
            \caption{} 
            \label{fig:deletion_1}
          \end{figure}

          \item The adjacent nodes may be empty as well, and in this case we want to \textbf{coalesce} or merge the nodes together. This results in a dangling pointer, so we delete the pointer and remove a value from the parent node.\footnote{In practice, this is not done every deletion. This reorganizing process can be deferred and the B+ tree property may temporarily not hold.} 

          \begin{figure}[H]
            \centering 
            \includegraphics[scale=0.4]{img/deletion_2.png}
            \caption{} 
            \label{fig:deletion_2}
          \end{figure}

          \item We keep doing this until the B+ tree requirements are satisfied or we reach the root, at which point we delete the root node and our B+ tree height decreases by 1. 
        \end{enumerate}
      \end{algo}

      \begin{theorem}[IO Cost of Lookup, Insertion, Deletion]
        In general, all these operations have similar runtimes: 
        \begin{enumerate}
          \item They require us to go to the bottom of the tree, so it is $h$ operations, where $h$ is the height. 
          \item We also maybe have $+1$ or $+2$ to manipulate the actual records, plus $O(h)$ for reorganization like we saw before (which is rare if $f$ is large). 
          \item Minus 1 if we cache the root in memory, which can be decreased even further if we cache more blocks. 
        \end{enumerate}
        
        The actual size of $h$ is roughly $\log_{f/2} {N}$, where $N$ is the number of records. $f$ is the fanout, but the B+ tree properties guarantee that there are at least $f/2$ pointers, so it is $\log_{f/2} N$ at worst and $\log_{f} N$ at best. $f$ is very large usually so this is quite good. 
      \end{theorem}
      
      The reason we use B+ trees rather than B trees is that if we store data in non-leaf nodes, this decreases the fanout $f$ and increases $h$. Therefore, records in leaves require more I/Os to access. 

    \subsubsection{Clustered vs Unclustered Index} 

      Note that in a B+ tree, the leaf nodes always store the index in sorted order. This is so we can have fast lookup in indices. When we go into the disk, however, we may not have this assumption.  

      \begin{definition}[Clustered vs Unclustered Index]
        If the order of the data records on disk is the same as or close to the order of data entries in an index, then it is \textbf{clustered}, and otherwise \textbf{unclustered}. 

        \begin{figure}[H]
          \centering 
          \includegraphics[scale=0.4]{img/clustered_vs_unclustered.png}
          \caption{Even if the data entries (leaf nodes) are sorted, the memory addresses of the blocks on disk that they point to may not be sorted.} 
          \label{fig:clustered_vs_unclustered}
        \end{figure}

        Note that the B+ tree is still a search tree! It is sorted. The clustered is a property of the data on disk (blue squares). 
      \end{definition}

      The performance can really depend on whether the index is clustered or unclustered. 

  \subsection{Hash and Composite Index}

    \begin{definition}[Hash Index]
      So far, we have used tree indices. However, an alternative is to use a \textbf{hash index} which hashes the search keys for comparison. Hash indices can only handle equality queries. 
      \begin{enumerate}
        \item \texttt{SELECT * FROM R WHERE age = 5; } (requires hash index on \texttt{(age)})
        \item \texttt{SELECT * FROM R, S WHERE R.A = S.A; } (requires hash index on \texttt{R.A} or \texttt{S.A})
        \item \texttt{SELECT * FROM R WHERE age = 5 AND name = 'Bob' } (requires hash index on \texttt{(age, name)})
      \end{enumerate}
      They are more amenable to parallel processing but \textit{cannot handle range queries or prefixes}, e.g. \texttt{SELECT * FROM R WHERE age >= 5;}. Its performance depends on how good the hash function is (whether it distributes data uniformly and whether data has skew). 
    \end{definition}

    \begin{definition}[Composite Index]
      We've looked at queries in the form \texttt{SELECT \* FROM User WHERE age = 50;}, but what if there are multiple conditions. For example, if we have 
      \begin{lstlisting}
        SELECT * FROM User WHERE age >= 25 AND name = 'B'; 
      \end{lstlisting}
      then we can do a couple things: 
      \begin{enumerate}
        \item We have the index on \texttt{(age)}, at which point the leaf nodes will look something like 
          \begin{equation}
            25, 25, 25, 26, 26, 28, 29, 29
          \end{equation}
          We traverse through all the addresses in these leaf nodes and find the ones with name \texttt{B}. 

        \item If we have a \textbf{composite index} on \texttt{(age, name)}, then our leaf nodes will sort them as 
          \begin{equation}
            (25, A), (25, A), (25, B), (26, A), (26, C), (28, B), (29, A), (29, C)
          \end{equation}

        \item If we index on \texttt{(name, age)}, then our leaf nodes will sort them as 
          \begin{equation}
            (A, 25), (A, 25), (A, 26), (A, 29), (B, 25), (B, 28), (C, 26), (C, 29)
          \end{equation}
          Note that if we had the query \texttt{SELECT \* FROM R WHERE age >= 25}, then this sorting would not help since we do not prioritize ordering by age. So we cannot use tree indexing over name, age for this query. 
      \end{enumerate}
    \end{definition}

    \begin{example}
      Therefore, given a query, there are certain indices that we can use or cannot use for it.
      \begin{enumerate}
        \item If we have a query \texttt{A >= 5}, 
          \begin{enumerate}
            \item Can use hash index in general.  
            \item Can use tree index in general. 
          \end{enumerate}

        \item If we have query \texttt{A >= 5}, 
          \begin{enumerate}
            \item Can use tree with index (A). 
            \item Can use tree with index (A, B). 
            \item Cannot use tree with index (B, A) since A is not prefix. 
          \end{enumerate}

        \item If we have query \texttt{A = 5},
          \begin{enumerate}
            \item Can use hash with index (A). 
            \item Can use tree with index (A). 
            \item Cannot use hash with index (A, B) since hashing this tuple does not allow us to compare to A or retrieve it. It is one-way and pseudo-random.   
            \item Can use tree with index (A, B). 
          \end{enumerate}

        \item If we have \texttt{A = 5 AND B = 7}, 
          \begin{enumerate}
            \item Can use hash with index (A, B). 
          \end{enumerate}
      \end{enumerate}
    \end{example}

    Each index has its pros and cons, so why not just use both tree and hash indices? The problem is that when we modify a relation on the disk, we need to update the index as well. Therefore, having too many indices requires us to update more and takes more disk space. 

    Okay, so we can't use too many indices, but are indices \textit{always} better than table scans? Not exactly. 
    
    \begin{example}[Table Scans Wins]
      Consider $\sigma_{A > v} (R)$ and a secondary, non-clustered index on $R(A)$ with around 20\% of $R$ satisfying $A > v$ (could happen even for equality predicates). We need to follow pointers to get the actual result tuples. 
      \begin{enumerate}
        \item IOs for scan-based selection is simply $B(R)$ (where we can retrieve multiple tuples in this block), while 
        \item IOs for index-based selection is the lookup-cost (to traverse down the tree) plus $0.2 |R|$ (since for each tuple, we do a IO lookup, retrieve it, and then have to retrieve the next tuple which is likely not in the same block)
      \end{enumerate}
      So table scan wins if a block contains more than 5 tuples since we might as well grab everything rather than look them up one by one. 
    \end{example}

    Thankfully, the query optimizer will make these decisions for you. 

  \subsection{Index Only Plans}

    \begin{definition}[Index-Only Plans]
      There are queries that can be answered only by accessing the index pages and not the data pages, known as \textbf{index-only plans}. For index-only plans, clustering is not important since we are looking only at the leaf nodes at most. Therefore, we only need to compute the I/O cost of traversing the tree and not to access data. 

      For equality, we can just compute the number of tuples in the index pages where the equality condition is satisfied. For ranges, we may need to traverse the leaf nodes, which will lead to additional I/O cost to retrieve the leaf index pages. 
    \end{definition}

    \begin{example}[Index Only Queries]
      If we look at the following query 
      \begin{equation}
        \pi_A (\sigma_{A > v} (R))
      \end{equation}
      we see that we only care about the value of $A$ and not the rest of the tuples, so we only need to look at the index pages and not the data itself. 
    \end{example}

    \begin{example}[Primary Index Clustered According to Search Key]
      If we have a primary index, in most cases the actual records are also stored in the index pages/leaf nodes. If they are clustered according to attribute $A$, then one lookup can lead to all result tuples in their entirety. You can just hit a leaf and grab your records as you walk along the leaves. 
    \end{example}

    \begin{example}[Other Index-Only Queries]
      For example, if we just wanted to look at the count of users with age 50, then we don't need the data. We can just look at the number of values in the leaf nodes of the B+ tree with this value. 
      \begin{lstlisting}
        SELECT E.do COUNT(*) 
        FROM Emp E 
        GROUP BY E.dno;
      \end{lstlisting}

      If we have an index on \texttt{(E.dno, E.sal)}, then the two queries are also index-only plans. However, if we index on \texttt{(E.dno)}, then we need to retrieve \texttt{E.sal} on the data page, incurring more cost. 

      \noindent\begin{minipage}{.5\textwidth}
      \begin{lstlisting}[]{Code}
        SELECT E.dno, MIN(E.sal) 
        FROM Emp E 
        GROUP BY E.dno; 
      \end{lstlisting}
      \end{minipage}
      \hfill
      \begin{minipage}{.49\textwidth}
      \begin{lstlisting}[]{Output}
        SELECT AVG(E.sal) 
        FROM Emp E 
        GROUP BY E.dno;
      \end{lstlisting}
      \end{minipage}
    \end{example}

    \begin{example}[Halloween Problem]
      The Halloween problem refers to a phenomenon in databases where an update operation causes a change in the physical location of a row, potentially allowing the row to be visited again later in the same update operation. Look at the update. 
      \begin{lstlisting}
        UPDATE Payroll 
        SET salary = salary * 1.1 
        WHERE salary <= 25000;
      \end{lstlisting}
      This caused everyone to have a salary of $25000+$. This is because when we updated someone with salary of say $1000$, it went to $1100$ and is moved further right in the B+ tree. Therefore, this is revisited again is increased again in the same update. To fix this, we could update the values in reverse, from the rightmost leaf node to the leftmost one so that increasing values are visited once. Or we can just create a to-do/done list that keeps track of which ones have been updated. 
    \end{example}

  \subsection{Exercises}

    Now let's go through a bunch of questions to clarify some of the IO cost computation. 

    \begin{example}
      Assume that we have the query \texttt{SELECT * FROM User WHERE age = 50;} with the following assumptions: 
      \begin{enumerate}
        \item Assume 12 Users with \texttt{age = 50}. 
        \item Assume one data page (block) can hold 4 User tuples (so $f = 5$). 
        \item Suppose searching for data entry requires 3 IOs in a B+ tree, which contain pointers to the data records (so $h = 3$). 
      \end{enumerate}

      If the index is clustered, then we can just traverse down the B+ tree to get to the leaf node containing the value 50. 
      \begin{enumerate}
        \item We have a cost of 3 to traverse down the tree to access the index pages which show the memory addresses of the tuples on disk. 

        \item Then, we will find entries and we want to find the cost to access the data pages. There are 12 Users, and for every address, we load the entire block in memory, which will retrieve the other users of age 50 (since this is clustered). At best, we will retrieve 3 blocks (of 4 tuples each) and at worst, due to block overlap, we will retrieve 4 blocks and read the rest from memory. This gives us a cost of $+3$ or $+4$. 
      \end{enumerate}
      The total cost is 6. If the index is unclustered, then we are not guaranteed that the data with values 50 will be contiguous, so we will in the worst case have to look at 12 different blocks, leading to a total cost of $3 + 12 = 15$. 
    \end{example}

    \begin{example}[Index Problem]
      Consider a table \texttt{Orders(OrderID, CustomerID, OrderDate, TotalAmount)} with 5,000,000 records stored in 100,000 disk blocks (=pages or units of I/O). The rows are not sorted on any particular attribute. There is a \textbf{clustered} B+ tree index on OrderID (the primary key), and an \textbf{unclustered} B+ tree index on CustomerID.

      Assume the following:
      \begin{itemize}
        \item Each node in the B+ trees corresponds to one disk block.
        \item The OrderID B+ tree has 4 levels (including the root) and 10,000 leaf nodes.
        \item The CustomerID B+ tree has 5 levels (including the root) and 50,000 leaf nodes.
        \item You do not have enough space in memory to hold all data pages.
        \item The root nodes of both B+ trees are always kept in memory.
        \item The non-root index nodes and all data pages are initially on disk.
        \item All the leaves have pointers to the next left node both on the left and the right side.
        \item All data pages are fully utilized.
        \item All nodes in B+ trees are also fully utilized.
        \item Uniformity in all places. You can ignore page boundaries.
      \end{itemize}

      \noindent\textbf{Question 1.1}\\
      How many disk I/Os are required (index and data) to retrieve all order records for a specific CustomerID using the index on the CustomerID? Assume there are 100 orders for this customer.

      We must access 4 levels, with 5 levels in tree minus 1 for head already in memory (+4). There is only 1 leaf index pages containing the matching entries, since there are 5m records and 50k leaf nodes, meaning that each leaf node can contain 100 addresses at most when it is a dense index. Given that there are 100 matches, we just need to use this leaf node to access the disk. This is not clustered, so we must look through all 100 addresses, reading each block from disk for an addition cost of 100 IOs. Total is \textbf{104 IOs}. 

      \vspace{1em}
      \noindent\textbf{Question 1.2}\\
      Suppose the Orders table is frequently queried for recent orders, and the performance is critical. The current clustered index on OrderID is not providing optimal performance for these queries. You are considering reorganizing the Orders table to cluster it on OrderDate instead.

      Assume that orders are uniformly distributed over time. Everything else is the same as in the original question description \textbf{(i.e., the number of levels of the new B+ tree indexed on OrderDate is still 4 and the root is still in memory. And the number of leaf nodes is still 10,000)}. Ignore page boundaries.

      Consider the following query:
      \begin{lstlisting}
      SELECT * FROM Orders 
      WHERE OrderDate BETWEEN '2023-09-01' AND '2023-09-30';
      \end{lstlisting}

      How many disk I/Os are required in the worst case for the following query before and after the change? What is the impact of the change? Assume that 0.5\% of the orders were made in September 2023.

      For before, since the data is distributed uniformly, we must in the worst case check all data pages of the relation on disk. 
      \begin{enumerate}
        \item We first have 3 IOs to go down to the leaf (4 levels minus 1 head already in memory). 
        \item We scan through all 10,000 leaves, but we are already on leaf 1, so need to traverse 9999 pointers to load each index leaf. 
      \end{enumerate}
      Therefore, the total IO for traversal is \textbf{10,002 IOs}. If we include the IOs for loading data pages, we also incur more costs. There are 100,000 disk blocks, so as we load all of them we incur an additional IO cost of 100,000, giving us \textbf{110,002 IOs}. 


      For after, if we cluster on OrderDate, then out of the 100,000 disk blocks, we assume that 0.5\% of them, or 500 of them, will contain the relevant dates. Additionally, we assume that by uniformity, 0.5\% of the 10,000 leaf nodes will contain these relevant addresses. 
      \begin{enumerate}
        \item We traverse down to the leaf containing the address of the tuple with date attribute \texttt{2023-09-01}. This is 3 IOs. 
        \item We must traverse through the rest of the leaf nodes. Since we are already at the first one, we must load in an additional 49 leaf blocks, so 49 IOs. 
        \item As we load in each block, we are loading in a total of 500 data blocks from disk, so an additional 500 IOs. 
      \end{enumerate}
      We end up with \textbf{552 IOs}. If we exclude the IOs for loading the data pages, we incur fewer cost of \textbf{52 IOs} since this is only for traversal of the B+ tree. 

      The difference in these costs is just the first value minus the second value. For example, I can do 
      \begin{equation}
        10,002 - 552 = 9450
      \end{equation}
      reduced IO costs. 


      \vspace{1em}
      \noindent\textbf{Question 1.3}
      \begin{enumerate}[label=(\alph*)]
        \item How many disk I/Os are required in the worst case to insert a new order with order ID 2000 and customer ID 200 if updating the clustered index on the Order ID? (i.e., what is the I/O required for updating the index and inserting data records?)
        \item What would the result be if we updated the unclustered index on the CustomerID?
      \end{enumerate}

      Assume all memory blocks are being used for this update only. Then we do not throw away an index page once it is read.

      For (a), 
      \begin{enumerate}
        \item You should first get 3 IOs to traverse down to the leaf. 
        \item Then you load the relevant data page from disk onto memory (1 IO). 
        \item Since this block is full and you want to add a tuple to it, you must split it into two pages. So you initialize two buffers in memory and write two separate blocks. Then you must write these blocks back into disk (2 IOs). 
        \item Then you split the leaf (assuming it's already in memory) by writing its 2 splits back, again by splitting it in memory (2 IOs). You this for the next node (since in worst case it's full), for a total of 3 more times as you traverse up the tree (6 IOs). 
        \item You finally want to write a new root into the disk, so you make one in memory and write it (1 IO). 
      \end{enumerate}
      This gives a total of \textbf{15 IOs}. 

      For (b), should be 17 or 18? 
      \begin{enumerate}
        \item You should first traverse down to the leaf (+4 IOs). 
        \item Then you take a data page and write it in memory (+1 IO) since you don't need to split it to keep the clustering. Then you flush the new page to disk (+2 IOs). 
        \item Then you split the leaf node by writing its 2 splits back (+2). You do this 4 more times (+8 IOs). 
        \item You then want to write a new root node (+1). So in total +16 IOs. 
      \end{enumerate}
    \end{example}

\section{Query Processing and Optimization}

    \begin{definition}[Disk/Memory Blocks]
      To set up some notation, let $B(R)$ represent the number of disk blocks that relation $R$ takes up, and say $M$ is the number of blocks available in our memory. We start off with some simple operations. 
    \end{definition}

    \begin{example}[Basic Block Storage]
      Suppose we have relation $R$ with $|R| = 1000$ and each block can hold 30 tuples. Then $B(R) = \lceil 1000 / 30 \rceil = 34$, or 35 if there is overlap. 
    \end{example}

    \begin{definition}[Buffer Blocks]
      It turns out that whenever we output data to stdout, the blocks need to be stored in a \textbf{buffer block}. If the buffer block is full, then it is flushed to stdout. 
    \end{definition}

    \begin{example}[Cost of Querying Everything]
      If we do \texttt{SELECT * FROM R}, then 
      \begin{enumerate}
        \item we are at most retrieving $B(R)$ pages from disk to memory, so our IO cost is $B(R)$. 
        \item Our memory cost is 2 pages since we take each page, load it to memory, and put the answer in the output page. The next page (if any) can overwrite the previously loaded page. 
      \end{enumerate}
      We can stop early if we lookup by key. Remember that this is not counting the cost of writing the result out. 
    \end{example}

    It turns out that the efficiency of most operations defined on modern DBMS depends on two things: sorting and hashing. We'll divide up our analysis this way, focusing on their applications in joining, which tends to be the mostly used and expensive operation. Note that there are many ways to process the same query. We can in the most basic sense just scan the entire relation in disk. We can sort it. We can use a tree or hash index, and so on. All have different performance characteristics and make different assumptions about the data, so the choice is really problem-dependent. What a DBMS does is implements all alternatives and let the \textit{query optimizer} choose at runtime. We'll talk about the algorithms now and talk about the query optimizer later. 

  \subsection{Brute-Force Algorithms}  

    We start off with the most brute-force algorithms of theta joins. In here, we describe how to implement it, its IO cost, and its memory cost. In the following, when we compute $R \bowtie_p S$, $R$ is called the \textbf{outer table} and $S$ the \textbf{inner table}. Furthermore, another convention is that when we calculate IO cost, we \textit{do not} factor in the cost of writing our result back to disk.  

    \subsubsection{Nested Loop Joins} 

      Our first algorithm simply takes in every block from $R$, and for every tuple $r \in R$, we take in every tuple in $S$ to calculate the predicate $p(r, s)$. 

      \begin{algo}[Nested Loop Join]
        \textbf{Nested-loop join} just uses a brute-force nested loop when computing $R \bowtie_p S$. 
        \begin{algorithm}[H]
          \caption{Nested loop Join}
          \begin{algorithmic}
            \Require{Outer table $R$, Inner table $S$}
            \Function{NestedLoop}{R, S}
              \For{each block $B_R \in R$} 
                \State load $B_R$ into memory block $M_1$
                \For{each tuple $r \in B_R$}  
                  \For{each block $B_S \in S$} 
                    \State load $B_S$ into memory block $M_2$
                    \For{each tuple $s \in B_S$} 
                      \If{$p(r, s)$ is true} 
                        \State Write $r \bowtie s$ into buffer block $M_3$ 
                        \State Flush $M_3$ to stdout when it's full. 
                      \EndIf
                    \EndFor
                  \EndFor
                \EndFor
              \EndFor
            \EndFunction
          \end{algorithmic}
        \end{algorithm} 
        The IO cost of this is 
        \begin{enumerate}
          \item $B(R)$ to load $R$ 
          \item For every tuple $r \in R$, we run through all of $s \in S$, requiring $B(S)$ IOs.
        \end{enumerate} 
        Therefore
        \begin{equation}
          \mathrm{IO} = B(R) + |R| \cdot B(S)
        \end{equation}
        with memory cost $3$ (since we use $M_1, M_2, M_3$ blocks).
      \end{algo}

      This is clearly not efficient since it requires a lot of IOs. We can make a slight improvement by trying to do as much as we can with the loaded blocks in memory. 

      \begin{algo}[Block-Based Nested-Loop Join]
        \textbf{Block-based nested-loop join} loops over the blocks rather than the tuples in the outer loops.  
        \begin{algorithm}[H]
          \begin{algorithmic}
            \Require{Outer table $R$, Inner table $S$}
            \Function{BlockNestedLoopJoin}{$R$, $S$}
              \For{each block $B_R \in R$} 
                \State load $B_R$ into memory block $M_1$ 
                \For{each block $B_S \in S$} 
                  \State load $B_S$ into memory block $M_2$ 
                  \For{each $r \in B_R, s \in B_S$} 
                    \If{$p(r, s)$ is true} 
                      \State Write $r \bowtie s$ into buffer block $M_3$ 
                      \State Flush $M_3$ to stdout when full. 
                    \EndIf 
                  \EndFor
                \EndFor
              \EndFor
            \EndFunction
          \end{algorithmic}
        \end{algorithm}
        The IO cost of this is 
        \begin{enumerate}
          \item $B(R)$ to load $R$. 
          \item For every block $B_R$, we run through all of $s \in S$, requiring $B(S)$ IOs. 
        \end{enumerate}
        Therefore
        \begin{equation}
          \mathrm{IO} = B(R) + B(R) \cdot B(S)
        \end{equation}
        The memory cost is $3$ (since we use $M_1, M_2, M_3$ blocks). 
      \end{algo}

      \begin{algo}[Saturated Block-Based Nested-Loop Join]
        The next optimization is to use more memory by basically stuffing the memory with as much of $R$ as possible, stream $S$ by, and join every $S$ tuple with all $R$ tuples in memory. 
        \begin{algorithm}[H]
          \begin{algorithmic}
            \Require{Outer table $R$, Inner table $S$}
            \Function{SatBlockNestedLoopJoin}{$R$, $S$}
              \For{each set of blocks $\mathbf{B} = \{B_{i+1}, \ldots, B_{i + (M-2)}\} \subset R$} 
                \State load $\mathbf{B}$ into memory blocks $M_1, \ldots, M_{M-2}$. 
                \For{each $B_S \in S$}
                  \State load $B_S$ into memory block $M_{M-1}$. 
                  \For{each $r \in \mathbf{B}, s \in B_S$}  
                    \If{$p(r, s)$ is true}
                      \State Write $r \bowtie s$ into buffer block $M_M$. 
                      \State Flush $M_M$ to stdout when it's full. 
                    \EndIf
                  \EndFor
                \EndFor
              \EndFor 
            \EndFunction
          \end{algorithmic}
        \end{algorithm}
        The total IO cost of this is 
        \begin{enumerate}
          \item $B(R)$ to load $R$. 
          \item For every set of $M-2$ blocks, we run through all of $s \in S$, requiring $B(S)$ IOs. 
        \end{enumerate}
        Therefore 
        \begin{equation}
          B(R) + \bigg\lceil \frac{B(R)}{M-2} \bigg\rceil \cdot B(S) \approx B(R) \cdot B(S) / M
        \end{equation}
        You want to pick the bigger table as $R$ since you want the smaller table $S$ to be loaded/streamed in multiple times. 
      \end{algo}

      \begin{algo}[Index Nested Loop Join]
        If we want to compute $R \bowtie_{R.A = S.B} S$, the idea is to use the value of $R.A$ to probe the index on $S(B)$. That is, for each block of $R$, we load it into memory, and for each $r$ in the block, we use the index on $S(B)$ to retrieve $s$ with $s.B = r.A$, and output $rs$. 

        The IO runtime, assuming that $S$ is unclustered and secondary, is 
        \begin{equation}
          B(R) + |R| \cdot (\mathrm{index lookup}) 
        \end{equation}
        Since typically the cost of an index lookup is 2-4 IOs, it beats other join methods mentioned later if $|R|$ is not too big. Since this does not scale at all with $S$, it is better to pick $R$ to be the smaller relation. The memory requirement as with other operations is $3$ blocks. 
      \end{algo}
  
  \subsection{Sort-Based Algorithms}

    \subsubsection{External Merge Sort}

      Now let's talk about processing of queries, namely how sorting works in a database system, called \textbf{external merge sort}. In an algorithm course, we know that the runtime is $O(n \log{n})$, but this is for CPU comparisons where the entire list is loaded in memory. This is extremely trivial in comparison to the IO commands we use in databases, so we will compute the runtime of sorting a relation by an attribute in terms of IO executions. 

      The problem is that we want to sort $R$ but $R$ does not fit in memory. We divide this algorithm into \textbf{passes} which deals with intermediate sequences of sorted blocks called \textbf{runs}. 
      \begin{enumerate}
        \item Pass 0: Read $M$ blocks of $R$ at a time, sort them in memory, and write out the sorted blocks, which are called \textit{level-0 runs}. 
        \item Pass 1: Read $M-1$ blocks of the level $0$ runs at a time, sort/merge them in memory, and write out the sorted blocks, which are called \textit{level-1 runs}.\footnote{The reason we need $M-1$ rather than $M$ is that now we are merging. We are not merging in-place so we need this extra buffer to store as we traverse our pointers down each of the $M-1$ blocks.}
        \item Pass 2: Read $M-1$ blocks of the level $1$ runs at a time, sort/merge them in memory, and write out the sorted blocks, which are called \textit{level-2 runs}. 
        \item ...  
        \item Final pass produces one sorted run. 
      \end{enumerate}

      \begin{figure}[H]
        \centering 
        \includegraphics[scale=0.4]{img/merge.png}
        \caption{} 
        \label{fig:merge}
      \end{figure}

      \begin{algo}[External Merge Sort]
        The implementation has a lot of details. Not finished yet. 
        \begin{algorithm}[H]
          \begin{algorithmic}
            \Require{Relation $R$}
            \Function{ExternalMergeSort}{$R$} 
              \State $L = [0]$ \Comment{Array storing number of level-$i$ runs}
              \While{while there are blocks to read from $R$} \Comment{First pass}
                \State read the next $M$ blocks $B = \{B_1, \ldots, B_M \}$ at a time and store it in memory.
                \State sort $B$ to generate a level-0 run $B^{(0)} = \{B_1^\prime, \ldots, B_M^\prime \}$.
                \State write $B^{(0)}$ to disk. 
                \State $L[0] += 1$
              \EndWhile

              \While{$L[-1] \geq M$} 
                \State append $0$ to $L$ 
                \State let $\mathbf{B} = \{B\}$ be the set of previous runs 
                \While{there exists blocks to be read in $\mathbf{B}$} 
                  \State read $M-1$ blocks starting from the beginning of each run into memory. 
                  \State sort them to produce the $i$th run $B^{(i)}$. 
                  \State write $B^{(i)}$ to disk. 
                \EndWhile
              \EndWhile

            \EndFunction
          \end{algorithmic}
        \end{algorithm}
        To compute the cost, we know that 
        \begin{enumerate}
          \item in pass 0, we read $M$ blocks of $R$ at a time, sort them, and write out a level 0 run, so there are 
            \begin{equation}
              \lceil B(R) / M \rceil
            \end{equation}
            level 0 sorted runs, or passes. 

          \item in pass $i$, we merge $M-1$ level $(i-1)$ runs at a time, and write out a level $i$ run. We have $M-1$ memory blocks for input and $1$ to buffer output, so 
            \begin{equation}
              \text{Num. of level i runs} = \bigg\lceil \frac{\text{Num. of level (i-1) runs}}{M-1} \bigg\rceil 
            \end{equation}

          \item The final pass produces 1 sorted run. 
        \end{enumerate}

        Therefore, the number of passes is approximately 
        \begin{equation}
          \bigg\lceil \log_{M-1} \Big\lceil \frac{B(R)}{M} \Big\rceil \bigg\rceil + 1
        \end{equation}
        and the number of IOs is $2 B(R)$ since each pass reads the entire relation once and write it once. The memory requirement is $M$ (as much as possible). 
      \end{algo}

      \begin{example}[Baby Merge]
        Assume $M = 3$, with each block able to hold at most 1 number. Assume that we have an input (relation) 
        \begin{equation}
          1, 7, 4, 5, 2, 8, 3, 6, 9
        \end{equation}
        Then we go through multiple passes. 
        \begin{enumerate}
          \item Pass 0 will consist of 3 runs. You load each of the 3 numbers in memory and sort them.  
            \begin{align}
              1, 7, 4 & \mapsto 1, 4, 7 \\ 
              5, 2, 8 & \mapsto 2, 5, 8 \\ 
              9, 6, 3 & \mapsto 3, 6, 9
            \end{align}

          \item Pass 1. You merge them together by first taking 1 and 2, loading them in memory, and then comparing which one should go first. Once 1 is outputted, then the next number 4 overwrites 1 in memory, and then 2 is outputted, and so on. 
            \begin{align}
              & 1, 4, 7 + 2, 5, 8 \mapsto 1, 2, 4, 5, 7, 8 \\ 
              & 3, 6, 9
            \end{align}

          \item Pass 2. Merges the final two relations. 
            \begin{align}
              1, 2, 3, 4, 5, 7, 8 + 3, 6, 9 \mapsto 1, 2, 3, 4, 5, 6, 7, 8, 9
            \end{align}
        \end{enumerate}
        Therefore, pass 0 uses all $M$ pages to sort, and after that, when we merge, we only use $M-1$ pages to merge the inputs together and $1$ page for the output. 
      \end{example}

      Some performance improvements include: 
      \begin{enumerate}
        \item \textit{Double Buffering}. You allocate an additional block for each run, and while you are processing (merging the relations in memory), you run the IO concurrently and store it in the new block to save some time. 
        \item \textit{Blocked IO}. Instead of reading/writing one disk block at a time, we can read/write a bunch of them in clusters. This is sort of like parallelization where you don't output just one block, but multiple blocks done from multiple processing. 
      \end{enumerate}
      The problem with both of these is that we have smaller fan-in, i.e. more passes. Since we are using more blocks per run than we have, we can look at fewer runs at once. 

    \subsubsection{Sort Merge Joins}

      Now that we know how to sort, let's exploit this to optimize joins beyond nested-loops. We introduce a naive version of sort-merge join. 

      \begin{algo}[Naive Sort-Merge Join]
        A clever way is to first sort $R$ and $S$ by their join attributes, and then merge. Given that the first tuples in sorted $R, S$ is $r, s$, we do repeat until one of the $R$ or $S$ is exhausted. 
        \begin{enumerate}
          \item If $r.A > s.B$, then $s =$ next tuple in $S$ 
          \item Else if $r.A < s.B$, then $r =$ next tuple in $R$. 
          \item Else output all matching tuples and $r, s$ = next in $R, S$, which is basically a nested loop. 
        \end{enumerate}
        Therefore, given that it takes $\mathrm{sort}(R), \mathrm{sort}(S)$ to sort $R, S$ (the equations above is too cluttered to write), the IO cost consists of
        \begin{enumerate}
          \item Sorting $R$ and $S$. 
          \item We then must write $R$ and $S$ to disk in order to prepare for merging, so $B(R) + B(S)$. 
          \item We then must write $R$ and $S$ back into memory, one at a time, to merge them, so $B(R) + B(S)$. 
        \end{enumerate}
        Therefore, the IO is really just $2 B(R) + 2 B(S)$ more than it takes to sort both $R$ and $S$. 
        \begin{equation}
          \mathrm{IO} = \mathrm{sort}(R) + \mathrm{sort}(S) + 2 B(R) + 2 B(S)
        \end{equation}
        which is worst case $B(R) \cdot B(S)$ when everything joins. 
        \begin{algorithm}[H]
          \caption{}
          \label{alg:}
          \begin{algorithmic}
            \Require{}
            \State 
            \Function{Func}{x}
            \EndFunction
          \end{algorithmic}
        \end{algorithm}
      \end{algo}

      \begin{example}[Worst Case]
        To see the worst case when the IO cost is $B(R) \cdot B(S)$, look at the following example. By the time we got to the first $3$, we can't just increment the pointers for both relations. We must scan through all of the tuples of A with value 3 and all those in B with value 3 and do a nested loop to join them. 
        
        \begin{figure}[H]
          \centering 
          \includegraphics[scale=0.4]{img/nested_loop.png}
          \caption{} 
          \label{fig:nested_loop}
        \end{figure}
      \end{example}

      We have completely isolated the sorting phase and the merging phase, but we don't have to do this. Just like regular merge-sort, we can integrate them together to save IO in the last merge phase. 

      \begin{algo}[Optimized Sort-Merge Join] 
        The algorithm is just slightly modified from the naive implementation. After the final pass from sorting both $R$ and $S$, say that we have $W_R$ and $W_S$ final runs such that 
        \begin{equation}
          M > W_R + W_S
        \end{equation} 
        We can assume that this is true since if it wasn't, we can just add another pass of external sort to reduce one of the $W$'s. Then, we can do these 3 things simultaneously. 
        \begin{enumerate}
          \item We can load in the next smallest block from $R$ from its runs, merge them together.
          \item We can load in the next smallest block from $S$ form its runs, merge them together. 
          \item We can join the merged blocks \textit{in memory} and output to the buffer to be flushed. 
        \end{enumerate}

        This saves us the IO cost of writing the sorted runs back into memory and then loading them again to write, giving us a total IO cost that is equal to that of simply sorting $R$ and $S$. 
        \begin{equation}
          \mathrm{IO} = \mathrm{sort}(R) + \mathrm{sort}(S)
        \end{equation} 
        The memory varies depending on how many passes, but if $R$ and $S$ are moderately big in that we need full memory to sort them, then the memory cost is $M$ (we use everything). 

        \begin{figure}[H]
          \centering 
          \includegraphics[scale=0.4]{img/smj_optim.png}
          \caption{Sorting: produce sorted runs for $R$ and $S$ such that there are fewer than $M$ of them total. Then in merge and join, we merge the runs of $R$, merge the runs of $S$, and merge-join the result streams as they are generated!} 
          \label{fig:smj_optim}
        \end{figure}
      \end{algo}

      \begin{example}[Two-Pass SMJ] 
        If SMJ completes in two passes, then the IOs is really cheap since we are basically getting a $2 B(R) + 2 B(S)$ cost of a level-0 pass, plus the final merge-join step which takes another $B(R) + B(S)$. 
        \begin{equation}
          \mathrm{IO} = 3 ( B(R) + B(S)) 
        \end{equation}
        If SMJ cannot complete in 2 passes, then we repeatedly merge to reduce the number of runs as necessary before the final merge and join. 
      \end{example}

    \subsubsection{Zig-Zag Join} 

      \begin{definition}[Zig Zag Join using Ordered Indices]
        To compute $R \bowtie_{R.A = S.B} S$, the idea is to use the ordering provided by the indices on $R(A)$ and $S(B)$ to eliminate the sorting step of merge-join. The idea is similar to sort-merge join. We start at the leftmost leaf node of both indices of $R$ and $S$, and traverse (right) through the leaves, querying both of the data at leaf $a$ in $R$ and $b$ in $S$ if the leaf values are equal. 

        Note that we don't even have to traverse through all leaves. If we find that a key is large, we can just start from the root node to traverse, possibly skipping many keys that don't match and not incurring all those IO costs. This can be helpful if the matching keys are distributed sparsely.

        \begin{figure}[H]
          \centering 
          \includegraphics[scale=0.4]{img/zig_zag.png}
          \caption{We see that the B+ tree of $B$ has value $7$ while that of $A$ has value $2$. Rather than traversing $2 \mapsto 3 \mapsto 4 \mapsto 7$, we can just traverse to $7$ from the root node of $A$. This can give us a stronger bound. } 
          \label{fig:zig_zag}
        \end{figure}
      \end{definition}

    \subsubsection{Other Sort Based Algorithms}

      The set union, intersection, difference is pretty much just like SMJ. 

      For duplicate elimination, you simply modify it so that during both the sort and merge steps, you eliminate duplicates if you find any. 

      For grouping and aggregation, you do external merge sort by the group-by columns. The trick is you produce ``partial'' aggregate values in each run and combine them using merge. 

  \subsection{Hash-Based Algorithms}

    \subsubsection{Hash Join} 

      Hash joining is useful when dealing with equality predicates: $R \bowtie_{R.A = S.B} S$. 

      \begin{definition}[Hash Join]
        The main idea of \textbf{hash join} is that we want to partition $R$ and $S$ by hashing (using a hash function that maps to $M-1$ values) their join attributes and then consider corresponding partitions (that get mapped to the same hash value) of $R$ and $S$. If $r.A$ and $s.B$ get hashed to the same number, they might join, and if they don't, then they definitely won't join. The figure below nicely visualizes this. 
      
        \begin{figure}[H]
          \centering 
          \includegraphics[scale=0.4]{img/hash_join.png}
          \caption{Say that the orange points represent the hashed values of $r.A$ and $s.B$ for $r, s \in R, S$. The nested loop considers all slots (all pairs of orange points between $R$ and $S$), but hash join considers only those along the diagonal. } 
          \label{fig:hash_join}
        \end{figure}

        Then, in the \textbf{probing phase}, we simply read in each partition (the set of tuples that map to the same hash) of $R$ into memory, stream in the corresponding partition of $S$, compare their \textit{values} (not hashes since they are equal), and join if they are equal. If we cannot fit in a partition into memory, we just take a second hash function and hash it again to divide it into even smaller partitions (parallels merge-sort join). 

        \begin{figure}[H]
          \centering 
          \includegraphics[scale=0.4]{img/hash_join2.png}
          \caption{} 
          \label{fig:hash_join2}
        \end{figure}

        Therefore, if the hash join completes in two passes, the IO runtime is 
        \begin{equation}
          3 ( B(R) + B(S)) 
        \end{equation}
        which is similar to merge-sort join. As for the memory requirement, let's first assume that in the probing phase, we should have enough memory to fit one partition of $R$, i.e. $M-1 > \lceil B(R) / (M-1) \rceil$, so solving for it roughly gives $M > \sqrt{B(R)} + 1$. We can always pick $R$ to be the smaller relation, so roughly 
        \begin{equation}
          M > \sqrt{\min\{ B(R), B(S)\}} + 1
        \end{equation}
      \end{definition}

      \begin{theorem}[Hash vs Sort-Merge Join]
        To compare hash join and SMU, note that their IOs are the same, but for memory requirements, hash join is lower, especially when the two relations have very different sizes. 
        \begin{equation}
          \sqrt{\min\{B(R), B(S)\}} + 1 < \sqrt{B(R) + B(S)}
        \end{equation}
        Some other factors include the quality of the hash (may not generate evenly sized partitions). Furthermore, hash join does not support inequality joins unlike SMJ, and SMJ wins if either $R$ and/or $S$ is already sorted. SMJ also wins if the result needs to be in sorted order. 
      \end{theorem}

      Sometimes, even block nested loop join may win in the following cases. 
      \begin{enumerate}
        \item if many tuples join (as in the size of the join is $|S| \cdot |R|$) since we are doing unnecessary processing in hash/merge-sort joins. 
        \item if we have black-box predicates where we may not know the truth/false values of the $\theta$
      \end{enumerate}

    \subsubsection{Other Hash Based Algorithms}

      The union, difference, and intersection are more or less like hash join. 

      For duplicate elimination, we can check for duplicates within each partition/bucket. 

      For grouping/aggregation, we can apply the hash functions to the group-by columns. Tuples in the same group must end up in the same partition/bucket. Or we may keep a running aggregate value for each group. 

      To compare the duality of sort and hash, note that 
      \begin{enumerate}
        \item in sorting, we have a physical division and logical combination 
        \item in hashing, we have a logical division and physical combination
      \end{enumerate}
      When handling large inputs, 
      \begin{enumerate}
        \item in sorting we have multi-level merge 
        \item in hashing we have recursive partitioning 
      \end{enumerate}
      For IO patterns, 
      \begin{enumerate}
        \item in sorting we have sequential write and random read (merge) 
        \item in hashing we have random write and sequential read (partition) 
      \end{enumerate}

  \subsection{Exercises} 

    Here are some exercises for calculating IO costs of these joins. 

    \begin{example}[Join Operations]
      Consider the two tables
      \begin{enumerate}
        \item \texttt{Orders(OrderID, CustomerID, OrderDate, TotalAmount)} 
        \item \texttt{Customers(CustomerID, Address)}
      \end{enumerate}
      \texttt{Orders.CustomerID} is a foreign key referring to \texttt{Customers.CustomerID}.

      The rows are not sorted on any particular attribute. We want to join the two tables using the attribute CustomerID:
      \[
      \text{Orders} \bowtie_{\text{Orders.CustomerID = Customers.CustomerID}} \text{Customers}
      \]
      The inner and outer tables can be swapped to reduce I/O costs in the following questions.

      Assume the following:
      \begin{itemize}
        \item The cost metric is the number of page/block I/Os unless otherwise noted.
        \item DO NOT count the I/O cost of writing out the final result unless otherwise noted.
        \item M = 19 blocks (= pages) available in memory unless otherwise noted.
        \item Table Orders contains 50,000 records (rows/tuples) on disk. One block can contain 20 Orders-tuples.
        \item Table Customers contains 20,000 records (rows/tuples) on disk. One block can contain 10 Customers-tuples.
        \item Assume uniform distribution for Orders.CustomerID and Customers.CustomerID -- i.e. the same number of Orders-tuples join with a Customers-tuple.
        \item Ignore page boundary wherever applicable.
        \item Assume uniform distributions wherever applicable.
      \end{itemize}

      \vspace{1em}
      \noindent\textbf{Question 2.1}
      \begin{enumerate}[label=(\alph*)]
        \item For the Orders and Customers tables, we want to perform a nested-loop join (\textbf{using 3 memory blocks}). What is the \textbf{minimum} total I/O cost? (Choose the table that will reduce the I/O cost as the outer table.)
        
        \item For the Orders and Customers tables, we want to perform a block-based nested-loop join (\textbf{using 3 memory blocks}). What is the \textbf{minimum} total I/O cost?
      \end{enumerate}

      For (a), we choose $R = $ Customers as the outer table since it is smaller. Therefore, our IO cost is 
      \begin{equation}
        B(R) + |R| \cdot B(S)  = 2000 + 20,000 \cdot 2500 = 50,002,000
      \end{equation}

      For (b), we also choose $R = $ Customers as outer since it's smaller. The IO cost is 
      \begin{equation}
        B(R) + B(R) \cdot B(S)= 2000 + 2000 * 2500 = 5,002,000
      \end{equation}

      \vspace{1em}
      \noindent\textbf{Question 2.2}
      \begin{enumerate}[label=(\alph*)]
        \item If we want to perform an external merge sort on the Orders table, how many level-0 runs does the external merge sort produce for the Orders table (M=19)?
        
        \item Continuing with the question (a), how many passes in total does the external merge sort take (including the first sorting pass)? Show the calculations for each pass (including the number of runs and size of each run).
        
        \item What is the total I/O cost of the external merge sort for the Orders table? Remember, do not count the cost of final write.
        
        \item Do an improved sort-merge-join, i.e., do merge and join in the same pass when the total number of sorted runs from Orders and Customers will fit in memory including an output block. Compute the minimum total I/O cost of sort-merge-join of table Orders and Customers.
      \end{enumerate}

      For (a), we read $M$ blocks of $R$ at a time, so we need to have $\lceil B(R) / M \rceil = \lceil 2500 / 19 \rceil =$ \textbf{132 passes}. 

      For (b), we saw that 
      \begin{enumerate}
        \item The level 0 pass takes 132 runs. 
        \item The level 1 pass takes $\lceil 132 / (19 - 1) \rceil = 8$ runs. 
        \item At this point we can just run once more since $8 < 18$, so our level 2 pass takes $1$ run. 
      \end{enumerate}
      So we have \textbf{3 passes} with \textbf{141 runs}. 

      For (c), we have $B(R) = 2500$ and just compute the following. 
      \begin{enumerate}
        \item For level $0$, we read all blocks, and write them all out: $2 B(R)$. 
        \item For level 1, it's the same thing since we again read all blocks and write them back to disk: $2 B(R)$. 
        \item For level 2, we just read through and do not include the final write, so $B(R)$.
      \end{enumerate}
      This is a total of \textbf{$5 B(R) = 5 \cdot 2500 =$ 12,500 IOs}. 

      For (d), 
      \begin{enumerate}
        \item For the first pass, you load all blocks of both relations in memory for the first iteration of sort merge and then write them to disk, giving us $2 (B(R) + B(S))$ IOs. 
        \item The second pass is the same, giving us $2 (B(R) + B(S))$ IOs. 
        \item By the third pass, we complete the sort-merge join giving us $B(R) + B(S)$. 
      \end{enumerate}
      This is a total of $5 (B(R) + B(S)) = 5 \cdot (2000 + 2500) =$ \textbf{22,500 IOs}. 

      \vspace{1em}
      \noindent\textbf{Question 2.3}
      Assume uniform distribution for the hash function(s).
      \begin{enumerate}[label=(\alph*)]
          \item For the Orders and Customers tables, we want to perform a multi-pass hash join. How many passes do we need (including partitioning phase and join phase)?
          
          \item What is the minimum I/O cost of joining Orders and Customers using a hash join?
          
          \item What is the minimum number of memory blocks required if we want to perform a 2-pass hash join? Note that M should be an integer.
      \end{enumerate}

      For (a), we must partition both relations. 
      \begin{enumerate}
        \item In the first partition pass, we 
          \begin{enumerate}
            \item take Customers and have $\lceil 2000 / (19 - 1) \rceil = 112 > 18$. Each partition contains 112 blocks, which is too big for our memory, so we must partition again.
            \item take Orders and have $\lceil 2500 / (19 - 1) \rceil = 139$. Each partition contains 139 blocks, which is too big for our memory, so we must partition again.
          \end{enumerate}
        \item In the second partition pass, we 
          \begin{enumerate}
            \item take Customers and have $\lceil 112 / 18 \rceil = 7 \leq 18$, so we are done since each partition (of a partition) of 7 blocks can fit in memory.  
            \item take Orders and have $\lceil 139 / 18 \rceil = 8 \leq 18$, so we are done since each partition (of a partition) of 8 blocks can fit in memory.  
          \end{enumerate}
        \item Finally, we load each partition of Customers in memory (7 blocks), and for each partition, we just iterate through the corresponding partition of Customers. This is one join pass. 
      \end{enumerate}
      There are a total of \textbf{3 passes}, or 5 passes if we consider each partition step of each relation as an individual pass. 

      For (b), we see that since \texttt{Orders.CustomerID} is a foreign key, we won't have a case where every row in \texttt{Orders} will trivially join with every row in \texttt{Customers}. We can compute the steps as such. 
      \begin{enumerate}
        \item The 1st partition pass. 
          \begin{enumerate}
            \item For Customers, it requires you to read all blocks (2000 IOs) and then write $112 \cdot 18$ blocks back onto disk (2016 IOs),\footnote{This is not exactly 2000 because $2000 / 18 = 111.11$, which means that after repeatedly flushing out the filled partitions to disk 111 times, at the end we will have a partially filled buffer block in memory for each of the 18 partitions. This will need to be flushed out as well.} or we are just approximating, this is about 2000 IOs as well.  
            \item For Orders, it requires you to read all blocks (2500 IOs) and then write $139 \cdot 18$ blocks back onto disk (2502 IOs), or just 2500 IOs as an approximation. This is flushed out as well. 
          \end{enumerate}
          This gives us a total of $2000 + 2016 + 2500 + 2502 = 9018$ IOs or with the approximations $9000$ IOs. 

        \item The 2nd partition pass. 
          \begin{enumerate}
            \item For Customers, now you have 18 partitions with each 112 blocks. For each partition, you load it again (112 IOs) and then partition it again to write back $18 \cdot 7$ blocks (126 IOs) for a total of $238$ IOs. You do this 18 times for each partition giving us $238 \cdot 18 = 4284$ IOs. Again, we can just approximate it by saying that loading all partitions is $2000$ and writing all is $2000$, giving us $4000$ IOs. 
            \item For Orders, now you have 18 partitions with each 139 blocks. For each partition, you load it again (139 IOs) and then partition it again to write back $18 \cdot 8$ blocks (144 IOs) for a total of $283$ IOs. You do this 18 times for each partition giving us $283 \cdot 18 = 5904$ IOs. Again, we can just approximate it by saying that loading all partitions is $2500$  and writing all is $2500$, giving us $5000$ IOs. 
          \end{enumerate}
          This gives us a total of $4284 + 5904 = 10188$ IOs, or with the approximations $9000$ IOs. 

        \item In the join phase, you load the partitions of $R$ and $S$ with the matching hashes once each to compare, so we have (not including write)  
          \begin{enumerate}
            \item the partitions of $R$, which is $18 \cdot 18 \cdot 7 = 2268$ IOs, or $2000$ IOs approximately.  
            \item the partitions of $S$, which is $18 \cdot 18 \cdot 8 = 2592$ IOs, or $2500$ IOs approximately.  
          \end{enumerate}
          giving us a total of $4860$ IOs. 
      \end{enumerate}
      Therefore, we have a total of $9018 + 10188 + 4860$ = \textbf{24066 IOs}, or if we use our approximations, it's simply $5 (B(R) + B(S)) = 5 \cdot 4500 =$ \textbf{22,500 IOs}. 

      For (c), $M$ must satisfy 
      \begin{equation}
        M \geq \lceil \sqrt{\min\{2500, 2000\}} \rceil + 1 = 46
      \end{equation} 
      giving us $M = 46$. Checking this is indeed the case, since now the image of our hash function is $\{1, \ldots, 45\}$, and assuming uniformity the number of blocks in each partition is $\lceil 2000 / 45 \rceil = 45$, which is just enough to fit in memory and then use the last block as an input buffer for the bigger relation when joining. 
    \end{example}

  \subsection{Logical Plans}

      When the DBMS chooses the best way to sort or merge two relations, it needs to choose it immediately. This can be done crudely by looking at the statistics of the relations that it is working with, but it doesn't guarantee that you will get the optimal plan. Therefore, it goes by the principal that you shouldn't try and waste time choosing the optimal one, but rather avoid the horrible ones. As a user of this DBMS, we should also take care in writing queries that are not too computationally or IO heavy. Two general heuristics that we should follow are: 
      \begin{enumerate}
        \item You want to \textit{push down}, i.e. use as early as possible, selections and projections. 
        \item You want to join smaller relations first and avoid using cross product, which can be devastating in memory. 
      \end{enumerate}

      \begin{definition}[Logical Plan]
        To have an approximate sense of how computationally heavy a query is, we can construct a high-level \textbf{logical plan}, which shows the computation DAG of the relational operators that we will perform for a query. We can optimize the logical plan by modifying our intermediate steps, such as optimizing our relational algebra logic or our implementation of SQL. 
      \end{definition}

    \subsubsection{Query Rewrite}

      Using what we know, we can get a bit more theoretical and use the following identities in relational algebra. However, this has already been optimized and only does so much in practice. 

      \begin{theorem}[Identities]
        The following hold:
        \begin{enumerate}
          \item Selection-Join Conversion: $\sigma_p(R \times S) = R \bowtie_p S$
          \item Selection Merge/Split: $\sigma_{p_1}(\sigma_{p_2}R) = \sigma_{p_1 \wedge p_2}R$
          \item Projection Merge/Split: $\pi_{L_1}(\pi_{L_2}R) = \pi_{L_1}R$, where $L_1 \subseteq L_2$
          \item Selection Push Down/Pull Up: $\sigma_{p \wedge p_r \wedge p_s}(R \bowtie_{p'} S) = (\sigma_{p_r}R) \bowtie_{p \wedge p'} (\sigma_{p_s}S)$, where:
            \begin{enumerate}
              \item $p_r$ is a predicate involving only R columns
              \item $p_s$ is a predicate involving only S columns
              \item $p$ and $p'$ are predicates involving both R and S columns
            \end{enumerate}
          \item Projection Push Down: $\pi_L(\sigma_p R) = \pi_L(\sigma_p(\pi_{L L'}R))$, where $L'$ is the set of columns referenced by $p$ that are not in $L$
        \end{enumerate}
      \end{theorem} 

      \begin{definition}[SQL Query Rewrite]
        We can rewrite SQL queries directly, though this is more complicated and requires knowledge of the nuances of the DBMS. 
        \begin{enumerate}
          \item Subqueries and views may not be efficient, as they divide a query into nested blocks. Processing each block separately forces the DBMS to use join methods, which may not be optimal for the entire query though it may be optimal for each block. 
          \item Unnest queries convert subqueries/views to joins. 
        \end{enumerate}
        Therefore, it is usually easier to deal with select-project-join queries, where the rules of relational algebra can be cleanly applied. 
      \end{definition} 

      \begin{example}[Query Rewrite]
        Given the query, we wish to rewrite it. 
        \begin{lstlisting}
          SELECT name 
          FROM User 
          WhERE uid = ANY(SELECT uid FROM Member);
        \end{lstlisting}

        The following is wrong since there may be one user in two groups, so it will be duplicated.\footnote{A bit of review: when testing whether two queries are equal, think about if the two queries treat duplicates, null values, and empty relations in the same way. } 
        \begin{lstlisting}
          SELECT name 
          FROM User, Member 
          WHERE User.uid = Member.uid;
        \end{lstlisting} 

        The following is correct assuming \texttt{User.uid} is a key. 
        \begin{lstlisting}
          SELECT name 
          FROM (SELECT DISTINCT User.uid, name) 
          FROM User, Member 
          WHERE User.uid = Member.uid); 
        \end{lstlisting}
      \end{example}

      \begin{example}[Correlated Subqueries]
        Look at this query where we want to select all group ids with name like Springfield and having less than some number of members. 
        \begin{lstlisting}
          SELECT gid 
          FROM Group, (SELECT gid, COUNT(*) AS cnt FROM Member GROUP BY gid) t 
          WHERE t.gid = Group.gid AND min_size > t.cnt 
          AND name LIKE 'Springfield%';
        \end{lstlisting}
        This is inefficient since for every \texttt{gid}, we are making an entire extra query to select the counts. This is called a \textbf{non-correlated} query since this subquery is being run independently for every run. It ends up computing the size of \textit{every} group, unlike the following one, where it filters out groups named Springfield first and then computes their size. 
        \begin{lstlisting}
          SELECT gid FROM Group 
          WHERE name LIKE 'Springfield%' 
          AND min_size > (SELECT COUNT(*) FROM Member WHERE Member.gid = Group.gid);
        \end{lstlisting}
      \end{example}

    \subsubsection{Search Strategies}  

      Given a set of operations we have to do, the number of permutations that we can apply these operations grows super-exponentially. The problem of finding the best permutation is called a \textbf{search strategy}. 

      \begin{example}[Left-Deep Plans]
        Say that we have relations $R_1, \ldots, R_n$ that we want to join. The set of all sequences in which we can join them is bijective to the set of all binary trees with leaves $R_i$. This grows super-exponentially, reading $30,240$ for $n = 6$. There are too many logical plans to choose from, so we must reduce this search space. Here are some heuristics. 
        \begin{enumerate}
          \item We consider only \textbf{left-deep} plans, in which case every time we join two relations, it is the outer relation in the next join.\footnote{Note that since the right/inner relation is the one that is being scanned, we want the right one to be smaller since for each block of the left relation, we are looping over all blocks of the right relation. Therefore, left-deep plans are much more efficient since we don't have to scan the huge relation from the disk multiple times. We can just send it directly to the next join.} 

          \begin{figure}[H]
            \centering 
            \includegraphics[scale=0.6]{img/left_deep.png}
            \caption{Left deep plans have a search space of only $n!$, which is better than before. } 
            \label{fig:left_deep}
          \end{figure} 

          \item We can consider a balanced binary tree, which can be parallel processed, but this causes more runtime on the CPU in sort-merge joins, you must materialize the result in the disk, and finally the search space of binary trees may be larger than that of the left-deep tree. 
        \end{enumerate}
      \end{example}

      Even left-deep plans are still pretty bad, and so optimizing this requires a bit of DP (dynamic programming), using \textit{Selinger's algorithm}. 

      \begin{algo}[Selinger's Algorithm]
        Given $R_1, \ldots, R_n$, we must choose a permutation from the $n!$ permutations. Say that the cost of the optimal join of a set $\mathbb{R}$ is $f(\mathbb{R})$. Note the recursive formula for some $S \subset [n]$. 
        \begin{equation}
          f(\{R_i\}_{i \in S}) = \min_i f(\{R_j\}_{j \in S, j \neq i}) + f(R_i, \bowtie_{j \in S, j \neq i} R_j) 
        \end{equation}
        Where we sum up the cost of getting the accumulated relation and add it to the additional cost of joining once more with $R_i$. Therefore, given the $R_i$'s, 
        \begin{enumerate}
          \item We compute all $f(\{R_i, R_j\})$ for $i < j$ (since $j > i$ requires us the larger one to be inner). 
          \item Then we apply the recursive formula for all 3-combinations and so on, until we get to $n$-combinations. 
        \end{enumerate}
      \end{algo}

      Given a certain logical plan, the DBMS tries to choose an optimal physical plan as we will see later. However, the globally optimal plan may not be achieved with a greedy approach of first choosing the optimal logical plan and then its optimal physical plan. Due to the sheer size of the search space, we tend to go for ``good'' plans rather than optimal ones. 

  \subsection{Physical Plan}

      The logical plan gives us an abstract view of the operations that we need to perform. It is mainly defined at the relational algebra or language level. But simply sorting or joining two relations is not done 
      in just one way (e.g. we can scan, hash, or sort in different ways). Optimizing the logical plan may or may not help in the runtime, since operations are dependent on the size of the intermediate inputs. 

      \begin{definition}[Physical Plan]
        The actual sub-decisions needed to execute these operations constitute the \textbf{physical plan}, which is the actual implementation including even more operations in between each node of the logical plan. Here are a few terms to know. 
        \begin{enumerate}
          \item \textit{On the fly}. The computations are done in memory and are not written back to disk. 
          \item \textit{(Index) Scan}. We scan for index value using a clustered/unclustered index on a B+ tree. 
          \item \textit{Sort}. Usually means external merge sort. 
          \item \textit{Filter}. Means the same as selection. 
        \end{enumerate}
      \end{definition}

      The difference between the logical and physical plan is that the logical plan represents \textit{what} needs to be done (which we write SQL) and not \textit{how} (which the DBMS chooses). Consider the two approaches. 

      \begin{example}[Query Plans]
        Consider the following SQL query:
        \begin{lstlisting}
          SELECT Group.title
          FROM User
          JOIN Member ON User.uid = Member.uid
          JOIN Group ON Member.gid = Group.gid
          WHERE User.name = 'Bart';
        \end{lstlisting}

        \begin{figure}[H]
          \centering 
          \begin{tikzpicture}[
              level distance=1.0cm,
              every node/.style={draw, rounded corners}, 
              sibling distance=4cm,
              edge from parent/.style={draw, latex-}  % Changed from -latex to latex-
          ]
          \node {$\pi_{\text{Group.title}}$}
              child {
                  node {$\bowtie_{\text{gid}}$}
                  child {
                      node {$\bowtie_{\text{uid}}$}
                      child {
                          node {$\sigma_{\text{name="Bart"}}$}
                          child {
                              node {User}
                          }
                      }
                      child {
                          node {Member}
                      }
                  }
                  child {
                      node {Group}
                  }
              };
          \end{tikzpicture}
          \caption{Logical Plan. We first take User, select Bart, and join it to Member over uid. Then we join it with Group on gid, and finally project the title attribute.} 
          \label{fig:logical_plan}
        \end{figure}

        \begin{figure}[H]
          \centering 
          \begin{tikzpicture}[
              level distance=1.0cm,
              sibling distance=4cm,
              every node/.style={draw, rounded corners},
              edge from parent/.style={draw, latex-}  % Changed from -latex to latex-
          ]
          \node {PROJECT (Group.title)}
              child {
                  node {MERGE-JOIN (gid)}
                  child {
                      node {SORT (gid)}
                      child {
                          node {MERGE-JOIN (uid)}
                          child {
                              node {FILTER (name = "Bart")}
                              child {
                                  node {SCAN (User)}
                              }
                          }
                          child {
                              node {SORT (uid)}
                              child {
                                  node {SCAN (Member)}
                              }
                          }
                      }
                  }
                  child {
                      node {SCAN (Group)}
                  }
              };
          \end{tikzpicture}
          \caption{Physical Plan. We first take User and do a scan before filtering/selecting tuples with Bart. We also scan Member and sort it by uid in order to prepare for merge-join. Once we merge-join over uid, we sort it again to prepare a second merge-join with Group (which we scan first). Once we do this, we finally project the title attribute. } 
          \label{fig:physical_plan}
        \end{figure}
      \end{example} 

    \subsubsection{SQL Rewrite}

      At the language level, SQL provides some APIs to force the DBMS to use a certain physical plan if desired. This requires expertise and should not be done by beginners, however. 

    \subsubsection{Cardinality Estimation}

      In the physical plan, we need to have a cost estimation for each operator. For example, we know that \texttt{SORT(gid)} takes $O(B(\text{input}) \cdot \log_M B(\text{input}))$, but we should find out what $B$, the number of blocks needed to store our input relation, is. To do this, we need the size of intermediate results through cardinality estimation.  

      Usually we cannot do quick and accurate cardinality estimation without strong assumptions, the first of which is uniformity of data. 

      \begin{example}[Selection with Equality Predicates]
        Suppose you have a relation $R$ with $|R| = 100,000$ tuples. Assume that it has an attribute $A$ taking integer values in $[50, 100)$ \textit{distributed uniformly}. Then, there are 50 distinct values, and when we want to do $\sigma_{A = a} (R)$, then we would expect it to return 
        \begin{equation}
          |\sigma_{A = a} (R)| = \frac{|R|}{|\pi_A(R)|} = 2000
        \end{equation}
        tuples.
      \end{example}

      The second assumption is \textit{independence} of the distributions over each attribute. 

      \begin{example}[Selection with Conjunctive Predicates]
        If we have the same relation $R$ with integer attributes $A \in [50, 100), B \in [10, 20)$ independently and uniformly distributed. Then, 
        \begin{equation}
          |\sigma_{A = a, B = b} (R) = \frac{|R|}{|\pi_A (R)| \cdot |\pi_B(R)|} = \frac{100,000}{50 \cdot 10} = 200
        \end{equation}
      \end{example}

      At this point, we are just using inclusion-exclusion principle and this becomes a counting problem. 

      \begin{example}[Negated, Disjunctive Predicates]
        We list these identities for brevity. The math is pretty simple. 
        \begin{equation}
          |\sigma_{A \neq a} (R)| = |R| \cdot \bigg( 1 - \frac{1}{|\pi_A (R)|} \bigg)
        \end{equation}
        and using I/E principle, we have 
        \begin{equation}
          |\sigma_{A = a \lor B = b} (R)| = |R| \cdot \bigg( \frac{1}{|\pi_A (R)|}  + \frac{1}{|\pi_B (R)|} - \frac{1}{|\pi_A (R)| \cdot |\pi_B(R)|} \bigg)
        \end{equation}
      \end{example}

      \begin{example}[Range Predicates]
        Range also works similarly, but only if we know the actual bounds of the attribute values.  
        \begin{equation}
          |\sigma_{A > a} (R)| = |R| \cdot \frac{\max(R.A) - a}{\max(R.A) - \min(R.A)}
        \end{equation}
      \end{example}

      Clearly, if we know that an attribute follows, say a Gaussian or a Poisson distribution, we can just calculate the difference in the CDFs and scale up by the relation size to get the approximate cardinality. I think this is what the professor refers to as \textit{histogram estimation}. 

      For joins, we need yet another assumption, called \textit{containment of value sets}. This means that if we are natural joining $R(A, B) \bowtie S(A, C)$, every tuple in the smaller (as in fewer distinct values for the join attribute $A$) joins with some tuple in the other relation. In other words, 
      \begin{equation}
        |\pi_A (R)| \leq |\pi_A (S)| \implies \pi_A (R) \subset \pi_A (S)
      \end{equation}
      which again is a very strong assumption in general but holds in the case of foreign key joins. 

      \begin{example}[Two Way Equi-Join] 
        With the containment assumption, we have 
        \begin{equation}
          |R \bowtie_{A} S| = \frac{|R| \cdot |S|}{\max( |\pi_A (R)|, |\pi_A(S)|)}
        \end{equation} 
        Think of this as looking at the cross product between the two relations, and then filtering out the actual tuples that don't belong there.  
      \end{example}

  \subsection{Exercises} 

    Let's do a more comprehensive exercise. 

    \begin{example}[Cost Estimation of Physical Query Plan]
      Say we have three relations 
      \begin{enumerate}
        \item \texttt{Student(\underline{sid}, name, age, addr)}. $T(S) = 10,000$ tuples, $B(S) = 1,000$ pages. 
        \item \texttt{Book(\underline{bid}, title, author)}. $T(B) = 50,000$ tuples, $B(S) = 5,000$ pages. 
        \item \texttt{Checkout(\underline{sid}, \underline{bid}, date)}. $T(C) = 300,000$ tuples, $B(C) = 15,000$ pages. 
      \end{enumerate}
      And say that the number of \texttt{author} attribute values in \texttt{Book} with $7 \leq \texttt{age} \leq 24$ is 500 tuples. There is an unclustered B+ tree index on \texttt{B.author}, a clustered B+ tree index on \texttt{C.bid}, and all index pages are in memory. Also we assume unlimited memory to simplify things. 
      \begin{figure}[H]
        \centering 
        \includegraphics[scale=0.4]{img/book_plan.png}
        \caption{We have the following physical plan.} 
        \label{fig:book_plan}
      \end{figure} 
      Okay, so let's do this. Note that since all index pages are in memory, we are storing the entire B+ tree in memory and don't need extra IOs to traverse it.  
      \begin{enumerate}
        \item[a)] For the selection, the author is an unclustered B+ tree. There are 50,000 tuples and 500 distinct authors. Since we're querying 1 author, by uniformity we would need to access $100$ book which may all be in their own disk page, so we need $100$ IOs.\footnote{If this was clustered, then each page can store $10$ tuples, so we actually need $10$ IOs. } We end up with an output relation of 100 tuples in memory, so we also have a cardinality of $100$. 
          \begin{equation}
            \mathrm{IO}(a) = \frac{T(B)}{500} = \frac{50,000}{500} = 100, \; \mathrm{Card}(a) = 100 
          \end{equation}

        \item[b)] For the projection on \texttt{bid}, we have already loaded in our relation in memory, so the IO cost is $0$. We are still working with 100 pages, so our cardinality is still $100$. 
          \begin{equation}
            \mathrm{IO} = 0, \; \mathrm{Card}(b) = 100 
          \end{equation} 

        \item[c)] Now we do a join with an index-nested loop join on \texttt{bid}. Recall that we want to use the value of the outer table \texttt{R.bid} to probe the index on the inner table \texttt{C.bid}, which is clustered. We already have our outer table in memory, and we use the index \texttt{bid} to probe our inner table \texttt{C}. For each of the 50,000 book tuples in \texttt{B}, there are 300,000 checkout tuples in \texttt{C}, meaning that there are about $300,000/50,000 = 6$ checkout per book. For each of the 100 book tuples in (a), we expect to get 6 checkouts per book. There are $300,000 / 15,000 = 20$ checkout tuples per page, so counting for page boundaries we assume that 6 tuples will fit in at most 2 pages (or maybe 1). Therefore, we have 
          \begin{equation}
            \mathrm{IO}(c) = 100 \cdot 2 = 200, \; \mathrm{Card}(c) = 100 \cdot 6 = 600
          \end{equation}

        \item[d)] This is done in memory so IO is $0$. Note that we have a total of 600 checkout/book tuples. Since this is a projection, the cardinality also remains the same. 
          \begin{equation}
            \mathrm{IO}(d) = 0, \; \mathrm{Card}(d) = 600 
          \end{equation}

        \item[e)] Now we have a block nested loop join. Since (d) is already in memory (on the fly), all we have to do is load all of \texttt{S} into memory (unlimited), which means our IO cost is $B(S) = 1000$. We are joining with the student relation, and since there is 1 student per checkout, our output relation is still 600 tuples long. 
          \begin{equation}
            \mathrm{IO}(e) = 1000, \; \mathrm{Card}(e) = 600
          \end{equation}

        \item[f)] Finally we select, and assuming that the ages are uniformly distributed, we expect $(20 - 12 - 1)/ (24 - 7 + 1) = 7/18$ of the relations to remain after selection. IO is $0$ since this is on the fly. 
          \begin{equation}
            \mathrm{IO}(f) = 0, \; \mathrm{Card}(f) = 600 \cdot \frac{7}{18} \approx 234
          \end{equation}

        \item[g)] Finally, we project onto name. IO is $0$ since on the fly. We are projecting on names and assuming we don't remove duplicates\footnote{Is this really a realistic assumption?} our output relation is still the same length. 
          \begin{equation}
            \mathrm{IO}(g) = 0, \; \mathrm{Card}(g) = 234
          \end{equation}
      \end{enumerate}
      The total cost is $1000 + 200 + 100 = 1300$ and the final cardinality is $234$. 
    \end{example}

\section{XML} 

    So far, we have talked about relational data, which is a type of \textbf{structured data} with a schema, attributes, tuples, etc along with their types and constraints. For example, just trying to add a new attribute to a relation may require you to add the attribute for each tuple if it cannot be null. Some \textbf{unstructured data} is just plaintext, which doesn't confirm to any schema, and is on the other extreme end. Some types of data in between is HTML, XML, or JSON, which we call \textbf{semi-structured}, which may contain sections, subsections, etc. 

    \begin{definition}[XML]
      The \textbf{extensible markup language (XML)} is a semi-structured data that is similar to HTML, where the data self-describes the structure.\footnote{The names and nesting of its tags describe its structure.} 
      \begin{enumerate}
        \item There are \textbf{tags} marked by a start (\texttt{<tag>}) and end (\texttt{</tag>}) tags. 
        \item An \textbf{element} is enclosed by a pair of start and end tags(\texttt{<t>...</t>}), and elements can be nested (\texttt{<t><r></r></t>}or empty(\texttt{<t></t>}.  
        \item Elements can also have \textbf{attributes} (\texttt{<book ISBN="...", price="80.00">}).\footnote{This is not the same attributes in relational databases.} Attributes must be unique, i.e. there cannot be duplicate attributes in a tag. 
      \end{enumerate}
      Note that there are some conventions that we must follow to get a \textbf{well-formed} XML document. First, the tags should be closed appropriately like parantheses matching, and we should not have the characters \texttt{<} or \texttt{>} as a part of the element. Rather, we should use \texttt{\&lt;} and \texttt{\&gt;}. 
    \end{definition}

    \begin{example}
      An example of XML data regarding a book is 
      \begin{lstlisting}
        <?xml version="1.0" encoding="UTF-8"?>
        <book>
            <title>The Great Gatsby</title>
            <author>
                <firstName>F. Scott</firstName>
                <lastName>Fitzgerald</lastName>
            </author>
            <published>
                <year>1925</year>
                <publisher>Charles Scribner's Sons</publisher>
            </published>
            <genre>Literary Fiction</genre>
            <price currency="USD">14.99</price>
            <inStock>true</inStock>
            <rating>4.5</rating>
        </book> 
      \end{lstlisting}
      This can be represented by a tree. 

      \begin{figure}[H]
        \centering 
        \includegraphics[scale=0.5]{img/tree_rep.png}
        \caption{Tree representation of an XML file.} 
        \label{fig:tree_rep}
      \end{figure}
    \end{example}

    Let's do a quick comparison of relations and XML. 
    \begin{enumerate}
      \item One advantage of XML is portability/exchangeability, since it self-describes itself, so all you really need is this text, unlike relations where you should also send the schema and other metadata. On the other hand, exchanging is problematic. 
      \item Second, its has flexibility to represent any information (e.g. structured, semi-structured, documents, etc.). Unlike a relation where each attribute of each tuple is atomic (meaning you can't place another relation in there), you can make a new XML tree. 
      \item Finally, since the data describes itself, you can change the schema easily. In contrast, the schema of a relational database is always fixed in advance and difficult to change. 
      \item Almost all major DBMS supports relations, and XML is often implemented as an add-on on top of relations. 
      \item Unlike the relational database where the order of the rows don't matter, the order does matter for XML. 
    \end{enumerate}
    
    \begin{definition}[DTD]
      Just because we don't need metadata does not mean that we cannot define one. This is done with the \textbf{document type definitions (DTD)}, which species the schema and constraints for XML just like relational databases. It has the following syntax, which should be written in the beginning of the XML file. 
      \begin{lstlisting}
        <!DOCTYPE root-element [
           <!ELEMENT element-name (content-specification)>
        ]> 
      \end{lstlisting} 
      Here are the common element specifications. 
      \begin{enumerate}
        \item \texttt{EMPTY}: Element has no content
        \item \texttt{ANY}: Element can contain any content
        \item \texttt{(\#PCDATA)}: Element contains parsed character data
        \item \texttt{Child elements}: Listed in parentheses
      \end{enumerate}
      with an example of a simple DTD being 
      \begin{lstlisting}
        <!DOCTYPE library [
           <!ELEMENT library (book+)>
           <!ELEMENT book (title, author, year)>
           <!ELEMENT title (#PCDATA)>
           <!ELEMENT author (#PCDATA)>
           <!ELEMENT year (#PCDATA)>
        ]> 
      \end{lstlisting}
    \end{definition}

    Really, this is like a tree directory structure, so to query data, it makes sense to try and traverse the paths. There are three major query languages for XML, which are 
    \begin{enumerate}
      \item \textbf{XPath}, which uses path expressions with conditions and are the building blocks of the rest of the standards. 
      \item \textbf{XQuery}, which is XPath plus a full-fledged SQL-like query language. 
      \item \textbf{XSLT}, or eXtensible Stylesheet Language Transformations and is the recommended style sheet language for XML. 
    \end{enumerate}

  \subsection{XPath}

    \begin{definition}[Basic XPath Constructs]
      The syntax for building a XPath is as follows: 
      \begin{enumerate}
        \item \texttt{/}: used as a separator between steps in a path (traversing down). 
        \item \texttt{name}: matches any child element with this tag name. 
        \item \texttt{*}: matches any child element 
        \item \texttt{@name}: matches the attribute with this name. 
        \item \texttt{@*}: matches any attribute 
        \item \texttt{//}: matches any descendent element of the current element itself. 
        \item \texttt{. }: matches current element.
        \item \texttt{.. }: matches parent element.\footnote{This is just like when we are navigating directories in a shell.}
      \end{enumerate}
      Note that these names are all case sensitive, and another important fact is that NULL evaluates to NOT (so if there are no elements/attributes), then a condition on that element/attribute will evaluate to NOT. 
    \end{definition}

    \begin{example}
      Let's look at this example, which shows bibliographies for different books. 
      \begin{lstlisting}
        <?xml version="1.0" encoding="UTF-8"?>
        <bibliography>
            <book ISBN="ISBN-10" price="70">
                <title>Foundations of Databases</title>
                <author>Abiteboul</author>
                <author>Hull</author>
                <author>Vianu</author>
                <publisher>Addison Wesley</publisher>
                <year>1995</year>
                <section>abc</section>
            </book>
            <book ISBN="ISBN-11" price="20">
                <title>DBSTS</title>
                <author>Ramakrishnan</author>
                <author>Gehrke</author>
                <publisher>Addison Wesley</publisher>
                <year>1999</year>
                <section>
                  <title>bruh</title>
                </section>
            </book>
            <report>
              <author>Muchang</author>
              <addon>
                <author>Jon</author>
              </addon>
            </report>
        </bibliography> 
      \end{lstlisting}
      Let's look at some XPaths. 
      \begin{enumerate}
        \item \texttt{/bibliography/book/author} gets all author element reachable via this path.
          \begin{lstlisting}
            <author>Abiteboul</author>
            <author>Hull</author>
            <author>Vianu</author>
            <author>Ramakrishnan</author>
            <author>Gehrke</author>
          \end{lstlisting}

        \item \texttt{/bibliography/book/title} gets all book titles reachable via this path. 
          \begin{lstlisting}
            <title>Foundations of Databases</title>
            <title>DBSTS</title>
          \end{lstlisting}

        \item \texttt{/bibliography/book/@ISBN} gets all book ISBN attributes (we need the \texttt{\@} to search the \textit{attribute}, not the element)
          \begin{lstlisting}
            ISBN="70"
            ISBN="ISBN:11"
          \end{lstlisting}

        \item \texttt{//title} returns all title elements, anywhere in the document. Note that the title of the section is also added. 
          \begin{lstlisting}
            <title>Foundations of Databases</title>
            <title>DBSTS</title>
            <title>bruh</title>
          \end{lstlisting}

        \item \texttt{//section/title} returns all section titles, anywhere in the document.
          \begin{lstlisting}
            <title>bruh</title>
          \end{lstlisting}

        \item \texttt{/bibliography/*/author} returns all authors in bibliography that is a grandchild of bibliography. Note that Muchang is retrieved but not Jon. 
          \begin{lstlisting}
            <author>Abiteboul</author>
            <author>Hull</author>
            <author>Vianu</author>
            <author>Ramakrishnan</author>
            <author>Gehrke</author>
            <author>Muchang</author>
          \end{lstlisting}

        \item \texttt{/bibliography/book[@price<50]} returns all books that are children of bibliography with attribute price less than 50. 
          \begin{lstlisting}
            <book ISBN="ISBN-11" price="20">
                <title>DBSTS</title>
                <author>Ramakrishnan</author>
                <author>Gehrke</author>
                <publisher>Addison Wesley</publisher>
                <year>1999</year>
                <section>
                  <title>bruh</title>
                </section>
            </book>
          \end{lstlisting}

        \item \texttt{/bibliography/book[author='Abiteboul']} returns all books that are children of bibliography that has an author tag children with element Abiteboul. 

        \item \texttt{/bibliography/book[author]} returns all books that are children of bibliography with some child author tag. 

        \item \texttt{/bibliography/book[40 <= @price and @price <= 50]} returns books with price attribute between 40 and 50. 

        \item \texttt{/bibliography/book[author='Abiteboul' or @price >=50]} returns books either authored by Abiteboul or price at least 50. Note that the first condition is on an element and the second is on an attribute. 

        \item \texttt{/bibliography/book[author='Abiteboul' or not (@price < 50)]} returns books authored by Abiteboul or price not below 50. Note that this is different from the query above since if a book does not have a price, then \texttt{\@price < 50} is null (which is NOT in XPath), so it returns the book. 
      \end{enumerate}
    \end{example}

    Hopefully you get a good feel for how XPaths work. Here's a trickier example. 

    \begin{example}[Conditions on Any vs All Attributes/Elements]
      Say you want to get all books with some price in the range $[20, 50]$. Then you would think of writing 
      \begin{lstlisting}
        /bibliography/book/[price>=20 and price <=50]
      \end{lstlisting}
      This may not work if we have a XML element of this form, with multiple prices. 
      \begin{lstlisting}
        <book> 
          <title>newbooktitle</title>
          <price>10</price>
          <price>70</price> 
        </book>
      \end{lstlisting} 
      since it compares is any element satisfies the conditions. Therefore, we can think of these conditions all as the \texttt{any} condition over child elements. If we want to use the \texttt{all} condition, we can write 
      \begin{lstlisting}
        /bibliography/book/[price[.>=20 and .<=50]]
      \end{lstlisting}

      Similarly, if we have the query \texttt{/bibliography/book[author='A' and author!='A']}, this is an any clause, so it will return all books with author A and another author not A. 
    \end{example}

    \begin{definition}[XPath Operators and Functions] 
      In conditions, we can use the following operations. 
      \begin{enumerate}
        \item Arithmetic: \texttt{x + y}, \texttt{x - y}, \texttt{x * y}, \texttt{x div y}, \texttt{x mod y} 
        \item \texttt{contains(x, y)} returns true if string \texttt{x} contains string \texttt{y} 
        \item \texttt{count(node-set)} counts the number of nodes in \texttt{node-set} 
        \item \texttt{position()} returns the \textit{context position} (i.e the position of the node amongst its siblings) 
        \item \texttt{last()} returns the \textit{context size} (roughly the size of the node-set)
        \item \texttt{name()} returns the tag name of the current element
      \end{enumerate}
    \end{definition}

    \begin{example}
      Some more queries is 
      \begin{enumerate}
        \item \texttt{/bibliography/book[count(section)<10]} returns books with fewer than 10 sections. 
        \item \texttt{//*[contains(name(), 'Ab')]} returns all elements whose tag names contain \texttt{Ab} (or \texttt{section}?) 
        \item \texttt{/bibliography/book/section[position()=1]/title} returns the title of the first section in each book. 
        \item \texttt{/bibliography/book/section[position()=last()]/title} returns the title of the last section in each book.
      \end{enumerate}
    \end{example}

  \subsection{XQuery}
    
    XQuery is a superset of the XPath, but it also encompasses FLWOR  expressions, quantified expressions, aggregation, sorting, etc. An XQuery expression can even return a new result XML document. Let's put the XML example file again for convenience. 

    \begin{lstlisting}
      <?xml version="1.0" encoding="UTF-8"?>
      <bibliography>
          <book ISBN="ISBN-10" price="70">
              <title>Foundations of Databases</title>
              <author>Abiteboul</author>
              <author>Hull</author>
              <author>Vianu</author>
              <publisher>Addison Wesley</publisher>
              <year>1995</year>
              <section>abc</section>
          </book>
          <book ISBN="ISBN-11" price="20">
              <title>DBSTS</title>
              <author>Ramakrishnan</author>
              <author>Gehrke</author>
              <publisher>Addison Wesley</publisher>
              <year>1999</year>
              <section>
                <title>bruh</title>
              </section>
          </book>
      </bibliography> 
    \end{lstlisting}

    \begin{example}
      For now, let's start with a simple XQuery based on XPath. To find all books, we can write the following, where the first \texttt{doc} refers to the specific document to query.
      \begin{lstlisting}
        <result>{
          doc("bib.xml")/bibliography/book
        }</result>
      \end{lstlisting}
      Note that like Python string interpolation, text outside of \texttt{\{\}} are copied to output verbatim, while those inside are evaluated and replaced by the result. We can use conditionals the same way. 
      \begin{lstlisting}
        <result>{
          doc("bib.xml")/bibliography/book[@price<50]
        }</result>
      \end{lstlisting}
      which will return 
      \begin{lstlisting}
        <book ISBN="ISBN-11" price="20">
            <title>DBSTS</title>
            <author>Ramakrishnan</author>
            <author>Gehrke</author>
            <publisher>Addison Wesley</publisher>
            <year>1999</year>
            <section>
              <title>bruh</title>
            </section>
        </book>
      \end{lstlisting}
    \end{example}

    \begin{definition}[FLWOR Expressions]
      The fundamental building block of XQuery is the FLWOR expression, which stands for:
      \begin{enumerate}
        \item \texttt{FOR}: Iterates over sequences
        \item \texttt{LET}: Binds variables to values
        \item \texttt{WHERE}: Filters the tuples
        \item \texttt{ORDER BY}: Sorts the results
        \item \texttt{RETURN}: Constructs the result
      \end{enumerate}
      Note that only the \texttt{RETURN} clause is mandatory; all others are optional.
    \end{definition}

    \begin{example}[FLWOR with Loops and Conditionals]
      To retreive the titles of books published before 2000, together with their publisher, we can write either of the following. The logic is pretty self explanatory. 

      \noindent\begin{minipage}{.5\textwidth}
      \begin{lstlisting}[]{Code}
        <result>{
          for $b in /bibliography/book 
          let $p := $b/publisher 
          where $b/year < 2000 
          return 
            <book>
            { $b/title }
            { $p }
            </book>
        }</result>
      \end{lstlisting}
      \end{minipage}
      \hfill
      \begin{minipage}{.49\textwidth}
      \begin{lstlisting}[]{Output}
        <result>{
          for $b in /bibliography/book[year<2000]
          return 
            <book>
            { $b/title }
            { $p/publisher }
            </book>
        }</result>
        .
        .
      \end{lstlisting}
      \end{minipage}
      which will return 
      \begin{lstlisting}
        <result>
          <book>
            <title>Foundations of Databases</title>
            <publisher>Addison Wesley</publisher>
          </book>
          <book>
            <title>DBSTS</title>
            <publisher>Addison Wesley</publisher>
          </book>
        </result> 
      \end{lstlisting}
      Note that \texttt{\$p} may be assigned to a set of elements. It does not have to be one element. 
    \end{example}

    Just like XPath, a conditional expression (with where) only checks the \texttt{any} condition. 
    
    \begin{example}[Nested Loops and List Comprehension]
      We can also use nested loops to solve the query above, but note the logic. If a book of price < 2000 has 2 publishers, then the book may be returned 2 times. On the other hand, if a book has a price < 2000 but has no publishers, then a null is really a false, so that book will not be returned. 

      \noindent\begin{minipage}{.5\textwidth}
        \begin{lstlisting}[]{Code}
          <result>
          {
            for $b in /bibliography/book,
            $p in $b/publisher
            where $b/year < 2000
            return 
              <book>{ $b/title }{ $p }</book>
          }
          </result> 
          .
          .
          .
          .
          .
        \end{lstlisting}
        \end{minipage}
        \hfill
        \begin{minipage}{.49\textwidth}
        \begin{lstlisting}[]{Output}
          <result>
            <book>
              <title>Foundations of Databases</title>
              <publisher>Addison Wesley</publisher>
            </book>
            <book>
              <title>Foundations of Databases</title>
              <publisher>Springer</publisher>
            </book>
            <book>
              <title>DBSTS</title>
              <publisher>Addison Wesley</publisher>
            </book>
          </result>          
        \end{lstlisting}
      \end{minipage}

      If we use a comprehension statement, we can let \texttt{\$b} be all the books and try to parse them element by element where its year is < 2000. This is also wrong since the where clause is an any expression, so if at least 2 book has year < 2000, then it will return all the books. In fact, it doesn't even return it in the right format, since it returns all the book titles first and then all the publishers. 

      \noindent\begin{minipage}{.5\textwidth}
        \begin{lstlisting}[]{Code}
          <result>
          {
            let $b := /bibliography/book
            where $b/year < 2000
            return 
              <book>
                { $b/title }
                { $b/publisher }
              </book>
          }
          </result>          
        \end{lstlisting}
        \end{minipage}
        \hfill
        \begin{minipage}{.49\textwidth}
        \begin{lstlisting}[]{Output}
          <result>
            <book>
              <title>Found of Databases</title>
              <title>DBSTS</title>
              <publisher>Addison Wesley</publisher>
            </book>
          </result>
          .
          .
          .
          .
        \end{lstlisting}
      \end{minipage}
    \end{example}

    Now that we went over FLWOR, let's talk about how we can do joins. 

    \begin{example}[Explicit Join]
      To find all pairs of books that have common authors, we can write 
      \begin{lstlisting}
        <result>
        {
          for $b1 in //book
          for $b2 in //book
          where $b1/author = $b2/author
          and $b1/title > $b2/title
          return
            <pair>
              { $b1/title }
              { $b2/title }
            </pair>
        }
        </result> 
      \end{lstlisting}
      Remember that since the where is an any condition, this works to our advantage now. We also use a string comparison of the authors and titles to remove duplicates. This gives us the result. 
      \begin{lstlisting}
        <result>
          <pair>
            <title>Foundations of Databases</title>
            <title>DBSTS</title>
          </pair>
        </result> 
      \end{lstlisting}
    \end{example}

    We will stop here, but there are more features such as subqueries, existential (some) vs universal (all) queries, aggregation, conditional, etc. 

  \subsection{Converstion of XML to Relational Data} 
    
    Relational to XML is trivial, but XML to relational isn't really since there can be different data types that are siblings (e.g. books vs articles, vs papers all under bibliography), there may be repeats of the same tag, and there are both child elements and attributes. 
    
    The most trivial thing to do is to store the entire XML in a column (called a CLOB, character large Object type), but this isn't useful since it first does not satisfy the condition that each element must be atomic in a relation. There are two alternatives. 
    \begin{enumerate}
      \item \textbf{Schema-Oblivious Mapping} takes a well-formed XML and converts it to a generic relational schema. In here, there are more subtypes. 
        \begin{enumerate}
          \item \textbf{Node/edge based mapping} for graphs. 
          \item \textbf{Interval based mapping} for trees.
          \item \textbf{Path based mapping} for trees. 
        \end{enumerate}
      \item \textbf{Schema-Aware Mapping} takes a valid XML and converts it to a special relational schema based on DTD. 
    \end{enumerate} 

    Let's go over the node/edge based mapping. This is quite intuitive since we can visualize a XML structure as a directed graph. 
    \begin{figure}[H]
      \centering 
      \includegraphics[scale=0.25]{img/node_based.png}
      \caption{} 
      \label{fig:node_based}
    \end{figure}

    We can convert the XML to the following 4 relations. We also put the bibliography example to make it easier to follow along. 
    \begin{enumerate}
      \item \textbf{Element(\underline{eid}, tag)}. All elements will have a certain id with their tag type. Note that elements can have text inside of them, but we will consider this a ``child'' of the element, stored in the \texttt{Text} relation. 

        \begin{table}[H]
          \centering
          \caption{Element Table}
          \begin{tabular}{|l|l|}
            \hline
            \textbf{eid} & \textbf{tag} \\
            \hline
            e0 & bibliography \\
            e1 & book \\
            e2 & title \\
            e3 & author \\
            e4 & author \\
            e5 & author \\
            e6 & publisher \\
            e7 & year \\
            \hline
          \end{tabular}
          \caption{}
          \label{tab:}
        \end{table}

      \item \textbf{Attribute(eid, attrName, attrValue)}. All attributes will need to have their name and type, along with which eid that they are a part of. Note that there is only one functional dependency \texttt{(eid, attrName) -> attrValue}.

        \begin{table}[H]
          \centering
          \caption{Attribute Table}
          \begin{tabular}{|l|l|l|}
          \hline
          \textbf{eid} & \textbf{attrName} & \textbf{attrValue} \\
          \hline
          e1 & ISBN & ISBN-10 \\
          e1 & price & 80 \\
          \hline
          \end{tabular}
        \end{table}

      \item \textbf{ElementChild(eid, pos, child)}. Each element will have a children, with pos referring to the position of the children. Child references either \texttt{Element(eid)} or \texttt{Text(tid)}. 

        \begin{table}[H]
          \centering
          \caption{ElementChild Table}
          \begin{tabular}{|l|l|l|}
          \hline
          \textbf{eid} & \textbf{pos} & \textbf{child} \\
          \hline
            e0 & 1 & e1 \\
            e1 & 1 & e2 \\
            e1 & 2 & e3 \\
            e1 & 3 & e4 \\
            e1 & 4 & e5 \\
            e1 & 5 & e6 \\
            e1 & 6 & e7 \\
            e2 & 1 & t0 \\
            e3 & 1 & t1 \\
            e4 & 1 & t2 \\
            e5 & 1 & t3 \\
            e6 & 1 & t4 \\
            e7 & 1 & t5 \\
          \hline
          \end{tabular}
        \end{table}

      \item \textbf{Text(\underline{tid}, value)}. All text in an element, with an id value (which cannot be the same as any eid) and the actual text in the value. 

        \begin{table}[H]
          \centering
          \caption{Text Table}
          \begin{tabular}{|l|l|}
          \hline
          \textbf{tid} & \textbf{value} \\
          \hline
          t0 & Foundations of Databases \\
          t1 & Abiteboul \\
          t2 & Hull \\
          t3 & Vianu \\
          t4 & Addison Wesley \\
          t5 & 1995 \\
          \hline
          \end{tabular}
        \end{table}
    \end{enumerate}
    Note that we need to invent a lot of ids and need indices for efficiency. 


    Given this, we can write the equivalent SQL queries from these XPath queries. 
    \begin{enumerate}
      \item \texttt{//title} 
        \begin{lstlisting}
          SELECT eid FROM Element WHERE tag='title';
        \end{lstlisting}

      \item \texttt{//section/title} 
        \begin{lstlisting}
          SELECT e2.eid 
          FROM Element e1, ElemtnChild c, Element e2 
          WHERE e1.tag = 'section' 
          AND e2.tag = 'title' 
          AND e1.eid = c.eid 
          AND c.child = e2.eid;
        \end{lstlisting}
        Therefore, path expression becomes joins. The number of joins is proportional to the length of the path expression. 
    \end{enumerate}

\section{JSON}

  NoSQL just stands for not SQL or not relational, like XML or \textbf{JSON}.\footnote{I think it's called this because it's literally how JS objects are stored and printed.} This relaxes some constraints and may be more flexible/efficient, with some popular data stores being MongoDB, CouchDB, Dynamo, etc. They are designed to scale simple OLTP (online transaction processing) style application loads and provide good horizontal scalability. An example of where this is used is in pretty much all blockchains. Every transaction and block is accessible in a JSON format on say \texttt{etherscan.io}. 

  \begin{definition}[JSON]
    A \textbf{JSON}, short for \textbf{JavaScript Object Notation}, data model is an object with the following properties: 
    \begin{enumerate} 
      \item At the top level, it is an array of \textit{objects}. 
      \item Each object contains a set of key-value pairs of form \texttt{\{key : value\}}, where key (attribute) names should be unique within an object. 
      \item It supports most primitive types (numbers, strings, double, booleans are stored in quotes e.g. \texttt{"true"}), along with arrays \texttt{[...]}, and finally \textbf{objects} \texttt{\{...\}} (which defines a recursive structure). 
      \item The order is unimportant. 
      \item You can't comment in JSON files. 
    \end{enumerate}
  \end{definition}

  \begin{example}[Example JSON]
    Here is an example with users, groups, and members. 
    \begin{lstlisting}
      {
        "users": [
          {"uid": 1, "name": "Bart"},
          {"uid": 2, "name": "Lisa"}
        ],
        "groups": [
          {"gid": 101, "title": "Skateboarding Club"},
          {"gid": 102, "title": "Chess Club"}
        ],
        "members": [
          {"uid": 1, "gid": 101},
          {"uid": 1, "gid": 102},
          {"uid": 2, "gid": 102}
        ]
      } 
    \end{lstlisting}
  \end{example}

  \begin{definition}[MongoDB Database]
    A database has collections of similarly structured documents, similar to tables of records as opposed to one big XML document that contains all data.  
    \begin{enumerate}
      \item \textbf{Database} is a list of collections. (analogous to a database)
      \item \textbf{Collection} is a list a documents (analogous to a table)
      \item \textbf{Document} is a JSON object (analogous to a row/tuple)
    \end{enumerate}
    MongoDB actually stores this data with \textbf{BSON} (Binary JSON), which is the binary encoding of it. 
  \end{definition}  

  MongoDB provides a rich set of operations for querying and manipulating data. 

  \begin{definition}[find()]
    The \texttt{find()} operation is MongoDB's basic query mechanism. It returns a cursor to the matching documents.
    \begin{enumerate}
      \item Takes a query document that specifies the selection criteria
      \item Can include projection to specify which fields to return
      \item Supports comparison operators like \texttt{\$eq}, \texttt{\$gt}, \texttt{\$lt}
      \item Can query nested documents and arrays
    \end{enumerate}
  \end{definition}

  \begin{example}[Basic Query]
    Find all users named "Bart":
    \begin{lstlisting}
      db.users.find({"name": "Bart"})
    \end{lstlisting}
    Find users with uid greater than 1:
    \begin{lstlisting}
      db.users.find({"uid": {$gt: 1}})
    \end{lstlisting}
  \end{example}

  \begin{definition}[sort()]
    The \texttt{sort()} method orders the documents in the result set.
    \begin{enumerate}
      \item Takes a document specifying the fields to sort by
      \item Use 1 for ascending order, -1 for descending
      \item Can sort by multiple fields
      \item Applied after the query but before limiting results
    \end{enumerate}
  \end{definition}

  \begin{example}[Sorting]
    Sort users by name in ascending order:
    \begin{lstlisting}
      db.users.find().sort({"name": 1})
    \end{lstlisting}
  \end{example}


  \begin{definition}[Aggregation Pipeline]
    An aggregation pipeline consists of stages that transform sequences of documents. Each stage performs an operation on the input documents and passes the results to the next stage. Despite its name, it handles more than just aggregation operations.
  \end{definition}

  MongoDB supports several types of pipeline stages:

  \begin{enumerate}
    \item \textbf{Selection and Filtering} (\texttt{\$match})
    \begin{itemize}
        \item Filters documents based on specified conditions
        \item Similar to \texttt{find()} but within the pipeline context
    \end{itemize}

    \item \textbf{Projection} (\texttt{\$project})
    \begin{itemize}
        \item Reshapes documents by including, excluding, or transforming fields
        \item Can create computed fields
    \end{itemize}

    \item \textbf{Sorting} (\texttt{\$sort})
    \begin{itemize}
        \item Orders documents based on specified fields
        \item Equivalent to the \texttt{sort()} method
    \end{itemize}

    \item \textbf{Grouping} (\texttt{\$group})
    \begin{itemize}
        \item Groups documents by a specified expression
        \item Supports aggregation operators:
            \begin{itemize}
                \item \texttt{\$sum}: Calculates numeric totals
                \item \texttt{\$push}: Accumulates values into an array
            \end{itemize}
    \end{itemize}

    \item \textbf{Document Transformation}
    \begin{itemize}
        \item \texttt{\$project}/\texttt{\$addFields}: Adds computed fields
        \item \texttt{\$unwind}: Deconstructs arrays into individual documents
        \item \texttt{\$replaceRoot}: Promotes an embedded document to the top level
        \item Array operators:
            \begin{itemize}
                \item \texttt{\$map}: Applies an expression to each array element
                \item \texttt{\$filter}: Selects array elements matching a condition
            \end{itemize}
    \end{itemize}

    \item \textbf{Joining} (\texttt{\$lookup})
    \begin{itemize}
        \item Performs left outer joins with other collections
    \end{itemize}
  \end{enumerate}

  \begin{example}[Basic Pipeline]
    A pipeline that finds users in the "Chess Club" and sorts them by name:
    \begin{lstlisting}
      db.members.aggregate([
          {$lookup: {
              from: "users",
              localField: "uid",
              foreignField: "uid",
              as: "user"
          }},
          {$match: {"gid": 102}},
          {$sort: {"user.name": 1}}
      ])
    \end{lstlisting}
  \end{example}

  \begin{example}[Complex Pipeline]
    A pipeline using multiple stages to group and transform data:
    \begin{lstlisting}
      db.members.aggregate([
          {$group: {
              _id: "$gid",
              members: {$push: "$uid"},
              count: {$sum: 1}
          }},
          {$lookup: {
              from: "groups",
              localField: "_id",
              foreignField: "gid",
              as: "group_info"
          }},
          {$project: {
              _id: 0,
              group: {$arrayElemAt: ["$group_info.title", 0]},
              member_count: "$count",
              members: 1
          }}
      ])
    \end{lstlisting}
  \end{example}
  
  The pipeline stages must be carefully ordered as each stage's output becomes the input for the next stage. For optimal performance:
  \begin{enumerate}
    \item Use \texttt{\$match} early to reduce the number of documents
    \item Place \texttt{\$project} and \texttt{\$unwind} before \texttt{\$group} when possible
    \item Consider memory limitations when using \texttt{\$sort}
  \end{enumerate}

  \begin{definition}[\$lookup]
    The \texttt{\$lookup} operation performs a left outer join to another collection.
    \begin{enumerate}
      \item Must be used within an aggregation pipeline
      \item Specifies foreign collection to join with
      \item Defines local and foreign fields to join on
      \item Results stored in an array field
    \end{enumerate}
  \end{definition}

  \begin{example}[Join Operation]
    Join members with users:
    \begin{lstlisting}
      db.members.aggregate([
        {$lookup: {
          from: "users",
          localField: "uid",
          foreignField: "uid",
          as: "user_info"
        }}
      ])
    \end{lstlisting}
  \end{example}

  \begin{definition}[Aggregation Operators]
    Special operators used within aggregation pipelines for computations.
    \begin{enumerate}
      \item \texttt{\$sum}: Calculates sum of numeric values
      \item \texttt{\$push}: Adds value to an array
      \item \texttt{\$avg}: Calculates average
      \item \texttt{\$first}/\texttt{\$last}: Returns first/last value in group
    \end{enumerate}
  \end{definition}

  \begin{example}[Aggregation Operators]
    Calculate total members and collect group IDs:
    \begin{lstlisting}
      db.members.aggregate([
        {$group: {
          _id: "$uid",
          total: {$sum: 1},
          groups: {$push: "$gid"}
        }}
      ])
    \end{lstlisting}
  \end{example}

\section{Transactions} 

    So far, we've had one query/update on one machine, but this is not the case in reality. In modern systems, you have users interacting with a database all the time and databases may be prone to failure. These two situations requires us to develop a safer way of interacting with the database. 

    \begin{example}[Simultaneous Interaction]
      In an airline, say that we have two parties A and B trying to book the same seat. A looks at the website, which queries the tuple representing the seat. B does the same. A and B then both book it at the same time, sending an update request to the database, causing an overbooking. 

      In a bank, say that we have some events that must be recorded in a database. Say that $A = 0$ and $B = 100$. 
      \begin{enumerate}
        \item T1. B sends \$100 to A. So $A \mapsto A + 100, B \mapsto B - 100$. 
        \item T2. The bank sends all users an interest of 6\%. $A \mapsto 1.06 A, B \mapsto 1.06B$. 
      \end{enumerate}
      This is sensitive to order and perhaps at the end we expect $A = 100$. Consider the following cases. 
      \begin{enumerate} 
        \item $A \mapsto A + 100, B \mapsto B - 100, A \mapsto 1.06A, B \mapsto 1.06B$. This is fine. 
        \item $A \mapsto A + 100, A \mapsto 1.06A, B \mapsto 1.06B, B \mapsto B - 100$. This is not fine since both A and B got interest and the bank lost \$6. 
      \end{enumerate}
    \end{example}

    \begin{example}[Database Crash]
      If $A \mapsto A + 100$ happened first and the bank crashed, then the system would find that $A$ just gained \$100! This is clearly not good, so we must undo what we have done so far. 
    \end{example}

    These two examples hint at some nice properties that we want in our database. 
    \begin{enumerate}
      \item \textit{Serializability}. The first example shows that we do not like parallelism and rather we want things to run \textit{serially} in a way such that T1 and T2 occur separately, like a mutex lock. In practice, there are so many requests that parallelism is a must, so we must find ways to not run serially, but to modify our parallel computing to make it serializable, as if it is running serially.  
      \item \textit{Atomicity}. The second example shows that we want these operations to be \textit{atomic}, i.e. it is fully done or not done at all. That is, we do not want to update the disk (which preserves its state upon powering off) until  T1 is completely finished. A simple solution is to do all the operations in memory (which is wiped upon crashing anyways) and then \textit{commit} (announce that it is done) these changes. \footnote{Committing is not the same as disk writing, actually. It is not necessarily the case that one follows the other, so you can commit before writing and write before committing. There can also be some completed transactions with updated data still in memory and therefore lost in crash.} 
    \end{enumerate} 

    Great, so let's review some actions that our DBMS should support. To read/write some tuple or relation $X$, the disk block containing $X$ must first be brought into memory. $X$ is read/written in memory. If it is written, it is called a \textbf{dirty block}. Finally, this dirty block must be flushed to disk. 

    \begin{definition}[Actions]
      Some examples of actions that can occur in a schedule is: 
      \begin{enumerate}
        \item \texttt{READ}: read in a memory block from disk
        \item \texttt{WRITE}: write/flush out a dirty memory block 
        \item \texttt{COMMIT}
        \item \texttt{ABORT} 
      \end{enumerate}
      Note that committing is not the same as writing! 
    \end{definition}

    \begin{definition}[Transaction]
      The solution to both serializability and atomicity is to use \textbf{transactions}, which is a collection of one or more operations, called \textbf{actions}, on the database that must be executed atomically as a whole. At the end of every transaction, the DBMS always puts either a commit (indicating successful completion) or abort signal. 
    \end{definition} 

    \begin{definition}[Schedule]
      Now that we've cleared up on transactions, a schedule is simply a sequence of actions gotten from interweaving the transactions on the DBMS. It must obviously be consistent, and two actions from the same transaction T must appear in the schedule in the same order that they appear in T. Some terminology to compare schedules: 
      \begin{enumerate}
        \item \textbf{Serial Schedule}. No interweaving of transactions. 
        \item \textbf{Equivalent Schedule}. Two schedules that have the same effect on the database state after completion. 
        \item \textbf{Serializable Schedule}. A schedule that is equivalent to a serial schedule. 
      \end{enumerate}
      In a schedule, the transactions are usually written in $T, S, \ldots$ and they act on \textbf{elements} (data, tuples or relations, on disk or memory). 
    \end{definition}

    \begin{example}[Bank Schedule]
      For the bank example above, we can construct a serial schedule $S_1$. 
      \begin{lstlisting}
        T1: R(A) W(A) R(B) W(B) C(T1)
        T2:                           R(A) W(A) R(B) W(B) C(T2)

        T:  R1(A) W1(A) R1(B) W1(B) C(T1) R2(A) W2(A) R2(B) W2(B) C(T2)
      \end{lstlisting}
      and a serializable schedule $S_2$. 
      \begin{lstlisting}
        T1: R(A) W(A)           R(B) W(B) C(T1) 
        T2:           R(A) W(A)                 R(B) W(B) C(T2)
        T:  R1(A) W1(A) R2(B) W2(B) R1(B) W1(B) C(T1) R2(B) W2(B) C(T2)
      \end{lstlisting}
      We can verify that $S_1 \equiv S_2$ since in the second transaction, we always read in A after it is written by the first transaction. However, the following is not a serializable schedule since T2 reads the unmodified A block from disk before flushed by T1.  
      \begin{lstlisting}
        T1: R(A)                           W(A) R(B) W(B) C(T1)
        T2:      R(A) W(A) R(B) W(B) C(T2)
      \end{lstlisting}
    \end{example} 

    These properties are a subset of the general ones described here. In the next sections, we will go over implementing features that ensure each of these properties. 

    \begin{definition}[ACID]
      These properties are a subset of the \textbf{ACID} properties.
      \begin{enumerate}
        \item \textit{Atomicity}. A user can think of a transaction as always executing all actions in one stop or not executing any at all. In an abort, this can be undone by storing recovery logs and going through the history to undo. 
        \item \textit{Consistency}. Each transaction, when run by itself with no concurrent execution of other actions, must preserve the consistency of the database.\footnote{e.g. if you transfer money from one account to another, the total amount in circulation still remains the same. The constraints must also be maintained. } 
        \item \textit{Isolation}. A user should be able to understand a transaction without considering the effect of any other concurrently running transaction. This allows us to have \textit{concurrency control}, which handles multiple transactions by interweaving them in a serializable way (like the bank example we saw above), and sometimes they may ensure isolation with locks.  

        \item \textit{Durability}. Once the DBMS informs the user that a transaction is completed, its effect should persist.\footnote{Even if the system crashes before writing to disk, we can recover it by using the history logs and redoing them.}
      \end{enumerate}

      It turns out that making one stricter usually leads to a decrease in performance in the other, so we should maintain some balance. C/I are usually improved in conjunction with concurrency controls, and A/D are improved using recovery. 
    \end{definition}

  \subsection{Consistency: Serizability and Precedence Graph}

    Okay, so we've seen in the bank schedule example that serial schedules are trivial to construct and ideally we want to construct serializable schedules. How do we detect them? Well it turns out checking for serializability is NP-complete, so we must resort to something called \textit{conflict-serializability}, which is a stronger condition but can be solved in polynomial time. Conflict-serializability assures the absence of \textit{conflicts}, which are certain properties of schedules that result in a corrupt schedule.   

    \begin{definition}[Conflict]
      When we interweave actions, \textbf{conflicts} cause schedules to be not equivalent to the serial schedule. There are 3 types of conflicts. 
      \begin{enumerate}
        \item Write-Read (WR): $T_1$ writes $A$ and then $T_2$ reads $A$ before $T_1$ commits. This causes reading uncommitted (dirty) data. 
        \item Read-Write (RW): $T_1$ reads $A$ and then $T_2$ writes $A$ before $T_1$ commits. This causes unrepeatable reads. 
        \item Write-Write (WW): $T_1$ writes $A$ and then $T_2$ writes $A$ before $T_1$ commits. This overwrites uncommitted data or lost updates. 
        \item 
      \end{enumerate}
      Note that RR (read read) has no conflicts since there are no writes. 
    \end{definition}

    We can make a \textit{precedence graph}. 

    \begin{definition}[Precedence Graph]
      A \textbf{precedence graph} contains 
      \begin{enumerate}
        \item a node for each transaction 
        \item a directed edge from $T_i$ to $T_j$ if an operation of $T_i$ precedes and conflicts with an operation of $T_j$ in the schedule. 
      \end{enumerate}
    \end{definition} 

    \begin{example}[Precedence Graph]
      Consider the following schedule between 2 transactions. 
      \begin{lstlisting}
        R1(A) R2(A) W1(A) C1 C2
      \end{lstlisting}
      Then, our precedence graph $G(V, E)$ is defined $V = \{T_1, T_2\}$ and $E = \{(T_2, T1)\}$ since there is a RW conflict. Now consider the following. 
      \begin{lstlisting}
        R1(A) R2(A) W1(A) W2(A) C1 C2
      \end{lstlisting}
      Its precedence graph has both edges $(T_1, T_2)$ due to \texttt{R1(A)...W2(A)} and $(T_2, T_1)$ due to \texttt{R2(A) W1(A)}. 
    \end{example}

    \begin{theorem}[Conflict-Serializability]
      A schedule is \textbf{conflict-serializable} (which implies serizability) if its precedence graph is acyclic. 
    \end{theorem} 

    \begin{example}[Not Serializable]
      The following schedule is not conflict serializable since $T_1 \mapsto T_2$ from the transactions on $A$ and $T_2 \mapsto T_1$ from the transactions on $B$. 
      \begin{lstlisting}
        T1: R(A) W(A)                     R(B) W(B)
        T2:           R(A) W(A) R(B) W(B) 
      \end{lstlisting}
    \end{example}

    We established a method to check if a certain schedule $S_1$ is conflict-serializable. Given two serial schedules $S_1, S_2$, we can check if they are equivalent serial schedules as well. 

    \begin{theorem}[Equivalent Serial Schedules]
      Given 2 serial schedules $S_1, S_2$, let $G_1, G_2$ be their precedence graphs. If $G_1$ and $G_2$ have a common existing topological ordering, then $S_1 \equiv S_2$. 
    \end{theorem} 

    \begin{theorem}[Swapping Non-Conflicting Actions Doesn't Change Precedence Graph]
      You can also swap adjacent non-conflicting actions to reach an equivalent serial schedule. 

      \begin{figure}[H]
        \centering 
        \includegraphics[scale=0.4]{img/swap_serial.png}
        \caption{You can first swap W(B) and R(C) since they act on different elements. You keep swapping to push the actions of T1 up as far as possible, and if you can get a serial schedule, you are good. Swapping non-conflicting actions does not affect the precedence graph of the schedule(?)} 
        \label{fig:swap_serial}
      \end{figure}
    \end{theorem} 

  \subsection{Isolation and Concurrency Control} 

      We've seen that ideally, we would like to have a schedule that is conflict-serializable. The avoiding of conflicts is just one problem that we must avoid when implementing isolation protocols, and in here we will focus on implementing a protocol that enforces three properties: 
      \begin{enumerate}
        \item conflict-serializability 
        \item avoiding cascading rollbacks 
        \item recoverability
      \end{enumerate}
      While looking at precedence graphs allow us to compare already existing schedules, it does not give us instructions to \textit{create} one. We can do this using locks. 

    \subsubsection{Locks}

      Let's introduce locks in the basic sense. 

      \begin{definition}[Locks]
        The rules for \textbf{locks} are as follows: 
        \begin{enumerate}
          \item If a transaction wants to read an object, it must request a \textbf{shared lock} (S mode) on that object. 
          \item If a transaction wants to read/modify an object, it must first request an \textbf{exclusive lock} (X mode) on that object. 
          \item You can allow one exclusive lock, or multiple shared locks. 
        \end{enumerate}
      \end{definition}

      \begin{example}[Basic Locking]
        An implementation of basic locking is shown below. 
        \begin{lstlisting}
          T1: X(A) R(A) W(A) U(A)                                         X(B) R(B) W(B) U(B) 
          T2:                     X(A) R(A) W(A) U(A) X(B) R(B) W(B) U(B) 
        \end{lstlisting}
      \end{example}

      However, notice that basic locking does not really enforce conflict-serializability. Naive locks still allow for this non-serializable schedule to occur. Therefore, we can do two-phase locking. 

      \begin{definition}[Two-Phase Locking (2PL)]
        The basic rule is that \textit{for each individual transaction}, all lock requests precede all unlock requests. Therefore, every transaction is divided into a phase of obtaining locks and releasing locks.   
      \end{definition}

      \begin{example}[2PL]
        Look at this previous schedule. 
        \begin{lstlisting}
          T1: R(A) W(A)                     R(B) W(B)
          T2:           R(A) W(A) R(B) W(B) 
        \end{lstlisting}
        \begin{enumerate}
          \item T1 will first requests a $X$ lock on $A$. 
          \item Then T2 wants to write to $A$ so it must have the lock. In order to do this T1 must unlock $A$, but as soon as it unlocks $A$, it cannot request for any more locks. What can it do? It has no choice but to request for a lock on $B$ \textit{now}, and then unlock $A$ to give to T2. 
          \item T2 unlocks $A$, reads/writes it, and then wants to unlock $A$, but again it must request $B$ before unlocking $A$. However, T1 already holds the lock on $B$ and cannot unlock it since it must write $B$ later. 
        \end{enumerate}
        \begin{lstlisting} 
          T1: X(A) R(A) W(A) X(B) U(A) 
          T2:                          X(A) R(A) W(A) ... not allowed
        \end{lstlisting}
        This block really comes from the fact that this schedule is not serializable. Consider a serializable schedule instead. 
        \begin{lstlisting} 
          T1: R(A) W(A)           R(B) W(B)
          T2:           R(A) W(A)           R(B) W(B) 
        \end{lstlisting} 
        Then we can construct the 2P locks as such. 
        \begin{lstlisting}
          T1: X(A) R(A) W(A) X(B) U(A)                R(B) W(B) U(B) 
          T2:                          X(A) R(A) W(A)                X(B) U(A) R(B) W(B) U(B) 
        \end{lstlisting}
        Fundamentally, what we are trying to do by having all locks come first is to push all actions on $A$ (and their associated locks) as far up as possible so we can finish them first. By pushing them up, we can isolate $R$ and not have two-way conflicts between T1 and T2. 
      \end{example}

      2PL is great, but we still have to look at the remaining two desired properties. Let's show why it doesn't satisfy them. 

      \begin{example}[Cascading Rollback and Irrecoverability in 2PL]
        Consider the following problems with 2PL.  
        \begin{enumerate}
          \item Say that T1 is reading $A$ and adds 5 to it to write. Then T2 will read $A=10$ and double it for write. If T1 decides to abort before writing, it should look like T1 had never happened, so T2 should have been reading $A=5$. This is problematic, and the only way to deal with this is to abort T2 as well. If there are other transactions, this is called a \textbf{cascading rollback}. 
            \begin{lstlisting}
              T1: R(A=5) W(A=10)         Abort 
              T2:                R(A=10)       W(A=20) Must abort T2
            \end{lstlisting}

          \item A schedule may not be recoverable sometimes. We can see that T1 aborts, but by the time it aborts, it's too late: T2 had already committed changes using uncommitted data from T1, which must now be undone. 
            \begin{lstlisting}
              T1: R(A=5) W(A=10)                        Abort
              T2:                R(A=10) W(A=20) Commit
            \end{lstlisting}
        \end{enumerate}
      \end{example}

      Therefore, we would like to 
      \begin{enumerate}
        \item \textit{Avoid cascading rollbacks}. All transactions should only read data by committed transactions. 
        \item \textit{Recoverable}. A transaction commits only after all transactions it had read from commits. 
      \end{enumerate} 
      By enforcing even stricter conditions, we can get this as well. 

      \begin{definition}[Strict 2PL]
        \textbf{Strict 2PL} only releases locks at commit/abort time. A writer will block all other readers until the writer commits or aborts.
      \end{definition}

      \begin{example}[2PL vs Strict 2PL]
        Consider the following schedule. 
        \begin{lstlisting}
          T1: X(A) W(A) U(A)                C
          T2:                X(A) W(A) U(A)   C
        \end{lstlisting}
        \begin{enumerate}
          \item 2PL allows this schedule since for each transaction, all unlocks come after all locks. 
          \item Strict 2PL does not allow this schedule since it asserts that you must commit before unlocking (and having other transactions use the lock). 
        \end{enumerate} 
        With strict 2PL, you must use this schedule, which requires the commits to be before the transaction. 
        \begin{lstlisting}
          T1: X(A) W(A) C U(A) 
          T2:                  X(A) W(A) C U(A)
        \end{lstlisting}
      \end{example}

      It seems that we are regressing more and more back to the serial schedule, and yes this is true. By enforcing stricter conditions, we are forced to isolate more and more until it is serial. However, this is used in many commercial DBMS because reliability is often more important than efficiency.\footnote{Oracle is a notable exception.} 

    \subsubsection{Snapshots} 
      
      Oracle uses snapshot isolation. Covered in 516. It uses a private snapshot or a local copy. If there are no conflicts, it makes global changes and aborts otherwise. It is more efficient than locks, but may lead to aborts.  

    \subsubsection{Timestamp-Based Concurrency Control} 
      
      

    \subsubsection{Conflicts and Isolation Levels}

      We can actually enforce isolation levels in SQL directly. In SQL a transaction is automatically started when a user executes a SQL statement, and subsequent statements in the same session are executed as part of this transaction. Statements see changes made by earlier ones in the same transaction and statements in other concurrently running transactions do not. \texttt{COMMIT} and \texttt{ROLLBACK} commands are self explanatory.  
       
      SQL also supports isolation levels, which increases performance by eliminating overhead and allowing higher levels of concurrency, albeit at the cost of getting inconsistent answers if you don't know what you're doing. We start off with the most restrictive and go down. 
      \begin{enumerate}
        \item \texttt{SERIALIZABLE} is the strongest isolation level, with complete isolation. 

        \item \texttt{REPEATABLE READ} allows repeated reads, and you are safe with updates, but you aren't safe with new insertions, which are called \textit{phantoms}. 

        \begin{figure}[H]
          \centering 
          \includegraphics[scale=0.4]{img/rep_read.png}
          \caption{You still have a different average due to the insertion of a new element. } 
          \label{fig:rep_read}
        \end{figure}

        \item \texttt{READ COMMITTED} does not allow dirty reads, but non-repeatable reads are allowed (RW conflicts), which means that reading the same data twice can produce different results and is allowed. 

        \begin{figure}[H]
          \centering 
          \includegraphics[scale=0.4]{img/read_c.png}
          \caption{In T2, we first compute an average of say 0.6. Then in T1 we update the popularity and commit. Then in T2, we take the average again, getting 0.8, and finally commit. T2 ends up reading two different states of the database.} 
          \label{fig:read_committed}
        \end{figure}

        \item \texttt{READ UNCOMMITTED} allows you to read uncommitted/dirty data (WR conflict). 

        \begin{figure}[H]
          \centering 
          \includegraphics[scale=0.4]{img/read_unc.png}
          \caption{In T1, you are setting the popularity to 0.99. In T2, you read this uncommitted data to compute the average. However, T1 aborts this write, and in T2 you are left with the wrong average. } 
          \label{fig:read_uncommitted}
        \end{figure}
      \end{enumerate}

      Here is a summary of the isolation levels.\footnote{Postgres defaults to read committed.}

      \begin{figure}[H]
        \centering 
        \includegraphics[scale=0.4]{img/isolation.png}
        \caption{} 
        \label{fig:isolation}
      \end{figure}

  \subsection{Atomicity and Durability} 

      Note that whenever we read from or write to disk, we are incurring an IO cost and therefore a delay. Since committing and writing to disk are not the same thing, there are two risks: 
      \begin{enumerate}
        \item A system may crash in the middle of transaction $T$ before committing, keeping only partial effects of $T$. This violates atomicity, and we should know how to \textit{undo} $T$. 
        \item A system may crash right after a transaction $T$ commits, and not all effects of $T$ were written to disk. This violates durability, and we should know how to \textit{complete} $T$. 
      \end{enumerate} 

      \begin{example}[No Steal and Force]
        A naive approach to these two risks are as such.  
        \begin{enumerate}
          \item \textit{No steal}. Writes of a transaction can only be flushed to disk at commit time. With steal, if the system crashes before $T$ commits but after some writes of $T$ have been flushed to disk, there is no way to undo these writes. 
          \item \textit{Force}. When a transaction commits, all writes of this transaction must be reflected on disk. Without force, if a system crashes right after $T$ commits, the effects of $T$ will be lost. 
        \end{enumerate}
      \end{example}

    \subsection{Logging} 

      Just as we enforced isolation and concurrency control with locking, we can kill two birds with one stone here with \textit{logging}. 

      \begin{definition}[Log]
        A \textbf{log} is a sequence of \textbf{records} that records all changes made to the database.\footnote{This is stored in a special data structure called \textit{aries}, developed at IBM.} There are 3 types of logs.
        \begin{enumerate}
          \item \texttt{Start}: When a transaction starts, we log 
            \begin{equation}
              \langle T_i, \texttt{start} \rangle
            \end{equation}
          \item \texttt{Write}: When transaction $T$ takes element $A$ and changes value $5$ to $10$, we log 
            \begin{equation}
              (T, A, 5, 10)
            \end{equation}
          \item \texttt{Commit}, \texttt{Abort}: When a transaction $T$ is committed or aborted, we log 
            \begin{equation}
              \langle T_i , \texttt{commit} \rangle \text{ or } \langle T_i , \texttt{abort} \rangle
            \end{equation}
        \end{enumerate}
        Logs are stored in disk (stable storage) and used in recovery. 
        It satisfies \textbf{write-ahead logging (WAL)}, which states that before $A$ is modified in disk, the log record pertaining to $A$ must be flushed. 
      \end{definition}

      At first glance, logging may sound like a bad idea. Since every time we do a transaction, we don't do 1 IO but 2, so this may hurt our performance. However, log writing is really sequential since we're appending to the end of a log only, so we can use dedicated disks with fast append operations to minimize this performance impact. 

      \begin{theorem}[No Force and Seal of Logging]
        Counterintuitively, logging applies no force and steal (as opposed to no steal and force in our naive approach). 
        \begin{enumerate}
          \item \textit{Steal}. Modified memory blocks can be flushed to disk anytime. This is fine since undo information is logged due to WAL. 
          \item \textit{No Force}. A transaction can commit even if its modified memory blocks have not been written to disk. This is fine since redo information is logged due to WAL. 
        \end{enumerate}
      \end{theorem} 

      \begin{example}[Bank Transfer]
        Say that we have two elements/tuples $A = 800, B = 400$ on disk. The schedule is just one transaction that will look something like this. Let $D, L, M$ refer to our main disk, log disk, and memory. 
        \begin{enumerate}
          \item We log that $T$ has started. 

            \noindent\begin{minipage}{.46\textwidth}
              \begin{lstlisting}[]{Code}
                Disk    Memory
                A=800   
                B=400
              \end{lstlisting}
              \end{minipage}
              \hfill
              \begin{minipage}{.45\textwidth}
              \begin{lstlisting}[]{Output}
                Log 
                [T, start]
                .
              \end{lstlisting}
            \end{minipage}

          \item $T$ loads $A=800$ from disk to memory. 

            \noindent\begin{minipage}{.46\textwidth}
              \begin{lstlisting}[]{Code}
                Disk    Memory
                A=800   A=800
                B=400
              \end{lstlisting}
              \end{minipage}
              \hfill
              \begin{minipage}{.45\textwidth}
              \begin{lstlisting}[]{Output}
                Log 
                [T, start]
                .
              \end{lstlisting}
            \end{minipage}

          \item $T$ writes $A=700$ in memory and logs the write at the same time. This is fine since WAL forces logging before \textit{flushing} to disk, not writes on memory. 

            \noindent\begin{minipage}{.46\textwidth}
              \begin{lstlisting}[]{Code}
                Disk    Memory
                A=800   A=700
                B=400
              \end{lstlisting}
              \end{minipage}
              \hfill
              \begin{minipage}{.45\textwidth}
              \begin{lstlisting}[]{Output}
                Log 
                [T, start]
                [T, A, 800, 700]
              \end{lstlisting}
            \end{minipage}

          \item We load $B=400$ into memory. 

            \noindent\begin{minipage}{.46\textwidth}
              \begin{lstlisting}[]{Code}
                Disk    Memory
                A=800   A=700
                B=400   B=400
              \end{lstlisting}
              \end{minipage}
              \hfill
              \begin{minipage}{.45\textwidth}
              \begin{lstlisting}[]{Output}
                Log 
                [T, start]
                [T, A, 800, 700]
              \end{lstlisting}
            \end{minipage}

          \item $T$ writes $B=500$ in memory and logs the write. 

            \noindent\begin{minipage}{.46\textwidth}
              \begin{lstlisting}[]{Code}
                Disk    Memory
                A=800   A=700
                B=400   B=500
                .
              \end{lstlisting}
              \end{minipage}
              \hfill
              \begin{minipage}{.45\textwidth}
              \begin{lstlisting}[]{Output}
                Log 
                [T, start]
                [T, A, 800, 700]
                [T, b, 400, 500]
              \end{lstlisting}
            \end{minipage}

          \item We flush the value of $A$ to disk. Note that by steal, we can flush before committing $T$.  

            \noindent\begin{minipage}{.46\textwidth}
              \begin{lstlisting}[]{Code}
                Disk    Memory
                A=700   A=700
                B=400   B=500
                .
              \end{lstlisting}
              \end{minipage}
              \hfill
              \begin{minipage}{.45\textwidth}
              \begin{lstlisting}[]{Output}
                Log 
                [T, start]
                [T, A, 800, 700]
                [T, b, 400, 500]
              \end{lstlisting}
            \end{minipage} 

          \item We commit $T$, which goes to the log. This marks the end of the transaction. 

            \noindent\begin{minipage}{.46\textwidth}
              \begin{lstlisting}[]{Code}
                Disk    Memory
                A=700   A=700
                B=400   B=500
                .
              \end{lstlisting}
              \end{minipage}
              \hfill
              \begin{minipage}{.45\textwidth}
              \begin{lstlisting}[]{Output}
                Log 
                [T, start]
                [T, A, 800, 700]
                [T, b, 400, 500] 
                [T, commit]
              \end{lstlisting}
            \end{minipage} 

          \item We flush the value of $B$ to disk. Note that by no force, we can flush after committing $T$. Note that this does not log. 

            \noindent\begin{minipage}{.46\textwidth}
              \begin{lstlisting}[]{Code}
                Disk    Memory
                A=700   A=700
                B=500   B=500
                .
              \end{lstlisting}
              \end{minipage}
              \hfill
              \begin{minipage}{.45\textwidth}
              \begin{lstlisting}[]{Output}
                Log 
                [T, start]
                [T, A, 800, 700]
                [T, b, 400, 500] 
                [T, commit]
              \end{lstlisting}
            \end{minipage} 

        \end{enumerate}
      \end{example}

      With this flexibility to flush anytime, we as the programmers really just care about the correctness about the schedule and don't care about the order.  

    \subsubsection{Checkpointing} 
      
      We've seen the advantages of log files, but they can get quite long very fast. If a system crashes and we must recover our operations, it may not be ideal to start the recovery from the beginning of a schedule. Rather, we want to use \textit{checkpointing}. Let's go through a naive approach. 

      \begin{definition}[Naive Checkpointing]
         To checkpoint, 
         \begin{enumerate}
           \item we stop accepting new transactions 
           \item finish all active transactions 
           \item take a database dump
         \end{enumerate}
         To recover, we can start from the last checkpoint. 
      \end{definition} 

      We can already see a few problems with this approach, especially the performance impact on not accepting new transactions. Therefore, we use a more sophisticated approach. 

      \begin{definition}[Fuzzy Checkpointing]
        To place a fuzzy checkpoint, we do the following. 
        \begin{enumerate}
          \item We determine $S$, the set (ids of) currently active transactions. 
          \item We log $\langle \texttt{START CKPT S} \rangle$ 
          \item We flush all dirty blocks up to the checkpoint to disk (however, actions within the checkpoint may or may not get flushed within the checkpoint). 
          \item Transactions normally proceed and new transactions can start during checkpointing. 
          \item We log $\langle \texttt{END CKPT START-CKPT\_location} \rangle$, where the start checkpoint location allows you to easily access the location of the start (otherwise we can read the log backwards to find it). 
        \end{enumerate}
        Now once a bad event like a crash happens, there are three recovery steps. 
        \begin{enumerate}
          \item \textit{Analysis}. Go backward from crash point. 
          \item \textit{Repeating History}. Forward takes care of REDO for committed transactions. 
          \item \textit{UNDO}. Backward takes care of UNDO of uncommitted transactions and removes their effects. 
        \end{enumerate}
      \end{definition} 

      \begin{example}[Crash After T2]
        Consider the logs below. The left log shows the full log, and the right log shows a log up until a system crash right before committing transaction T3. 
        
        \noindent\begin{minipage}{.5\textwidth}
          \begin{lstlisting}[]{Code}
            <START T1>
            <T1, A, 4, 5>
            <START T2>
            <COMMIT T1>
            <T2, B, 9, 10>
            <START CKPT(T2)>
            <T2, C, 14, 15>
            <START T3>
            <T3, D, 19, 20>
            <END CKPT>
            <COMMIT T2>
            <COMMIT T3>
          \end{lstlisting}
          \end{minipage}
          \hfill
          \begin{minipage}{.49\textwidth}
          \begin{lstlisting}[]{Output}
            <START T1>
            <T1, A, 4, 5>
            <START T2>
            <COMMIT T1>
            <T2, B, 9, 10>
            <START CKPT(T2)>
            <T2, C, 14, 15>
            <START T3>
            <T3, D, 19, 20>
            <END CKPT>
            <COMMIT T2>
            -- CRASH
          \end{lstlisting}
        \end{minipage}

        In the full log, we see that 
        \begin{enumerate}
          \item At the time of checkpoint, T2 is active but T1 is already committed, so the current set of transactions is just T2. 
          \item All logs before \texttt{START CKPT} goes to disk. 
          \item During the checkpoint, T2 writes to $C$. 
          \item We can also see that during the checkpoint, a new transaction T3 starts. 
          \item Then we can commit T2 and T3. 
        \end{enumerate}

        In the log with the crash, we go through the recovery steps. 
        \begin{enumerate}
          \item We look at line 11 at the end of the log where T2 commits. We go back until we reach an \texttt{END CKPT} and use it to go back to \texttt{START CKPT} at line 6. If there is no \texttt{CKPT}, then go back to the beginning of the log. 
          \item Now for the redo step. 
            \begin{enumerate}
              \item After analysis, we first construct a set $U$ of uncommitted transaction to be used in the UNDO step later. Initially, $U = S = \{T_2\}$ since $T_1$ has already committed and writes are already on disk.  
              \item We scan forward from \texttt{START CKPT(T2)} and repeat every action (redo) from here until the end of the log. If we see a log record of form \texttt{<T, A, old, new>}, we can simply write \texttt{(A, new)}, flush to disk, and ignore the old value, as it may or may not be preserved (depending on whether we flushed before crash or not). 
              \item We see that T3 starts at line 8, so we add $T_3$ to $U$. If a transaction $T$ committed or aborted, we remove $T$ from $U$. 
              \item We see after the \texttt{CKPT} that T2 is committed. We remove $T_2$ from $U$. 
            \end{enumerate}
          \item Finally, we do the undo step since some transactions are still uncommitted. We run backwards from the end of log, to the earliest \texttt{<START T>} of the uncommitted transactions stored in set $U$ (which may be before or after the \texttt{START CKPT}). 
            \begin{enumerate}
              \item We have $U = \{T_3\}$, and so we go backwards an undo all the writes. That is, for each log record \texttt{<T, A, old, new>}, where $T \in U$, we can simply write \texttt{(A, old)} and log this operation. 
              \item We do this until we hit the log \texttt{START T3} at line 8. 
              \item We finally log \texttt{<abort T>} when all effects of $T$ have been undone. 
            \end{enumerate}
        \end{enumerate}
      \end{example}

      \begin{example}[Crash After T2 and T3]
        Say that our crash happens after committing T3. 

        \begin{lstlisting}
          <START T1>
          <T1, A, 4, 5>
          <START T2>
          <COMMIT T1>
          <T2, B, 9, 10>
          <START CKPT(T2)>
          <T2, C, 14, 15>
          <START T3>
          <T3, D, 19, 20>
          <END CKPT>
          <COMMIT T2>
          <COMMIT T3>
          -- CRASH
        \end{lstlisting} 
        In this case, $U = \{T_2\}$ after analysis, and after redo, $U = \{\}$, and so there is no undo phase. 
      \end{example}

      \begin{example}[Crash After T3]
        Consider when we commit T3 and then it crashes before committing T2. 
        \begin{lstlisting}
          <START T1>
          <T1, A, 4, 5>
          <START T2>
          <COMMIT T1>
          <T2, B, 9, 10>
          <START CKPT(T2)>
          <T2, C, 14, 15>
          <START T3>
          <T3, D, 19, 20>
          <END CKPT>
          <COMMIT T3>
          -- CRASH
        \end{lstlisting} 
        T1 has committed and writes are already on disk. After analysis, $U = S = \{T_2\}$. When we do redo all actions, we again take line 7 and write \texttt{T2(C=15)}, then we add $T_3$ to $U$, then we write \texttt{T3(D=20)}. Then we end checkpoint and commit T3, removing $T_3$ from $U$. We have $U = \{T_2\}$ after redo, so in our undo phase, we go backwards. 
        \begin{enumerate}
          \item In line 7, we write \texttt{T2(C=14)} (which was already flushed to disk by redo step) and flush to disk.  
          \item Beyond the checkpoint start, we write \texttt{T2(B=9)} (which was already flushed to disk since it's before the start CKPT) and flush to disk. 
          \item We finally write \texttt{<abort T2>} at the end of the checkpoint
        \end{enumerate} 
        We are done, and the effects of T2 have vanished. 
      \end{example}

      \begin{example}[Crash Before T2 and T3]
        Now we consider when we have a crash before both commits. 
        \begin{lstlisting}
          <START T1>
          <T1, A, 4, 5>
          <START T2>
          <COMMIT T1>
          <T2, B, 9, 10>
          <START CKPT(T2)>
          <T2, C, 14, 15>
          <START T3>
          <T3, D, 19, 20>
          <END CKPT>
          -- CRASH
        \end{lstlisting} 
        The analysis is the same with $U = \{T_2\}$. Then we redo and flush changes to disk up until the end and find $U = \{T_2, T_3\}$. Finally, we have to undo these uncommitted transactions, where we go up until \texttt{<START T2>}, and rewrite the old values and flush them back the disk. 
      \end{example}

\section{Big Data} 

  As powerful relational models are, the sheer growth of data in modern times requires the use of hundreds or thousands of database servers working together to modify or query data. This field is known as \textit{big data}. Unlike transactions, which deal with multiple queries on one machine, big data deals with one query on multiple machines. 

  \subsection{Parallel Databases} 

  \subsection{Map-Reduce}

\end{document}

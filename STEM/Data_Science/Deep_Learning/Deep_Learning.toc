\babel@toc {english}{}\relax 
\contentsline {section}{\numberline {1}Multi-Layered Perceptrons}{3}{section.1}%
\contentsline {subsection}{\numberline {1.1}Generalized Linear Models}{3}{subsection.1.1}%
\contentsline {subsection}{\numberline {1.2}Architecture}{4}{subsection.1.2}%
\contentsline {subsection}{\numberline {1.3}Forward and Back Propagation}{7}{subsection.1.3}%
\contentsline {subsubsection}{\numberline {1.3.1}Summary}{12}{subsubsection.1.3.1}%
\contentsline {subsection}{\numberline {1.4}Neural Net from Scratch}{12}{subsection.1.4}%
\contentsline {subsection}{\numberline {1.5}Quick Start to PyTorch}{16}{subsection.1.5}%
\contentsline {subsubsection}{\numberline {1.5.1}Datasets and Features/Label Transformations}{16}{subsubsection.1.5.1}%
\contentsline {subsubsection}{\numberline {1.5.2}Building a Neural Net}{18}{subsubsection.1.5.2}%
\contentsline {subsubsection}{\numberline {1.5.3}Automatic Differentiation}{20}{subsubsection.1.5.3}%
\contentsline {subsubsection}{\numberline {1.5.4}Optimizing Model Parameters}{20}{subsubsection.1.5.4}%
\contentsline {section}{\numberline {2}Training Stability}{22}{section.2}%
\contentsline {subsection}{\numberline {2.1}Weight Initialization}{22}{subsection.2.1}%
\contentsline {subsection}{\numberline {2.2}Optimizers}{22}{subsection.2.2}%
\contentsline {subsection}{\numberline {2.3}Vanishing Gradient Problem}{22}{subsection.2.3}%
\contentsline {subsubsection}{\numberline {2.3.1}Activation Functions}{22}{subsubsection.2.3.1}%
\contentsline {subsubsection}{\numberline {2.3.2}Residual Connections}{23}{subsubsection.2.3.2}%
\contentsline {subsection}{\numberline {2.4}Exploding Gradient Problem}{24}{subsection.2.4}%
\contentsline {subsubsection}{\numberline {2.4.1}Normalization Layers}{24}{subsubsection.2.4.1}%
\contentsline {subsubsection}{\numberline {2.4.2}Max Norm Regularization}{25}{subsubsection.2.4.2}%
\contentsline {section}{\numberline {3}Regularization}{25}{section.3}%
\contentsline {subsection}{\numberline {3.1}Early Stopping}{25}{subsection.3.1}%
\contentsline {subsection}{\numberline {3.2}L1 and L2 Regularization}{25}{subsection.3.2}%
\contentsline {subsection}{\numberline {3.3}Dropout}{25}{subsection.3.3}%
\contentsline {subsection}{\numberline {3.4}Data Augmentation}{27}{subsection.3.4}%
\contentsline {subsection}{\numberline {3.5}Network Pruning}{27}{subsection.3.5}%
\contentsline {subsection}{\numberline {3.6}Summary}{27}{subsection.3.6}%
\contentsline {section}{\numberline {4}Convolutional Neural Networks}{28}{section.4}%
\contentsline {subsection}{\numberline {4.1}Kernels}{28}{subsection.4.1}%
\contentsline {subsection}{\numberline {4.2}Convolutional Layers}{31}{subsection.4.2}%
\contentsline {subsection}{\numberline {4.3}Pooling Layers}{33}{subsection.4.3}%
\contentsline {subsection}{\numberline {4.4}Backpropagation}{33}{subsection.4.4}%
\contentsline {subsection}{\numberline {4.5}Implementation from Scratch}{33}{subsection.4.5}%
\contentsline {subsection}{\numberline {4.6}Total Architecture with PyTorch}{33}{subsection.4.6}%
\contentsline {section}{\numberline {5}Recurrent Neural Networks}{34}{section.5}%
\contentsline {subsection}{\numberline {5.1}Unidirectional RNNs}{35}{subsection.5.1}%
\contentsline {subsubsection}{\numberline {5.1.1}Loss Functions}{36}{subsubsection.5.1.1}%
\contentsline {subsubsection}{\numberline {5.1.2}Backpropagation Through Time}{37}{subsubsection.5.1.2}%
\contentsline {subsubsection}{\numberline {5.1.3}Stacked Unidirectional RNNs}{38}{subsubsection.5.1.3}%
\contentsline {subsection}{\numberline {5.2}Bidirectional RNNs}{39}{subsection.5.2}%
\contentsline {subsubsection}{\numberline {5.2.1}PyTorch Implementation}{39}{subsubsection.5.2.1}%
\contentsline {subsection}{\numberline {5.3}Long Short Term Memory (LSTMs)}{39}{subsection.5.3}%
\contentsline {subsubsection}{\numberline {5.3.1}Multilayer LSTMs}{42}{subsubsection.5.3.1}%
\contentsline {subsection}{\numberline {5.4}Gated Recurrent Units}{43}{subsection.5.4}%
\contentsline {section}{\numberline {6}Encoder-Decoder Models}{43}{section.6}%
\contentsline {subsection}{\numberline {6.1}Sequence to Sequence}{44}{subsection.6.1}%
\contentsline {subsubsection}{\numberline {6.1.1}Decoding Schemes}{46}{subsubsection.6.1.1}%
\contentsline {subsection}{\numberline {6.2}Autoencoders}{48}{subsection.6.2}%
\contentsline {subsection}{\numberline {6.3}Image Captioning}{48}{subsection.6.3}%
\contentsline {section}{\numberline {7}Attention Models}{49}{section.7}%
\contentsline {subsection}{\numberline {7.1}Seq2Seq with Attention}{49}{subsection.7.1}%
\contentsline {subsection}{\numberline {7.2}Self-Attention}{51}{subsection.7.2}%
\contentsline {section}{\numberline {8}Transformers}{51}{section.8}%
\contentsline {section}{\numberline {9}Generative Models}{51}{section.9}%
\contentsline {subsection}{\numberline {9.1}Variational Autoencoders}{51}{subsection.9.1}%
\contentsline {subsection}{\numberline {9.2}Generative Adversarial Networks (GANs)}{51}{subsection.9.2}%
\contentsline {section}{\numberline {10}Deep Reinforcement Learning}{51}{section.10}%

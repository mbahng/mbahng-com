\begin{thebibliography}{}

\bibitem[Frankle and Carbin, 2019]{frankle2019lottery}
Frankle, J. and Carbin, M. (2019).
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural networks.

\bibitem[Lecun et~al., 1998]{Lecun1998ConvNets}
Lecun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998).
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324.

\bibitem[Srivastava et~al., 2014]{srivastava14a}
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2014).
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock {\em Journal of Machine Learning Research}, 15(56):1929--1958.

\bibitem[Tam and Dunson, 2020]{tam2020fiedler}
Tam, E. and Dunson, D. (2020).
\newblock Fiedler regularization: Learning neural networks with graph sparsity.

\bibitem[Vaswani et~al., 2017]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N., Kaiser, {\L}., and Polosukhin, I. (2017).
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30.

\end{thebibliography}

\relax 
\providecommand{\transparent@use}[1]{}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Overview}{3}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Types of Models}{4}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Loss Functions}{5}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1}Maximum Likelihood Estimation}{6}{subsubsection.1.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.2}Optimization}{7}{subsubsection.1.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.3}Generalization Error and Statistical Learning Theory}{7}{subsubsection.1.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.4}Bias Variance Noise Decomposition of MSE Loss}{8}{subsubsection.1.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Model Selection}{9}{subsection.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1}Model Complexity}{10}{subsubsection.1.3.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:1d}{{1a}{11}{1st Degree\relax }{figure.caption.2}{}}
\newlabel{sub@fig:1d}{{a}{11}{1st Degree\relax }{figure.caption.2}{}}
\newlabel{fig:3d}{{1b}{11}{3rd Degree\relax }{figure.caption.2}{}}
\newlabel{sub@fig:3d}{{b}{11}{3rd Degree\relax }{figure.caption.2}{}}
\newlabel{fig:5e}{{1c}{11}{5th Degree\relax }{figure.caption.2}{}}
\newlabel{sub@fig:5e}{{c}{11}{5th Degree\relax }{figure.caption.2}{}}
\newlabel{fig:7d}{{1d}{11}{7th Degree\relax }{figure.caption.2}{}}
\newlabel{sub@fig:7d}{{d}{11}{7th Degree\relax }{figure.caption.2}{}}
\newlabel{fig:9d}{{1e}{11}{9th Degree\relax }{figure.caption.2}{}}
\newlabel{sub@fig:9d}{{e}{11}{9th Degree\relax }{figure.caption.2}{}}
\newlabel{fig:11e}{{1f}{11}{11th Degree\relax }{figure.caption.2}{}}
\newlabel{sub@fig:11e}{{f}{11}{11th Degree\relax }{figure.caption.2}{}}
\newlabel{fig:d}{{2a}{11}{$M = 9, N = 15$\relax }{figure.caption.3}{}}
\newlabel{sub@fig:d}{{a}{11}{$M = 9, N = 15$\relax }{figure.caption.3}{}}
\newlabel{fig:d}{{2b}{11}{$M = 9, N = 100$\relax }{figure.caption.3}{}}
\newlabel{sub@fig:d}{{b}{11}{$M = 9, N = 100$\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Increasing the number of data points helps the overfitting problem. Now, we can afford to fit a 9th degree polynomial with reasonable accuracy.\relax }}{11}{figure.caption.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.2}Train Test Split and Cross Validation}{11}{subsubsection.1.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Degree of Polynomial vs RMS on Training and Testing Sets\relax }}{12}{figure.caption.4}\protected@file@percent }
\newlabel{fig:poly_deg_vs_rms}{{3}{12}{Degree of Polynomial vs RMS on Training and Testing Sets\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.3}Regularization}{12}{subsubsection.1.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Even with a slight increase in the regularization term $\lambda $, the RMS error on the testing set heavily decreases. \relax }}{13}{figure.caption.5}\protected@file@percent }
\newlabel{fig:enter-label}{{4}{13}{Even with a slight increase in the regularization term $\lambda $, the RMS error on the testing set heavily decreases. \relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.4}Ensemble Learning}{14}{subsubsection.1.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Preprocessing Data}{14}{subsection.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.1}Feature Extraction}{14}{subsubsection.1.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Gaussian basis functions over the interval $[-1, 1]$ with standard deviation of $0.3$\relax }}{15}{figure.caption.6}\protected@file@percent }
\newlabel{fig:Gaussian_basis_functions}{{5}{15}{Gaussian basis functions over the interval $[-1, 1]$ with standard deviation of $0.3$\relax }{figure.caption.6}{}}
\newlabel{fig:d}{{6a}{18}{Fitting with 10 Gaussian basis functions. \relax }{figure.caption.7}{}}
\newlabel{sub@fig:d}{{a}{18}{Fitting with 10 Gaussian basis functions. \relax }{figure.caption.7}{}}
\newlabel{fig:d}{{6b}{18}{Fitting with 10 Fourier basis functions. \relax }{figure.caption.7}{}}
\newlabel{sub@fig:d}{{b}{18}{Fitting with 10 Fourier basis functions. \relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \relax }}{18}{figure.caption.7}\protected@file@percent }
\newlabel{Coincide_mean}{{6}{18}{\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.2}Standardizing Data}{18}{subsubsection.1.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Information Theory}{19}{subsection.1.5}\protected@file@percent }
\newlabel{fig:d}{{7a}{20}{Original Data\relax }{figure.caption.8}{}}
\newlabel{sub@fig:d}{{a}{20}{Original Data\relax }{figure.caption.8}{}}
\newlabel{fig:d}{{7b}{20}{StandardScaler\relax }{figure.caption.8}{}}
\newlabel{sub@fig:d}{{b}{20}{StandardScaler\relax }{figure.caption.8}{}}
\newlabel{fig:d}{{7c}{20}{MinMaxScaler\relax }{figure.caption.8}{}}
\newlabel{sub@fig:d}{{c}{20}{MinMaxScaler\relax }{figure.caption.8}{}}
\newlabel{fig:d}{{7d}{20}{MaxAbsScaler\relax }{figure.caption.8}{}}
\newlabel{sub@fig:d}{{d}{20}{MaxAbsScaler\relax }{figure.caption.8}{}}
\newlabel{fig:d}{{7e}{20}{RobustScaler\relax }{figure.caption.8}{}}
\newlabel{sub@fig:d}{{e}{20}{RobustScaler\relax }{figure.caption.8}{}}
\newlabel{fig:d}{{7f}{20}{QuantileTransformer\relax }{figure.caption.8}{}}
\newlabel{sub@fig:d}{{f}{20}{QuantileTransformer\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The StandardScaler simply standardizes the data to have $0$ mean and unit variance.\relax }}{20}{figure.caption.8}\protected@file@percent }
\newlabel{Scalers}{{7}{20}{The StandardScaler simply standardizes the data to have $0$ mean and unit variance.\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.5.1}Kullback Leibler Divergence}{21}{subsubsection.1.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.5.2}Mutual Information}{21}{subsubsection.1.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Linear Regression}{22}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Construction}{22}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Least Squares}{23}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Likelihood Estimation}{24}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Simple Linear Regression}{25}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Significance Tests}{26}{subsection.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.1}T Test}{26}{subsubsection.2.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.2}F Test}{28}{subsubsection.2.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Perceptron}{28}{section.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Perceptron Trained on Different Standardized Data\relax }}{30}{figure.caption.9}\protected@file@percent }
\newlabel{fig:Percepton_on_Standardized_data}{{8}{30}{Perceptron Trained on Different Standardized Data\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Logistic Regression}{30}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Likelihood Estimation}{31}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Softmax Regression}{31}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Likelihood Estimation}{33}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Generalized Linear Models}{34}{section.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Fitting a linear model for Bernoulli random variables will predict a mean that is outside of $[0, 1]$ when getting new datapoints. \relax }}{36}{figure.caption.10}\protected@file@percent }
\newlabel{fig:Bernoulli_GLM}{{9}{36}{Fitting a linear model for Bernoulli random variables will predict a mean that is outside of $[0, 1]$ when getting new datapoints. \relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Exponential Family}{37}{subsection.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.1}Canonical Exponential Family}{38}{subsubsection.6.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Link Functions}{40}{subsection.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Logit and Probit Functions\relax }}{41}{figure.caption.11}\protected@file@percent }
\newlabel{fig:logit_probit}{{10}{41}{Logit and Probit Functions\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.1}Canonical Link Functions}{41}{subsubsection.6.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Likelihood Optimization}{42}{subsection.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Discriminant Analysis}{43}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.0.1}Fisher Discriminant Analysis}{43}{subsubsection.7.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.0.2}Gaussian Discriminant Analysis (Generative Model)}{43}{subsubsection.7.0.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces GDA of Data Generated from 2 Gaussisans centered at $(-2.3, 0.4)$ and $(1.4, -0.9)$ with unit covariance. The decision boundary is slightly off since MLE approximates the true means. \relax }}{44}{figure.caption.12}\protected@file@percent }
\newlabel{fig:enter-label}{{11}{44}{GDA of Data Generated from 2 Gaussisans centered at $(-2.3, 0.4)$ and $(1.4, -0.9)$ with unit covariance. The decision boundary is slightly off since MLE approximates the true means. \relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}K Nearest Neighbors}{44}{section.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}Decision Trees}{46}{section.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}Splitting Criteria}{47}{subsection.9.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.1.1}Misclassification Error}{47}{subsubsection.9.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.1.2}Information Gain}{47}{subsubsection.9.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.1.3}Gini Index}{49}{subsubsection.9.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}Regularization}{50}{subsection.9.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3}Splitting on Continuous Values}{51}{subsection.9.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4}Random Forests}{51}{subsection.9.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5}Boosting}{51}{subsection.9.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10}Kernel Methods}{51}{section.10}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11}Support Vector Machines}{52}{section.11}\protected@file@percent }
\newlabel{fig:svm_intro}{{12a}{53}{Planes such as (1) and (4) are ``too close" to the positive and negative samples. \relax }{figure.caption.13}{}}
\newlabel{sub@fig:svm_intro}{{a}{53}{Planes such as (1) and (4) are ``too close" to the positive and negative samples. \relax }{figure.caption.13}{}}
\newlabel{fig:svm_intro2}{{12b}{53}{SVMs try to find the separating hyperplane with the best minimum margin.\relax }{figure.caption.13}{}}
\newlabel{sub@fig:svm_intro2}{{b}{53}{SVMs try to find the separating hyperplane with the best minimum margin.\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces \relax }}{53}{figure.caption.13}\protected@file@percent }
\newlabel{}{{12}{53}{\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.0.1}Functional and Geometric Margins}{54}{subsubsection.11.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.1}Lagrange Duality}{54}{subsection.11.1}\protected@file@percent }
\newlabel{fig:original_scaled}{{13a}{55}{$f(x) = x_1 + x_2 + 1$\relax }{figure.caption.14}{}}
\newlabel{sub@fig:original_scaled}{{a}{55}{$f(x) = x_1 + x_2 + 1$\relax }{figure.caption.14}{}}
\newlabel{fig:two_times_scaled}{{13b}{55}{$f(x) = 2 x_1 + 2 x_2 + 2$\relax }{figure.caption.14}{}}
\newlabel{sub@fig:two_times_scaled}{{b}{55}{$f(x) = 2 x_1 + 2 x_2 + 2$\relax }{figure.caption.14}{}}
\newlabel{fig:something_else}{{13c}{55}{$f(x) = -2x_1 + x_2 - 3$\relax }{figure.caption.14}{}}
\newlabel{sub@fig:something_else}{{c}{55}{$f(x) = -2x_1 + x_2 - 3$\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces From (a), you can see that simply multiplying everything by two automatically increases our confidence by $2$, meaning that the functional margin can be scaled arbitrarily by scaing $(\mathbf  {w}, b)$. There are still too many degrees of freedom in here and so extra constraints must be imposed. \relax }}{55}{figure.caption.14}\protected@file@percent }
\newlabel{fig:scaling_problem}{{13}{55}{From (a), you can see that simply multiplying everything by two automatically increases our confidence by $2$, meaning that the functional margin can be scaled arbitrarily by scaing $(\mathbf {w}, b)$. There are still too many degrees of freedom in here and so extra constraints must be imposed. \relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2}Nonseparable Case}{55}{subsection.11.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12}Naive Bayes}{55}{section.12}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {13}Introduction}{55}{section.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1}Bayesian Probability}{55}{subsection.13.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {13.2}Density Estimation}{56}{subsection.13.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {13.2.1}Frequentist Approach}{56}{subsubsection.13.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {13.2.2}Bayesian Approach}{57}{subsubsection.13.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {13.3}Regression with Regularization}{57}{subsection.13.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {13.3.1}Frequentist's Maximum Likelihood Approach}{57}{subsubsection.13.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {13.3.2}Bayesian Approach}{58}{subsubsection.13.3.2}\protected@file@percent }
\newlabel{LastPage}{{}{59}{}{page.59}{}}
\xdef\lastpage@lastpage{59}
\xdef\lastpage@lastpageHy{59}
\gdef \@abspage@last{59}

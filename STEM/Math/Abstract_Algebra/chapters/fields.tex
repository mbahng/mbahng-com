\section{Fields}

  Our final structure is field, which are usually pretty tame compared to groups and rings. 

  \begin{definition}[Field]
    A \textbf{field} $(F, +, \times)$ is a commutative, associative ring where every nonzero element is a unit. 
  \end{definition}

  \begin{theorem}
    Every field is a Euclidean domain. 
  \end{theorem}
  \begin{proof}
    Given $x, y \in \mathbb{F}$, assume $x y = 0$ with $x \neq 0$. Since $x$ is invertible,
    \begin{equation}
      0 = x^{-1} 0 = x^{-1} (x y) = y
    \end{equation}
    Now assuming that $y \neq 0$, since $y$ is invertible, 
    \begin{equation}
      0 = 0 y^{-1} = (x y) y^{-1} = x
    \end{equation}
  \end{proof}

  Let's give a few examples of fields. 

  \begin{theorem}[Wedderburn's little theorem]
    Every finite Euclidean domain is a field. 
  \end{theorem} 

  \begin{example}[Finite Fields]
    $\mathbb{Z}_p$ with $p$ prime is a field. 
  \end{example}

  \begin{example}[Numbers]
    The rationals, reals, and complex numbers are all fields.\footnote{Quaternions are not!}
  \end{example}

  Note the subfield structure $\mathbb{Q} \subset \mathbb{R} \subset \mathbb{C}$. However, we will find that there are tons of other fields lurking in between $\mathbb{Q}$ and $\mathbb{C}$ other than $\mathbb{R}$. We can actually say that there are no subfields of $\mathbb{Q}$. 

  \begin{lemma}[Rationals are a Minimal Field]
    Every subfield of $\mathbb{C}$ contains $\mathbb{Q}$. 
  \end{lemma}
  \begin{proof}
    Must contain $0$ and $1$. Keep adding $1$ and inverting it to get $\mathbb{Z}$. Now $\mathbb{Z}$ must contain units so $1/n$ also contained. Then multiply the elements to get $\mathbb{Q}$. 
  \end{proof} 

\subsection{Rational Functions}

  Given a field $F$, we have constructed the Euclidean domain $F[x]$. However, this is one step away from being a field. We mimick the construction of the rational numbers $\mathbb{Q}$ as a quotient space over $\mathbb{Z} \times (\mathbb{Z} \setminus \{0\})$ by taking $F[x] \times (F[x] \setminus \{0\})$ and putting a quotient on it. 
  
  \begin{definition}[Rational Functions]
    The \textbf{rational functions} are defined to be the field of quotients (really just 2-tuples) of the form 
    \begin{equation}
      F(x) \coloneqq \bigg\{ \frac{f(x)}{g(x)} \; \bigg| \; f(x), g(x) \in F[x], g(x) \neq 0 \bigg\}
    \end{equation}
    where addition and multiplication is defined in the usual sense.
  \end{definition}

  \begin{theorem}[Partial Fractions Decomposition]
    Let $f(x), g(x) \in F[x]$ where $\deg(f(x)) < \deg(g(x))$. If $g(x) = u(x) v(x)$ where $u, v$ are relatively prime, then there are polynomials $a(x), b(x)$ with $\deg(a) < \deg(u), \deg(b) < \deg(v)$ s.t. 
    \begin{equation}
      \frac{f(x)}{g(x)} = \frac{a(x)}{u(x)} + \frac{b(x)}{v(x)}
    \end{equation}
    By induction, we can prove this for any finite set of irreducible polynomials. 
  \end{theorem}
  \begin{proof}
    We describe an algorithm to get this decomposition. There are polynomials $s(x), t(x)$ s.t. $1 = s(x) u(x) + t(x) v(x)$. Therefore, 
    \begin{equation}
      \frac{f(x)}{ u(x) v(x)} = \frac{f(x) t(x)}{u(x)} + \frac{f(x) s(x)}{v(x)}
    \end{equation}
    and we can use the Euclidean algorithm to write 
    \begin{align}
      \frac{f(x) t(x)}{u(x)} & = q(x) + \frac{a(x)}{u(x)}, \qquad \deg(a) < \deg(u) \\
      \frac{f(x) s(x)}{v(x)} & = q(x) + \frac{a(x)}{u(x)}, \qquad \deg(b) < \deg(v)
    \end{align}
    which implies 
    \begin{equation}
      \frac{f(x)}{u(x) v(x)} = \frac{a(x)}{u(x)} + \frac{b(x)}{v(x)}
    \end{equation}
  \end{proof}

  \begin{example}
    Consider the rational function $\frac{x + 3}{x^3 (x - 1)^2}$. Applying the Euclidean algorithm, we find that 
    \begin{equation}
      1 = (3x^2 + 2x + 1) (x - 1)^2 - (3x - 4) x^3
    \end{equation}
    and so 
    \begin{align}
      \frac{x + 3}{x^3 (x - 1)^2} & = \frac{(x + 3)(3x^2 + 2x + 1)}{x^3} - \frac{(x + 3)(3x - 4)}{(x - 1)^2} \\
                                  & = \frac{11x^2 + 7x + 3}{x^3} + \frac{-11x + 15}{(x - 1)^2}
    \end{align}
  \end{example}

\subsection{Algebraically Closed Fields} 

  Now that we have seen some examples of fields, what properties would we like it to have? Going back to polynomials, recall that if $F$ is a field, then $F[x]$ as a Euclidean domain gave us a lot of nice properties, such as admitting a unique factorization of irreducible polynomials. However, we have only proved that the number of roots is \textit{at most} the degree $n$, but not that it actually reaches $n$. In fact, in a more extreme case, a polynomial may not even factor \textit{at all} in $F[x]$, since it could be irreducible. So while we have defined an upper bound for the number of roots for a polynomial, we have not determined whether a polynomial has any roots at all, i.e. a lower bound. 

  We don't have much \textit{control} over what these irreducible polynomials can look like. We may have to check---either through theorems or manually---that a polynomial or arbitrary degree is irreducible. If we would like to assert that all irreducible polynomials must be of smallest degree---that is, linear---then such a field is called \textit{algebraically closed}. 
  This algebraic closed property asserts also that the lower bound on the number of (non-unique) factors is $n$. 

  \begin{definition}[Algebraically Closed Field]
    A field $F$ is \textbf{algebraically closed} if every polynomial of positive degree (i.e. non-constant) in $F[x]$ has at least one root in $F$. 
  \end{definition}

  This is equivalent to saying that every polynomial can be expressed as a product of first degree polynomials. To extend our analysis more, we can talk about the multiplicity of these factors, which just tells us more about how many unique and non-unique factors a polynomial has. 

  \begin{definition}[Multiplicity]
    A root $c$ of polynomial $f(x) \in F[x]$ is called simple if $f(x)$ is not divisible by $(x - c)^2$ and multiple otherwise. The \textbf{multiplicity} of a root $c$ is the maximum k such that $(x - c)^k$ divides $f(x)$ .
  \end{definition} 

  To restate the root-factor theorem for $R[x]$ with arbitrary commutative ring $R$, the number of roots of a polynomial---counted with multiplicity---does not exceed the degree of this polynomial. Furthermore, these numbers are equal if and only if the polynomial is a product of linear factors.

  \begin{example}[Reals are not Algebraically Closed]
    $\mathbb{R}$ is not algebraically closed since we can identify the polynomial $f(x) = x^2 + 1 \in \mathbb{R}[x]$ which does not have any roots in $\mathbb{R}$. Consequently, any subfield of $\mathbb{R}$ (which contains $1$) such as $\mathbb{Q}, \mathbb{Q}(\sqrt{2}), \ldots$ are not algebraically closed. 
  \end{example}

  It turns out that the complex numbers are algebraically closed, which is presented with the following grand name. Ironically, this theorem cannot be proven with algebra alone. We need complex analysis.\footnote{Gauss proved this for the first time in 1799.} 

  \begin{theorem}[Fundamental Theorem of Algebra]
    Suppose $f \in \mathbb{C}[x]$ is a polynomial of degree $n \geq 1$. Then $f(x)$ has a root in $\mathbb{C}$. It immediately follows from induction that it can be factored as a product of linear polynomials in $\mathbb{C}[x]$. 
  \end{theorem}
  \begin{proof}
    WLOG we can assume that $f$ is monic: $f(z) = z^n + a_{n-1} z^{n-1} + \ldots + a_1 z + a_0$. Since $\mathbb{C}$ is a field, we can set 
    \begin{equation}
      f(z) = z^n \bigg( 1 + \frac{a_{n-1}}{z} + \frac{a_{n-2}}{z^2} + \ldots + \frac{a_0}{z_n} \bigg)
    \end{equation} 
    Since 
    \begin{equation}
      \lim_{|z| \rightarrow \infty} \bigg( 1 + \frac{a_{n-1}}{z} + \frac{a_{n-2}}{z^2} + \ldots + \frac{a_0}{z_n} \bigg) = 0
    \end{equation}
    there exists a $R > 0$ s.t. 
    \begin{equation}
      |z| > R \implies \bigg| 1 + \frac{a_{n-1}}{z} + \frac{a_{n-2}}{z^2} + \ldots + \frac{a_0}{z_n} \bigg| < \frac{1}{2}
    \end{equation}
    and hence 
    \begin{equation}
      |z| > R \implies |f(z)| > |z|^n \cdot \bigg( 1 - \frac{1}{2} \bigg) > \frac{R^n}{2}
    \end{equation}
    So $z$ cannot be a root if $|z| > R$. On the other hand, $f(z)$ is continuous (under the Euclidean topology) and so on the compact set $\{z \in \mathbb{C} \mid |z| \leq R\}$, $|f(z)|$ achieves a minimum value say at the point $z_0$. We claim that $\min_z f(z) = 0$. 

    For convenience, we let $z_0 = 0$ (we can do a change of basis on the polynomial) and assume that the minimum is some positive number, i.e. $f(0) = a_0 \neq 0$. Let $j$ be the smallest positive integer such that $a_j = 0$. Let 
    \begin{equation}
      g(z) = \frac{a_{j+1}}{a_j} z + \ldots + \frac{a_n}{a_j} z^{n-j} \implies f(z) = a_0 + a_j z^j \big( 1 + g(z) \big) 
    \end{equation}
    We set $\gamma = \sqrt[j]{-a_0/a_j}$ and consider the values of 
    \begin{align}
      f(t \gamma) & = a_0 + a_j (t\gamma)^j \big( 1 + g(t\gamma) \big) \\
                  & = a_0 - a_0 t^j \big(1 + g(t \gamma) \big) \\
                  & = a_0 \big\{ 1 - t^j \big(1 + g(t \gamma) \big) \big\}
    \end{align} 
    for $t > 0$. For $t$ sufficiently small, we have 
    \begin{equation}
      |g(t \gamma)| = \bigg| \frac{a_{j+1}}{a_j} (t \gamma) + \ldots + \frac{a_n}{a_j} (t \gamma)^{n-j} \bigg| < \frac{1}{2} 
    \end{equation}
    and for such $t$, this implies 
    \begin{equation}
      |f(t \gamma)| = |a_0| |1 - t^j (1 + g(t \gamma))| \leq |a_0| |1 - t^j/2| < |a_0|
    \end{equation}
    and so $z_0$ cannot have been the minimum of $|f(z)|$. Therefore, the minimum value must be $0$.  
  \end{proof}

  Great, so through this theorem, we can work in any subfield of $\mathbb{C}$ and guarantee that will have all of its roots in $\mathbb{C}$. 

  \begin{corollary}[$\mathbb{C}$ is algebraically closed]
    $\mathbb{C}$ is algebraically closed, i.e. $\mathbb{C}$ is a splitting field of $\mathbb{C}[x]$. 
  \end{corollary}

  Put more succinctly, the impossibility of defining division on the ring of integers motivates its extension into the field of rational numbers. Similarly, the inability to take square roots of negative real numbers forces us to extend the field of real numbers to the bigger field of complex numbers. 

  \begin{theorem}[Eigenvector Conditions for Algebraic Closedness]
    A field $F$ is algebraically closed if and only if for each natural number $n$, every endomorphism of $F^n$ (that is, ever linear map from $F^n$ to itself) has at least one eigenvector. 
  \end{theorem}
  \begin{proof}
    An endomorphism of $F^n$ has an eigenvector if and only if its characteristic polynomial has some root. $(\rightarrow)$ So, when $F$ is algebraically closed, every characteristic polynomial, which is an element of $F[x]$, must have a root. $(\leftarrow)$ Assume that every characteristic polynomial has some root, and let $p \in F[x]$. Dividing the polynomial by a scalar doesn't change its roots, so we can assume $p$ to have leading coefficient $1$. If $p(x) = a_0 + a_1 x + ... + x^n$, then we can identify matrix 
    \begin{equation}
      A = \begin{pmatrix}
      0 & 0 & ... & 0 & -a_0 \\
      1 & 0 & ... & 0 & -a_1 \\
      0 & 1 & ... & 0 & -a_2 \\
      ... & ... & ... & ... & ... \\
      0 & 0 & ... & 1 & -a_{n-1}
      \end{pmatrix}
    \end{equation}
    such that the characteristic polynomial of $A$ is $p$. 
  \end{proof}

  With this splitting condition, we can get a nice set of formulas often introduced in high-school math competitions. 

  \begin{theorem}[Viete's Formulas]
    Given that a polynomial $f$ factors into linear terms, that is 
    \begin{equation}
      f(x) = a_0 \prod_{i = 1}^{n} (x - c_i), c_i \text{ roots of } f
    \end{equation}
    Then the coefficients of $f$ can be presented with the formulas
    \begin{align*}
      & \sum_{i=1}^n c_i = - \frac{a_1}{a_0} \\
      & \sum_{i_1 < i_2} c_{i_1} c_{i_2} = \frac{a_2}{a_0} \\
      & \sum_{i_1< ...< i_k} \prod_{j = 1}^{k} c_{i_j} = (-1)^k \frac{a_k}{a_0} \\
      & c_1 c_2 c_3 ... c_n = (-1)^n \frac{a_n}{a_0}
    \end{align*}
  \end{theorem}

\subsection{Extensions and Splitting Fields} 

  Great, so by establishing the fact that $\mathbb{C}$ is algebraically closed, this gives us a ``safe space'' to work in, in the sense that if we take any subfield $F \subset \mathbb{C}$ and find a polynomial $f(x) \in F[x]$, we are \textit{guaranteed} to find a linear factorization of $f$ in $\mathbb{C}[x]$. Let's define this a bit more generally for arbitrary fields $F \subset K$. 

  \begin{definition}[Field Extension]
    The pair of fields $F \subset K$ is called a \textbf{field extension}. 
  \end{definition}

  Therefore, if $K$ is algebraically closed and $F \subset K$ is a field extension, $f(x) \in F[x]$ is guaranteed to \textit{split} completely into linear factors. This is true for \textit{all} $f(x) \in F[x]$, but now if we \textit{fix} $f(x) \in F[x]$, perhaps we don't need the entire field $K$ to split $f(x)$. Maybe we can work in a slightly larger field $E$---such that $F \subset E \subset K$---where $f(x)$ splits in $E$. This process of finding such a minimal field is important to understand the behavior of roots of such polynomials. 

  \begin{definition}[Splitting Field]
    Given a field extension $F \subset K$ and a polynomial $f \in F[x]$, 
    \begin{enumerate}
      \item $f$ \textbf{splits} in $K$ if $f$ can be written as the product of linear polynomials in $K[x]$. 
      \item If $f$ splits in $K$ and there exists no field $E$ s.t. $F \subsetneq E \subsetneq K$, then $K$ is called a \textbf{splitting field} of $f$.\footnote{i.e. the splitting field is the smallest field that splits $f$.} 
    \end{enumerate}
  \end{definition}

  \begin{example}[Don't Need(?) Complex]
    Consider the following. 
    \begin{enumerate}
      \item Let $f(x) = x^2 - 1$. If $f(x) \in \mathbb{R}[x]$, it does split in $\mathbb{R}$. In fact, even if we consider it as an element of $\mathbb{Z}_2 [x]$, it still splits into $(x + 1)(x - 1)$. 
      \item Let $f(x) = x^2 - 2$. If $f(x) \in \mathbb{Q}[x]$, it doesn't split in $\mathbb{Q}$ since the roots $\pm \sqrt{2} \not\in \mathbb{Q}$, but $\pm \sqrt{2}$ are real numbers, so $f(x)$ does in fact split in $\mathbb{R}$ since it splits into $(x + \sqrt{2}) (x - \sqrt{2})$. However, maybe it is not the (smallest) splitting field. 
      \item Let $f(x) = x^2 + 1$. We can see that if we consider it as an element of $\mathbb{Q}[x]$ or $\mathbb{R}[x]$, neither fields split $f(x)$ since $\pm i$ are its roots and therefore are contained in the coefficients of its linear factors. We know that it definitely splits in $\mathbb{C}$, but can we find a smaller field that splits $f(x)$? Perhaps.  
    \end{enumerate}
  \end{example}

  So how does one find a splitting field? Note that in the example above, we have found that there were some roots $\alpha$ of certain polynomials $f(x) \in F[x]$ are not contained in $F$. Therefore, what we want to do is find the smallest field $F$ containing both $F$ and $\alpha$ (plus any other $\alpha$'s). This smallest such field is called an \textit{adjoining field}. 

\subsubsection{Ring Extensions}

  We will introduce this in a slightly different way, but by building up some theorems, we will unify these two soon enough. 
  
  \begin{definition}[Ring of Univariate Polynomial Elements] 
    Let $F \subset K$ be fields, $F[x]$ a polynomial ring, and a constant $\alpha \in K$, 
    \begin{equation}
      F[\alpha] \coloneqq \{ f(\alpha) \in F \mid f \in F[x]\} \subset K
    \end{equation}
  \end{definition} 

  \begin{lemma}[Ring Extension] 
    We have the following subring structure. 
    \begin{equation}
      F \subset F[\alpha] \subset K
    \end{equation}
    Furthermore, if $\alpha \not\in F$, then $F \subsetneq F[\alpha]$. 
  \end{lemma}
  \begin{proof}
    Note that $F \subset F[\alpha]$ since we can just take the constant polynomials, so this is not very interesting. Given two elements $\phi, \gamma \in F[\alpha]$, there exists polynomials $f, g \in F[x]$ s.t. $\phi = f(\alpha), \gamma = g(\alpha)$. Since $F[x]$ is a ring, we see that 
    \begin{align}
      \phi + \gamma & = f(\alpha) + g(\alpha) = (f + g)(\alpha) \\
      \phi \cdot \gamma & = f(\alpha) \cdot g(\alpha) = (fg)(\alpha)
    \end{align} 
    Furthermore, it is easy to check that $0$ and $1$ are the images of $\alpha$ through the $0$ and $1$ polynomials. What allows us to make this inclusion proper is that the $\alpha \in K$, which does not necessarily have to be in $F$, \textit{extends} this field a bit further, but since we can only map the one element $\alpha$, it may not cover all of $K$. 
  \end{proof} 

  Let's go through some examples. 

  \begin{example}[Radical Extensions of $\sqrt{2}$]
    Let $F = \mathbb{Q}$ and $K = \mathbb{C}$. We claim $\mathbb{Q}[\sqrt{2}] = \{a + b \sqrt{2} \mid a, b \in \mathbb{Q} \}$.
    \begin{enumerate}
      \item $\mathbb{Q}[\sqrt{2}] \subset \{a + b \sqrt{2} \mid a, b \in \mathbb{Q} \}$. $\mathbb{Q}[\sqrt{2}]$ are elements of the form
      \begin{equation}
        f(\sqrt{2}) = a_n (\sqrt{2})^n + a_{n-1} (\sqrt{2})^{n-1} + \ldots + a_2 (\sqrt{2})^2 + a_1 \sqrt{2} + a_0
      \end{equation} 
      This can be written by collecting terms, of the form $a + b \sqrt{2}$. 

      \item $\mathbb{Q}[\sqrt{2}] \supset \{a + b \sqrt{2} \mid a, b \in \mathbb{Q} \}$. Given an element $a + b \sqrt{2}$, this is clearly in $\mathbb{Q}[\sqrt{2}]$ since it is the image of $\sqrt{2}$ under the polynomial $f(x) = a + bx$. 
    \end{enumerate}
  \end{example} 

  Given this, we may extrapolate this pattern and claim that $\mathbb{Q}[\sqrt{2} + \sqrt{3}]$ consists of all numbers of form $a + (\sqrt{2} + \sqrt{3}) b$. However, this is \textit{not} the case. 

  \begin{example}
    Given any element $\beta \in \mathbb{Q}[\sqrt{2} + \sqrt{3}]$, it is by definition of the form 
    \begin{equation}
      \beta = \sum_{k=0}^n a_k (\sqrt{2} + \sqrt{3})^k 
    \end{equation} 
    Clearly $1, \sqrt{2} + \sqrt{3} \in \mathbb{Q}[\sqrt{2} + \sqrt{3}]$ by mapping $\sqrt{2} + \sqrt{3}$ through the polynomials $f(x) = 1$ and $f(x) = $. However, we can see that $(\sqrt{2} + \sqrt{3})^2 = 5 + \sqrt{6}$,\footnote{where we use $\sqrt{6}$ as notation for $\sqrt{2} \cdot \sqrt{3}$} and so $\sqrt{6} \in \mathbb{Q}[\sqrt{2} + \sqrt{3}]$. Furthermore, we have $(\sqrt{2} + \sqrt{3})^3 = 11 \sqrt{2} + 9 \sqrt{3}$, and so with the ring properties we can conclude that 
    \begin{align}
      \frac{1}{2} \big[ (11 \sqrt{2} + 9 \sqrt{3}) - 9 (\sqrt{2} + \sqrt{3})\big] = \sqrt{2} & \in \mathbb{Q}[\sqrt{2} + \sqrt{3}] \\
      -\frac{1}{2} \big[ (11 \sqrt{2} + 9 \sqrt{3}) - 11 (\sqrt{2} + \sqrt{3})\big] = \sqrt{3} & \in \mathbb{Q}[\sqrt{2} + \sqrt{3}] \\
    \end{align} 
    If we go a bit further, we can show that 
    \begin{equation}
      \mathbb{Q}[\sqrt{2} + \sqrt{3}] = \{a + b \sqrt{2} + c \sqrt{3} + d\sqrt{6} \mid a, b, c, d \in \mathbb{Q} \}
    \end{equation}
  \end{example}

  This method in which we have taken higher powers of $\alpha$ to reveal elements in $\mathbb{Q}$ reveals a deeper structure of a finite-dimensional vector space, which will be useful for analyzing certain fields in the examples below. 

  \begin{lemma}[Vector Space Structure]
    $F[\alpha]$ is a finite-dimensional vector space over $F$. If $f(x) = a_n x^n + \ldots a_0$, then $S = \{1, \alpha, \ldots, \alpha^{n-1}\}$ spans $F[\alpha]$.\footnote{Note that this does not mean that it is a basis.} 
  \end{lemma}
  \begin{proof}
    An element of $F[\alpha]$ is of the form 
    \begin{equation}
      f(\alpha) = \sum_{k=0}^n a_k \alpha^k
    \end{equation} 
    for some $f \in F[x]$, and so it is immediate that $\{\alpha^k\}_{k \in \mathbb{N}_0}$ spans $F[\alpha]$. We claim that $\alpha^{n-1+i}$ is in $S$ for all $i > 0$. By induction, if $i = 1$, then 
    \begin{equation}
      \alpha^n = -\frac{1}{a_n} \big( a_{n-1} \alpha^{n-1} + \ldots + a_0 \big)
    \end{equation}
    which proves the claim. Now assume that $\alpha^n, \alpha^{n+1}, \ldots, \alpha^{n-1+i} \in \Span\{1, \ldots, \alpha^{n-1}\}$. Then 
    \begin{equation}
      \alpha^i f(\alpha) = 0 \implies a_n \alpha^{n+i} + \alpha_{n-1} \alpha^{n+i-1} + \ldots + a_0 \alpha^i = 0 
    \end{equation}
    and so 
    \begin{equation}
      \alpha^{n+i} = -\frac{1}{a_n} \big(a_{n-1} \alpha^{n+i-1} + \ldots + a_0 \alpha^i)
    \end{equation}
    which means that $\alpha^{n+i} \in \Span\{1, \ldots, \alpha^{n-1}\}$, completing the proof. 
  \end{proof} 

\subsubsection{Field Extensions} 
  
  Great, so we automatically have the ring and vector space structures on $F[\alpha]$. However, what we would really like is a field structure since that was our original goal. Remember that $F[\alpha]$ is a ring that contains both $F$ and $\alpha$. With one more assumption, we can claim that it is a field. 

  \begin{theorem}[Adjoining Fields]
    Given fields $F \subset K$, if there exists a $f \in F[x]$ s.t. $\alpha \in K$ is a root of $f$, then $F[\alpha] \subset K$ is a field. To emphasize that it is a field, we usually denote it as $F(\alpha)$ and refer it as the field obtained by \textbf{adjoining} $\alpha$ to $F$. 
  \end{theorem}
  \begin{proof}
    It is clear that $F[\alpha]$ is a commutative ring since $F$ is a field. So it remains to show that every nonzero element of $\beta \in F[\alpha]$ is a unit. By definition $\beta = p(\alpha)$ for some polynomial $p \in F[x]$.  Factor $f \in F[x]$ as the product of irreducible polynomials. Then $\alpha$ must be a root of one of those irreducible factors, say $g(x)$. Note that $g(x) \nmid p(x)$ since $p(\alpha) \neq 0$. Since $g$ is irreducible, we know that $\gcd(g, p) = 1$ and so $\exists s, t \in F[x]$ s.t. 
    \begin{equation}
      1 = s p + t g \implies 1 = s(\alpha) p(\alpha) + t(\alpha) g(\alpha) = s(\alpha) p(\alpha)
    \end{equation}  
    Therefore we have found a multiplicative inverse $s = p^{-1} \in F[\alpha]$. 
  \end{proof} 
  \begin{proof}
    We can prove it using the vector space structure. Treating $F[\alpha]$as a finite-dimensional vector space over $F$, let us define the $F$-linear function\footnote{linearity is easy to check}
    \begin{equation}
      m_b: F[\alpha] \rightarrow F[\alpha], \qquad m_b (\beta) = b\beta
    \end{equation} 
    Since $F[\alpha] \subset K$, $F[\alpha]$ is an integral domain. Thus $\not\exists \beta \in F[\alpha] \setminus \{0\}$ s.t. $b \beta = 0$. This means that the kernel of $m_b$ is $0$, and so $m_b$ is injective. By the rank-nullity theorem, it is bijective, and so there exists a $\beta \in F[\alpha]$ s.t. $b \beta = 1 \implies b$ is a unit. 
  \end{proof}

  \begin{corollary}[Adjoining Field is Minimal]
    $F[\alpha]$ is the smallest field containing $F$ and $\alpha$. 
  \end{corollary}

  \begin{example}[$\mathbb{Q}\lbrack \sqrt{3} i\rbrack$ is a Field]
    $\mathbb{Q}[\sqrt{3} i]$ is a field, hence denoted $\mathbb{Q}(\sqrt{3} i)$ since $\sqrt{3}i$ is a root of the polynomial $f(x) = x^2 + 3$. 
  \end{example}

  \begin{example}[$\mathbb{Q}\lbrack \pi \rbrack$ not a Field]
    However, $\mathbb{Q}[\pi]$ is not a field. 
  \end{example} 

  \begin{example}[Finding Multiplicative Inverses of elements in $\mathbb{Q}\lbrack \alpha \rbrack$]
    Given $\beta = p(\alpha) = \alpha^2 + \alpha - 1 \in \mathbb{Q}[\alpha]$, where $\alpha$ is a root of $f(\alpha) = \alpha^3 + \alpha + 1$, we first know that $\beta$ must have a multiplicative inverse since $\mathbb{Q}[\alpha]$ is a field. Applying the Euclidean algorithm, we have 
    \begin{equation}
      1 = \frac{1}{3} \big\{ (x+1) f(x) - (x^2 + 2) p(x)\big\} = -\frac{1}{3} (\alpha^2 + 2) p(\alpha)
    \end{equation}
    and so $\beta^{-1} = (\alpha^2 + \alpha - 1)^{-1} = -\frac{1}{3} (\alpha^2 + 2)$. We can check that 
    \begin{align}
      -\frac{1}{3} (\alpha^2 + 2) (\alpha^2 + \alpha - 1) & = -\frac{1}{3} (\alpha^4 + \alpha^3 + \alpha^2 + 2 \alpha - 2) \\
                                                          & = -\frac{1}{3} (\alpha^3 + \alpha - 2) \\
                                                          & = -\frac{1}{3} (-3) = 1
    \end{align}
  \end{example}

  Intuitively, the extra $\alpha \in K$ allows us to ``expand'' our field $F$ into a bigger field of $K$. We can also define this for multivariate polynomials.  

  \begin{definition}[Ring of Multivariate Polynomial Elements]
    Given a polynomial ring $F[x, y]$ over a field $F$ and constants $\alpha, \beta \in F$, the following definitions are equivalent. 
    \begin{align}
      F[\alpha, \beta] & \coloneqq \{ f(\alpha, \beta) \in F \mid f \in F[x, y] \} \\ 
                       & = (F[\alpha])[\beta] \\
                       & = (F[\beta])[\alpha]
    \end{align}
  \end{definition}
  \begin{proof}
    
  \end{proof} 
  
  \begin{example}[Extensions of $\sqrt{2}$ and $i$]
    We claim that 
    \begin{equation}
      \mathbb{Q}[\sqrt{2}, i] = \{ a + b \sqrt{2} + ci + d(\sqrt{2} i) \mid a, b, c, d \in \mathbb{Q}\}
    \end{equation}
    From the previous example, we know that $\mathbb{Q}[\sqrt{2}]$ are all numbers of the form $a + b\sqrt{2}$. Now we take $i \in \mathbb{C}$ and map it through all polynomials with coefficients in $\mathbb{Z}[\sqrt{2}]$, which will be of form 
    \begin{equation}
      f(i) = (a_n + b_n \sqrt{2}) i^n + (a_{n-1} + b_{n-1}\sqrt{2}) i^{n-1} + \ldots + (a_2 + b_2 \sqrt{2}) i^2 + (a_1 + b_1 \sqrt{2}) i + (a_0 + b_0 \sqrt{2})
    \end{equation} 
    However, we can see that since $i^2 = -1$, we only need to consider up to degree 1 polynomials of form 
    \begin{equation}
      (a + b \sqrt{2}) + (c + d \sqrt{2}) i 
    \end{equation}
    which is clearly of the desired form. For the other way around, this is trivial since we can construct a linear polynomial as before. 
  \end{example} 

  \begin{example}
    We claim $\mathbb{Q}[\sqrt{3} + i] = \mathbb{Q}[\sqrt{3}, i]$. 
    \begin{enumerate}
      \item $\mathbb{Q}[\sqrt{3} + i] \subset \mathbb{Q}[\sqrt{3}, i]$
      \item $\mathbb{Q}[\sqrt{3} + i] \supset \mathbb{Q}[\sqrt{3}, i]$. Note that 
        \begin{align}
          (\sqrt{3} + i)^3 = 8i & \implies i \in \mathbb{Q}[\sqrt{3} + i] \\
                                & \implies (\sqrt{3} + i) - i = \sqrt{3} \in \mathbb{Q}[\sqrt{3} + i] 
        \end{align}
        Therefore, $\mathbb{Q}[\sqrt{3} + i]$ contains the elements $1, \sqrt{3}, i$, which form the basis of $\mathbb{Q}[\sqrt{3}, i]$. 
    \end{enumerate}
  \end{example}

  \begin{example}[Extensions of $\sqrt{3}i$ and $\sqrt{3}, i$]
    We claim that $\mathbb{Q}[\sqrt{3} i] \subsetneq \mathbb{Q}[\sqrt{3}, i]$. 
    \begin{enumerate}
      \item We can see that $\{1, \sqrt{3}i \}$ span $\mathbb{Q}[\sqrt{3}i ]$ as a $\mathbb{Q}$-vector space. Therefore, 
      \begin{equation}
        \sqrt{3}, i \in \mathbb{Q}[\sqrt{3}, i] \implies \sqrt{3} i \in \mathbb{Q}[\sqrt{3}, i]
      \end{equation} 
      implies that $\mathbb{Q}[\sqrt{3} i] \subset \mathbb{Q}[\sqrt{3}, i]$. 

      \item To prove proper inclusion, we claim that $i \not\in \mathbb{Q}[\sqrt{3}i]$. Assuming that it can, we represent it in the basis $i = b_0 + b_1 \sqrt{3} i$, and so
      \begin{equation}
        -1 = (b_0 + b_1 \sqrt{3} i)^2 = (b_0^2 - 3b_1^2) + 2b_0 b_1 \sqrt{3} i
      \end{equation}
      Therefore we must have $2b_0 b_1 \sqrt{3} = 0 \implies b_0$ or $b_1$ should be $0$. If $b_0 = 0$, then $b_0^2 - 3b_1^2 = -3 b_1^2 \implies b_1^2 = 1/3$, which is not possible since $b_1^2 \in \mathbb{Q}$. If $b_1 = 0$, then $b_0 - 3 b_1^2 = b_0^2 > 0$, and so it cannot be $-1$. 
    \end{enumerate}
  \end{example}

\subsubsection{Splitting Fields}

  Now we return to the problem of taking a polynomial $f \in \mathbb{Q}[x]$ and finding the \textit{smallest} possible field $K \subset \mathbb{C}$ s.t. $f$ can be factored as a product of linear polynomials in $K[x]$. 

  \begin{example}[Simple Splitting Fields]
    We provide some simple examples to gain intuition. 
    \begin{enumerate}
      \item Let $f(x) = x^2 + 2x + 2 \in \mathbb{Q}[x]$. Then the roots of $f(x)$ are $-1 \pm i$, so 
      \begin{equation}
        f(x) = (x - (-1 + i)) (x - (-1 - i)) 
      \end{equation}
      and we can show that $\mathbb{Q}[-1 - i, -1+i] = \mathbb{Q}[i]$ is the splitting field of $f$. 

      \item Let $f(x) = x^2 - 2x - 1 \in \mathbb{Q}[x]$. The roots are $1 \pm \sqrt{2}$, and so 
      \begin{equation}
        f(x) = (x - (1 + \sqrt{2})) (x - (1 - \sqrt{2}))
      \end{equation}
      and so $\mathbb{Q}[\sqrt{2}]$ is the splitting field of $f$. 

      \item Let $f(x) = x^6 - 1 \in \mathbb{Q}[x]$. We can factor 
        \begin{equation}
          f(x) = (x-1) (x + 1) (x^2 + x + 1) (x^2 - x + 1)
        \end{equation} 
        and the non-rational roots are $\frac{\pm 1 \pm \sqrt{3} i}{2}$. Thus the splitting field of $f$ is $\mathbb{Q}[\sqrt{3} i]$. 
    \end{enumerate}
  \end{example}

  \begin{example}
    Let $f(x) = x^4 - 2 \in \mathbb{Q}[x]$. It follows that the roots are 
    \begin{equation}
      \{ \sqrt[4]{2}, \sqrt[4]{2}, -\sqrt[4]{2}, - \sqrt[4]{2} i \} = \Big\{ \sqrt[4]{2}, \sqrt[4]{2} e^{\frac{2\pi i}{4}}, \sqrt[4]{2} e^{\frac{4\pi i}{4}}, \sqrt[4]{2} e^{\frac{6\pi i}{4}} \Big\}
    \end{equation}
    thus the splitting field of $f$ is 
    \begin{equation}
      \mathbb{Q} \big( \sqrt[4]{2}, \sqrt[4]{2} e^{\frac{2\pi i}{4}}, \sqrt[4]{2} e^{\frac{4\pi i}{4}}, \sqrt[4]{2} e^{\frac{6\pi i}{4}} \big) \subset \mathbb{Q}(\sqrt[4]{2}, e^{\frac{2\pi i}{4}})
    \end{equation}
    since $\sqrt[4]{2} e^{\frac{m \pi i}{4}} \in \mathbb{Q}(\sqrt[4]{2}, e^{\frac{2\pi i}{4}})$. In fact, the two are equal, and to prove this we can see that since we are working in a field, 
    \begin{equation}
      e^{2 \pi i / 4} = \frac{\sqrt[4]{2} e^{2\pi i/4}}{\sqrt[4]{2}} \in \mathbb{Q} \big( \sqrt[4]{2}, \sqrt[4]{2} e^{\frac{2\pi i}{4}}, \sqrt[4]{2} e^{\frac{4\pi i}{4}}, \sqrt[4]{2} e^{\frac{6\pi i}{4}} \big) 
    \end{equation}
    which implies that $\sqrt[4]{2} \in \mathbb{Q} \big( \sqrt[4]{2}, \sqrt[4]{2} e^{\frac{2\pi i}{4}}, \sqrt[4]{2} e^{\frac{4\pi i}{4}}, \sqrt[4]{2} e^{\frac{6\pi i}{4}} \big)$. Therefore we can conclude that the splitting field is 
    \begin{equation}
      \mathbb{Q} \big( \sqrt[4]{2}, \sqrt[4]{2} e^{\frac{2\pi i}{4}}, \sqrt[4]{2} e^{\frac{4\pi i}{4}}, \sqrt[4]{2} e^{\frac{6\pi i}{4}} \big) = \mathbb{Q}(\sqrt[4]{2}, e^{\frac{2\pi i}{4}})
    \end{equation}
  \end{example} 

\subsection{Reducibility of Real Polynomials}

  \begin{theorem}
    If $c$ is a complex root of polynomial $f \in \mathbb{R}[x]$, then $\bar{c}$ is also a root of the polynomial. Moreover, $\bar{c}$ has the same multiplicity as $c$. 
  \end{theorem}

  \begin{corollary}
    Every nonzero polynomial in $\mathbb{R}[x]$ factors into a product of linear terms and quadratic terms with negative discriminants. 
  \end{corollary}

  \begin{example}
    \begin{align*}
      x^5 - 1 & = (x-1) \bigg( x - \Big( \cos{\frac{2\pi}{5}} + i \sin{\frac{2\pi}{5}}\Big) \bigg) \bigg( x - \Big( \cos{\frac{2\pi}{5}} - i \sin{\frac{2\pi}{5}}\Big) \bigg) \\
      & \times \bigg( x - \Big( \cos{\frac{4\pi}{5}} + i \sin{\frac{4\pi}{5}}\Big) \bigg) \bigg( x - \Big( \cos{\frac{4\pi}{5}} - i \sin{\frac{4\pi}{5}}\Big) \bigg) \\
      & = (x-1) \bigg( x^2 - \frac{\sqrt{5} - 1}{2} x + 1\bigg) \bigg( x^2 + \frac{\sqrt{5} + 1}{2} x + 1\bigg) 
    \end{align*}
  \end{example}

  \begin{corollary}
    Every polynomial $f \in \mathbb{R}[x]$ of odd degree has at least one real root. 
  \end{corollary}
  \begin{proof}
    This is a direct result of Theorem **. Alternatively, without loss of generality we can assume that the leading coefficient of $f$ is positive. Then
    \begin{equation}
      \lim_{x \rightarrow + \infty} f(x) = + \infty, \; \lim_{x \rightarrow -\infty} f(x) = -\infty
    \end{equation}
    By the intermediate value theorem, there must be some point where $f$ equals $0$. 
  \end{proof}

  \begin{theorem}[Descartes' Rule of Signs] 
    \label{thm:descartes}
    Let $f(x) = x^n + a_{n-1}x^{n-1} + \cdots + a_1x + a_0 \in \mathbb{R}[x]$. Let $C_+$ be the number of times the coefficients of $f(x)$ change signs (here we ignore the zero coefficients); let $Z_+$ be the number of positive roots of $f(x)$, counting multiplicities. Then $Z_+ \leq C_+$ and $Z_+ \equiv C_+ \pmod{2}$. Moreover, if we set $g(x) = f(-x)$, let $C_-$ be the number of times the coefficients of $g(x)$ change signs, and $Z_-$ the number of negative roots of $f(x)$. Then $Z_- \leq C_-$ and $Z_- \equiv C_- \pmod{2}$.
  \end{theorem}

  \begin{theorem}
    The number of positive roots of $f(x)$ is the same as the number of negative roots of $f(-x)$.
  \end{theorem}

  \begin{example}[Easy Way to Find Number of Positive Roots]
    Given $f(x) = x^5 + x^4 - x^2 - 1$, 
    \begin{enumerate}
      \item We have $C_+ = 1$. By Descartes' rule of signs, it must be the case that $Z_+ \leq 1$ and $Z_+ \equiv 1 \pmod{2} \implies Z_+ = 1$. 
      \item Since $f(-x) = -x^5 + x^4 - x^2 - 1$, we have $C_- = 2$, so $Z_- = 0$ or $2$. This is the best that we can do, though it turns out that it actually has $0$ negative roots.\footnote{On the other hand, $x^5 + 3x^3 - x^2 - 1$ has 2 negative roots.} 
    \end{enumerate}
  \end{example}

  Note that if a polynomial has a multiple root but its coefficients are known only approximately (but with any degree of precision), then it is impossible to prove that the multiple roots exists because under any perturbation of the coefficients, however small, it may separate into simple roots or simply cease to exist. This fact leads to the "instability" of the Jordan Normal form because under any perturbation of the elements of a matrix $A$, the change may drastically affect the characteristic polynomial, hence affecting the geometric multiplicities of its eigenvectors. 

\subsection{Reducibility of Integer Polynomials} 

  Even though we have covered a more general theory of polynomials with rational coefficients, it is worthwhile to visit integer polynomials for two reasons. First, there are a few specialized theorems that allow us to easily determine reducibility in $\mathbb{Z}[x]$. Second, Gauss's lemma allows us to check for reducibility in $\mathbb{Q}[x]$ by checking for reducibility in $\mathbb{Z}[x]$, at which point we can abuse the specialized theorems we have developed. 

  \begin{theorem}[Rational Root Theorem]
    Let $a_n x^n + \ldots + a_0 \in \mathbb{Z}[x]$. If $r/s \in \mathbb{Q}$ with $\gcd(r, s) = 1$, then $r \mid a_0$ and $s \mid a_n$. 
  \end{theorem}
  \begin{proof}
    Given that $r/s$ is a root, we have 
    \begin{equation}
      a_n (r/s)^n + \ldots + a_0 = 0
    \end{equation}
    Multiplying by $s^n$, we get 
    \begin{equation}
      a_n r^n + a_{n-1} r^{n-1} s + \ldots + a_1 s^{n-1} r + a_0 s^n = 0
    \end{equation}
    and putting this equation on mod $r$ and mod $s$ implies that $r | a_0 s^n$ and $s | a_n r^n$, respectively. But since we assumed that $\gcd (r, s) = 1$, $r | a_0$ and $s | a_n$. 
  \end{proof}

  The next is quite a remarkable result, since it says that decompositions in $\mathbb{Q}[x]$ imply decompositions in $\mathbb{Z}[x]$! Therefore, to check irreducibility in $\mathbb{Q}[x]$, it suffices to check irreducibility in $\mathbb{Z}[x]$. 

  \begin{lemma}[Gauss's Lemma]
    Let $f \in \mathbb{Z}[x]$. If $\exists g, h \in \mathbb{Q}[x]$ s.t. $f(x) = g(x) h(x)$, then $\exists \bar{g}, \bar{h} \in \mathbb{Z}[x]$ s.t. $f(x) = \bar{g}(x) \bar{h}(x)$. 
  \end{lemma}
  \begin{proof}
    We can find $k, l \in \mathbb{Z}$ s.t. $g_1 (x) = k g(x)$ and $h_1 (x) = l h(x)$ have integer coefficients, i.e. $g_1, h_1 \in \mathbb{Z}[x]$. Then, $k l f(x) = g_1 (x) h_1 (x) \in \mathbb{Z}[x]$. Let $p$ be a prime factor of $kl$. We have 
    \begin{equation}
      0 \equiv \bar{k} \bar{l} \bar{f} (x) \equiv \bar{g}_1 (x) \bar{h}_1 (x) \text{ in } \mathbb{Z}_p [x]
    \end{equation}
    Since $\mathbb{Z}_p$ is an integral domain, $\mathbb{Z}_p [x]$ is an integral domain, and so $\bar{g}_1$ or $\bar{h}_1$ must be $0$. WLOG let it be $\bar{g}_1$. Then every coefficient of $g_1 (x)$ is divisible by $p$, and we can write it in the form $g_2(x) = p g_1 (x)$. Therefore, 
    \begin{equation}
      p(x) \cdot \frac{kl}{p} = \underbrace{\frac{g_1 (x)}{p}}_{g_2 (x)} \cdot \underbrace{h_1 (x)}_{h_2 (x)} \iff f(x) \frac{kl}{p} = g_2 (x) h_2 (x)
    \end{equation}
    Since there are only finitely many prime divisors, we do this for all prime factors of $kl$, and we have 
    \begin{equation}
      f(x) = g_n (x) h_n (x), \qquad g_n, h_n \in \mathbb{Z}[x]
    \end{equation}
  \end{proof}

  \begin{example}[Reducibility of Integer Polynomials]
    Let $f(x) = x^4 - x^3 + 2$. The rational roots are in the set $S = \{\pm 1, \pm2 \}$, but none of them work since $f(\pm1), f(\pm2) \neq 0$. By degree considerations and Gauss's lemma, if $f(x)$ is reducible, then 
    \begin{equation}
      f(x) = (x^2 + ax + b) (x^2 + cx + d), \qquad a, b, c, d \in \mathbb{Z}
    \end{equation}
    We know that $bd \in S$, with $a + c = -1$, $d + b + ac = 0$, and so on for each coefficients. We can brute force this finite set of possibilities. 
  \end{example}

  A great way to check irreducibility is to check in mod $p$. 

  \begin{theorem}
    Let $f(x) = a_n x^n + \ldots + a_0 \in \mathbb{Z}[x]$. If $p \nmid a_n$ and $f \in \mathbb{Z}_p [x]$ is irreducible, then $f$ is irreducible in $\mathbb{Q}[x]$.\footnote{May need to verify this again.}
  \end{theorem}
  \begin{proof}
    Suppose that $f(x) = g(x) h(x) \in \mathbb{Z}[x]$ with $\deg(g), \deg(h) > 0$. Then 
    \begin{equation}
      f(x) \equiv g(x) h(x) \text{ in } \mathbb{Z}_p [x]
    \end{equation}
    Since $f(x)$ is irreducible in $\mathbb{Z}_p [x]$, we must have that one of $g(x)$ or $h(x)$ has degree $0$ in $\mathbb{Z}_p [x]$. WLOG let it be $g(x)$, but this means that the leading coefficient of $g(x)$ must be divisible by $p \implies$ leading coefficient of $f(x)$ is divisible by $p \iff p \mid a_n$. 
  \end{proof}

  \begin{example}
    $x^4 + x + 1$ is irreducible in $\mathbb{Z}_2 [x]$. So we can extend this to $\mathbb{Z}[x]$ to see that \textit{all} fourth degree polynomials of form $a x^4 + b x^3 + c x^2 + dx + e$, which $a, d, e$ odd and $b, c$ even is irreducible in $\mathbb{Q}[x]$. 
  \end{example}

  This is a powerful theorem to quickly find a large class of polynomials that are irreducible. However, being reducible in $\mathbb{Z}_p [x]$ does not imply reducibility in $\mathbb{Q}$. In fact, there are polynomials $f(x) \in \mathbb{Z}[x]$ which are irreducible but reducible in $\mathbb{Z}_p$ for \textit{every} prime $p$. 

  \begin{theorem}[Eisenstein's Criterion]
    Let $f(x) = a_n x^n + \ldots + a_0 \in \mathbb{Z}[x]$ and $p \in \mathbb{Z}$ a prime s.t. $p \nmid a_n$, $p \mid a_i$ for $i = 0, \ldots, a_{n-1}$, and $p^2 \nmid a_0$. Then $f(x)$ is irreducible in $\mathbb{Q}[x]$. 
  \end{theorem}
  \begin{proof}
    Suppose that $f(x) = g(x) h(x) \in \mathbb{Q}[x]$ with $\deg(g), \deg(h) > 0$. Then, by Gauss's lemma, $g, h \in \mathbb{Z}[x]$. Reducing the equations mod $p$, 
    \begin{equation}
      f(x) = g(x) h(x) \text{ in } \mathbb{Z}_p [x]
    \end{equation}
    But $f(x) = a_n x^n$. By unique factorization theorem in $\mathbb{Z}_p [x]$, $g, h \in \mathbb{Z}_p [x]$ must be products of units and prime factors of $a_n x^n$, which are $\{x\}$. Therefore, let 
    \begin{equation}
      g(x) = b_m x^m, h(x) = \frac{a_n}{b_m} x^{n-m} \in \mathbb{Z}_p [x]
    \end{equation}
    with $\deg(g) = m > 0$ and $\deg(h) = n - m > 0$ in $\mathbb{Z}[x]$. This implies that the constant coefficients of $g(x), h(x)$ are divisible by $p$, which implies that the constant coefficients of $f(x) = g(x) h(x)$ are divisible by $p^2$, a contradiction. 
  \end{proof}

  \begin{example}[Easy Checks for Irreducibility with Eisenstein]
    Listed. 
    \begin{enumerate}
      \item $x^{13} + 2x^{10} + 4x + 6$ is irreducible in $\mathbb{Q}[x]$ by Eisenstein for $p = 2$. 
      \item $x^3 + 9x^2 + 12x + 3$ is irreducible in $\mathbb{Q}[x]$ by Eisenstein for $p = 3$. 
      \item Let $f(x) = x^4 + x^3 + x^2 + x + 1$. Then, we know that $f(x) = \frac{x^5 - 1}{x-1}$ and so 
      \begin{align}
        f(x + 1) & = \frac{(x + 1)^5 - 1}{(x + 1) - 1} \\
                 & = \frac{1}{x} \bigg( x^5 + \binom{5}{1} x^4 + \binom{5}{2} x^3 + \binom{5}{3} x^2 + \binom{5}{4} x + \binom{5}{5} - 1 \bigg) \\
                 & = x^4 + 5x^3 + 10 x^2 + 10x + 5
      \end{align}
      So all nonleading coefficients are divisible by $5$ exactly once, which by Eisenstein implies that $f(x+1)$ is irreducible which implies that $f(x)$ is irreducible. 
    \end{enumerate}
  \end{example}

  We have prod that for $\alpha \in \mathbb{C}$, subfield $F \subset \mathbb{C}$, and $f(x) \in F[x]$, with $f(\alpha) = 0$, then $B = \{1, \alpha, \ldots, \alpha^{\deg(f) - 1}\}$ spans $F[\alpha]$ as a $F$-vector space. If $f(x)$ is irreducible then $B$ is a basis. 



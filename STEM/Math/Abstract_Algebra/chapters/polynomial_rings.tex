\section{Polynomial Rings} 

  In ring theory, the idea of \textit{adjoining} an existing ring $R$ with an arbitrary element $x$ will appear frequently. That is, given a ring $R$ and some $x$, can we try and construct a new ring $S$, denoted $R[x]$ that is the minimal ring containing both $R$ and $x$? What kind of elements would be in $R[x]$? 
  \begin{enumerate}
    \item Since $R \subset R[x]$, it must be the case that for $a \in R$, $a \in R[x]$. 
    \item Since $x \in R[x]$, it must be the case that $a x \in R[x]$ for all $a \in R$. 
    \item $x \times x = x^2 \in R[x]$, so it must be the case that $a x^2 \in R[x]$ 
    \item In general, for any $n \in \mathbb{N}$, $x^n \in R[x]$, so it must be the case that $a x^n \in R[x]$. 
  \end{enumerate}
  If we add these terms up, we have elements of the general form 
  \begin{equation}
    a_n x^n + a_{n-1} x^{n-1} + \ldots + a_1 x + a_0 
  \end{equation}
  This is what we refer to as a polynomial, and it indeed does have a ring structure. More generally, given a set of elements $S = \{x_1, \ldots, x_n\}$, $R[S]$ can be defined accordingly. Now let's formally define them. 

  \begin{definition}[Polynomial Ring]
    Given ring $R$ and a set of \textbf{indeterminates} $S = \{x_1, \ldots, x_n\}$, the \textbf{ring of polynomials} $R[S] = R[x_1, \ldots, x_n]$ is defined in the equivalent ways. 
    \begin{enumerate}
      \item It is the minimal ring containing $R$ as a subring and $x$. 
      \item It is the ring of formal expressions of the form 
      \begin{equation}
        f(x_1, \ldots, x_n) = \sum_{0 \leq k_i \leq n} a_{k_1 \ldots k_n} x_1^{k_1} x_2^{k_2} \ldots x_n^{k_n}, \quad a_i \in R
      \end{equation}
      which for univariate polynomials simplifies to 
      \begin{equation}
        f(x) = a_nx^n + a_{n-1}x^{n-1} + \dots + a_1 x + a_0, \quad a_i \in R
      \end{equation}
    \end{enumerate}
    From both definitions is becomes clear through the ring properties that 
    \begin{enumerate}
      \item Addition is defined component-wise: $a x^i + b x^i = (a + b) x^i$. 
      \item Multiplication is defined component-wise: $x^i x^j = x^{i + j}$. 
      \item Additive and multiplicative identities is $0$ and $1$. 
    \end{enumerate}
  \end{definition}

  Note that $x$ is just a formal symbol, whose powers $x^i$ are just placeholders for the corresponding coefficients $a_i$ so that the given formal expression is a way to encode the finitary sequence. $(a_0, a_1, a_2, ..., a_n)$. Two polynomials are equal if and only if the sequences of their corresponding coefficients are equal. Also note that unlike a function, which we write as $f$, for a polynomial we should write the indeterminate $f(x)$. We can however interpret $f(x)$ as a function as well (which may not be unique), and doing so allows us to determine special properties of $f(x) \in R[x]$. Before we move on, let's get some terms out of the way. 

  \begin{definition}[Some Terms for Polynomials]
    Given a univariate polynomial $f(x) \in R[x]$. 
    \begin{enumerate} 
      \item The \textbf{leading coefficient} is the last nonzero coefficient  
      \item The \textbf{degree} of $f$---denoted $\deg f$---is the index of the leading coefficient.
      \item A \textbf{monomial} is a polynomial of a single term $a_j x^j$. 
      \item A \textbf{linear} polynomial is a polynomial of degree 1. 
      \item A \textbf{quadratic} polynomial is a polynomial of degree 2. 
      \item A \textbf{cubic} polynomial is a polynomial of degree 3. 
    \end{enumerate}
  \end{definition}

  We need to be very careful about the properties that hold for polynomials, as they may not be intuitive. For example, for certain finite fields (which are rings), some formally different polynomials may be indistinguishable in terms of mappings.\footnote{$x$ and $x^2$ are equivalent in  $\mathbb{Z}_2 [x]$. } Second, a polynomial may have more roots than its degree. Therefore, we will work in different rings $R$ and provide conditions where our intuition is true in $R[x]$. It is clear that if you have two polynomials of degree $n$ and $m$, their sum may be degree $k < n, m$. This is not always true for multiplication. 

  \begin{example}[Product of Two Linear Polynomials is $0$]
    Given $f, g \in \mathbb{Z}_6 [x]$ with $f(x) = 2x + 4$ and $g(x) = 3x + 3$, we have 
    \begin{equation}
      f(x) \cdot g(x) = (2x + 4)(3x + 3) = 6x^2 + 18 x + 12 = 0
    \end{equation}
  \end{example}

  There is a simple condition in which the degree is additive, however. 

  \begin{theorem}[Bounds on Degrees From Operations]
    Given that $R$ is a ring and $f, g \in R[x]$, 
    \begin{equation}
      \deg(f+g) \leq \max\{\deg f, \deg g\} \\
    \end{equation}
    If $R$ is a domain, then 
    \begin{equation}
      \deg (f g) = \deg f + \deg g
    \end{equation}
    Note that this automatically implies that $R[x]$ is a domain. Combined with the lemma above, we have: $R$ is an integral domain $\implies R[x]$ is an integral domain. 
  \end{theorem}
  \begin{proof}
    The second may not be true if $R$ has zero divisors. 
  \end{proof}

  Just working in domains do not make things all better. Sometimes, we may have two different polynomials but they may define the same function from $R$ to $R$! 

  \begin{example}[Polynomials as Same Function]
    Given $f, g \in \mathbb{Z}_2 [x]$, 
    \begin{equation}
      f(x) = x \sim g(x) = x^2
    \end{equation} 
  \end{example}

  As shown in the example above, it is not so simple as to restrict which underlying set you are working on. Some rings $R$ may or may not assert uniqueness of functions in $F[x]$, and vice versa. Therefore, here are some special theorems. 

  \begin{theorem}[Uniqueness of Polynomials over Field]
    If the field $\mathbb{F}$ is infinite, then different polynomials in $\mathbb{F}[x]$ determine different functions. 
  \end{theorem}
  
  Unsurprisingly, the properties of the base ring $R$ determines the properties on $R$ that we see above. 

  \begin{lemma}[Ring Properties Extending to Polynomials]
    We have the following. 
    \begin{enumerate}
      \item $R$ is a commutative ring iff $R[x]$ is a commutative ring. 
      \item $R$ is an integral domain iff $R[x]$ is an integral domain. 
    \end{enumerate}
  \end{lemma}
  \begin{proof}
    For the first claim, 
    \begin{enumerate}
      \item $(\rightarrow)$ If $R$ is commutative then given $a, b \in R$, we have $(ax^i) (b x^j) = a (x^i b) x^j = (ab)(x^i x^j) = (ba) (x^j x^i) = (b x^j) (a x^i)$, and so from distributive property $R[x]$ is commutative. 
      \item $(\leftarrow)$ This is trivial since given $R[x]$ commutative, take $a, b \in R \subset R[x]$, and so $ab = ba$ in $R[x]$ implies commutativity in $R$. 
    \end{enumerate}
    For the second claim, it suffices to prove the domain property since commutativity is proved above. 
    \begin{enumerate}
      \item $(\rightarrow)$. Assume $R$ is a domain. Now take any two nonzero polynomials $f(x), g(x) \in R[x]$. Then look at their leading term $ax^n$ and $bx^n$. The leading coefficient of $(fg)(x)$ is $(ab) x^{n+m}$, and since $R$ is a domain $ab \neq = 0 \implies (fg)(x) \neq 0$. So $R[x]$ is a domain. 
      \item $(\leftarrow)$. This is trivial since given $R[x]$ integral domain, take $a, b \in R \subset R[x]$ with $a, b \neq 0$, and so $ab \neq 0$ since $R[x]$ is a domain. Therefore $R$ is a domain. 
    \end{enumerate}
  \end{proof} 

  With this theorem we unlock all the properties that we have studied in general for the subclasses of rings. Almost always we will assume that $R$ is at least commutative. 

\subsection{Euclidean Division} 

  Just like how we can do Euclidean division with integers, there is an analogous result for polynomials. However, we require to work with a \textit{field} $F$ rather than an arbitrary ring $R$. 

  \begin{theorem}[Polynomials as Euclidean Domain]
    $F$ is a field $\implies F[x]$ is a Euclidean domain. That is, given polynomials $f(x), g(x) \in F[x]$, there are unique polyomials $q(x), r(x) \in F[x]$ s.t. 
    \begin{equation}
      f(x) = q(x) g(x) + r(x), \qquad \deg(r(x)) < \deg(g(x))
    \end{equation} 
  \end{theorem} 
  \begin{proof}
    We first prove existence. If $\deg(f(x)) < \deg(g(x))$, then we can trivially set $q(x) = 0, r(x) = f(x)$. Therefore we can assume that $\deg(f(x)) \geq \deg(g(x))$. We can prove this by strong induction on $k = \deg(f(x))$. Assume that $\deg(f(x)) = 1$. Then if $\deg(g(x)) > 1$ it is trivial as before, so we show for $\deg(g(x)) = 1$. So let 
    \begin{equation}
      f(x) = a_1 x + a_0, \qquad g(x) = b_1 x + b_0 
    \end{equation}
    and we can find the solutions 
    \begin{equation}
      f(x) = \frac{a_1}{b_1} g(x) + \bigg( a_0 - \frac{a_1 b_0}{b_1} \bigg)
    \end{equation} 
    Now suppose that the results is known for whenver $\deg(f(x)) \leq k$ and we have a polynomial $F(x) = a_{k+1} x^{k+1} + \ldots a_0$ of degree $k+1$. Then we must check that there exists a quotient and remainder for $0 \leq \deg(g(x)) = m \leq k + 1$. Note that the coefficients of $x^{k+1}$ in $F(x)$ and in the polynomial $\frac{a_{k+1}}{b_m} x^{k+1-m} g(x)$ are the same, so the polynomial 
    \begin{equation}
      f(x) = F(x) - \frac{a_{k+1}}{b_m} x^{k+1-m} g(x) 
    \end{equation}
    has degree at most $k$. Thus by our induction hypothesis we can write $f(x) = q(x) g(x) + r(x)$, and so 
    \begin{align}
      F(x) & = f(x) + \frac{a_{k+1}}{b_m} x^{k+1-m} g(x) \\
           & = q(x) g(x) + r(x) + \frac{a_{k+1}}{b_m} x^{k+1-m} g(x) \\ 
           & = \bigg( q(x) + \frac{a_{k+1}}{b_m} x^{k+1-m} \bigg) g(x) + r(x)
    \end{align} 
    which is indeed a decomposition. Now to prove uniqueness, suppose we had two different decompositions 
    \begin{equation}
      f(x) = q(x) g(x) + r(x) = q^\prime (x) g(x) + r^\prime (x) \implies \big( q(x) - q^\prime (x) \big) g(x) = r(x) - r^\prime (x)  
    \end{equation}  
    IF $q(x) \neq q^\prime (x)$, then the degree of the LHS is at least $\deg(g(x))$, while the degree of the RHS must be strictly less, a contradiction. 
  \end{proof}

  \begin{example}[Polynomial Long Division] 
    The algorithimc way to get such $q(x), r(x)$ is through \textit{polynomial long division}. 
    
    \begin{center}
      \polylongdiv{x^3 + 4x^2 - x + 7}{x - 2}
    \end{center}

    Given field $\mathbb{Z}_5$, $\mathbb{Z}_5[x]$ is a Euclidean domain, with Euclidean division.  
  \end{example} 

  In fact, it turns out that you don't necessarily require a polynomial to always come from a field in order to do long division. You can do polynomial long division over \textit{any} commutative rings, as long as the leading coefficient of the divisor is a unit (and since all elements of a field are units, we can do so). This is because at each step, you only need to divide the leading coefficient of the divisor into the leading coefficient of the polynomial you have left. An immediate consequence of this theorem is the following. 

  \begin{corollary}[Remainder Theorem]
    Let $c \in F$ and $f(x) \in F[x]$. When we divide $f(x)$ by $g(x) = x - c$, the remainder is $f(c)$. 
  \end{corollary}
  \begin{proof}
    By the Euclidean algorithm, 
    \begin{equation}
      f(x) = (x - c) q(x) + r(x) \implies f(c) = (c - c) q(c) + r(c) = r(c)
    \end{equation}
  \end{proof} 

\subsection{Roots and Factorization} 

  Next, we can define the all too familiar factors and roots of a polynomial. 

  \begin{definition}[Factor]
    Given a ring $R$ and a polynomial $f(x) \in R[x]$, if there exists $g(x), h(x)$ of degree at least 1 such that 
    \begin{equation}
      f(x) = g(x) h(x)
    \end{equation}
    then $g, h$ are said to be \textbf{factors}, or \textbf{divisors}, of $f$. If there are no such factors of $f$, then $f(x)$ is said to be \textbf{irreducible}. 
  \end{definition}

  Irreducible polynomials are analogous to prime numbers in $\mathbb{Z}$. 

  \begin{definition}[Polynomial Root]
    An element $r \in R$ is a \textbf{root} of polynomial $f \in R[x]$ if and only if 
    \begin{equation}
      f(r) = 0
    \end{equation}
  \end{definition} 

  Note that both factors and roots are intimately tied to Euclidean division, so the two are closely related. 

  \begin{theorem}[Root-Factor Theorem]
    Given a commutative ring $R$ (usually $R$ is a field) and $f(x) \in R[x]$, $(x - c)$ is a factor of $f(x)$, i.e. can be factored into 
    \begin{equation}
      f(x) = (x - c) q(x) 
    \end{equation}
    for some $q(x) \in R[x]$ of degree $\deg(f) - 1$ if and only if $f(c) = 0$.\footnote{Note that this is not true for an arbitrary ring. $R$ must be commutative at least.}
  \end{theorem} 
  \begin{proof}
    We prove for when $R$ is a field $F$, but it turns out that the theorem also holds for commutative rings $R$. 
    \begin{enumerate}
      \item $(\rightarrow)$. Given that $(x - c)$ is a factor of $f(x)$, this means that by the Euclidean algorithm $f(x) = (x - c) q(x)$ for some $q(x)$, and so $f(c) = (c - c) q(c) = 0$. 
      \item $(\leftarrow)$. Given that $f(c) = 0$. By the remainder theorem this means that when we divide $f(x)$ by $(x - c)$, the remainder is $f(c) = 0$, and so $f(x) = (x - c) q(x) + 0 = (x - c) q(x) \implies (x - c)$ is a factor of $f(x)$. 
    \end{enumerate}
  \end{proof}

  Notice how these polynomials mimick integers, and to drive this point even further, let's introduce the greatest common divisor. 

  \begin{theorem}[GCD of Two Polynomials Exist]
    Given nonzero polynomials $f(x), g(x) \in F[x]$, let 
    \begin{equation}
      S = \{h(x) \in F[x] \mid h(x) = a(x) f(x) + b(x) g(x) \text{ for some } a(x), b(x) \in F[x] \}
    \end{equation} 
    Then there exists some polynomial $d(x) \in S$ of smallest degree, and every $h(x) \in S$ is divisible by $d(x)$. 
  \end{theorem}
  \begin{proof}
    The existence is trivial since by the well-ordering principle on the degrees of polynomials in $S$, such a minimal degree must exist. Now we prove the second claim by proving $d(x) \mid f(x)$. We apply the division algorithm to write 
    \begin{equation}
      f(x) = q(x) d(x) + r(x)
    \end{equation}
    If $r(x) = 0$, then by root factor theorem we are done. If $r(x) \neq 0$, we then write 
    \begin{align}
      r(x) & = f(x) - q(x) d(x) \\ 
           & = f(x) - \big( s(x) f(x) + t(x) g(x) \big) q(x) \\ 
           & = \big( 1 - s(x) q(x) \big) f(x) - \big( t(x) q(x) \big) g(x) \in S
    \end{align}
    Since $r(x) \in S$ due to its form, the fact that $\deg(r(x)) < \deg(d(x))$ contradicts the way that $d(x)$ was chosen. Therefore $r(x) = 0$. It turns out that $d(x)$ is unique up to a constant factor. 
  \end{proof}

  \begin{definition}[GCD]
    $d(x)$ as above is called the \textbf{greatest common divisor} of $f(x), g(x)$, denoted $d(x) = \gcd(f(x), g(x))$ satisfying 
    \begin{enumerate}
      \item $d(x) \mid f(x)$, $d(x) \mid g(x)$, and 
      \item $\forall e(x) \in F[x]$, if $e(x) \mid f(x)$ and $e(x) \mid g(x)$, then $e(x) \mid d(x)$. 
    \end{enumerate}
    $f(x), g(x)$ are said to be \textbf{relatively prime} if $\gcd(f(x), g(x)) = 1$. 
  \end{definition}

  The algorithmic way for computing the GCD is done the same way by performing Euclidean algorithm on two polynomials: dividing on by the other, taking the remainder, and dividing the lesser degree by the remainder again, until the remainder is $0$. 

  \begin{lemma}
    Suppose $f(x)$ is irreducible and $f(x) \mid g(x) h(x)$. Then $f(x) \mid g(x)$ or $f(x) \mid h(x)$. 
  \end{lemma}

  Now, we show an extremely important theorem. This should be intuitive since $F$ a field implies $F[x]$ a Euclidean domain, which is a PID, which has the unique factorization theorem. 

  \begin{theorem}[Unique Factorization of Polynomials over Fields]
    Given field $F$ and nonconstant polynomial $f(x) \in F[x]$ of degree $n$, we can always write $f(x)$ as a unique\footnote{up to constant factors and rearrangement} product of at most $n$ irreducible polynomials in $F[x]$. 
  \end{theorem}
  \begin{proof}
    To prove the bound, the general idea is that by the root factor theorem, each root gives rise to a linear factor, and so inductively we cannot have more than $n$ linear factors.  
    Strong induction on degree of $f(x)$ by starting with linear. 
  \end{proof}

  Note that this is \textit{not} true in arbitrary rings. 

  \begin{example}[Linear Polynomial with 3 Roots]
    Consider $f(x) = x^2 - 1 \in \mathbb{Z}_8 [x]$, a commutative ring. Then $1, 3, 5, 7$ are all roots of $f(x)$, which is greater than its degree. Furthermore, it has two different factorizations 
    \begin{equation}
      x^2 - 1 = (x + 1)(x - 1) = (x + 3)(x - 3)
    \end{equation}
  \end{example} 

  \begin{theorem}[Interpolation]
    For any collection of given field values $y_1, y_2, ..., y_n \in F$ at given distinct points $x_1, x_2, ..., x_n \in F$, there exists a unique polynomial $f \in F[x]$ with deg$\, f < n$ such that
    \begin{equation}
      f(x_i) = y_i, \quad i = 1, 2, ..., n
    \end{equation}
    This is commonly known as the \textbf{interpolation problem}, and when $n = 2$, this is called \textbf{linear interpolation}. 
  \end{theorem} 

\subsection{Algebraically Closed Fields} 

  Now that we have seen some examples of fields, what properties would we like it to have? Going back to polynomials, recall that if $F$ is a field, then $F[x]$ as a Euclidean domain gave us a lot of nice properties, such as admitting a unique factorization of irreducible polynomials. However, we have only proved that the number of roots is \textit{at most} the degree $n$, but not that it actually reaches $n$. In fact, in a more extreme case, a polynomial may not even factor \textit{at all} in $F[x]$, since it could be irreducible. So while we have defined an upper bound for the number of roots for a polynomial, we have not determined whether a polynomial has any roots at all, i.e. a lower bound. 

  We don't have much \textit{control} over what these irreducible polynomials can look like. We may have to check---either through theorems or manually---that a polynomial or arbitrary degree is irreducible. If we would like to assert that all irreducible polynomials must be of smallest degree---that is, linear---then such a field is called \textit{algebraically closed}. 
  This algebraic closed property asserts also that the lower bound on the number of (non-unique) factors is $n$. 

  \begin{definition}[Algebraically Closed Field]
    A field $F$ is \textbf{algebraically closed} if every polynomial of positive degree (i.e. non-constant) in $F[x]$ has at least one root in $F$. 
  \end{definition}

  This is equivalent to saying that every polynomial can be expressed as a product of first degree polynomials. To extend our analysis more, we can talk about the multiplicity of these factors, which just tells us more about how many unique and non-unique factors a polynomial has. 

  \begin{definition}[Multiplicity]
    A root $c$ of polynomial $f(x) \in F[x]$ is called simple if $f(x)$ is not divisible by $(x - c)^2$ and multiple otherwise. The \textbf{multiplicity} of a root $c$ is the maximum k such that $(x - c)^k$ divides $f(x)$ .
  \end{definition} 

  To restate the root-factor theorem for $R[x]$ with arbitrary commutative ring $R$, the number of roots of a polynomial---counted with multiplicity---does not exceed the degree of this polynomial. Furthermore, these numbers are equal if and only if the polynomial is a product of linear factors.

  \begin{example}[Reals are not Algebraically Closed]
    $\mathbb{R}$ is not algebraically closed since we can identify the polynomial $f(x) = x^2 + 1 \in \mathbb{R}[x]$ which does not have any roots in $\mathbb{R}$. Consequently, any subfield of $\mathbb{R}$ (which contains $1$) such as $\mathbb{Q}, \mathbb{Q}(\sqrt{2}), \ldots$ are not algebraically closed. 
  \end{example}

  It turns out that the complex numbers are algebraically closed, which is presented with the following grand name. Ironically, this theorem cannot be proven with algebra alone. We need complex analysis.\footnote{Gauss proved this for the first time in 1799.} 

  \begin{theorem}[Fundamental Theorem of Algebra]
    Suppose $f \in \mathbb{C}[x]$ is a polynomial of degree $n \geq 1$. Then $f(x)$ has a root in $\mathbb{C}$. It immediately follows from induction that it can be factored as a product of linear polynomials in $\mathbb{C}[x]$. 
  \end{theorem}
  \begin{proof}
    WLOG we can assume that $f$ is monic: $f(z) = z^n + a_{n-1} z^{n-1} + \ldots + a_1 z + a_0$. Since $\mathbb{C}$ is a field, we can set 
    \begin{equation}
      f(z) = z^n \bigg( 1 + \frac{a_{n-1}}{z} + \frac{a_{n-2}}{z^2} + \ldots + \frac{a_0}{z_n} \bigg)
    \end{equation} 
    Since 
    \begin{equation}
      \lim_{|z| \rightarrow \infty} \bigg( 1 + \frac{a_{n-1}}{z} + \frac{a_{n-2}}{z^2} + \ldots + \frac{a_0}{z_n} \bigg) = 0
    \end{equation}
    there exists a $R > 0$ s.t. 
    \begin{equation}
      |z| > R \implies \bigg| 1 + \frac{a_{n-1}}{z} + \frac{a_{n-2}}{z^2} + \ldots + \frac{a_0}{z_n} \bigg| < \frac{1}{2}
    \end{equation}
    and hence 
    \begin{equation}
      |z| > R \implies |f(z)| > |z|^n \cdot \bigg( 1 - \frac{1}{2} \bigg) > \frac{R^n}{2}
    \end{equation}
    So $z$ cannot be a root if $|z| > R$. On the other hand, $f(z)$ is continuous (under the Euclidean topology) and so on the compact set $\{z \in \mathbb{C} \mid |z| \leq R\}$, $|f(z)|$ achieves a minimum value say at the point $z_0$. We claim that $\min_z f(z) = 0$. 

    For convenience, we let $z_0 = 0$ (we can do a change of basis on the polynomial) and assume that the minimum is some positive number, i.e. $f(0) = a_0 \neq 0$. Let $j$ be the smallest positive integer such that $a_j = 0$. Let 
    \begin{equation}
      g(z) = \frac{a_{j+1}}{a_j} z + \ldots + \frac{a_n}{a_j} z^{n-j} \implies f(z) = a_0 + a_j z^j \big( 1 + g(z) \big) 
    \end{equation}
    We set $\gamma = \sqrt[j]{-a_0/a_j}$ and consider the values of 
    \begin{align}
      f(t \gamma) & = a_0 + a_j (t\gamma)^j \big( 1 + g(t\gamma) \big) \\
                  & = a_0 - a_0 t^j \big(1 + g(t \gamma) \big) \\
                  & = a_0 \big\{ 1 - t^j \big(1 + g(t \gamma) \big) \big\}
    \end{align} 
    for $t > 0$. For $t$ sufficiently small, we have 
    \begin{equation}
      |g(t \gamma)| = \bigg| \frac{a_{j+1}}{a_j} (t \gamma) + \ldots + \frac{a_n}{a_j} (t \gamma)^{n-j} \bigg| < \frac{1}{2} 
    \end{equation}
    and for such $t$, this implies 
    \begin{equation}
      |f(t \gamma)| = |a_0| |1 - t^j (1 + g(t \gamma))| \leq |a_0| |1 - t^j/2| < |a_0|
    \end{equation}
    and so $z_0$ cannot have been the minimum of $|f(z)|$. Therefore, the minimum value must be $0$.  
  \end{proof}

  Great, so through this theorem, we can work in any subfield of $\mathbb{C}$ and guarantee that will have all of its roots in $\mathbb{C}$. 

  \begin{corollary}[$\mathbb{C}$ is algebraically closed]
    $\mathbb{C}$ is algebraically closed, i.e. $\mathbb{C}$ is a splitting field of $\mathbb{C}[x]$. 
  \end{corollary}

  Put more succinctly, the impossibility of defining division on the ring of integers motivates its extension into the field of rational numbers. Similarly, the inability to take square roots of negative real numbers forces us to extend the field of real numbers to the bigger field of complex numbers. 

  \begin{theorem}[Eigenvector Conditions for Algebraic Closedness]
    A field $F$ is algebraically closed if and only if for each natural number $n$, every endomorphism of $F^n$ (that is, ever linear map from $F^n$ to itself) has at least one eigenvector. 
  \end{theorem}
  \begin{proof}
    An endomorphism of $F^n$ has an eigenvector if and only if its characteristic polynomial has some root. $(\rightarrow)$ So, when $F$ is algebraically closed, every characteristic polynomial, which is an element of $F[x]$, must have a root. $(\leftarrow)$ Assume that every characteristic polynomial has some root, and let $p \in F[x]$. Dividing the polynomial by a scalar doesn't change its roots, so we can assume $p$ to have leading coefficient $1$. If $p(x) = a_0 + a_1 x + ... + x^n$, then we can identify matrix 
    \begin{equation}
      A = \begin{pmatrix}
      0 & 0 & ... & 0 & -a_0 \\
      1 & 0 & ... & 0 & -a_1 \\
      0 & 1 & ... & 0 & -a_2 \\
      ... & ... & ... & ... & ... \\
      0 & 0 & ... & 1 & -a_{n-1}
      \end{pmatrix}
    \end{equation}
    such that the characteristic polynomial of $A$ is $p$. 
  \end{proof}

  With this splitting condition, we can get a nice set of formulas often introduced in high-school math competitions. 

  \begin{theorem}[Viete's Formulas]
    Given that a polynomial $f$ factors into linear terms, that is 
    \begin{equation}
      f(x) = a_0 \prod_{i = 1}^{n} (x - c_i), c_i \text{ roots of } f
    \end{equation}
    Then the coefficients of $f$ can be presented with the formulas
    \begin{align*}
      & \sum_{i=1}^n c_i = - \frac{a_1}{a_0} \\
      & \sum_{i_1 < i_2} c_{i_1} c_{i_2} = \frac{a_2}{a_0} \\
      & \sum_{i_1< ...< i_k} \prod_{j = 1}^{k} c_{i_j} = (-1)^k \frac{a_k}{a_0} \\
      & c_1 c_2 c_3 ... c_n = (-1)^n \frac{a_n}{a_0}
    \end{align*}
  \end{theorem}

\subsection{Reducibility of Real Polynomials}

  \begin{theorem}
    If $c$ is a complex root of polynomial $f \in \mathbb{R}[x]$, then $\bar{c}$ is also a root of the polynomial. Moreover, $\bar{c}$ has the same multiplicity as $c$. 
  \end{theorem}

  \begin{corollary}
    Every nonzero polynomial in $\mathbb{R}[x]$ factors into a product of linear terms and quadratic terms with negative discriminants. 
  \end{corollary}

  \begin{example}
    \begin{align*}
      x^5 - 1 & = (x-1) \bigg( x - \Big( \cos{\frac{2\pi}{5}} + i \sin{\frac{2\pi}{5}}\Big) \bigg) \bigg( x - \Big( \cos{\frac{2\pi}{5}} - i \sin{\frac{2\pi}{5}}\Big) \bigg) \\
      & \times \bigg( x - \Big( \cos{\frac{4\pi}{5}} + i \sin{\frac{4\pi}{5}}\Big) \bigg) \bigg( x - \Big( \cos{\frac{4\pi}{5}} - i \sin{\frac{4\pi}{5}}\Big) \bigg) \\
      & = (x-1) \bigg( x^2 - \frac{\sqrt{5} - 1}{2} x + 1\bigg) \bigg( x^2 + \frac{\sqrt{5} + 1}{2} x + 1\bigg) 
    \end{align*}
  \end{example}

  \begin{corollary}
    Every polynomial $f \in \mathbb{R}[x]$ of odd degree has at least one real root. 
  \end{corollary}
  \begin{proof}
    This is a direct result of Theorem **. Alternatively, without loss of generality we can assume that the leading coefficient of $f$ is positive. Then
    \begin{equation}
      \lim_{x \rightarrow + \infty} f(x) = + \infty, \; \lim_{x \rightarrow -\infty} f(x) = -\infty
    \end{equation}
    By the intermediate value theorem, there must be some point where $f$ equals $0$. 
  \end{proof}

  \begin{theorem}[Descartes' Rule of Signs] 
    \label{thm:descartes}
    Let $f(x) = x^n + a_{n-1}x^{n-1} + \cdots + a_1x + a_0 \in \mathbb{R}[x]$. Let $C_+$ be the number of times the coefficients of $f(x)$ change signs (here we ignore the zero coefficients); let $Z_+$ be the number of positive roots of $f(x)$, counting multiplicities. Then $Z_+ \leq C_+$ and $Z_+ \equiv C_+ \pmod{2}$. Moreover, if we set $g(x) = f(-x)$, let $C_-$ be the number of times the coefficients of $g(x)$ change signs, and $Z_-$ the number of negative roots of $f(x)$. Then $Z_- \leq C_-$ and $Z_- \equiv C_- \pmod{2}$.
  \end{theorem}

  \begin{theorem}
    The number of positive roots of $f(x)$ is the same as the number of negative roots of $f(-x)$.
  \end{theorem}

  \begin{example}[Easy Way to Find Number of Positive Roots]
    Given $f(x) = x^5 + x^4 - x^2 - 1$, 
    \begin{enumerate}
      \item We have $C_+ = 1$. By Descartes' rule of signs, it must be the case that $Z_+ \leq 1$ and $Z_+ \equiv 1 \pmod{2} \implies Z_+ = 1$. 
      \item Since $f(-x) = -x^5 + x^4 - x^2 - 1$, we have $C_- = 2$, so $Z_- = 0$ or $2$. This is the best that we can do, though it turns out that it actually has $0$ negative roots.\footnote{On the other hand, $x^5 + 3x^3 - x^2 - 1$ has 2 negative roots.} 
    \end{enumerate}
  \end{example}

  Note that if a polynomial has a multiple root but its coefficients are known only approximately (but with any degree of precision), then it is impossible to prove that the multiple roots exists because under any perturbation of the coefficients, however small, it may separate into simple roots or simply cease to exist. This fact leads to the "instability" of the Jordan Normal form because under any perturbation of the elements of a matrix $A$, the change may drastically affect the characteristic polynomial, hence affecting the geometric multiplicities of its eigenvectors. 

\subsection{Reducibility of Integer Polynomials} 

  Even though we have covered a more general theory of polynomials with rational coefficients, it is worthwhile to visit integer polynomials for two reasons. First, there are a few specialized theorems that allow us to easily determine reducibility in $\mathbb{Z}[x]$. Second, Gauss's lemma allows us to check for reducibility in $\mathbb{Q}[x]$ by checking for reducibility in $\mathbb{Z}[x]$, at which point we can abuse the specialized theorems we have developed. 

  \begin{theorem}[Rational Root Theorem]
    Let $a_n x^n + \ldots + a_0 \in \mathbb{Z}[x]$. If $r/s \in \mathbb{Q}$ with $\gcd(r, s) = 1$, then $r \mid a_0$ and $s \mid a_n$. 
  \end{theorem}
  \begin{proof}
    Given that $r/s$ is a root, we have 
    \begin{equation}
      a_n (r/s)^n + \ldots + a_0 = 0
    \end{equation}
    Multiplying by $s^n$, we get 
    \begin{equation}
      a_n r^n + a_{n-1} r^{n-1} s + \ldots + a_1 s^{n-1} r + a_0 s^n = 0
    \end{equation}
    and putting this equation on mod $r$ and mod $s$ implies that $r | a_0 s^n$ and $s | a_n r^n$, respectively. But since we assumed that $\gcd (r, s) = 1$, $r | a_0$ and $s | a_n$. 
  \end{proof}

  The next is quite a remarkable result, since it says that decompositions in $\mathbb{Q}[x]$ imply decompositions in $\mathbb{Z}[x]$! Therefore, to check irreducibility in $\mathbb{Q}[x]$, it suffices to check irreducibility in $\mathbb{Z}[x]$. 

  \begin{lemma}[Gauss's Lemma]
    Let $f \in \mathbb{Z}[x]$. If $\exists g, h \in \mathbb{Q}[x]$ s.t. $f(x) = g(x) h(x)$, then $\exists \bar{g}, \bar{h} \in \mathbb{Z}[x]$ s.t. $f(x) = \bar{g}(x) \bar{h}(x)$. 
  \end{lemma}
  \begin{proof}
    We can find $k, l \in \mathbb{Z}$ s.t. $g_1 (x) = k g(x)$ and $h_1 (x) = l h(x)$ have integer coefficients, i.e. $g_1, h_1 \in \mathbb{Z}[x]$. Then, $k l f(x) = g_1 (x) h_1 (x) \in \mathbb{Z}[x]$. Let $p$ be a prime factor of $kl$. We have 
    \begin{equation}
      0 \equiv \bar{k} \bar{l} \bar{f} (x) \equiv \bar{g}_1 (x) \bar{h}_1 (x) \text{ in } \mathbb{Z}_p [x]
    \end{equation}
    Since $\mathbb{Z}_p$ is an integral domain, $\mathbb{Z}_p [x]$ is an integral domain, and so $\bar{g}_1$ or $\bar{h}_1$ must be $0$. WLOG let it be $\bar{g}_1$. Then every coefficient of $g_1 (x)$ is divisible by $p$, and we can write it in the form $g_2(x) = p g_1 (x)$. Therefore, 
    \begin{equation}
      p(x) \cdot \frac{kl}{p} = \underbrace{\frac{g_1 (x)}{p}}_{g_2 (x)} \cdot \underbrace{h_1 (x)}_{h_2 (x)} \iff f(x) \frac{kl}{p} = g_2 (x) h_2 (x)
    \end{equation}
    Since there are only finitely many prime divisors, we do this for all prime factors of $kl$, and we have 
    \begin{equation}
      f(x) = g_n (x) h_n (x), \qquad g_n, h_n \in \mathbb{Z}[x]
    \end{equation}
  \end{proof}

  \begin{example}[Reducibility of Integer Polynomials]
    Let $f(x) = x^4 - x^3 + 2$. The rational roots are in the set $S = \{\pm 1, \pm2 \}$, but none of them work since $f(\pm1), f(\pm2) \neq 0$. By degree considerations and Gauss's lemma, if $f(x)$ is reducible, then 
    \begin{equation}
      f(x) = (x^2 + ax + b) (x^2 + cx + d), \qquad a, b, c, d \in \mathbb{Z}
    \end{equation}
    We know that $bd \in S$, with $a + c = -1$, $d + b + ac = 0$, and so on for each coefficients. We can brute force this finite set of possibilities. 
  \end{example}

  \begin{example}
    We claim that $x^4 - 22x^2 + 1 = (x - (\sqrt{6} + \sqrt{5})) (x - (\sqrt{6} - \sqrt{5})) (x - (-\sqrt{6} + \sqrt{5})) (x - (-\sqrt{6} - \sqrt{5})) $ is irreducible in $\mathbb{Q} [x]$. By the rational root theorem $\pm 1$ are the possible rational roots, but plugging it into reveals that they aren't roots. Now if $x^4 - 22x^2 + 1$ factors it must factor as a product of monic quadratic polynomials in $\mathbb{Z}[x]$ by Gauss's Lemma. Therefore, 
    \begin{equation}
      (x^2 + ax + b) (x^2 + cx + d) = x^4 - 22x^2 + 1
    \end{equation}
    Then $a + c = 0, bd = 1, ad + bc = 0, d + b + ac = 22$. We can derive this and find that there are no solutions, and so it is irreducible. 
  \end{example}

  A great way to check irreducibility is to check in mod $p$. 

  \begin{theorem}
    Let $f(x) = a_n x^n + \ldots + a_0 \in \mathbb{Z}[x]$. If $p \nmid a_n$ and $f \in \mathbb{Z}_p [x]$ is irreducible, then $f$ is irreducible in $\mathbb{Q}[x]$.\footnote{May need to verify this again.}
  \end{theorem}
  \begin{proof}
    Suppose that $f(x) = g(x) h(x) \in \mathbb{Z}[x]$ with $\deg(g), \deg(h) > 0$. Then 
    \begin{equation}
      f(x) \equiv g(x) h(x) \text{ in } \mathbb{Z}_p [x]
    \end{equation}
    Since $f(x)$ is irreducible in $\mathbb{Z}_p [x]$, we must have that one of $g(x)$ or $h(x)$ has degree $0$ in $\mathbb{Z}_p [x]$. WLOG let it be $g(x)$, but this means that the leading coefficient of $g(x)$ must be divisible by $p \implies$ leading coefficient of $f(x)$ is divisible by $p \iff p \mid a_n$. 
  \end{proof}

  \begin{example}
    $x^4 + x + 1$ is irreducible in $\mathbb{Z}_2 [x]$. So we can extend this to $\mathbb{Z}[x]$ to see that \textit{all} fourth degree polynomials of form $a x^4 + b x^3 + c x^2 + dx + e$, which $a, d, e$ odd and $b, c$ even is irreducible in $\mathbb{Q}[x]$. 
  \end{example}

  This is a powerful theorem to quickly find a large class of polynomials that are irreducible. However, being reducible in $\mathbb{Z}_p [x]$ does not imply reducibility in $\mathbb{Q}$. In fact, there are polynomials $f(x) \in \mathbb{Z}[x]$ which are irreducible but reducible in $\mathbb{Z}_p$ for \textit{every} prime $p$. 

  \begin{theorem}[Eisenstein's Criterion]
    Let $f(x) = a_n x^n + \ldots + a_0 \in \mathbb{Z}[x]$ and $p \in \mathbb{Z}$ a prime s.t. $p \nmid a_n$, $p \mid a_i$ for $i = 0, \ldots, a_{n-1}$, and $p^2 \nmid a_0$. Then $f(x)$ is irreducible in $\mathbb{Q}[x]$. 
  \end{theorem}
  \begin{proof}
    Suppose that $f(x) = g(x) h(x) \in \mathbb{Q}[x]$ with $\deg(g), \deg(h) > 0$. Then, by Gauss's lemma, $g, h \in \mathbb{Z}[x]$. Reducing the equations mod $p$, 
    \begin{equation}
      f(x) = g(x) h(x) \text{ in } \mathbb{Z}_p [x]
    \end{equation}
    But $f(x) = a_n x^n$. By unique factorization theorem in $\mathbb{Z}_p [x]$, $g, h \in \mathbb{Z}_p [x]$ must be products of units and prime factors of $a_n x^n$, which are $\{x\}$. Therefore, let 
    \begin{equation}
      g(x) = b_m x^m, h(x) = \frac{a_n}{b_m} x^{n-m} \in \mathbb{Z}_p [x]
    \end{equation}
    with $\deg(g) = m > 0$ and $\deg(h) = n - m > 0$ in $\mathbb{Z}[x]$. This implies that the constant coefficients of $g(x), h(x)$ are divisible by $p$, which implies that the constant coefficients of $f(x) = g(x) h(x)$ are divisible by $p^2$, a contradiction. 
  \end{proof}

  \begin{example}[Easy Checks for Irreducibility with Eisenstein]
    Listed. 
    \begin{enumerate}
      \item $x^{13} + 2x^{10} + 4x + 6$ is irreducible in $\mathbb{Q}[x]$ by Eisenstein for $p = 2$. 
      \item $x^3 + 9x^2 + 12x + 3$ is irreducible in $\mathbb{Q}[x]$ by Eisenstein for $p = 3$. 
      \item Let $f(x) = x^4 + x^3 + x^2 + x + 1$. Then, we know that $f(x) = \frac{x^5 - 1}{x-1}$ and so 
      \begin{align}
        f(x + 1) & = \frac{(x + 1)^5 - 1}{(x + 1) - 1} \\
                 & = \frac{1}{x} \bigg( x^5 + \binom{5}{1} x^4 + \binom{5}{2} x^3 + \binom{5}{3} x^2 + \binom{5}{4} x + \binom{5}{5} - 1 \bigg) \\
                 & = x^4 + 5x^3 + 10 x^2 + 10x + 5
      \end{align}
      So all nonleading coefficients are divisible by $5$ exactly once, which by Eisenstein implies that $f(x+1)$ is irreducible which implies that $f(x)$ is irreducible. 
    \end{enumerate}
  \end{example}

  We have prod that for $\alpha \in \mathbb{C}$, subfield $F \subset \mathbb{C}$, and $f(x) \in F[x]$, with $f(\alpha) = 0$, then $B = \{1, \alpha, \ldots, \alpha^{\deg(f) - 1}\}$ spans $F[\alpha]$ as a $F$-vector space. If $f(x)$ is irreducible then $B$ is a basis. 

\subsection{Ring Extensions} 

  We have already seen the idea of taking ring $R$ and adjoining an element $x$ to it to get a minimal ring containing both $R$ (as a subring) and $x$. This was the polynomial ring. Now consider a field $F$ and rather than adjoining an indeterminant $x$, we consider a bigger field $K \supset F$ with some $\alpha \in K$ (with $\alpha \not\in F$). Then, we would like to similarly extend $F$ to a bigger field $F[\alpha]$ containing both $F$ (as a subring) and $\alpha$. This can be naturally constructed when mapping $\alpha$ through polynomials $f(x) \in F[x]$. For ease of notation, we introduce the following. 

  \begin{definition}[Ring Extension]
    A pair of rings $R, S$ where $R$ is a subring of $S$ is called a \textbf{ring extension}, denoted $R \hookrightarrow S$. 
  \end{definition}

  Now we introduce the ring $R$ adjoined by $\alpha$. 
  
  \begin{definition}[Adjoining Ring of Polynomial Elements] 
    Given ring extension $R \hookrightarrow S$ with finite $\alpha = \{\alpha_1, \ldots, \alpha_n\} \subset S$, the \textbf{ring $R$ adjoined by $\alpha$} is defined 
    \begin{equation}
      R[\alpha] = R[\alpha_1, \ldots, \alpha_n] \coloneqq \{ f(\alpha_1, \ldots, \alpha_n) \in K \mid f \in F[x_1, \ldots, x_n]\} \subset K
    \end{equation} 
    That is, we take the $\alpha_i$'s and map it through all polynomials in $F[x_1, \ldots, x_n]$. This induces a subring structure. 
  \end{definition} 

  Most of the times, we will work with adjoining rings of univariate polynomial elements. 
  \begin{equation}
    F[\alpha] \coloneqq \{ f(\alpha) \in F \mid f \in F[x]\} \subset K
  \end{equation} 
  Note that if $R \hookrightarrow S$ is a ring extension, then $R[\alpha]$ is a ring, and so it makes sense to write $R[\alpha][\beta] \subset S$. It had better be the case that this ring is consistent with equivalent constructions. 

  \begin{lemma}[Consistency of Adjoning Rings]
    It is the case that 
    \begin{equation}
      R[\alpha, \beta] = R[\alpha][\beta] = R[\beta][\alpha]
    \end{equation}
    It immediately follows for any finite set $X$ as above, $F[\alpha_1, \ldots, \alpha_n]$ characterizes the $(n-1)$-fold adjoin operations. 
  \end{lemma}
  \begin{proof}
    
  \end{proof}

  Since $R$ being a ring implies $R[x]$ is also a ring, we can intuit some subring structure. Furthermore, $R[\alpha]$ is a minimal ring containing $\alpha$. 

  \begin{lemma}[Adjoining Fields are a Ring Extension] 
    Given an adjoining ring, we claim two things. 
    \begin{enumerate}
      \item It is true that $F \subset F[\alpha] \subset K$. Furthermore, if $\alpha \not\in F$, then $F \subsetneq F[\alpha]$. 
      \item $R[\alpha]$ is the minimal ring containing $R$ and $\alpha$. 
    \end{enumerate}
  \end{lemma}
  \begin{proof}
    Note that $F \subset F[\alpha]$ since we can just take the constant polynomials, so this is not very interesting. Given two elements $\phi, \gamma \in F[\alpha]$, there exists polynomials $f, g \in F[x]$ s.t. $\phi = f(\alpha), \gamma = g(\alpha)$. Since $F[x]$ is a ring, we see that 
    \begin{align}
      \phi + \gamma & = f(\alpha) + g(\alpha) = (f + g)(\alpha) \\
      \phi \cdot \gamma & = f(\alpha) \cdot g(\alpha) = (fg)(\alpha)
    \end{align} 
    Furthermore, it is easy to check that $0$ and $1$ are the images of $\alpha$ through the $0$ and $1$ polynomials. 
  \end{proof} 

  What allows us to make this inclusion proper is that the $\alpha \in K$, which does not necessarily have to be in $F$, \textit{extends} this field a bit further, but since we can only map the one element $\alpha$, it may not cover all of $K$. Let's go through some examples. 

  \begin{example}[Radical Extensions of $\sqrt{2}$]
    Let $F = \mathbb{Q}$ and $K = \mathbb{C}$. We claim $\mathbb{Q}[\sqrt{2}] = \{a + b \sqrt{2} \mid a, b \in \mathbb{Q} \}$.
    \begin{enumerate}
      \item $\mathbb{Q}[\sqrt{2}] \subset \{a + b \sqrt{2} \mid a, b \in \mathbb{Q} \}$. $\mathbb{Q}[\sqrt{2}]$ are elements of the form
      \begin{equation}
        f(\sqrt{2}) = a_n (\sqrt{2})^n + a_{n-1} (\sqrt{2})^{n-1} + \ldots + a_2 (\sqrt{2})^2 + a_1 \sqrt{2} + a_0
      \end{equation} 
      This can be written by collecting terms, of the form $a + b \sqrt{2}$. 

      \item $\mathbb{Q}[\sqrt{2}] \supset \{a + b \sqrt{2} \mid a, b \in \mathbb{Q} \}$. Given an element $a + b \sqrt{2}$, this is clearly in $\mathbb{Q}[\sqrt{2}]$ since it is the image of $\sqrt{2}$ under the polynomial $f(x) = a + bx$. 
    \end{enumerate}
  \end{example} 

  Given this, we may extrapolate this pattern and claim that $\mathbb{Q}[\sqrt{2} + \sqrt{3}]$ consists of all numbers of form $a + (\sqrt{2} + \sqrt{3}) b$. However, this is \textit{not} the case. 

  \begin{example}[Radical Extensions of $\sqrt{2} + \sqrt{3}$]
    Given any element $\beta \in \mathbb{Q}[\sqrt{2} + \sqrt{3}]$, it is by definition of the form 
    \begin{equation}
      \beta = \sum_{k=0}^n a_k (\sqrt{2} + \sqrt{3})^k 
    \end{equation} 
    Clearly $1, \sqrt{2} + \sqrt{3} \in \mathbb{Q}[\sqrt{2} + \sqrt{3}]$ by mapping $\sqrt{2} + \sqrt{3}$ through the polynomials $f(x) = 1$ and $f(x) = $. However, we can see that $(\sqrt{2} + \sqrt{3})^2 = 5 + \sqrt{6}$,\footnote{where we use $\sqrt{6}$ as notation for $\sqrt{2} \cdot \sqrt{3}$} and so $\sqrt{6} \in \mathbb{Q}[\sqrt{2} + \sqrt{3}]$. Furthermore, we have $(\sqrt{2} + \sqrt{3})^3 = 11 \sqrt{2} + 9 \sqrt{3}$, and so with the ring properties we can conclude that 
    \begin{align}
      \frac{1}{2} \big[ (11 \sqrt{2} + 9 \sqrt{3}) - 9 (\sqrt{2} + \sqrt{3})\big] = \sqrt{2} & \in \mathbb{Q}[\sqrt{2} + \sqrt{3}] \\
      -\frac{1}{2} \big[ (11 \sqrt{2} + 9 \sqrt{3}) - 11 (\sqrt{2} + \sqrt{3})\big] = \sqrt{3} & \in \mathbb{Q}[\sqrt{2} + \sqrt{3}] \\
    \end{align} 
    If we go a bit further, we can show that 
    \begin{equation}
      \mathbb{Q}[\sqrt{2} + \sqrt{3}] = \{a + b \sqrt{2} + c \sqrt{3} + d\sqrt{6} \mid a, b, c, d \in \mathbb{Q} \}
    \end{equation}
  \end{example}

  \begin{example}[Showing Two Extensions are Equal]
    We claim that $\mathbb{Q}[\sqrt{6}, \sqrt{5}] = \mathbb{Q}[\sqrt{6} + \sqrt{5}]$. 
    \begin{enumerate}
      \item $\mathbb{Q}[\sqrt{6}, \sqrt{5}] \subset \mathbb{Q}[\sqrt{6} + \sqrt{5}]$ because the sum of $\sqrt{6}$ and $\sqrt{5}$ lies in $\mathbb{Q}[\sqrt{6}, \sqrt{5}]$. 
      \item $\mathbb{Q}[\sqrt{6} + \sqrt{5}] \subset \mathbb{Q}[\sqrt{6}, \sqrt{5}]$. We have 
        \begin{equation}
          \frac{1}{\sqrt{5} + \sqrt{6}} = \frac{1}{\sqrt{5} + \sqrt{6}} \frac{\sqrt{5} - \sqrt{6}}{\sqrt{5} - \sqrt{6}} = \sqrt{6} - \sqrt{5} \in \mathbb{Q}[\sqrt{5} + \sqrt{6}] 
        \end{equation}
        which implies that 
        \begin{equation}
          \sqrt{6} = \frac{1}{2} ((\sqrt{6} + \sqrt{5}) + (\sqrt{6} - \sqrt{5})) \in \mathbb{Q}[\sqrt{5} + \sqrt{6}] 
        \end{equation}
        and so $\sqrt{5} = (\sqrt{6} + \sqrt{5}) - \sqrt{6} \in \mathbb{Q}[\sqrt{5} + \sqrt{6}]$. 
    \end{enumerate}
  \end{example}

  \begin{example}[Quotient Ring as Product Rings with Chinese Remainder Theorem]
    Consider the following ring $\mathbb{Z}_5 [x] / \langle x^2 + 1 \rangle$. Then $x^2 + 1$ is reducible since $x^2 + 1 = (x + 2) (x + 3)$, which implies $\langle x^2 + 1 \rangle = \langle x + 2 \rangle \cap \langle x + 3 \rangle$, and so by the Chinese remainder theorem, we have 
    \begin{equation}
      \frac{\mathbb{Z}_5 [x]}{\langle x^2 + 1 \rangle} = \frac{\mathbb{Z}_5 [x]}{\langle x + 2 \rangle \cap \langle x + 3 \rangle} = \frac{\mathbb{Z}_5 [x]}{\langle x + 2 \rangle} \times \frac{\mathbb{Z}_5 [x]}{\langle x + 3 \rangle} \simeq \mathbb{Z}_5 \times \mathbb{Z}_5
    \end{equation}
  \end{example} 

  \begin{example}[Irreducible Quotient Rings]
    On the other hand, if we consider the ring $\mathbb{Z}_5 [x] / \langle x^2 - 2 \rangle$, then $x^2 - 2$ is irreducible in $\mathbb{Z}_5 [x]$ (just plug in $0, 1, 2, 3, 4$), and so since $\mathbb{Z}_5$ is a field, the quotient ring is a field. Then we have 
    \begin{equation}
      \frac{\mathbb{Z}_5 [x]}{\langle x^2 - 2 \rangle} \simeq \mathbb{Z}_5 [\sqrt{2}] = \{a + b \sqrt{2} \mid a, b \in \mathbb{Z}_5 \}
    \end{equation}
  \end{example} 


\section{Polynomial Rings} 

  One of the most widely studied rings are the ring of polynomials. Let's reintroduce them. 

  \begin{definition}[Univariate Polynomials]
    For a ring $R$, the \textbf{univariate polynomial ring over $R$}, denoted $R[x]$ consists of elements called \textbf{polynomials} which are formal expressions of the form 
    \begin{equation}
      f(x) = a_nx^n + a_{n-1}x^{n-1} + \dots + a_1x + a_0 \text{ where } a_i \in R
    \end{equation}
    with coefficients $a_i \in R$ and $x$ is called a \textbf{variable}, or \textbf{indeterminant}.\footnote{Note that $x$ is just a formal symbol, whose powers $x^i$ are just placeholders for the corresponding coefficients $a_i$ so that the given formal expression is a way to encode the finitary sequence. $(a_0, a_1, a_2, ..., a_n)$.} Two polynomials are equal if and only if the sequences of their corresponding coefficients are equal. We can also see a polynomial as a function $f: R \rightarrow R$ as well. 

    Furthermore, $R[x]$ is a ring, with addition and multiplication defined
    \begin{equation}
      a_i x^i + b_i x^i = (a_i + b_i) x^i, \qquad x^ix^j = x^{i+j}
    \end{equation}
    along with $0$ as the additive identity and $1$ as the multiplicative identity.   
  \end{definition}

  While we will mainly deal with univariate polynomials, we can also define multivariate polynomials similarly. 

  \begin{definition}[Multivariate Polynomials] 
    For a ring $R$, the \textbf{multivariate polynomial ring over $R$}, denoted $R[x_1, \ldots, x_n]$ consists of elements called \textbf{polynomials} which are formal expressions of the form 
    \begin{equation}
      f(x_1, \ldots, x_n) = \sum_{0 \leq k_i \leq n} a_{k_1 \ldots k_n} x_1^{k_1} x_2^{k_2} \ldots x_n^{k_n}
    \end{equation}
    with coefficients $a \in R$ and $x_i$'s the \textbf{variables}. We can treat an element $f \in R[x_1, \ldots, x_n]$ as a function $f: R^n \rightarrow R$.  

    Furthermore, $R[x_1, \ldots, x_n]$ is a ring, with addition and multiplication defined 
    \begin{align}
      a_{k_1 \ldots k_n} x_1^{k_1} x_2^{k_2} \ldots x_n^{k_n} + b_{k_1 \ldots k_n} x_1^{k_1} x_2^{k_2} \ldots x_n^{k_n} & = (a_{k_1 \ldots k_n} + b_{k_1 \ldots k_n}) x_1^{k_1} x_2^{k_2} \ldots x_n^{k_n} \\
      x^{k_1 \ldots k_n} x^{l_1 \ldots l_n} & = x^{k_1 + l_1, k_2 + l_2, \ldots, k_n + l_n}
    \end{align}
  \end{definition}

  Usually, the properties of the base ring $R$ determines the properties on $R$. 

  \begin{lemma}[Commutativity Extends to Polynomials]
    We have the following. 
    \begin{enumerate}
      \item $R$ is a commutative ring iff $R[x]$ is a commutative ring. 
      \item $R$ is an integral domain iff $R[x]$ is an integral domain. 
      \item $F$ is a field iff $F[x]$ is a Euclidean domain.  
    \end{enumerate}
  \end{lemma}
  \begin{proof}
    TBD. 
  \end{proof} 

  With this theorem we unlock all the properties that we have studied in general for the subclasses of rings. Almost always we will assume that $R$ is at least commutative, so let's get that out of the way. Before we move on, let's get some terms out of the way. 

  \begin{definition}[Some Terms for Polynomials]
    Given a univariate polynomial $f(x)$. 
    \begin{enumerate} 
      \item The \textbf{leading coefficient} is the last nonzero coefficient  
      \item The \textbf{degree} of $f$---denoted $\deg f$---is the index of the leading coefficient.
      \item A \textbf{monomial} is a polynomial of a single term $a_j x^j$. 
      \item A \textbf{linear} polynomial is a polynomial of degree 1. 
      \item A \textbf{quadratic} polynomial is a polynomial of degree 2. 
      \item A \textbf{cubic} polynomial is a polynomial of degree 3. 
    \end{enumerate}
  \end{definition}

\subsection{Basic Properties of Polynomials}

  We need to be very careful about the properties that hold for polynomials, as they may not be intuitive. For example, for certain finite fields (which are rings), some formally different polynomials may be indistinguishable in terms of mappings.\footnote{$x$ and $x^2$ are equivalent in the polynomial algebra defined on the domain $\mathbb{Z}_2$. } Second, a polynomial may have more roots than its degree. Therefore, we will work in different rings $R$ and provide conditions where our intuition is true in $R[x]$. It is clear that if you have two polynomials of degree $n$ and $m$, their sum may be degree $k < n, m$. This is not always true for multiplication. 

  \begin{example}[Product of Two Linear Polynomials is $0$]
    Given $f, g \in \mathbb{Z}_6 [x]$ with $f(x) = 2x + 4$ and $g(x) = 3x + 3$, we have 
    \begin{equation}
      f(x) \cdot g(x) = (2x + 4)(3x + 3) = 6x^2 + 18 x + 12 = 0
    \end{equation}
  \end{example}

  There is a simple condition in which the degree is additive, however. 

  \begin{theorem}[Bounds on Degrees From Operations]
    Given that $R$ is a ring and $f, g \in R[x]$, 
    \begin{equation}
      \deg(f+g) \leq \max\{\deg f, \deg g\} \\
    \end{equation}
    If $R$ is a domain, then 
    \begin{equation}
      \deg (f g) = \deg f + \deg g
    \end{equation}
    Note that this automatically implies that $R[x]$ is a domain. Combined with the lemma above, we have: $R$ is an integral domain $\implies R[x]$ is an integral domain. 
  \end{theorem}
  \begin{proof}
    The second may not be true if $R$ has zero divisors. 
  \end{proof}

  Just working in domains do not make things all better. Sometimes, we may have two different polynomials but they may define the same function from $R$ to $R$! 

  \begin{example}[Polynomials as Same Function]
    Given $f, g \in \mathbb{Z}_2 [x]$, 
    \begin{equation}
      f(x) = x \sim g(x) = x^2
    \end{equation} 
  \end{example}

  As shown in the example above, it is not so simple as to restrict which underlying set you are working on. Some rings $R$ may or may not assert uniqueness of functions in $F[x]$, and vice versa. Therefore, here are some special theorems. 

  \begin{theorem}[Uniqueness of Polynomials over Field]
    If the field $\mathbb{F}$ is infinite, then different polynomials in $\mathbb{F}[x]$ determine different functions. 
  \end{theorem}

\subsection{Euclidean Division} 

  Just like how we can do Euclidean division with integers, there is an analogous result for polynomials. However, we require to work with a \textit{field} $F$ rather than an arbitrary ring $R$. 

  \begin{theorem}[Polynomials as Euclidean Domain]
    Given a field $F$, $F[x]$ is a Euclidean domain. That is, given polynomials $f(x), g(x) \in F[x]$, there are unique polyomials $q(x), r(x) \in F[x]$ s.t. 
    \begin{equation}
      f(x) = q(x) g(x) + r(x), \qquad \deg(r(x)) < \deg(g(x))
    \end{equation} 
  \end{theorem} 
  \begin{proof}
    We first prove existence. If $\deg(f(x)) < \deg(g(x))$, then we can trivially set $q(x) = 0, r(x) = f(x)$. Therefore we can assume that $\deg(f(x)) \geq \deg(g(x))$. We can prove this by strong induction on $k = \deg(f(x))$. Assume that $\deg(f(x)) = 1$. Then if $\deg(g(x)) > 1$ it is trivial as before, so we show for $\deg(g(x)) = 1$. So let 
    \begin{equation}
      f(x) = a_1 x + a_0, \qquad g(x) = b_1 x + b_0 
    \end{equation}
    and we can find the solutions 
    \begin{equation}
      f(x) = \frac{a_1}{b_1} g(x) + \bigg( a_0 - \frac{a_1 b_0}{b_1} \bigg)
    \end{equation} 
    Now suppose that the results is known for whenver $\deg(f(x)) \leq k$ and we have a polynomial $F(x) = a_{k+1} x^{k+1} + \ldots a_0$ of degree $k+1$. Then we must check that there exists a quotient and remainder for $0 \leq \deg(g(x)) = m \leq k + 1$. Note that the coefficients of $x^{k+1}$ in $F(x)$ and in the polynomial $\frac{a_{k+1}}{b_m} x^{k+1-m} g(x)$ are the same, so the polynomial 
    \begin{equation}
      f(x) = F(x) - \frac{a_{k+1}}{b_m} x^{k+1-m} g(x) 
    \end{equation}
    has degree at most $k$. Thus by our induction hypothesis we can write $f(x) = q(x) g(x) + r(x)$, and so 
    \begin{align}
      F(x) & = f(x) + \frac{a_{k+1}}{b_m} x^{k+1-m} g(x) \\
           & = q(x) g(x) + r(x) + \frac{a_{k+1}}{b_m} x^{k+1-m} g(x) \\ 
           & = \bigg( q(x) + \frac{a_{k+1}}{b_m} x^{k+1-m} \bigg) g(x) + r(x)
    \end{align} 
    which is indeed a decomposition. Now to prove uniqueness, suppose we had two different decompositions 
    \begin{equation}
      f(x) = q(x) g(x) + r(x) = q^\prime (x) g(x) + r^\prime (x) \implies \big( q(x) - q^\prime (x) \big) g(x) = r(x) - r^\prime (x)  
    \end{equation}  
    IF $q(x) \neq q^\prime (x)$, then the degree of the LHS is at least $\deg(g(x))$, while the degree of the RHS must be strictly less, a contradiction. 
  \end{proof}

  \begin{example}[Polynomials over Fields] 
    The algorithimc way to get such $q(x), r(x)$ is through \textit{polynomial long division}. 
    
    \begin{center}
      \polylongdiv{x^3 + 4x^2 - x + 7}{x - 2}
    \end{center}

    Given field $\mathbb{Z}_5$, $\mathbb{Z}_5[x]$ is a Euclidean domain, with Euclidean division.  
  \end{example} 

  In fact, it turns out that you don't necessarily require a polynomial to always come from a field in order to do long division. You can do polynomial long division over \textit{any} commutative rings, as long as the leading coefficient of the divisor is a unit (and since all elements of a field are units, we can do so). This is because at each step, you only need to divide the leading coefficient of the divisor into the leading coefficient of the polynomial you have left. An immediate consequence of this theorem is the following. 

  \begin{corollary}[Remainder Theorem]
    Let $c \in F$ and $f(x) \in F[x]$. When we divide $f(x)$ by $g(x) = x - c$, the remainder is $f(c)$. 
  \end{corollary}
  \begin{proof}
    By the Euclidean algorithm, 
    \begin{equation}
      f(x) = (x - c) q(x) + r(x) \implies f(c) = (c - c) q(c) + r(c) = r(c)
    \end{equation}
  \end{proof} 

\subsection{Roots and Factorization}

  Next, we can define the all too familiar factors and roots of a polynomial. 

  \begin{definition}[Factor]
    Given a ring $R$ and a polynomial $f(x) \in R[x]$, if there exists $g(x), h(x)$ of degree at least 1 such that 
    \begin{equation}
      f(x) = g(x) h(x)
    \end{equation}
    then $g, h$ are said to be \textbf{factors}, or \textbf{divisors}, of $f$. If there are no such factors of $f$, then $f(x)$ is said to be \textbf{irreducible}. 
  \end{definition}

  Irreducible polynomials are analogous to prime numbers in $\mathbb{Z}$. 

  \begin{definition}[Polynomial Root]
    An element $r \in R$ is a \textbf{root} of polynomial $f \in R[x]$ if and only if 
    \begin{equation}
      f(r) = 0
    \end{equation}
  \end{definition} 

  Note that both factors and roots are intimately tied to Euclidean division, so the two are closely related. 

  \begin{theorem}[Root-Factor Theorem]
    Given a commutative ring $R$ (usually $R$ is a field) and $f(x) \in R[x]$, $(x - c)$ is a factor of $f(x)$, i.e. can be factored into 
    \begin{equation}
      f(x) = (x - c) q(x) 
    \end{equation}
    for some $q(x) \in R[x]$ of degree $\deg(f) - 1$ if and only if $f(c) = 0$.\footnote{Note that this is not true for an arbitrary ring. $R$ must be commutative at least.}
  \end{theorem} 
  \begin{proof}
    We prove for when $R$ is a field $F$, but it turns out that the theorem also holds for commutative rings $R$. 
    \begin{enumerate}
      \item $(\rightarrow)$. Given that $(x - c)$ is a factor of $f(x)$, this means that by the Euclidean algorithm $f(x) = (x - c) q(x)$ for some $q(x)$, and so $f(c) = (c - c) q(c) = 0$. 
      \item $(\leftarrow)$. Given that $f(c) = 0$. By the remainder theorem this means that when we divide $f(x)$ by $(x - c)$, the remainder is $f(c) = 0$, and so $f(x) = (x - c) q(x) + 0 = (x - c) q(x) \implies (x - c)$ is a factor of $f(x)$. 
    \end{enumerate}
  \end{proof}

  Notice how these polynomials mimick integers, and to drive this point even further, let's introduce the greatest common divisor. 

  \begin{theorem}[GCD of Two Polynomials Exist]
    Given nonzero polynomials $f(x), g(x) \in F[x]$, let 
    \begin{equation}
      S = \{h(x) \in F[x] \mid h(x) = a(x) f(x) + b(x) g(x) \text{ for some } a(x), b(x) \in F[x] \}
    \end{equation} 
    Then there exists some polynomial $d(x) \in S$ of smallest degree, and every $h(x) \in S$ is divisible by $d(x)$. 
  \end{theorem}
  \begin{proof}
    The existence is trivial since by the well-ordering principle on the degrees of polynomials in $S$, such a minimal degree must exist. Now we prove the second claim by proving $d(x) \mid f(x)$. We apply the division algorithm to write 
    \begin{equation}
      f(x) = q(x) d(x) + r(x)
    \end{equation}
    If $r(x) = 0$, then by root factor theorem we are done. If $r(x) \neq 0$, we then write 
    \begin{align}
      r(x) & = f(x) - q(x) d(x) \\ 
           & = f(x) - \big( s(x) f(x) + t(x) g(x) \big) q(x) \\ 
           & = \big( 1 - s(x) q(x) \big) f(x) - \big( t(x) q(x) \big) g(x) \in S
    \end{align}
    Since $r(x) \in S$ due to its form, the fact that $\deg(r(x)) < \deg(d(x))$ contradicts the way that $d(x)$ was chosen. Therefore $r(x) = 0$. It turns out that $d(x)$ is unique up to a constant factor. 
  \end{proof}

  \begin{definition}[GCD]
    $d(x)$ as above is called the \textbf{greatest common divisor} of $f(x), g(x)$, denoted $d(x) = \gcd(f(x), g(x))$ satisfying 
    \begin{enumerate}
      \item $d(x) \mid f(x)$, $d(x) \mid g(x)$, and 
      \item $\forall e(x) \in F[x]$, if $e(x) \mid f(x)$ and $e(x) \mid g(x)$, then $e(x) \mid d(x)$. 
    \end{enumerate}
    $f(x), g(x)$ are said to be \textbf{relatively prime} if $\gcd(f(x), g(x)) = 1$. 
  \end{definition}

  The algorithmic way for computing the GCD is done the same way by performing Euclidean algorithm on two polynomials: dividing on by the other, taking the remainder, and dividing the lesser degree by the remainder again, until the remainder is $0$. 

  \begin{lemma}
    Suppose $f(x)$ is irreducible and $f(x) \mid g(x) h(x)$. Then $f(x) \mid g(x)$ or $f(x) \mid h(x)$. 
  \end{lemma}

  Now, we show an extremely important theorem. This should be intuitive since $F$ a field implies $F[x]$ a Euclidean domain, which is a PID, which has the unique factorization theorem. 

  \begin{theorem}[Unique Factorization of Polynomials over Fields]
    Given field $F$ and nonconstant polynomial $f(x) \in F[x]$ of degree $n$, we can always write $f(x)$ as a unique\footnote{up to constant factors and rearrangement} product of at most $n$ irreducible polynomials in $F[x]$. 
  \end{theorem}
  \begin{proof}
    To prove the bound, the general idea is that by the root factor theorem, each root gives rise to a linear factor, and so inductively we cannot have more than $n$ linear factors.  
    Strong induction on degree of $f(x)$ by starting with linear. 
  \end{proof}

  Note that this is \textit{not} true in arbitrary rings. 

  \begin{example}[Linear Polynomial with 3 Roots]
    Consider $f(x) = x^2 - 1 \in \mathbb{Z}_8 [x]$, a commutative ring. Then $1, 3, 5, 7$ are all roots of $f(x)$, which is greater than its degree. Furthermore, it has two different factorizations 
    \begin{equation}
      x^2 - 1 = (x + 1)(x - 1) = (x + 3)(x - 3)
    \end{equation}
  \end{example} 

  \begin{theorem}[Interpolation]
    For any collection of given field values $y_1, y_2, ..., y_n \in F$ at given distinct points $x_1, x_2, ..., x_n \in F$, there exists a unique polynomial $f \in F[x]$ with deg$\, f < n$ such that
    \begin{equation}
      f(x_i) = y_i, \quad i = 1, 2, ..., n
    \end{equation}
    This is commonly known as the \textbf{interpolation problem}, and when $n = 2$, this is called \textbf{linear interpolation}. 
  \end{theorem} 

\subsection{Algebraically Closed Fields} 

  Now that we have seen some examples of fields, what properties would we like it to have? Going back to polynomials, recall that if $F$ is a field, then $F[x]$ as a Euclidean domain gave us a lot of nice properties, such as admitting a unique factorization of irreducible polynomials. However, we have only proved that the number of roots is \textit{at most} the degree $n$, but not that it actually reaches $n$. In fact, in a more extreme case, a polynomial may not even factor \textit{at all} in $F[x]$, since it could be irreducible. So while we have defined an upper bound for the number of roots for a polynomial, we have not determined whether a polynomial has any roots at all, i.e. a lower bound. 

  We don't have much \textit{control} over what these irreducible polynomials can look like. We may have to check---either through theorems or manually---that a polynomial or arbitrary degree is irreducible. If we would like to assert that all irreducible polynomials must be of smallest degree---that is, linear---then such a field is called \textit{algebraically closed}. 
  This algebraic closed property asserts also that the lower bound on the number of (non-unique) factors is $n$. 

  \begin{definition}[Algebraically Closed Field]
    A field $F$ is \textbf{algebraically closed} if every polynomial of positive degree (i.e. non-constant) in $F[x]$ has at least one root in $F$. 
  \end{definition}

  This is equivalent to saying that every polynomial can be expressed as a product of first degree polynomials. To extend our analysis more, we can talk about the multiplicity of these factors, which just tells us more about how many unique and non-unique factors a polynomial has. 

  \begin{definition}[Multiplicity]
    A root $c$ of polynomial $f(x) \in F[x]$ is called simple if $f(x)$ is not divisible by $(x - c)^2$ and multiple otherwise. The \textbf{multiplicity} of a root $c$ is the maximum k such that $(x - c)^k$ divides $f(x)$ .
  \end{definition} 

  To restate the root-factor theorem for $R[x]$ with arbitrary commutative ring $R$, the number of roots of a polynomial---counted with multiplicity---does not exceed the degree of this polynomial. Furthermore, these numbers are equal if and only if the polynomial is a product of linear factors.

  \begin{example}[Reals are not Algebraically Closed]
    $\mathbb{R}$ is not algebraically closed since we can identify the polynomial $f(x) = x^2 + 1 \in \mathbb{R}[x]$ which does not have any roots in $\mathbb{R}$. Consequently, any subfield of $\mathbb{R}$ (which contains $1$) such as $\mathbb{Q}, \mathbb{Q}(\sqrt{2}), \ldots$ are not algebraically closed. 
  \end{example}

  It turns out that the complex numbers are algebraically closed, which is presented with the following grand name. Ironically, this theorem cannot be proven with algebra alone. We need complex analysis.\footnote{Gauss proved this for the first time in 1799.} 

  \begin{theorem}[Fundamental Theorem of Algebra]
    Suppose $f \in \mathbb{C}[x]$ is a polynomial of degree $n \geq 1$. Then $f(x)$ has a root in $\mathbb{C}$. It immediately follows from induction that it can be factored as a product of linear polynomials in $\mathbb{C}[x]$. 
  \end{theorem}
  \begin{proof}
    WLOG we can assume that $f$ is monic: $f(z) = z^n + a_{n-1} z^{n-1} + \ldots + a_1 z + a_0$. Since $\mathbb{C}$ is a field, we can set 
    \begin{equation}
      f(z) = z^n \bigg( 1 + \frac{a_{n-1}}{z} + \frac{a_{n-2}}{z^2} + \ldots + \frac{a_0}{z_n} \bigg)
    \end{equation} 
    Since 
    \begin{equation}
      \lim_{|z| \rightarrow \infty} \bigg( 1 + \frac{a_{n-1}}{z} + \frac{a_{n-2}}{z^2} + \ldots + \frac{a_0}{z_n} \bigg) = 0
    \end{equation}
    there exists a $R > 0$ s.t. 
    \begin{equation}
      |z| > R \implies \bigg| 1 + \frac{a_{n-1}}{z} + \frac{a_{n-2}}{z^2} + \ldots + \frac{a_0}{z_n} \bigg| < \frac{1}{2}
    \end{equation}
    and hence 
    \begin{equation}
      |z| > R \implies |f(z)| > |z|^n \cdot \bigg( 1 - \frac{1}{2} \bigg) > \frac{R^n}{2}
    \end{equation}
    So $z$ cannot be a root if $|z| > R$. On the other hand, $f(z)$ is continuous (under the Euclidean topology) and so on the compact set $\{z \in \mathbb{C} \mid |z| \leq R\}$, $|f(z)|$ achieves a minimum value say at the point $z_0$. We claim that $\min_z f(z) = 0$. 

    For convenience, we let $z_0 = 0$ (we can do a change of basis on the polynomial) and assume that the minimum is some positive number, i.e. $f(0) = a_0 \neq 0$. Let $j$ be the smallest positive integer such that $a_j = 0$. Let 
    \begin{equation}
      g(z) = \frac{a_{j+1}}{a_j} z + \ldots + \frac{a_n}{a_j} z^{n-j} \implies f(z) = a_0 + a_j z^j \big( 1 + g(z) \big) 
    \end{equation}
    We set $\gamma = \sqrt[j]{-a_0/a_j}$ and consider the values of 
    \begin{align}
      f(t \gamma) & = a_0 + a_j (t\gamma)^j \big( 1 + g(t\gamma) \big) \\
                  & = a_0 - a_0 t^j \big(1 + g(t \gamma) \big) \\
                  & = a_0 \big\{ 1 - t^j \big(1 + g(t \gamma) \big) \big\}
    \end{align} 
    for $t > 0$. For $t$ sufficiently small, we have 
    \begin{equation}
      |g(t \gamma)| = \bigg| \frac{a_{j+1}}{a_j} (t \gamma) + \ldots + \frac{a_n}{a_j} (t \gamma)^{n-j} \bigg| < \frac{1}{2} 
    \end{equation}
    and for such $t$, this implies 
    \begin{equation}
      |f(t \gamma)| = |a_0| |1 - t^j (1 + g(t \gamma))| \leq |a_0| |1 - t^j/2| < |a_0|
    \end{equation}
    and so $z_0$ cannot have been the minimum of $|f(z)|$. Therefore, the minimum value must be $0$.  
  \end{proof}

  Great, so through this theorem, we can work in any subfield of $\mathbb{C}$ and guarantee that will have all of its roots in $\mathbb{C}$. 

  \begin{corollary}[$\mathbb{C}$ is algebraically closed]
    $\mathbb{C}$ is algebraically closed, i.e. $\mathbb{C}$ is a splitting field of $\mathbb{C}[x]$. 
  \end{corollary}

  Put more succinctly, the impossibility of defining division on the ring of integers motivates its extension into the field of rational numbers. Similarly, the inability to take square roots of negative real numbers forces us to extend the field of real numbers to the bigger field of complex numbers. 

  \begin{theorem}[Eigenvector Conditions for Algebraic Closedness]
    A field $F$ is algebraically closed if and only if for each natural number $n$, every endomorphism of $F^n$ (that is, ever linear map from $F^n$ to itself) has at least one eigenvector. 
  \end{theorem}
  \begin{proof}
    An endomorphism of $F^n$ has an eigenvector if and only if its characteristic polynomial has some root. $(\rightarrow)$ So, when $F$ is algebraically closed, every characteristic polynomial, which is an element of $F[x]$, must have a root. $(\leftarrow)$ Assume that every characteristic polynomial has some root, and let $p \in F[x]$. Dividing the polynomial by a scalar doesn't change its roots, so we can assume $p$ to have leading coefficient $1$. If $p(x) = a_0 + a_1 x + ... + x^n$, then we can identify matrix 
    \begin{equation}
      A = \begin{pmatrix}
      0 & 0 & ... & 0 & -a_0 \\
      1 & 0 & ... & 0 & -a_1 \\
      0 & 1 & ... & 0 & -a_2 \\
      ... & ... & ... & ... & ... \\
      0 & 0 & ... & 1 & -a_{n-1}
      \end{pmatrix}
    \end{equation}
    such that the characteristic polynomial of $A$ is $p$. 
  \end{proof}

  With this splitting condition, we can get a nice set of formulas often introduced in high-school math competitions. 

  \begin{theorem}[Viete's Formulas]
    Given that a polynomial $f$ factors into linear terms, that is 
    \begin{equation}
      f(x) = a_0 \prod_{i = 1}^{n} (x - c_i), c_i \text{ roots of } f
    \end{equation}
    Then the coefficients of $f$ can be presented with the formulas
    \begin{align*}
      & \sum_{i=1}^n c_i = - \frac{a_1}{a_0} \\
      & \sum_{i_1 < i_2} c_{i_1} c_{i_2} = \frac{a_2}{a_0} \\
      & \sum_{i_1< ...< i_k} \prod_{j = 1}^{k} c_{i_j} = (-1)^k \frac{a_k}{a_0} \\
      & c_1 c_2 c_3 ... c_n = (-1)^n \frac{a_n}{a_0}
    \end{align*}
  \end{theorem}

\subsection{Reducibility of Real Polynomials}

  \begin{theorem}
    If $c$ is a complex root of polynomial $f \in \mathbb{R}[x]$, then $\bar{c}$ is also a root of the polynomial. Moreover, $\bar{c}$ has the same multiplicity as $c$. 
  \end{theorem}

  \begin{corollary}
    Every nonzero polynomial in $\mathbb{R}[x]$ factors into a product of linear terms and quadratic terms with negative discriminants. 
  \end{corollary}

  \begin{example}
    \begin{align*}
      x^5 - 1 & = (x-1) \bigg( x - \Big( \cos{\frac{2\pi}{5}} + i \sin{\frac{2\pi}{5}}\Big) \bigg) \bigg( x - \Big( \cos{\frac{2\pi}{5}} - i \sin{\frac{2\pi}{5}}\Big) \bigg) \\
      & \times \bigg( x - \Big( \cos{\frac{4\pi}{5}} + i \sin{\frac{4\pi}{5}}\Big) \bigg) \bigg( x - \Big( \cos{\frac{4\pi}{5}} - i \sin{\frac{4\pi}{5}}\Big) \bigg) \\
      & = (x-1) \bigg( x^2 - \frac{\sqrt{5} - 1}{2} x + 1\bigg) \bigg( x^2 + \frac{\sqrt{5} + 1}{2} x + 1\bigg) 
    \end{align*}
  \end{example}

  \begin{corollary}
    Every polynomial $f \in \mathbb{R}[x]$ of odd degree has at least one real root. 
  \end{corollary}
  \begin{proof}
    This is a direct result of Theorem **. Alternatively, without loss of generality we can assume that the leading coefficient of $f$ is positive. Then
    \begin{equation}
      \lim_{x \rightarrow + \infty} f(x) = + \infty, \; \lim_{x \rightarrow -\infty} f(x) = -\infty
    \end{equation}
    By the intermediate value theorem, there must be some point where $f$ equals $0$. 
  \end{proof}

  \begin{theorem}[Descartes' Rule of Signs] 
    \label{thm:descartes}
    Let $f(x) = x^n + a_{n-1}x^{n-1} + \cdots + a_1x + a_0 \in \mathbb{R}[x]$. Let $C_+$ be the number of times the coefficients of $f(x)$ change signs (here we ignore the zero coefficients); let $Z_+$ be the number of positive roots of $f(x)$, counting multiplicities. Then $Z_+ \leq C_+$ and $Z_+ \equiv C_+ \pmod{2}$. Moreover, if we set $g(x) = f(-x)$, let $C_-$ be the number of times the coefficients of $g(x)$ change signs, and $Z_-$ the number of negative roots of $f(x)$. Then $Z_- \leq C_-$ and $Z_- \equiv C_- \pmod{2}$.
  \end{theorem}

  \begin{theorem}
    The number of positive roots of $f(x)$ is the same as the number of negative roots of $f(-x)$.
  \end{theorem}

  \begin{example}[Easy Way to Find Number of Positive Roots]
    Given $f(x) = x^5 + x^4 - x^2 - 1$, 
    \begin{enumerate}
      \item We have $C_+ = 1$. By Descartes' rule of signs, it must be the case that $Z_+ \leq 1$ and $Z_+ \equiv 1 \pmod{2} \implies Z_+ = 1$. 
      \item Since $f(-x) = -x^5 + x^4 - x^2 - 1$, we have $C_- = 2$, so $Z_- = 0$ or $2$. This is the best that we can do, though it turns out that it actually has $0$ negative roots.\footnote{On the other hand, $x^5 + 3x^3 - x^2 - 1$ has 2 negative roots.} 
    \end{enumerate}
  \end{example}

  Note that if a polynomial has a multiple root but its coefficients are known only approximately (but with any degree of precision), then it is impossible to prove that the multiple roots exists because under any perturbation of the coefficients, however small, it may separate into simple roots or simply cease to exist. This fact leads to the "instability" of the Jordan Normal form because under any perturbation of the elements of a matrix $A$, the change may drastically affect the characteristic polynomial, hence affecting the geometric multiplicities of its eigenvectors. 

\subsection{Reducibility of Integer Polynomials} 

  Even though we have covered a more general theory of polynomials with rational coefficients, it is worthwhile to visit integer polynomials for two reasons. First, there are a few specialized theorems that allow us to easily determine reducibility in $\mathbb{Z}[x]$. Second, Gauss's lemma allows us to check for reducibility in $\mathbb{Q}[x]$ by checking for reducibility in $\mathbb{Z}[x]$, at which point we can abuse the specialized theorems we have developed. 

  \begin{theorem}[Rational Root Theorem]
    Let $a_n x^n + \ldots + a_0 \in \mathbb{Z}[x]$. If $r/s \in \mathbb{Q}$ with $\gcd(r, s) = 1$, then $r \mid a_0$ and $s \mid a_n$. 
  \end{theorem}
  \begin{proof}
    Given that $r/s$ is a root, we have 
    \begin{equation}
      a_n (r/s)^n + \ldots + a_0 = 0
    \end{equation}
    Multiplying by $s^n$, we get 
    \begin{equation}
      a_n r^n + a_{n-1} r^{n-1} s + \ldots + a_1 s^{n-1} r + a_0 s^n = 0
    \end{equation}
    and putting this equation on mod $r$ and mod $s$ implies that $r | a_0 s^n$ and $s | a_n r^n$, respectively. But since we assumed that $\gcd (r, s) = 1$, $r | a_0$ and $s | a_n$. 
  \end{proof}

  The next is quite a remarkable result, since it says that decompositions in $\mathbb{Q}[x]$ imply decompositions in $\mathbb{Z}[x]$! Therefore, to check irreducibility in $\mathbb{Q}[x]$, it suffices to check irreducibility in $\mathbb{Z}[x]$. 

  \begin{lemma}[Gauss's Lemma]
    Let $f \in \mathbb{Z}[x]$. If $\exists g, h \in \mathbb{Q}[x]$ s.t. $f(x) = g(x) h(x)$, then $\exists \bar{g}, \bar{h} \in \mathbb{Z}[x]$ s.t. $f(x) = \bar{g}(x) \bar{h}(x)$. 
  \end{lemma}
  \begin{proof}
    We can find $k, l \in \mathbb{Z}$ s.t. $g_1 (x) = k g(x)$ and $h_1 (x) = l h(x)$ have integer coefficients, i.e. $g_1, h_1 \in \mathbb{Z}[x]$. Then, $k l f(x) = g_1 (x) h_1 (x) \in \mathbb{Z}[x]$. Let $p$ be a prime factor of $kl$. We have 
    \begin{equation}
      0 \equiv \bar{k} \bar{l} \bar{f} (x) \equiv \bar{g}_1 (x) \bar{h}_1 (x) \text{ in } \mathbb{Z}_p [x]
    \end{equation}
    Since $\mathbb{Z}_p$ is an integral domain, $\mathbb{Z}_p [x]$ is an integral domain, and so $\bar{g}_1$ or $\bar{h}_1$ must be $0$. WLOG let it be $\bar{g}_1$. Then every coefficient of $g_1 (x)$ is divisible by $p$, and we can write it in the form $g_2(x) = p g_1 (x)$. Therefore, 
    \begin{equation}
      p(x) \cdot \frac{kl}{p} = \underbrace{\frac{g_1 (x)}{p}}_{g_2 (x)} \cdot \underbrace{h_1 (x)}_{h_2 (x)} \iff f(x) \frac{kl}{p} = g_2 (x) h_2 (x)
    \end{equation}
    Since there are only finitely many prime divisors, we do this for all prime factors of $kl$, and we have 
    \begin{equation}
      f(x) = g_n (x) h_n (x), \qquad g_n, h_n \in \mathbb{Z}[x]
    \end{equation}
  \end{proof}

  \begin{example}[Reducibility of Integer Polynomials]
    Let $f(x) = x^4 - x^3 + 2$. The rational roots are in the set $S = \{\pm 1, \pm2 \}$, but none of them work since $f(\pm1), f(\pm2) \neq 0$. By degree considerations and Gauss's lemma, if $f(x)$ is reducible, then 
    \begin{equation}
      f(x) = (x^2 + ax + b) (x^2 + cx + d), \qquad a, b, c, d \in \mathbb{Z}
    \end{equation}
    We know that $bd \in S$, with $a + c = -1$, $d + b + ac = 0$, and so on for each coefficients. We can brute force this finite set of possibilities. 
  \end{example}

  A great way to check irreducibility is to check in mod $p$. 

  \begin{theorem}
    Let $f(x) = a_n x^n + \ldots + a_0 \in \mathbb{Z}[x]$. If $p \nmid a_n$ and $f \in \mathbb{Z}_p [x]$ is irreducible, then $f$ is irreducible in $\mathbb{Q}[x]$.\footnote{May need to verify this again.}
  \end{theorem}
  \begin{proof}
    Suppose that $f(x) = g(x) h(x) \in \mathbb{Z}[x]$ with $\deg(g), \deg(h) > 0$. Then 
    \begin{equation}
      f(x) \equiv g(x) h(x) \text{ in } \mathbb{Z}_p [x]
    \end{equation}
    Since $f(x)$ is irreducible in $\mathbb{Z}_p [x]$, we must have that one of $g(x)$ or $h(x)$ has degree $0$ in $\mathbb{Z}_p [x]$. WLOG let it be $g(x)$, but this means that the leading coefficient of $g(x)$ must be divisible by $p \implies$ leading coefficient of $f(x)$ is divisible by $p \iff p \mid a_n$. 
  \end{proof}

  \begin{example}
    $x^4 + x + 1$ is irreducible in $\mathbb{Z}_2 [x]$. So we can extend this to $\mathbb{Z}[x]$ to see that \textit{all} fourth degree polynomials of form $a x^4 + b x^3 + c x^2 + dx + e$, which $a, d, e$ odd and $b, c$ even is irreducible in $\mathbb{Q}[x]$. 
  \end{example}

  This is a powerful theorem to quickly find a large class of polynomials that are irreducible. However, being reducible in $\mathbb{Z}_p [x]$ does not imply reducibility in $\mathbb{Q}$. In fact, there are polynomials $f(x) \in \mathbb{Z}[x]$ which are irreducible but reducible in $\mathbb{Z}_p$ for \textit{every} prime $p$. 

  \begin{theorem}[Eisenstein's Criterion]
    Let $f(x) = a_n x^n + \ldots + a_0 \in \mathbb{Z}[x]$ and $p \in \mathbb{Z}$ a prime s.t. $p \nmid a_n$, $p \mid a_i$ for $i = 0, \ldots, a_{n-1}$, and $p^2 \nmid a_0$. Then $f(x)$ is irreducible in $\mathbb{Q}[x]$. 
  \end{theorem}
  \begin{proof}
    Suppose that $f(x) = g(x) h(x) \in \mathbb{Q}[x]$ with $\deg(g), \deg(h) > 0$. Then, by Gauss's lemma, $g, h \in \mathbb{Z}[x]$. Reducing the equations mod $p$, 
    \begin{equation}
      f(x) = g(x) h(x) \text{ in } \mathbb{Z}_p [x]
    \end{equation}
    But $f(x) = a_n x^n$. By unique factorization theorem in $\mathbb{Z}_p [x]$, $g, h \in \mathbb{Z}_p [x]$ must be products of units and prime factors of $a_n x^n$, which are $\{x\}$. Therefore, let 
    \begin{equation}
      g(x) = b_m x^m, h(x) = \frac{a_n}{b_m} x^{n-m} \in \mathbb{Z}_p [x]
    \end{equation}
    with $\deg(g) = m > 0$ and $\deg(h) = n - m > 0$ in $\mathbb{Z}[x]$. This implies that the constant coefficients of $g(x), h(x)$ are divisible by $p$, which implies that the constant coefficients of $f(x) = g(x) h(x)$ are divisible by $p^2$, a contradiction. 
  \end{proof}

  \begin{example}[Easy Checks for Irreducibility with Eisenstein]
    Listed. 
    \begin{enumerate}
      \item $x^{13} + 2x^{10} + 4x + 6$ is irreducible in $\mathbb{Q}[x]$ by Eisenstein for $p = 2$. 
      \item $x^3 + 9x^2 + 12x + 3$ is irreducible in $\mathbb{Q}[x]$ by Eisenstein for $p = 3$. 
      \item Let $f(x) = x^4 + x^3 + x^2 + x + 1$. Then, we know that $f(x) = \frac{x^5 - 1}{x-1}$ and so 
      \begin{align}
        f(x + 1) & = \frac{(x + 1)^5 - 1}{(x + 1) - 1} \\
                 & = \frac{1}{x} \bigg( x^5 + \binom{5}{1} x^4 + \binom{5}{2} x^3 + \binom{5}{3} x^2 + \binom{5}{4} x + \binom{5}{5} - 1 \bigg) \\
                 & = x^4 + 5x^3 + 10 x^2 + 10x + 5
      \end{align}
      So all nonleading coefficients are divisible by $5$ exactly once, which by Eisenstein implies that $f(x+1)$ is irreducible which implies that $f(x)$ is irreducible. 
    \end{enumerate}
  \end{example}

  We have prod that for $\alpha \in \mathbb{C}$, subfield $F \subset \mathbb{C}$, and $f(x) \in F[x]$, with $f(\alpha) = 0$, then $B = \{1, \alpha, \ldots, \alpha^{\deg(f) - 1}\}$ spans $F[\alpha]$ as a $F$-vector space. If $f(x)$ is irreducible then $B$ is a basis. 

\subsection{Rational Functions}

  Given a field $F$, we have constructed the Euclidean domain $F[x]$. However, this is one step away from being a field. We mimick the construction of the rational numbers $\mathbb{Q}$ as a quotient space over $\mathbb{Z} \times (\mathbb{Z} \setminus \{0\})$ by taking $F[x] \times (F[x] \setminus \{0\})$ and putting a quotient on it. 
  
  \begin{definition}[Rational Functions]
    The \textbf{rational functions} are defined to be the field of quotients (really just 2-tuples) of the form 
    \begin{equation}
      F(x) \coloneqq \bigg\{ \frac{f(x)}{g(x)} \; \bigg| \; f(x), g(x) \in F[x], g(x) \neq 0 \bigg\}
    \end{equation}
    where addition and multiplication is defined in the usual sense.
  \end{definition}

  \begin{theorem}[Partial Fractions Decomposition]
    Let $f(x), g(x) \in F[x]$ where $\deg(f(x)) < \deg(g(x))$. If $g(x) = u(x) v(x)$ where $u, v$ are relatively prime, then there are polynomials $a(x), b(x)$ with $\deg(a) < \deg(u), \deg(b) < \deg(v)$ s.t. 
    \begin{equation}
      \frac{f(x)}{g(x)} = \frac{a(x)}{u(x)} + \frac{b(x)}{v(x)}
    \end{equation}
    By induction, we can prove this for any finite set of irreducible polynomials. 
  \end{theorem}
  \begin{proof}
    We describe an algorithm to get this decomposition. There are polynomials $s(x), t(x)$ s.t. $1 = s(x) u(x) + t(x) v(x)$. Therefore, 
    \begin{equation}
      \frac{f(x)}{ u(x) v(x)} = \frac{f(x) t(x)}{u(x)} + \frac{f(x) s(x)}{v(x)}
    \end{equation}
    and we can use the Euclidean algorithm to write 
    \begin{align}
      \frac{f(x) t(x)}{u(x)} & = q(x) + \frac{a(x)}{u(x)}, \qquad \deg(a) < \deg(u) \\
      \frac{f(x) s(x)}{v(x)} & = q(x) + \frac{a(x)}{u(x)}, \qquad \deg(b) < \deg(v)
    \end{align}
    which implies 
    \begin{equation}
      \frac{f(x)}{u(x) v(x)} = \frac{a(x)}{u(x)} + \frac{b(x)}{v(x)}
    \end{equation}
  \end{proof}

  \begin{example}
    Consider the rational function $\frac{x + 3}{x^3 (x - 1)^2}$. Applying the Euclidean algorithm, we find that 
    \begin{equation}
      1 = (3x^2 + 2x + 1) (x - 1)^2 - (3x - 4) x^3
    \end{equation}
    and so 
    \begin{align}
      \frac{x + 3}{x^3 (x - 1)^2} & = \frac{(x + 3)(3x^2 + 2x + 1)}{x^3} - \frac{(x + 3)(3x - 4)}{(x - 1)^2} \\
                                  & = \frac{11x^2 + 7x + 3}{x^3} + \frac{-11x + 15}{(x - 1)^2}
    \end{align}
  \end{example}

\subsection{Symmetric Polynomials}

  \begin{definition}
    A polynomial $f \in \mathbb{F}[x_1, ..., x_n]$ is called \textbf{symmetric} if it is invariant under any permutation of the variables $x_i$. 
  \end{definition}

  \begin{example}
    Power sums are symmetric polynomials. 
    \begin{equation}
      p(x_1, x_2, ..., x_n) = \sum_{i=1}^n x_i^k
    \end{equation}
  \end{example}

  \begin{definition}
    An \textbf{elementary symmetric polynomial} is a symmetric polynomial of one of these forms: 
    \begin{align*}
      \sigma_1 & = x_1 + x_2 + ... + x_n \\
      \sigma_2 & = x_1 x_2 + x_1 x_3 + ... + x_{n-1} x_n \\
      ... & = ... \\
      \sigma_k & = \sum_{i_1 < ... < i_k} x_{i_1} x_{i_2} ... x_{i_k} \\
      ... & = ... \\
      \sigma_n & = x_1 x_2 ... x_n
    \end{align*}
  \end{definition}

  The following theorem presents an extremely useful result about the decomposition of symmetric polynomials. 

  \begin{theorem}
    Every symmetric polynomial can be written as a polynomial of elementary symmetric polynomials $\sigma_i$. 
  \end{theorem}

  \begin{example}
    The polynomial 
    \begin{equation}
      f \equiv \sum_{i=1}^n x_i^3
    \end{equation}
    can be expressed as 
    \begin{equation}
      f = \sigma_1^3 - 3 \sigma_1 \sigma 2 + 3 \sigma_3
    \end{equation}
  \end{example}


\section{Rings}

\subsection{Ring-like Structures}

  \begin{definition}[Ring]
    A \textbf{ring} is a set $(R, +, \times)$ equipped with two operations, called addition and multiplication. It has properties: 
    \begin{enumerate}
      \item $R$ is an abelian group with respect to $+$, where we denote the additive identity as $0$ and the additive inverse of $x$ as $-x$. 
      \item $R$ is closed under $\times$.
      \item $\times$ is distributive with respect to addition $+$
        \begin{equation}
          a \times (b + c) = a\times b + a\times c, (a+b)\times c = a\times c + b\times c \; \forall a, b, c \in R
        \end{equation}
      \item There is a multiplicative identity, also called the \textbf{unity}, denoted $1$ such that 
        \begin{equation}
          1 \times a = a \times 1 = a \; \forall a \in R
        \end{equation}
    \end{enumerate}
  \end{definition}

  \begin{lemma} 
    Additive inverses are unique and $-1 \times a$ is the additive inverse of $a$. 
  \end{lemma}
  \begin{proof}
    We can see that 
    \begin{align}
      -1 + 1 = 0 & \implies (-1 + 1) \times a = 0 \times a \\
                 & \implies -1 \times a + 1 \times a = 0 \\
                 & \implies -1 \times a + a = 0 
    \end{align}
    and therefore by definition $-1 \times a$ must be the additive inverse. 
  \end{proof}

  Note that unlike groups, commutativity and associativity are not assumed w.r.t. $\times$. In fact, in some cases the existence of the multiplicative identity is not even assumed, though we will do it here. They must be additionally endowed. 

  \begin{definition}[Commutative, Associative Ring]
    A ring $R$ is 
    \begin{enumerate}
      \item a \textbf{commutative ring} if and only if multiplication is commutative. 
      \item an \textbf{associative ring} if and only if multiplication is associative, i.e. $a \times (b \times c) = (a \times b) \times c$. 
    \end{enumerate}
  \end{definition}

  This is not to be confused with the unit of a ring. 

  \begin{definition}[Unit]
    A \textbf{unit} of a ring $R$ is an element $u \in R$ that has a multiplicative inverse in $R$. 
  \end{definition}

  \begin{example}[Numerical Rings] 
    \begin{enumerate}
      \item $\mathbb{Z}, \mathbb{Q}, \mathbb{R}$ are commutative, associative rings with respect to ordinary addition and multiplication.
      \item The set of even integers $2\mathbb{Z}$ is a commutative, associative ring without unity.
    \end{enumerate}
  \end{example}

  \begin{proposition}[Power Set]
    Given a set $X$, let $2^X$ be its power set, that is the set of all subsets of $X$. Then, $2^X$ is a commutative associative ring with respect to the operations of symmetric difference (i.e. the set of elements which is in exactly one of the sets) 
    \begin{equation}
      M \bigtriangleup N \equiv (M \setminus N) \cup (N \setminus M)
    \end{equation}
    and intersection $\cap$, taken for addition an multiplication, respectively. 
  \end{proposition}
  \begin{proof}
    We will not prove all of the axioms of the ring, but we can state some important facts about this structure. The additive identity is $\emptyset$ and the multiplicative identity is $X$. Finally, it is clear that 
    \begin{align*}
      & M \bigtriangleup N \equiv (M \setminus N) \cup (N \setminus M) \equiv N \bigtriangleup M \\
      & M \cap N = N \cap M \\
      & M \cap N \cap P = (M \cap N) \cap P = M \cap (N \cap P)
    \end{align*}
  \end{proof}

  \begin{example}
    A \textbf{division ring}, also called a \textbf{skew field}, is an associative ring with unity where every nonzero element is invertible with respect to $\times$. Division rings differ from fields in that multiplication is not required to be commutative. 
  \end{example}

  At first, a division ring may not seem different from a field. However, a classic example is the ring of invertible matrices, which is not necessarily commutative, but is a ring in which "division" can be done by right and left multiplication of a matrix inverse. 
  \begin{equation}
    a a^{-1} = a^{-1} a = I
  \end{equation}
  This implies that every element in the division ring commutes with the identity, but again commutativity does not necessarily hold for arbitrary elements $a, b$. 

  \begin{definition}
    A \textbf{field} $(F, +, \times)$ is a commutative, associative ring with unity where every nonzero element is invertible (with respect to $\times$). It is usually denoted as $\mathbb{F}$. Note that $F$ is now an abelian group with respect to $\times$. 
  \end{definition}

  \begin{definition}
    An element a of a ring $R$ is called a \textbf{left zero divisor} if there exists a nonzero $x$ such that $a x = 0$ and a \textbf{right zero divisor} if there exists a nonzero $x$ such that $x a = 0$. 
  \end{definition}

  \begin{definition}
    A ring $R$ with no zero divisors for every element is called a \textbf{domain}. 
  \end{definition}

  \begin{proposition}
    Every field is a domain. 
  \end{proposition}
  \begin{proof}
    Given $x, y \in \mathbb{F}$, assume $x y = 0$ with $x \neq 0$. Since $x$ is invertible,
    \begin{equation}
      0 = x^{-1} 0 = x^{-1} (x y) = y
    \end{equation}
    Now assuming that $y \neq 0$, since $y$ is invertible, 
    \begin{equation}
      0 = 0 y^{-1} = (x y) y^{-1} = x
    \end{equation}
  \end{proof}

  While the converse is not true, we can state the following result. 

  \begin{theorem}[Wedderburn's little theorem]
    Every finite domain is a field. 
  \end{theorem}

\subsection{Field of Complex Numbers}

  The impossibility of defining division on the ring of integers motivates its extension into the field of rational numbers. Similarly, the inability to take square roots of negative real numbers forces us to extend the field of real numbers to the bigger field of complex numbers. 

  \begin{definition}
    The \textbf{field of complex numbers} is a field $\mathbb{C}$ such that 
    \begin{enumerate}
      \item It contains the field $\mathbb{R}$ as a subfield. 
      \item It contains an element $i$ such that $i^2 = -1$.
      \item It is minimal with respect to properties (i) and (ii). That is, if $F$ is a subfield of $\mathbb{C}$ containing $\mathbb{R}$ and $i$, then $F = \mathbb{C}$. 
    \end{enumerate}
  \end{definition}

  Note that the identity $x^2 + 1 \equiv (x + i) (x - i)$ implies that the equation $x^2 = -1$ has exactly two solutions in $\mathbb{C}$, $i$ and $-i$. Therefore, if a subfield of $\mathbb{C}$ contains one of these solutions, it must contain the other (since $i$ and $-i$ are additive and multiplicative inverses). 

  Furthermore, since $i$ is defined to be $\sqrt{-1}$, we could replace $i$ with $-i$ and our calculations would still be consistent throughout the rest of mathematics. In fact, $i$ and $-i$ behave \textbf{exactly} identically and cannot be distinguished in an abstract sense. Visually, the complex plane "flipped" across the real number axis produces the same complex plane. 

  \begin{theorem}
    $\mathbb{C}$ exists and is unique up to an isomorphism that maps all real numbers to themselves. Every complex number can be uniquely written as $a + bi$, where $a, b \in \mathbb{R}$ and $i$ is a fixed element such that $i^2 = -1$. 
  \end{theorem}
  \begin{proof}
    We first assume that $\mathbb{C}$ exists. Consider the subset of $\mathbb{C}$
    \begin{equation}
      K \equiv \{ a + bi \; | \; a, b \in \mathbb{R}\}
    \end{equation}
    By evaluating its operations, we can check for closure, identity, and invertibility of nonzero elements to conclude that $K$ is a subfield of $\mathbb{C} \implies$ by prop. (iii), $K = \mathbb{C} \implies$ every element in $\mathbb{C}$ can be written in form $a + bi$. To prove uniqueness, we assume that $p \in \mathbb{C}$ can be written in distinct forms $p = a + bi = a^{\prime} + b^\prime i$. Then
    \begin{align*}
       a + bi = a^{\prime} + b^\prime i & \implies (a - a^\prime)^2 = (b^\prime i - b i)^2 = - (b^\prime - b)^2 \\
       & \implies a - a^\prime = b^\prime - b = 0
    \end{align*}
    To prove uniqueness of $\mathbb{C}$ up to ismorphism, we assume that $\mathbb{C}^\prime$ exists with $i^\prime$ such that $i^{\prime 2}$ containing elements $a + b i'$. Let $f: \mathbb{C} \longrightarrow \mathbb{C}^\prime$ defined 
    \begin{equation}
      f( a + bi) = a + bi^\prime
    \end{equation}
    Then, 
    \begin{align*}
      f\big((a + b i) + (c + d i) \big) & = f\big( (a + c) + (b + d)i \big) \\
      & = (a + c) + (b + d) i^\prime \\
      & = (a + b i^\prime) + (c + d i^\prime) \\
      & = f(a + b i) + f( c + d i) \\
      f\big( \kappa (a + b i)\big) & = f\big( \kappa a + \kappa b i\big) \\
      & = \kappa a + \kappa b i^\prime \\
      & = \kappa (a + b i^\prime) \\
      & = \kappa f(a + b i)
    \end{align*}
    So, $f$ is an isomorphism, and $\mathbb{C} \simeq \mathbb{C}^\prime$. From analysis, we can construct and prove the existence of $\mathbb{R}$. We then define the map
    \begin{equation}
      \rho: \mathbb{R}^2 \longrightarrow \mathbb{C}, \; \rho(a, b) \equiv a + bi
    \end{equation}
    with $\rho(1, 0)$ as the multiplicative identity and $\rho(0,1) \equiv i$. Therefore, every element of $\mathbb{C}$ can be uniquely represented as an element of $\mathbb{R}^2$. 
  \end{proof}

  \begin{definition}
    \textbf{Complex conjugation} is an automorphism of $\mathbb{C}$ defined
    \begin{equation}
      c = a + b i \mapsto \bar{c} = a - b i
    \end{equation}
    This is identically defined by replacing $i$ with $-i$. Clearly, $\bar{\bar{c}} = c$. 
  \end{definition}

  \begin{definition}
    Real numbers are elements in $\mathbb{C}$ that are equal to their own conjugates. 
  \end{definition}

  \begin{proposition}
    For any $c \in \mathbb{C}$, $c + \bar{c}$ and $c \bar{c}$ are real. 
  \end{proposition}
  \begin{proof}
    Using the fact that the complex conjugate is an isomorphism, 
    \begin{align*}
      & \bar{c + \bar{c}} = \bar{c} + \bar{\bar{c}} = \bar{c} + c = c + \bar{c} \\
      & \bar{ c \bar{c}} = \bar{c} \bar{\bar{c}} = \bar{c} c = c \bar{c}
    \end{align*}
  \end{proof}
  Note that we proved this abstractly using only the properties given above, and did not decompose $c$ to its \textbf{algebraic form} $a + b i$. 

  If $c = a + b i, \; a, b \in \mathbb{R}$, then 
  \begin{equation}
    c + \bar{c} = 2a, \; c \bar{c} = a^2 + b^2
  \end{equation}

  In case the reader is unaware, it is common to interpret complex numbers $c = a + b i$ as points or vectors $(a, b)$ on the complex plane. 

  \subsubsection{Polar Representations of Complex Numbers}

    \begin{definition}
      The \textbf{absolute value} of a complex number $c = a + b i$, denoted $|c|$, is the length of the vector representing $c$. 
      \begin{equation}
        |c| \equiv \sqrt{a^2 + b^2}
      \end{equation}
    \end{definition}

    \begin{definition}
      The \textbf{argument} of a complex number $c = a + b i$, denoted arg$c$, is the angle formed by the corresponding vector with the polar axis. It is defined within the interval $[0, 2\pi)$. 
      \begin{equation}
        \text{arg}(c) \equiv \tan^{-1}{\frac{b}{a}}
      \end{equation}
    \end{definition}

    \begin{definition}
      The \textbf{polar representation}, or \textbf{trigonometric representation}, of a complex number $c = a + b i$ is defined using the equations 
      \begin{equation}
        a = r \cos{\varphi}, \; b = r\sin{\varphi} \implies c = r (\cos{\varphi} + i \sin{\varphi})
      \end{equation}
      This mapping can be defined 
      \begin{equation}
        \rho: \mathbb{R} \times \frac{\mathbb{R}}{2 \pi} \longrightarrow \mathbb{C}, \; \rho(r, \varphi) = r (\cos{\varphi} + i \sin{\varphi})
      \end{equation}
    \end{definition}

    \begin{theorem}
      $\rho$ is "similar" to a homomorphism in the following way. By defining the domain and codomain as groups, 
      \begin{equation}
        \rho: \big( \mathbb{R}, \times \big) \times \Big( \frac{\mathbb{R}}{2 \pi} \Big) \longrightarrow \big( \mathbb{C}, \times \big)
      \end{equation}
      we can see that
      \begin{equation}
        \rho (r_1, \varphi_1) \times \rho(r_2, \varphi_2) = \rho(r_1 \times r_2, \varphi_1 + \varphi_2) 
      \end{equation}
      or equivalently, 
      \begin{equation}
        r_1 (\cos{\varphi_1} + i \sin{\varphi_1}) \cdot r_2 (\cos{\varphi_2} + i \sin{\varphi_2}) = r_1 r_2 (\cos{(\varphi_1 + \varphi_2)} + i \sin{(\varphi_1 + \varphi_2)})
      \end{equation}
    \end{theorem}

    \begin{corollary}
      The formula for the ratio of complex numbers is defined
      \begin{equation}
        \frac{r_1 (\cos{\varphi_1} + i \sin{\varphi_1})}{r_2 (\cos{\varphi_2} + i \sin{\varphi_2})} = \frac{r_1}{r_2} (\cos{(\varphi_1 - \varphi_2)} + i \sin{(\varphi_1 - \varphi_2)})
      \end{equation}
    \end{corollary}

    \begin{corollary}
      The positive integer power of a complex number can be written using \textbf{De Moivre's formula}. 
      \begin{equation}
        \big(r(\cos{\varphi} + i \sin{\varphi})\big)^n = r^n (\cos{n \varphi} + i \sin{n \varphi})
      \end{equation}
    \end{corollary}

    We can use this formula to extract a root of $n$th degree from a complex number $c = r(\cos{\varphi} + i \sin{\varphi})$, which means to solve the equation $z^n = c$. Let $z = s (\cos{\psi} + i \sin{\psi})$. Then by De Moivre's formula, 
    \begin{align*}
      z^n & = s^n (\cos{n \psi} + i \sin{n \psi}) = r(\cos{\varphi} + i \sin{\varphi}) \\
      & \implies s = \sqrt[n]{r}, \; \psi = \frac{\varphi + 2\pi k}{n} \\
      & \implies z = \sqrt[n]{r} \bigg( \cos{\frac{\varphi + 2\pi k}{n}} + i \sin{\frac{\varphi + 2\pi k}{n}}\bigg) \text{ for } k = 0, 1, ..., n-1
    \end{align*}
    Geometrically, the $n$ solutions lie at the vertices of a regular $n$-gon centered at the origin. When $c = 1$, the solutions are the $n$th roots of unity.

\subsection{Rings of Residue Class}

  \begin{definition}
    The quotient set $\mathbb{Z}$ by the relation of congruence modulo $n$ is denoted $\mathbb{Z}_{n}$. It is called the \textbf{ring of residue class modulo n} or \textbf{residue ring modulo n}. 
    \begin{equation}
      \mathbb{Z}_{n} = \{ [0]_{n}, [1]_{n}, ... , [n-1]_{n} \}
    \end{equation}
  \end{definition}

  By definition of the relation, congruence modulo $n$ has properties: 
  \begin{enumerate}
    \item $a \equiv a' \pmod{n}, b \equiv b' \pmod{n} \implies a + b \equiv a' + b' \pmod{n}$ . 
    \item With same hypothesis as (i) $a b \equiv a' b \equiv a b' \equiv a' b' \pmod{n}$. 
  \end{enumerate}
  We can furthermore define operations of addition and multiplication on the ring $\mathbb{Z}_{n}$ as such 
  \begin{align*}
    & [a]_{n} + [b]_{n} \equiv [a + b]_{n} \\
    & [a]_{n} [b]_{n} \equiv [ab]_{n}
  \end{align*}
  making $\mathbb{Z}_{n}$ is a commutative, associative ring with unity. 

  Note that the properties of the operation in $\frac{M}{R}$ inherits all the properties of the addition operation on $M$ that are expressed in the form of identities and inverses, along with the existence of the zero identity. 
  \begin{align*}
    0 \in M & \implies [0] \text{ is the additive identity in } \frac{M}{R} \\
    a + (-a) = 0 & \implies [a] + [-a] = [0] \\
    1 \in M & \implies [1] \text{ is the multiplicative identity in } \frac{M}{R}
  \end{align*}

  \begin{example}
    In $\mathbb{Z}_{5}$, the elements $[2]$ and $[3]$ are multiplicative inverses of each other since $[2] [3] = [6] = [1]$, and $[4]$ is its own inverse since $[4] [4] = [16] = [1]$. The addition and multiplication tables for $\mathbb{Z}_5$ is shown below. 
  \end{example}

  The ring $\mathbb{Z}_n$ has all the properties of a field except the property of having inverses for all of its nonzero elements. This leads to the following theorem. 

  \begin{theorem}
    The ring $\mathbb{Z}_{n}$ is a field if and only if $n$ is a prime number. 
  \end{theorem}
  \begin{proof}
    $(\rightarrow)$ Assume that $n$ is composite $\implies n = k l$ for $k, n \in \mathbb{N} \implies k, n \neq 0$, but 
    \begin{equation}
      [k]_n [l]_n = [k l]_n = [n]_n = 0
    \end{equation}
    meaning that $\mathbb{Z}_n$ contains $0$ divisors and is not a field. The contrapositive of this states $(\rightarrow)$. \\
    $(\leftarrow)$ Given that $n$ is prime, let $[a]_n \neq 0$, i.e. $[a]_n \neq [0]_n, [1]_n$. The set of $n$ elements 
    \begin{equation}
      [0]_n, [a]_n, [2a]_n, ..., [(n-1)a]_n
    \end{equation}
    are all distinct. Indeed, if $[k a]_n = [l a]_n$, then $[(k-l) a]_n = 0 \implies n = (k-l) a \iff n$ is not prime. Since the elements are distinct, exactly one of them must be $[1]_n$, say $[p a]_n \implies$ the inverse $[p]_n$ exists. 
  \end{proof}

  \begin{corollary}
    For any $n$, $[k]_n$ is invertible in the ring $\mathbb{Z}_n$ if and only if $n$ and $k$ are relatively prime. 
  \end{corollary}

  \begin{definition}
    The \textbf{characteristic} of ring $R$ (or a field $F$), denoted char$(R)$, is the smallest number of times one must successively add the multiplicative identity $1$ to get the additive identity $0$. That is char$(R)$ is the smallest positive number $n$ such that 
    \begin{equation}
      1 + 1 + ... + 1 = 0 
    \end{equation}
    If no such number $n$ exists, then char$(R) = 0$. The characteristic of $\mathbb{Z}_n = n$
  \end{definition}

  Note that the characteristic of the field $\mathbb{Z}_n$ must be prime. 

  \begin{theorem}[Freshman's Dream]
    Given a field $F$ with char$(F) = p$, 
    \begin{equation}
      (a + b)^p = a^p + b^p
    \end{equation}
  \end{theorem}
  \begin{proof}
    We have 
    \begin{equation}
      (a + b)^p = \sum_{k = 0}^p \binom{p}{k} a^{p-k} b^{k}
    \end{equation}
    It is clear that 
    \begin{equation}
      \binom{p}{k} = \frac{p (p-1) ... (p - k+1)}{k!}
    \end{equation}
    is divisible by $p$ for all $k \neq 0, p$, so all the middle terms must cancel out to $0$. 
  \end{proof}

\subsection{Polynomial Algebra}

  \subsubsection{Construction and Basic Properties}

    \begin{definition}
      A \textbf{polynomial $f$} of $x$ over a ring $R$ is defined as a formal expression 
      \begin{equation}
        f(x) = a_0 + a_1 x^1 + a_2 x^2 + ...  + a_{n-1} x^{n-1} + a_n x^n
      \end{equation}
      where $n$ is a natural number, the coefficients $a_0, a_1, ..., a_n$ are elements of $R$, $x$ is a formal symbol, whose powers $x^i$ are just placeholders for the corresponding coefficients $a_i$ so that the given formal expression is a way to encode the infinite finitary sequence. 
      \begin{equation}
        (a_0, a_1, a_2, ..., a_n, 0, 0, ...)
      \end{equation}
      Two polynomials are equal if and only if the sequences of their corresponding coefficients are equal.
    \end{definition}

    Note that this is really just a fancy way to write a finitary sequence. 

    \begin{definition}
      The set of polynomials with coefficients in the ring $R$ forms itself a ring, called the \textbf{ring of polynomials over $R$}, denoted $R[x]$. Addition on $R[x]$ is defined component-wise, and it suffices, by the distributive law, to define multiplication as
      \begin{equation}
        x^k x^l = x^{k + l}
      \end{equation}
      given that we have chosen $\{x^i\}$ as a basis of $R[x]$. If $R$ is a commutative associative ring (or a field), then $R[x]$ is called the \textbf{polynomial algebra}. From now, we will treat $R[x]$ and $F[x]$ as an algebra with $R$ denoting a commutative associative ring and $F$ denoting a field, respectively. 
    \end{definition}

    Note that the map from $R \longrightarrow R[x]$ sending $r \mapsto r x^0$ is an injective homomorphism of rings, by which $R$ is viewed as a subring of $R[x]$. 

    The ring of polynomials over field $\mathbb{R}$ is denoted $\mathbb{R}[x]$. $R[x]$ is a subalgebra within the algebra of all function of $\mathbb{R}$. 

    However, for certain finite fields, some formally different polynomials may be indistinguishable in terms of mappings. For example, $x$ and $x^2$ are equivalent in the polynomial algebra defined on the domain $\mathbb{Z}_2$.

    \begin{definition}
      The last nonzero coefficient is called the \textbf{leading coefficient}, and the degree of the polynomial $f$, denoted deg$f$, is the index of the leading coefficient. 
    \end{definition}

    \begin{theorem}
    \begin{align}
      \text{deg}(f+g) & \leq \text{max}\{\text{deg}\,f, \text{deg} \,g\} \\
      \text{deg} \,f g & = \text{deg} \,f + \text{deg} \,g
    \end{align}
    \end{theorem}
    \begin{proof}
      Simple when presenting polynomials if form $(1)$. 
    \end{proof}

    \begin{definition}
      The product of two finitary sequences $(a_0, a_1, a_2, ...)$ and $(b_0, b_1, b_2, ...)$ in the ring $F[x]$ is a sequence 
      \begin{equation}
        (c_0, c_1, c_2, ...), \; c_k = \sum_{l = 0}^{k} a_l b_{k-l}
      \end{equation}
      This formula works for infinite (non-finitary) sequences too, allowing us to define a commutative, associative algebra with unity called the \textbf{algebra of formal power series over $F$}, denoted $F[[x]]$. The elements of $F[[x]]$ are written in the form 
      \begin{equation}
        a_0 + a_1 x + a_2 x^2 + a_3 x^3...
      \end{equation}
    \end{definition}

    \begin{theorem}
      If the field $F$ is infinite, then different polynomials in $F[x]$ determine different functions. 
    \end{theorem}

    \begin{theorem}
      For any collection of given values $y_1, y_2, ..., y_n \in F$ at given distinct points $x_1, x_2, ..., x_n \in F$, there exists a unique polynomial $f \in F[x]$ with deg$\, f < n$ such that
      \begin{equation}
        f(x_i) = y_i, \; i = 1, 2, ..., n
      \end{equation}
      This is commonly known as the \textbf{interpolation problem}, and when $n = 2$, this is called \textbf{linear interpolation}. 
    \end{theorem}

    It is usually impossible to divide one polynomial by another in the algebra $F[x]$; the construction of it does not allow us to. However, division \textbf{with remainder} is possible, similarly to the procedure of division with remainder in the ring of integers. 

    \begin{theorem}
      Let $f, g \in F[x]$ and $g \neq 0$. Then, there exists polynomials $q, r$ such that 
      \begin{equation}
        f = q g + r, \; \text{deg}\, r < \text{deg}\, g \text{ (or } r = 0 \text{)}
      \end{equation}
      This procedure of finding such polynomials $q, r$ is called \textbf{division with a remainder}. A polynomial $f$ is divisible by $g$ in $F[x]$ if and only if $r = 0$. 
    \end{theorem}

    \begin{theorem}[Bezout's Theorem]
      Given that one divides (with remainder) polynomial $f$ by $g = x - c$, let the remainder be $r \in F$. That is, 
      \begin{equation}
        f(x) = (x-c) q(x) + r, \; r \in F
      \end{equation}
      This implies that the remainder equals the value of $f$ at point $c$. That is, 
      \begin{equation}
        f(c) = r
      \end{equation}
    \end{theorem}

  \subsubsection{Roots of Polynomials}

    \begin{definition}
      An element $c \in F$ is a \textbf{root} of polynomial $f$ if and only if 
      \begin{equation}
        f(c) = 0
      \end{equation}
    \end{definition}

    \begin{corollary}
      An element $c$ of a field $F$ is a root of polynomial $f$ if and only if $f$ is divisible by $x - c$. 
    \end{corollary}

    \begin{definition}
      A root $c$ of polynomial $f$ is called \textbf{simple} if $f$ is not divisible by $(x-c)^2$ and \textbf{multiple} otherwise. The \textbf{multiplicity} of a root $c$ is the maximum $k$ such that $(x-c)^k$ divides $f$. 
    \end{definition}

    \begin{theorem}
      The number of roots of a polynomial, counted with multiplicity, does not exceed the degree of this polynomial. Furthermore, these numbers are equal if and only if the polynomial is a product of linear factors.
    \end{theorem}

    \begin{definition}
      A \textbf{monic polynomial} is a polynomial with leading coefficient equal to $1$. 
    \end{definition}

    \begin{theorem}[Viete's Formulas]
      Given that a polynomial $f$ factors into linear terms, that is 
      \begin{equation}
        f(x) = a_0 \prod_{i = 1}^{n} (x - c_i), c_i \text{ roots of } f
      \end{equation}
      Then the coefficients of $f$ can be presented with the formulas
      \begin{align*}
        & \sum_{i=1}^n c_i = - \frac{a_1}{a_0} \\
        & \sum_{i_1 < i_2} c_{i_1} c_{i_2} = \frac{a_2}{a_0} \\
        & \sum_{i_1< ...< i_k} \prod_{j = 1}^{k} c_{i_j} = (-1)^k \frac{a_k}{a_0} \\
        & c_1 c_2 c_3 ... c_n = (-1)^n \frac{a_n}{a_0}
      \end{align*}
    \end{theorem}

    \begin{theorem}[Wilson's Theorem]
      Let $n$ be a prime number. Then 
      \begin{equation}
        (n-1)! \equiv -1 \pmod{n}
      \end{equation}
    \end{theorem}

    \begin{definition}
      The \textbf{derivative} of a polynomial is a map $D: \mathbb{R}[x] \longrightarrow \mathbb{R}[x]$ with the following properties:
      \begin{enumerate}
        \item It is linear. 
        \item $D(f g) = (D f) g + f (D g)$. 
        \item $D x = 1$. 
      \end{enumerate}
    \end{definition}

    In fact, there exists a unique map $D: F[x] \longrightarrow F[x]$ satisfying these properties for any field $F$. 

    \begin{proposition}
      If char$F = 0$, then the coefficients of $f \in F[x]$ regarded as a polynomial in $x - c$ can be expressed as 
      \begin{equation}
        b_k = \frac{ f^{(k)} (c)}{k!}
      \end{equation}
      where $f^{(k)}$ is the $k$th derivative of $f$. 
    \end{proposition}
    \begin{proof}
      We make the substitution $ y = x-c$ in the polynomial $f \in F[x]$ and then express it as a polynomial in $y$ 
      \begin{equation}
        f = b_0 + b_1 (x-c) + b_2 (x-c)^2 + ... + b_n (x-c)^n
      \end{equation}
      We differentiate this equation $k$ times and substitute at $x = c$ to get the corresponding values of the coefficients.
    \end{proof}

  \subsubsection{Fundamental Theorem of Algebra of Complex Numbers}

    While we have defined an upper bound for the number of roots for a polynomial, we have not determined whether a polynomial has any roots at all. Fortunately, it is sufficient to extend the field to $\mathbb{C}$ in order to strongly define a lower limit, too. 

    \begin{definition}
      A field $F$ is \textbf{algebraically closed} if every polynomial of positive degree (i.e. non-constant) in $F[x]$ has at least one root in $F$. This is equivalent to saying that every polynomial can be expressed as a product of first degree polynomials.
    \end{definition}

    \begin{proposition}
      A field $F$ is algebraically closed if and only if for each natural number $n$, every endomorphism of $F^n$ (that is, ever linear map from $F^n$ to itself) has at least one eigenvector. 
    \end{proposition}
    \begin{proof}
      An endomorphism of $F^n$ has an eigenvector if and only if its characteristic polynomial has some root. $(\rightarrow)$ So, when $F$ is algebraically closed, every characteristic polynomial, which is an element of $F[x]$, must have a root. $(\leftarrow)$ Assume that every characteristic polynomial has some root, and let $p \in F[x]$. Dividing the polynomial by a scalar doesn't change its roots, so we can assume $p$ to have leading coefficient $1$. If $p(x) = a_0 + a_1 x + ... + x^n$, then we can identify matrix 
      \begin{equation}
        A = \begin{pmatrix}
        0 & 0 & ... & 0 & -a_0 \\
        1 & 0 & ... & 0 & -a_1 \\
        0 & 1 & ... & 0 & -a_2 \\
        ... & ... & ... & ... & ... \\
        0 & 0 & ... & 1 & -a_{n-1}
        \end{pmatrix}
      \end{equation}
      such that the characteristic polynomial of $A$ is $p$. 
    \end{proof}

    \begin{proposition}
      $\mathbb{R}$ is not algebraically closed. 
    \end{proposition}
    \begin{proof}
      $x^2 + 1$ doesn't have any roots in $\mathbb{R}$. 
    \end{proof}

    \begin{theorem}
      Every polynomial of positive degree over field $\mathbb{C}$ has a root. 
    \end{theorem}

    \begin{corollary}
      In the algebra $\mathbb{C}[x]$, every polynomial splits into a product of linear factors. 
    \end{corollary}

    \begin{corollary}
      Every polynomial of degree $n$ over $\mathbb{C}$ has $n$ roots, counted with multiplicities. 
    \end{corollary}

    \begin{corollary}
      $\mathbb{C}$ is algebraically closed. 
    \end{corollary}

  \subsubsection{Roots of Polynomials with Real Coefficients}

    \begin{theorem}
      If $c$ is a complex root of polynomial $f \in \mathbb{R}[x]$, then $\bar{c}$ is also a root of the polynomial. Moreover, $\bar{c}$ has the same multiplicity as $c$. 
    \end{theorem}

    \begin{corollary}
      Every nonzero polynomial in $\mathbb{R}[x]$ factors into a product of linear terms and quadratic terms with negative discriminants. 
    \end{corollary}

    \begin{example}
    \begin{align*}
      x^5 - 1 & = (x-1) \bigg( x - \Big( \cos{\frac{2\pi}{5}} + i \sin{\frac{2\pi}{5}}\Big) \bigg) \bigg( x - \Big( \cos{\frac{2\pi}{5}} - i \sin{\frac{2\pi}{5}}\Big) \bigg) \\
      & \times \bigg( x - \Big( \cos{\frac{4\pi}{5}} + i \sin{\frac{4\pi}{5}}\Big) \bigg) \bigg( x - \Big( \cos{\frac{4\pi}{5}} - i \sin{\frac{4\pi}{5}}\Big) \bigg) \\
      & = (x-1) \bigg( x^2 - \frac{\sqrt{5} - 1}{2} x + 1\bigg) \bigg( x^2 + \frac{\sqrt{5} + 1}{2} x + 1\bigg) 
    \end{align*}
    \end{example}

    \begin{corollary}
      Every polynomial $f \in \mathbb{R}[x]$ of odd degree has at least one real root. 
    \end{corollary}
    \begin{proof}
      This is a direct result of Theorem **. Alternatively, without loss of generality we can assume that the leading coefficient of $f$ is positive. Then
      \begin{equation}
        \lim_{x \rightarrow + \infty} f(x) = + \infty, \; \lim_{x \rightarrow -\infty} f(x) = -\infty
      \end{equation}
      By the intermediate value theorem, there must be some point where $f$ equals $0$. 
    \end{proof}

    \begin{theorem}[Descartes' Theorem]
      The number of positive roots (counted with multiplicities) of a polynomial $f \in \mathbb{R}[x]$ (denote this $N(f)$) does not exceed the number of changes of sign in the sequence of its coefficients (denote this $L(f)$). Additionally, $L(f) \equiv N(f) \pmod{2}$. If all the complex roots of $f$ are real, then $L(f) = N(f)$. 
    \end{theorem}

    Note that if a polynomial has a multiple root but its coefficients are known only approximately (but with any degree of precision), then it is impossible to prove that the multiple roots exists because under any perturbation of the coefficients, however small, it may separate into simple roots or simply cease to exist. This fact leads to the "instability" of the Jordan Normal form because under any perturbation of the elements of a matrix $A$, the change may drastically affect the characteristic polynomial, hence affecting the geometric multiplicities of its eigenvectors. 

  \subsubsection{Factorization in Euclidean Domains}

    Factorization of polynomials over $\mathbb{C}$ into linear factors and polynomials over $\mathbb{R}$ into linear and quadratic factors is similar to the factoring of the integers to prime numbers. In fact, such a factorization exists for polynomials over any field $F$, but their factors can be of any degree. Moreover, there exists no general solution for the factoring of polynomials over any field. 

    \begin{definition}
      A commutative associative ring with unity and without zero divisors is called an \textbf{integral domain}. That is, the product of any two nonzero elements $x, y \in A$ must be nonzero. Integral domains are generalizations of the ring of integers $\mathbb{Z}$ and provide a natural setting for studying divisibility. 
    \end{definition}

    \begin{example}
      $\mathbb{Z}$ and $F[x]$ over field $F$ are integral domains. Any field $F$ is also an integral domain. 
    \end{example}

    \begin{example}
      The quotient ring $\mathbb{Z}_n$ is not an integral domain when $n$ is composite. 
    \end{example}

    \begin{example}
      A product of two nonzero commutative rings with unity $R \times S$ is not an integral domain since $(1,0) \cdot (0, 1) = (0, 0) \in R \times S$. 
    \end{example}

    \begin{example}
      The ring of $n \times n$ matrices over any nonzero ring when $ n \geq 2$ is not an integral domain. Given matrices $A, B$, if the image of $B$ is in the kernel of $A$, then $A B = 0$.
    \end{example}

    \begin{example}
      The ring of continuous functions on the interval is not an integral domain. To see why, notice that given the piecewise functions 
      \begin{equation}
        f (x) = \begin{cases}
        1 - 2x & x \in [0, \frac{1}{2}] \\
        0 & x \in [\frac{1}{2}, 1] 
        \end{cases}, \; \;\;g (x) = \begin{cases}
        0 & x \in [0, \frac{1}{2}] \\
        2x - 1 & x \in [\frac{1}{2}, 1] 
        \end{cases}
      \end{equation}
      $f, g \neq 0$, but $f g = g f = 0$. 
    \end{example}

    We can classify the rings
    \begin{equation}
      \text{Integral Domains} \subset \text{Commutative Rings} \subset \text{Rings}
    \end{equation}

    \begin{proposition}
      An integral domain is a ring that is isomorphic to a subring of a field. 
    \end{proposition}

    \begin{proposition}
      The characteristic of an integral domain is either $0$ or a prime number. 
    \end{proposition}

    \begin{definition}
       An element $r$ of a ring $R$ is \textbf{regular} if the mapping 
       \begin{equation}
         \rho: R \longrightarrow R, \; x \mapsto x r
       \end{equation}
      is injective for all $x \in R$. 
    \end{definition}

    \begin{proposition}
      An integral domain is a commutative associative ring where every element is regular. 
    \end{proposition}

    \begin{definition}
      Let $A$ be an integral domain. An element $a \in A$ is \textbf{divisible} by $b \in A$, denoted $b | a$ if there exists an element $q \in A$ such that $a = q b$. Elements $a$ and $b$ are \textbf{associated}, denoted $a \sim b$ if either of the following equivalent conditions holds
      \begin{enumerate}
          \item $a | b \text{ and } b | a$
          \item $a = c b, \text{ where } c$ is invertible
      \end{enumerate}
      The two conditions are equivalent because $c$ and $c^{-1}$ are both in $A$. 
    \end{definition}

    \begin{definition}
      Let $A$ be an integral domain which is not a field. $A$ is \textbf{Euclidean} if there exists a function 
      \begin{equation}
        N: A \setminus \{ 0 \} \longrightarrow \mathbb{Z}_+
      \end{equation}
      called a \textbf{norm} that satisfies the following conditions. 
      \begin{enumerate}
        \item $N(a b) \geq N(a)$ and the equality holds if and only if $b$ is invertible. 
        \item For any $a, b \in A, \; b \neq 0$, there exist $q, r \in A$ such that $a = q b + r$ with either $r = 0$ or $ N(r) < N(b)$, known as division with remainder. 
      \end{enumerate}
      Uniqueness of $q, r$ is not required in property 2. 
    \end{definition}

    \begin{example}
      The subring of $\mathbb{C}$, defined
      \begin{equation}
        \mathbb{Z}[i] \equiv \{ a + b i \; | \; a, b \in \mathbb{Z} \}
      \end{equation}
      is a Euclidean integral domain with respect to the norm 
      \begin{equation}
        N(c) \equiv a^2 + b^2
      \end{equation}
      since $N(c d) = N(c) N(d)$ and the invertible elements of $\mathbb{Z}[i]$ are $\pm 1, \pm i$. 
    \end{example}

    \begin{example}
      The ring of rational numbers of the form $2^{-n} m, \; n \in \mathbb{Z}_+, m \in \mathbb{Z}$, is a Euclidean domain. To define the norm, we can first assume that $m$ can be prime factorized into the form 
      \begin{equation}
        m = \pm \prod_{i} p_{i}^{k_i}, \; p \text{ prime}
      \end{equation}
      and the norm is defined 
      \begin{equation}
        N(\frac{m}{2^n}) \equiv 1 + \sum_i k_i
      \end{equation}
      We must further show that division with remainder is possible, but we will not show it here. 
    \end{example}

    \begin{definition}
      The \textbf{greatest common divisor} of elements $a$ and $b$ of an integral domain is a common divisor of $a$ and $b$ divisible by all their common divisors. It is denoted GCD$(a, b)$. 
    \end{definition}

    \begin{definition}
      A \textbf{Gaussian integer} is a complex number whose real part and imaginary part are both integers. That is, 
      \begin{equation}
        \mathbb{Z}[i] \equiv \{a + b i \;|\; a, b \in \mathbb{Z} \}
      \end{equation}
    \end{definition}

  \subsubsection{Polynomials in Several Variables}

    \begin{definition}
      A function of real variable $x_1, x_2, ..., x_n$ is called a \textbf{polynomial} if it can be represented as 
      \begin{equation}
        f(x_1, ..., x_n) =  \sum_{k_1, ..., k_n} a_{k_1 ... k_n} x_1^{k_1} x_2^{k_2} ... x_n^{k_n}
      \end{equation}
      where the summation is taken over a finite set of collections $(k_1, ..., k_n)$. The algebra of polynomials in $x_1, x_2, ..., x_n$ over $\mathbb{R}$ is denoted $\mathbb{R}[x_1, x_2, ..., x_n]$. 
    \end{definition}

    \begin{definition}
      More generally, an infinite dimensional polynomial algebra of variables $x_1, ..., x_n$ over field $\mathbb{F}$ is denoted
      \begin{equation}
        \mathbb{F}[x_1, ..., x_n]
      \end{equation}
      Like polynomials of one variable, it can be naturally identified with an abstract multi-dimensional "sequence." It has basis 
      \begin{equation}
        \{e_{k_1 k_2 ... k_n} \;|\; k_1, k_2, ..., k_n \in \mathbb{Z}_+
      \end{equation}
      with addition defined component-wise and the multiplication rule defined with the table
      \begin{equation}
        e_{k_1...k_n} e_{l_1 ... l_n} = e_{k_1 + l_1, k_2 + l_2, ..., k_n + l_n}
      \end{equation}
      Clearly each polynomial in its usual presentation is gotten by the linear mapping
      \begin{equation}
        e_{k_1 ... k_n} \mapsto x_1^{k_1} x_2^{k_2} ... x_n^{k_n}
      \end{equation}
    \end{definition}

    However, note that different polynomials may define the same functions if the field $\mathbb{F}$ is finite, similarly to polynomials with one variable. If $\mathbb{F}$ is infinite, then every polynomial will determine a different function. 

    \begin{definition}
      A polynomial is called \textbf{homogeneous} if degree $d$ if 
      \begin{equation}
        a_{k_1 k_2 ... k_n} = 0 \text{ for } k_1 + k_2 + ... + k_n \neq d
      \end{equation}
      The space of all homogeneous polynomials of fixed degree $d$ forms a finite dimensional subspace in $\mathbb{F}[x_1, ..., x_n]$ with dimension 
      \begin{equation}
        \frac{n(n+1)...(n+d-1)}{d!}
      \end{equation}
      The dimension can be calculated by thinking of the combinatorics problem of having $d$ indistinguishable balls to put into $n$ distinguishable urns. 
    \end{definition}

  \subsubsection{Symmetric Polynomials}

    \begin{definition}
      A polynomial $f \in \mathbb{F}[x_1, ..., x_n]$ is called \textbf{symmetric} if it is invariant under any permutation of the variables $x_i$. 
    \end{definition}

    \begin{example}
      Power sums are symmetric polynomials. 
      \begin{equation}
        p(x_1, x_2, ..., x_n) = \sum_{i=1}^n x_i^k
      \end{equation}
    \end{example}

    \begin{definition}
      An \textbf{elementary symmetric polynomial} is a symmetric polynomial of one of these forms: 
      \begin{align*}
        \sigma_1 & = x_1 + x_2 + ... + x_n \\
        \sigma_2 & = x_1 x_2 + x_1 x_3 + ... + x_{n-1} x_n \\
        ... & = ... \\
        \sigma_k & = \sum_{i_1 < ... < i_k} x_{i_1} x_{i_2} ... x_{i_k} \\
        ... & = ... \\
        \sigma_n & = x_1 x_2 ... x_n
      \end{align*}
    \end{definition}

    The following theorem presents an extremely useful result about the decomposition of symmetric polynomials. 

    \begin{theorem}
      Every symmetric polynomial can be written as a polynomial of elementary symmetric polynomials $\sigma_i$. 
    \end{theorem}

    \begin{example}
      The polynomial 
      \begin{equation}
        f \equiv \sum_{i=1}^n x_i^3
      \end{equation}
      can be expressed as 
      \begin{equation}
        f = \sigma_1^3 - 3 \sigma_1 \sigma 2 + 3 \sigma_3
      \end{equation}
    \end{example}

  \subsubsection{Cubic Equations}

    The well known discriminant of a quadratic equation 
    \begin{equation}
      f(x) = ax^2 + bx + c
    \end{equation}
    is known in the form $\nabla = b^2 - 4ac$. However, we will present it in a slightly different manner. 

    \begin{definition}
      The \textbf{discriminant} $D(\varphi)$ of a quadratic polynomial
      \begin{equation}
        \varphi = a_0 x^2 + a_1 x + a_2 \in \mathbb{C}[x]
      \end{equation}
      with $c_1, c_2 \in \mathbb{C}$ as its roots is defined
      \begin{equation}
        D(\varphi) = a_1^2 - 4 a_0 a_2 = a_0^2 \bigg( \Big(\frac{a_1}{a_0} \Big)^2 - \frac{4 a_2}{a_0} \bigg) = a_0^2 \big( (c_1 + c_2)^2 - 4 c_1 c_2 \big) = a_0^2 (c_1 - c_2)^2
      \end{equation}
      Clearly, the value of $D(\varphi)$ can tell us three things
      \begin{enumerate}
        \item $c_1, c_2 \in \mathbb{R}, c_1 \neq c_2$. Then $c_1 - c_2$ is a nonzero real number and $D(\varphi) > 0$. 
        \item $c_1 = c_2 \in \mathbb{R}$. Then $c_1 - c_2 = 0$ and $D(\varphi) = 0$. 
        \item $c_1, c_2 \in \mathbb{C}, c_1 = \bar{c}_2$. Then, $c_1 - c_2$ is a nonzero strictly imaginary number and $D(\varphi) < 0$. 
      \end{enumerate}
    \end{definition}

    \begin{definition}
      We can generalize this notion of the discriminant to arbitrary polynomials
      \begin{equation}
        \varphi = a_0 x^n + a_1 x^{n-1} + ... + a_{n-1} x + a_n \in \mathbb{F}[x], \; a_0 \neq 0
      \end{equation}
      The discriminant $D(\varphi)$ of the polynomial above is defined
      \begin{equation}
        D(\varphi) \equiv a_0^{2n-2} \prod_{i>j} (c_i - c_j)^2
      \end{equation}
      The $a_0$ term isn't very important in this formula, since it does not affect whether $D(\varphi)$ is positive, negative, or zero. 
    \end{definition}

    \begin{definition}
      A polynomial 
      \begin{equation}
        \varphi = a_0 x^n + a_1 x^{n-1} + ... + a_{n-1} x + a_n \in \mathbb{F}[x], \; a_0 \neq 0
      \end{equation}
      where $a_1 = 0$ is called \textbf{depressed}. A depressed cubic polynomial is of form
      \begin{equation}
        \varphi = x^3 + p x + q
      \end{equation}
    \end{definition}

    \begin{proposition}
      Every monic (leading coefficeint $=1$) polynomial (and non-monic ones) 
      \begin{equation}
        \varphi = x^n + a_1 x^{n-1} + ... + a_{n-1} x + a_n \in \mathbb{F}[x], \; a_0 \neq 0
      \end{equation}
      can be turned into a depressed polynomial with the change of variable
      \begin{equation}
        x = y - \frac{a_1}{n}
      \end{equation}
      to get the polynomial 
      \begin{equation}
        \psi = y^n + b_2 y^{n-2} + ... + b_{n-1} y + b_n
      \end{equation}
    \end{proposition}

    \begin{lemma}
      A cubic polynomial 
      \begin{equation}
        \varphi = a_0 x^3 + a_1 x^2 + a_2 x + a_3 \in \mathbb{R}[x]
      \end{equation}
      with roots $c_1, c_2, c_3 \in \mathbb{C}$ has discriminant
      \begin{equation}
        D(\varphi) \equiv a_0^4 (c_1 - c_2)^2 (c_1 - c_3)^2 (c_2 - c_3)^2
      \end{equation}
      With a bit of evaluation, it can also be expressed in terms of its coefficients as
      \begin{equation}
        D(\varphi) = a_1^2 a_2^2 - 4a_1^3 a_3 - 4a_0 a_2^3 + 18 a_0 a_1 a_2 a_3 - 27 a_0^2 a_3^2
      \end{equation}
      Again, three possibilities can occur (up to reordering of its roots). 
      \begin{enumerate}
          \item $c_1, c_2, c_3$ are distinct real numbers. Then $D(\varphi) > 0$. 
          \item $c_1, c_2, c_3 \in \mathbb{R}, c_1 = c_2$. Then $D(\varphi) = 0$. 
          \item $c_1 \in \mathbb{R}, c_2 = \bar{c}_3 \not\in \mathbb{R}$. Then $D(\varphi) < 0$. 
      \end{enumerate}
      Furthermore, the cubic formula used to find the roots of the polynomial is 
      \begin{equation}
        c_{1, 2, 3} = \sqrt[3]{-\frac{q}{2} + \sqrt{\frac{p^3}{27} + \frac{q^2}{4}}} + \sqrt[3]{-\frac{q}{2} - \sqrt{\frac{p^3}{27} + \frac{q^2}{4}}}
      \end{equation}
      known as \textbf{Cardano's formula}, after the mathematician Gerolamo Cardano. 
    \end{lemma}

\subsection{Ideals and Quotient Rings}

  \begin{definition}
    For an arbitrary ring $(R,+, \cdot)$, let $(R, +)$ be its additive group. A subset $I$ is called a \textbf{left ideal} of $R$ if it satisfies the two conditions. 
    \begin{enumerate}
      \item $(I, +)$ is a subgroup of $(R, +)$. 
      \item For every $r \in R$ and every $x \in I$ the left product $r \cdot x \in I$. 
    \end{enumerate}
    Similarly, a \textbf{right ideal} $I$ of $R$ satisfies
    \begin{enumerate}
      \item $(I, +)$ is a subgroup of $(R, +)$. 
      \item For every $r \in R$ and every $x \in I$, the right product $r \cdot x \in I$. 
    \end{enumerate}
    Note that left and right modules are equivalence relations defined on a ring. 
  \end{definition}

  A left/right ideal can also be seen as a left/right $R$-submodule of $R$ viewed as an $R$-module. 

  \begin{definition}
    A \textbf{two-sided ideal}, or more simply an \textbf{ideal}, is a left ideal that is also a right idea. 
  \end{definition}

  \begin{proposition}
    Every right or left ideal of a commutative ring is a two sided ideal. 
  \end{proposition}
  \begin{proof}
    Trivial. 
  \end{proof}

  \begin{example}
    The set of even integers $2 \mathbb{Z}$ is an ideal in the ring $\mathbb{Z}$, since the sum of any even integers is even and the product of any even integer with an integer is an even integer. However, the odd integers do not form an ideal. 
  \end{example}

  \begin{example}
    The set of all polynomials with real coefficients which are divisible by the polynomial $x^2 + 1$ is an ideal in the ring of all polynomials. 
  \end{example}

  \begin{example}
    The set of all $n \times n$ matrices whose last row is zero forms a right ideal in the ring of all $n \times n$ matrices. However, it is not a left ideal.

    The set of all $n\times n$ matrices whose last column is zero is a left ideal, but not a right ideal. 
  \end{example}

  \begin{proposition}
    The only ideals that exist in a field $\mathbb{F}$ is $\{0\}$ and $\mathbb{F}$ itself. 
  \end{proposition}
  \begin{proof}
    Given a nonzero element $x \in \mathbb{F}$, every element of $\mathbb{F}$ can be expressed in the form of $a x$ or $x a$ for some $a \in \mathbb{F}$. 
  \end{proof}

  \begin{definition}
    A left ideal generated by a single element $x$ is called the \textbf{principal left ideal generated by $x$} and is denoted $R x$. Principal right ideals are denoted $x R$, and principal (two-sided) ideals are denoted $R x R$. 
  \end{definition}

  \begin{definition}
    A \textbf{principal ideal domain}, also called a \textbf{PID}, is an integral domain in which every ideal is principal (i.e. can be generated by a single element). 

    More generally, a \textbf{principal ideal ring} is a nonzero commutative ring in which every ideal is principal (i.e. can be generated by a single element). 
  \end{definition}

  The distinction is that a principal ideal ring may have zero divisors whereas a principal ideal domain cannot. Principal ideal domains are thus mathematical objects that behave somewhat like the integers. That is, 
  \begin{enumerate}
    \item Any element of a PID has a unique decomposition into prime elements. 
    \item Any two elements of a PID have a greatest common divisor. 
    \item If $x$ and $y$ are elements of a PID without common divisors, then every element of the PID can be written in the form 
      \begin{equation}
        a x + b y
      \end{equation}
  \end{enumerate}

  \begin{proposition}
    Every Euclidean domain is also a principal ideal domain. 
  \end{proposition}

  \begin{example}
    The following are all examples of principal ideal domains. 
    \begin{enumerate}
      \item Any field $\mathbb{F}$. 
      \item The ring of integers $\mathbb{Z}$. 
      \item $\mathbb{F}[x]$, rings of polynomials in one variable with coefficients in a field $\mathbb{F}$. 
      \item Rings of formal power series $\mathbb{F}[[x]]$. 
      \item The ring of Gaussian integers $\mathbb{Z}[i]$. 
    \end{enumerate}
  \end{example}

  It is quite easy to see that a field $\mathbb{F}$ is a PID since the only two possible ideals are $\{0\}$ and $\mathbb{F}$, both of which are principal. For the integers $\mathbb{Z}$, every ideal is of the form $n\mathbb{Z}$, which is principal since it is generated by the integer $n$. The ring of polynomials $\mathbb{F}[x]$ is a PID since we can imagine a minimal polynomial $p$ in each ideal $I$. Every element in $I$ must be divisible by $p$, which means that the entire ideal $I$ can be generated by the minimal polynomial $p$, making $I$ principal. 

\subsection{The Algebra of Quaternions}

  \begin{definition}
    The \textbf{quaternions} form an algebra of $4$-dimensional vectors over $\mathbb{R}$, with elements of the form
    \begin{equation}
      (a, b, c, d) \equiv a + bi + cj + dk
    \end{equation}
    where $a$ is called the \textbf{scalar portion} and $bi + cj + dk$ is called the \textbf{vector/imaginary portion}. The algebra of quaternions is denoted $\mathbb{H}$, which stands for "Hamilton." $\mathbb{H}$ is a $4$-dimensional associative normed division algebra over $\mathbb{R}$. 
  \end{definition}

  From looking at the multiplication table, we can see that multiplication in $\mathbb{H}$ is not commutative. 
  \begin{center}
    \begin{tabular}{|c|c|c|c|c|}
    \hline
    i & i & -1 & k & -j \\ 
    \hline
    j & j & -k & -1 & i \\ 
    \hline
    k & k & j & -i & -1 \\ 
    \hline
    \end{tabular}
  \end{center}
  Note the identity 
  \begin{equation}
    i^2 = j^2 = k^2 = -1
  \end{equation}
  The algebra of quaternions are in fact the first noncommutative algebra to be discovered! 

  \begin{proposition}
    $\mathbb{H}$ and $\mathbb{C}$ are the only finite-dimensional divisions rings containing $\mathbb{R}$ as a proper subring. 
  \end{proposition}

  \begin{definition}
    The \textbf{quaternion group}, denoted $Q_8$ is a nonabelian group of order $8$, isomorphic to a certain $8$-element subset in $\mathbb{H}$ under multiplication. It's group presentation is 
    \begin{equation}
      Q_8 = \big\langle \bar{e}, i, j, k \;|\; \bar{e}^2 = e, i^2 = j^2 = k^2 = ijk = \bar{e} \big\rangle
    \end{equation}
  \end{definition}

  Going back to the algebra, we can set $\{1, i, j, k\}$ as a basis and define addition and scalar multiplication component-wise, and multiplication (called the \textbf{Hamilton product}) with properties
  \begin{enumerate}
    \item The real quaternion $1$ is the identity element. 
    \item All real quaternions commute with quaternions: $a q = q a$ for all $a \in \mathbb{R}, q \in \mathbb{H}$. 
    \item Every quaternion has an inverse with respect to the Hamilton product. 
      \begin{equation}
        (a + bi + cj + dk)^{-1} = \frac{1}{a^2 + b^2 + c^2 + d^2} \big( a - bi - cj - dk\big)
      \end{equation}
  \end{enumerate}
  Note that property 3 allows $\mathbb{H}$ to be a division algebra. 

  \begin{proposition}[Scalar and Vector Components]
    Let the quaternion be divided up into a scalar and vector part with the bjective mapping $a + bi + cj + dk \mapsto \big(a, (b, c, d)\big)$. 
    \begin{equation}
      q = (r, v), r \in \mathbb{R}, v \in \mathbb{R}^3
    \end{equation}
    Then, the formulas for addition and multiplication are
    \begin{align*}
      q_1 + q_2 & = (r_1, v_1) + (r_2, v_2) = (r_1 + r_2, v_1 + v_2) \\
      q_1 \cdot q_2 & = (r_1, v_1) \cdot (r_2, v_2) = (r_1 r_2 - v_1 \cdot v_2, r_1 v_2 + r_2 v_1 + v_1 \times v_2)
    \end{align*}
    where the $\cdot$ and $\times$ on the right hand side represnts the dot product and cross product, respectively. 
  \end{proposition}

  \begin{definition}
    The conjugate of a quaternion $q = a + bi + cj + dk$ is defined 
    \begin{equation}
      \bar{q}, q^* \equiv a - bi - cj - dk
    \end{equation}
    It has properties
    \begin{enumerate}
      \item $q^{**} = q$
      \item $(q p)^* = p^* q^*$
    \end{enumerate}
    $q^*$ can also be expressed in terms of addition and multiplication. 
    \begin{equation}
      q^* = -\frac{1}{2} \big( q + iqi + jqj + kqk \big)
    \end{equation}
  \end{definition}

  \begin{definition}
    The \textbf{norm} of $q$ is defined
    \begin{equation}
      ||q|| \equiv \sqrt{q^* q} = \sqrt{q q^*} = \sqrt{a^2 + b^2 + c^2 + d^2}
    \end{equation}
    with properties
    \begin{enumerate}
      \item Scaling factor. $||\alpha q|| = |\alpha| ||q||$
      \item Multiplicative. $||p q|| = ||p|| ||q||$
    \end{enumerate}
  \end{definition}

  The norm allows us to define a metric 
  \begin{equation}
    d(p, q) \equiv ||p - q||
  \end{equation}
  This makes $\mathbb{H}$ a metric space, with addition and multiplication continuous on the metric topology. 

  \begin{definition}
    The \textbf{unit quaternion} is defined to be
    \begin{equation}
      U_q = \frac{q}{||q||}
    \end{equation}
  \end{definition}

  \begin{corollary}
    Every quaternion has a polar decomposition
    \begin{equation}
      q = U_q \cdot ||q||
    \end{equation}
    With this, we can redefine the inverse as
    \begin{equation}
      q^{-1} = \frac{q^*}{||q||^2}
    \end{equation}
  \end{corollary}

  \subsubsection{Matrix Representations of Quaternions}

    We can represent $q$ with $2 \times 2$ matrices over $\mathbb{C}$ or $4\times 4 $ matrices over $\mathbb{R}$. 

    \begin{proposition}
      The following representation is an injective homomorphism $\rho: \mathbb{H} \longrightarrow \GL(2, \mathbb{C})$. 
      \begin{equation}
        \rho: a + bi + cj + dk \mapsto \begin{pmatrix}
        a+bi & c+ di \\ -c + di & a - bi
        \end{pmatrix}
      \end{equation}
      It has properties
      \begin{enumerate}
        \item Constraining any two of $b, c, d$ to $0$ produces a representation of the complex numbers. When $c = d = 0$, this is called the \textbf{diagonal representation}. 
        \begin{align*}
          \begin{pmatrix}
          a+bi & 0 \\ 0 & a-bi
          \end{pmatrix},  \begin{pmatrix}
          a & c \\ -c & a
          \end{pmatrix},  \begin{pmatrix}
          a & di \\ di & a
          \end{pmatrix}
        \end{align*}
        \item The norm of a quaternion is the square root of the determinant of its corresponding matrix representation. 
          \begin{equation}
            ||q|| = \sqrt{\det \begin{pmatrix}
            a+bi & c+di \\ -c+di & a-bi
            \end{pmatrix}} = \sqrt{(a^2 + b^2) + (c^2 + d^2)}
          \end{equation}
        \item The conjugate of a quaternion corresponds to the conjugate (Hermitian) transpose of its matrix representation. 
          \begin{equation}
            \rho(q^*) = \rho(q)^H \iff a-bi-cj-dk \mapsto \begin{pmatrix}
            a-bi & -c-di \\ c-di & a+bi
            \end{pmatrix}
          \end{equation}
        \item The restriction of this representation to only unit quaternions leads to an isomorphism between the subgroup of unit quaternions and their corresponding image in SU$(2)$. Topologically, the unit quaternions is the $3$-sphere, so the underlying space SU$(2)$ is also a $3$-sphere. More specifically, 
          \begin{equation}
            \frac{\text{SU}(2)}{2} \simeq \text{SO}(3)
          \end{equation}
      \end{enumerate}
    \end{proposition}

    \begin{proposition}
    The following representation of $\mathbb{H}$ is an injective homomorphism $\rho: \mathbb{H} \longrightarrow \GL(4, \mathbb{R})$. 
    \begin{equation}
      \rho: a+bi+cj+dk \mapsto \begin{pmatrix}
      a&-b&-c&-d \\
      b&a&-d&c\\
      c&d&a&-b\\
      d&-c&b&a
      \end{pmatrix}
    \end{equation}
    or also as
    \begin{equation}
      a \begin{pmatrix}
      1 &0 &0 &0 \\
      0& 1&0&0\\
      0&0&1&0\\
      0&0&0&1
      \end{pmatrix} + b \begin{pmatrix}
      0&-1&0&0\\1&0&0&0\\0&0&0&-1\\0&0&1&0
      \end{pmatrix} + c\begin{pmatrix}
      0&0&-1&0\\0&0&0&1\\1&0&0&0\\0&-1&0&0
      \end{pmatrix} + d \begin{pmatrix}
      0&0&0&-1\\0&0&-1&0\\0&1&0&0\\1&0&0&0
      \end{pmatrix}
    \end{equation}
    It has properties
    \begin{enumerate}
      \item $\rho(q^*) = \rho(q)^T$
      \item The fourth power of the norm is the determinant of the matrix 
        \begin{equation}
          ||q||^4 = \det\big( \rho (q)\big)
        \end{equation}
      \item Similarly, with the $2\times 2$ representation, complex number representations can be produced by restricting $2$ of $b, c, d$ to $0$. 
    \end{enumerate}
    \end{proposition}

    Note that this representation in $\GL(4, \mathbb{R})$ is not unique. There are in fact 48 distinct representation of this form where one of the component matrices represents the scalar part and the other 3 are skew symmetric. 

  \subsubsection{Square Roots of -1}

    In $\mathbb{C}$, there are two numbers, $i$ and $-i$, whose square is $-1$. However, in $\mathbb{H}$, infinitely many square roots of $-1$ exist, forming the unit sphere in $\mathbb{R}^3$. To see this, let $q = a+bi+cj+dk$ be a quaternion, and assume that its square is $-1$. Then this implies that
    \begin{equation}
      a^2 - b^2 -c^2 -d^2 = -1, 2ab = 2ac = 2ad = 0
    \end{equation}
    To satisfy the second equation, either $a=0$ or $b=c=d=0$. The latter is impossible since then $q$ would be real. Therefore, 
    \begin{equation}
      b^2 + c^2 + d^2 = 1
    \end{equation}
    which forms the unit sphere in $\mathbb{R}^3$. 
\section{Rings}

  \subsection{Ring-like Structures}

    \begin{definition}[Ring]
      A \textbf{ring} is a set $(R, +, \times)$ equipped with two operations, called addition and multiplication. It has properties: 
      \begin{enumerate}
        \item $R$ is an abelian group with respect to $+$, where we denote the additive identity as $0$ and the additive inverse of $x$ as $-x$. 
        \item $R$ is closed under $\times$.
        \item $\times$ is distributive with respect to addition $+$
          \begin{equation}
            a \times (b + c) = a\times b + a\times c, (a+b)\times c = a\times c + b\times c \; \forall a, b, c \in R
          \end{equation}
        \item There is a multiplicative identity, also called the \textbf{unity}, denoted $1$ such that 
          \begin{equation}
            1 \times a = a \times 1 = a \; \forall a \in R
          \end{equation}
      \end{enumerate}
    \end{definition}

    \begin{lemma} 
      Additive inverses are unique and $-1 \times a$ is the additive inverse of $a$. 
    \end{lemma}
    \begin{proof}
      We can see that 
      \begin{align}
        -1 + 1 = 0 & \implies (-1 + 1) \times a = 0 \times a \\
                   & \implies -1 \times a + 1 \times a = 0 \\
                   & \implies -1 \times a + a = 0 
      \end{align}
      and therefore by definition $-1 \times a$ must be the additive inverse. 
    \end{proof}

    Note that unlike groups, commutativity and associativity are not assumed w.r.t. $\times$. In fact, in some cases the existence of the multiplicative identity is not even assumed, though we will do it here. They must be additionally endowed. 

    \begin{definition}[Commutative, Associative Ring]
      A ring $R$ is 
      \begin{enumerate}
        \item a \textbf{commutative ring} if and only if multiplication is commutative. 
        \item an \textbf{associative ring} if and only if multiplication is associative, i.e. $a \times (b \times c) = (a \times b) \times c$. 
      \end{enumerate}
    \end{definition}

    This is not to be confused with the unit of a ring. 

    \begin{definition}[Unit]
      A \textbf{unit} of a ring $R$ is an element $u \in R$ that has a multiplicative inverse in $R$. 
    \end{definition}

    \begin{example}[Numerical Rings] 
      \begin{enumerate}
        \item $\mathbb{Z}, \mathbb{Q}, \mathbb{R}$ are commutative, associative rings with respect to ordinary addition and multiplication.
        \item The set of even integers $2\mathbb{Z}$ is a commutative, associative ring without unity.
      \end{enumerate}
    \end{example}

    \begin{proposition}[Power Set]
      Given a set $X$, let $2^X$ be its power set, that is the set of all subsets of $X$. Then, $2^X$ is a commutative associative ring with respect to the operations of symmetric difference (i.e. the set of elements which is in exactly one of the sets) 
      \begin{equation}
        M \bigtriangleup N \equiv (M \setminus N) \cup (N \setminus M)
      \end{equation}
      and intersection $\cap$, taken for addition an multiplication, respectively. 
    \end{proposition}
    \begin{proof}
      We will not prove all of the axioms of the ring, but we can state some important facts about this structure. The additive identity is $\emptyset$ and the multiplicative identity is $X$. Finally, it is clear that 
      \begin{align*}
        & M \bigtriangleup N \equiv (M \setminus N) \cup (N \setminus M) \equiv N \bigtriangleup M \\
        & M \cap N = N \cap M \\
        & M \cap N \cap P = (M \cap N) \cap P = M \cap (N \cap P)
      \end{align*}
    \end{proof}

    \begin{example}
      A \textbf{division ring}, also called a \textbf{skew field}, is an associative ring with unity where every nonzero element is invertible with respect to $\times$. Division rings differ from fields in that multiplication is not required to be commutative. 
    \end{example}

    At first, a division ring may not seem different from a field. However, a classic example is the ring of invertible matrices, which is not necessarily commutative, but is a ring in which "division" can be done by right and left multiplication of a matrix inverse. 
    \begin{equation}
      a a^{-1} = a^{-1} a = I
    \end{equation}
    This implies that every element in the division ring commutes with the identity, but again commutativity does not necessarily hold for arbitrary elements $a, b$. 

    \begin{definition}
      A \textbf{field} $(F, +, \times)$ is a commutative, associative ring with unity where every nonzero element is invertible (with respect to $\times$). It is usually denoted as $\mathbb{F}$. Note that $F$ is now an abelian group with respect to $\times$. 
    \end{definition}

    \begin{definition}
      An element a of a ring $R$ is called a \textbf{left zero divisor} if there exists a nonzero $x$ such that $a x = 0$ and a \textbf{right zero divisor} if there exists a nonzero $x$ such that $x a = 0$. 
    \end{definition}

    \begin{definition}
      A ring $R$ with no zero divisors for every element is called a \textbf{domain}. 
    \end{definition}

    \begin{proposition}
      Every field is a domain. 
    \end{proposition}
    \begin{proof}
      Given $x, y \in \mathbb{F}$, assume $x y = 0$ with $x \neq 0$. Since $x$ is invertible,
      \begin{equation}
        0 = x^{-1} 0 = x^{-1} (x y) = y
      \end{equation}
      Now assuming that $y \neq 0$, since $y$ is invertible, 
      \begin{equation}
        0 = 0 y^{-1} = (x y) y^{-1} = x
      \end{equation}
    \end{proof}

    While the converse is not true, we can state the following result. 

    \begin{theorem}[Wedderburn's little theorem]
      Every finite domain is a field. 
    \end{theorem}

  \subsection{Field of Complex Numbers}

    The impossibility of defining division on the ring of integers motivates its extension into the field of rational numbers. Similarly, the inability to take square roots of negative real numbers forces us to extend the field of real numbers to the bigger field of complex numbers. 

    \begin{definition}
      The \textbf{field of complex numbers} is a field $\mathbb{C}$ such that 
      \begin{enumerate}
        \item It contains the field $\mathbb{R}$ as a subfield. 
        \item It contains an element $i$ such that $i^2 = -1$.
        \item It is minimal with respect to properties (i) and (ii). That is, if $F$ is a subfield of $\mathbb{C}$ containing $\mathbb{R}$ and $i$, then $F = \mathbb{C}$. 
      \end{enumerate}
    \end{definition}

    Note that the identity $x^2 + 1 \equiv (x + i) (x - i)$ implies that the equation $x^2 = -1$ has exactly two solutions in $\mathbb{C}$, $i$ and $-i$. Therefore, if a subfield of $\mathbb{C}$ contains one of these solutions, it must contain the other (since $i$ and $-i$ are additive and multiplicative inverses). 

    Furthermore, since $i$ is defined to be $\sqrt{-1}$, we could replace $i$ with $-i$ and our calculations would still be consistent throughout the rest of mathematics. In fact, $i$ and $-i$ behave \textbf{exactly} identically and cannot be distinguished in an abstract sense. Visually, the complex plane "flipped" across the real number axis produces the same complex plane. 

    \begin{theorem}
      $\mathbb{C}$ exists and is unique up to an isomorphism that maps all real numbers to themselves. Every complex number can be uniquely written as $a + bi$, where $a, b \in \mathbb{R}$ and $i$ is a fixed element such that $i^2 = -1$. 
    \end{theorem}
    \begin{proof}
      We first assume that $\mathbb{C}$ exists. Consider the subset of $\mathbb{C}$
      \begin{equation}
        K \equiv \{ a + bi \; | \; a, b \in \mathbb{R}\}
      \end{equation}
      By evaluating its operations, we can check for closure, identity, and invertibility of nonzero elements to conclude that $K$ is a subfield of $\mathbb{C} \implies$ by prop. (iii), $K = \mathbb{C} \implies$ every element in $\mathbb{C}$ can be written in form $a + bi$. To prove uniqueness, we assume that $p \in \mathbb{C}$ can be written in distinct forms $p = a + bi = a^{\prime} + b^\prime i$. Then
      \begin{align*}
         a + bi = a^{\prime} + b^\prime i & \implies (a - a^\prime)^2 = (b^\prime i - b i)^2 = - (b^\prime - b)^2 \\
         & \implies a - a^\prime = b^\prime - b = 0
      \end{align*}
      To prove uniqueness of $\mathbb{C}$ up to ismorphism, we assume that $\mathbb{C}^\prime$ exists with $i^\prime$ such that $i^{\prime 2}$ containing elements $a + b i'$. Let $f: \mathbb{C} \longrightarrow \mathbb{C}^\prime$ defined 
      \begin{equation}
        f( a + bi) = a + bi^\prime
      \end{equation}
      Then, 
      \begin{align*}
        f\big((a + b i) + (c + d i) \big) & = f\big( (a + c) + (b + d)i \big) \\
        & = (a + c) + (b + d) i^\prime \\
        & = (a + b i^\prime) + (c + d i^\prime) \\
        & = f(a + b i) + f( c + d i) \\
        f\big( \kappa (a + b i)\big) & = f\big( \kappa a + \kappa b i\big) \\
        & = \kappa a + \kappa b i^\prime \\
        & = \kappa (a + b i^\prime) \\
        & = \kappa f(a + b i)
      \end{align*}
      So, $f$ is an isomorphism, and $\mathbb{C} \simeq \mathbb{C}^\prime$. From analysis, we can construct and prove the existence of $\mathbb{R}$. We then define the map
      \begin{equation}
        \rho: \mathbb{R}^2 \longrightarrow \mathbb{C}, \; \rho(a, b) \equiv a + bi
      \end{equation}
      with $\rho(1, 0)$ as the multiplicative identity and $\rho(0,1) \equiv i$. Therefore, every element of $\mathbb{C}$ can be uniquely represented as an element of $\mathbb{R}^2$. 
    \end{proof}

    \begin{definition}
      \textbf{Complex conjugation} is an automorphism of $\mathbb{C}$ defined
      \begin{equation}
        c = a + b i \mapsto \bar{c} = a - b i
      \end{equation}
      This is identically defined by replacing $i$ with $-i$. Clearly, $\bar{\bar{c}} = c$. 
    \end{definition}

    \begin{definition}
      Real numbers are elements in $\mathbb{C}$ that are equal to their own conjugates. 
    \end{definition}

    \begin{proposition}
      For any $c \in \mathbb{C}$, $c + \bar{c}$ and $c \bar{c}$ are real. 
    \end{proposition}
    \begin{proof}
      Using the fact that the complex conjugate is an isomorphism, 
      \begin{align*}
        & \bar{c + \bar{c}} = \bar{c} + \bar{\bar{c}} = \bar{c} + c = c + \bar{c} \\
        & \bar{ c \bar{c}} = \bar{c} \bar{\bar{c}} = \bar{c} c = c \bar{c}
      \end{align*}
    \end{proof}
    Note that we proved this abstractly using only the properties given above, and did not decompose $c$ to its \textbf{algebraic form} $a + b i$. 

    If $c = a + b i, \; a, b \in \mathbb{R}$, then 
    \begin{equation}
      c + \bar{c} = 2a, \; c \bar{c} = a^2 + b^2
    \end{equation}

    In case the reader is unaware, it is common to interpret complex numbers $c = a + b i$ as points or vectors $(a, b)$ on the complex plane. 

    \subsubsection{Polar Representations of Complex Numbers}

      \begin{definition}
        The \textbf{absolute value} of a complex number $c = a + b i$, denoted $|c|$, is the length of the vector representing $c$. 
        \begin{equation}
          |c| \equiv \sqrt{a^2 + b^2}
        \end{equation}
      \end{definition}

      \begin{definition}
        The \textbf{argument} of a complex number $c = a + b i$, denoted arg$c$, is the angle formed by the corresponding vector with the polar axis. It is defined within the interval $[0, 2\pi)$. 
        \begin{equation}
          \text{arg}(c) \equiv \tan^{-1}{\frac{b}{a}}
        \end{equation}
      \end{definition}

      \begin{definition}
        The \textbf{polar representation}, or \textbf{trigonometric representation}, of a complex number $c = a + b i$ is defined using the equations 
        \begin{equation}
          a = r \cos{\varphi}, \; b = r\sin{\varphi} \implies c = r (\cos{\varphi} + i \sin{\varphi})
        \end{equation}
        This mapping can be defined 
        \begin{equation}
          \rho: \mathbb{R} \times \frac{\mathbb{R}}{2 \pi} \longrightarrow \mathbb{C}, \; \rho(r, \varphi) = r (\cos{\varphi} + i \sin{\varphi})
        \end{equation}
      \end{definition}

      \begin{theorem}
        $\rho$ is "similar" to a homomorphism in the following way. By defining the domain and codomain as groups, 
        \begin{equation}
          \rho: \big( \mathbb{R}, \times \big) \times \Big( \frac{\mathbb{R}}{2 \pi} \Big) \longrightarrow \big( \mathbb{C}, \times \big)
        \end{equation}
        we can see that
        \begin{equation}
          \rho (r_1, \varphi_1) \times \rho(r_2, \varphi_2) = \rho(r_1 \times r_2, \varphi_1 + \varphi_2) 
        \end{equation}
        or equivalently, 
        \begin{equation}
          r_1 (\cos{\varphi_1} + i \sin{\varphi_1}) \cdot r_2 (\cos{\varphi_2} + i \sin{\varphi_2}) = r_1 r_2 (\cos{(\varphi_1 + \varphi_2)} + i \sin{(\varphi_1 + \varphi_2)})
        \end{equation}
      \end{theorem}

      \begin{corollary}
        The formula for the ratio of complex numbers is defined
        \begin{equation}
          \frac{r_1 (\cos{\varphi_1} + i \sin{\varphi_1})}{r_2 (\cos{\varphi_2} + i \sin{\varphi_2})} = \frac{r_1}{r_2} (\cos{(\varphi_1 - \varphi_2)} + i \sin{(\varphi_1 - \varphi_2)})
        \end{equation}
      \end{corollary}

      \begin{corollary}
        The positive integer power of a complex number can be written using \textbf{De Moivre's formula}. 
        \begin{equation}
          \big(r(\cos{\varphi} + i \sin{\varphi})\big)^n = r^n (\cos{n \varphi} + i \sin{n \varphi})
        \end{equation}
      \end{corollary}

      We can use this formula to extract a root of $n$th degree from a complex number $c = r(\cos{\varphi} + i \sin{\varphi})$, which means to solve the equation $z^n = c$. Let $z = s (\cos{\psi} + i \sin{\psi})$. Then by De Moivre's formula, 
      \begin{align*}
        z^n & = s^n (\cos{n \psi} + i \sin{n \psi}) = r(\cos{\varphi} + i \sin{\varphi}) \\
        & \implies s = \sqrt[n]{r}, \; \psi = \frac{\varphi + 2\pi k}{n} \\
        & \implies z = \sqrt[n]{r} \bigg( \cos{\frac{\varphi + 2\pi k}{n}} + i \sin{\frac{\varphi + 2\pi k}{n}}\bigg) \text{ for } k = 0, 1, ..., n-1
      \end{align*}
      Geometrically, the $n$ solutions lie at the vertices of a regular $n$-gon centered at the origin. When $c = 1$, the solutions are the $n$th roots of unity.

  \subsection{Rings of Residue Class}

    \begin{definition}
      The quotient set $\mathbb{Z}$ by the relation of congruence modulo $n$ is denoted $\mathbb{Z}_{n}$. It is called the \textbf{ring of residue class modulo n} or \textbf{residue ring modulo n}. 
      \begin{equation}
        \mathbb{Z}_{n} = \{ [0]_{n}, [1]_{n}, ... , [n-1]_{n} \}
      \end{equation}
    \end{definition}

    By definition of the relation, congruence modulo $n$ has properties: 
    \begin{enumerate}
      \item $a \equiv a' \pmod{n}, b \equiv b' \pmod{n} \implies a + b \equiv a' + b' \pmod{n}$ . 
      \item With same hypothesis as (i) $a b \equiv a' b \equiv a b' \equiv a' b' \pmod{n}$. 
    \end{enumerate}
    We can furthermore define operations of addition and multiplication on the ring $\mathbb{Z}_{n}$ as such 
    \begin{align*}
      & [a]_{n} + [b]_{n} \equiv [a + b]_{n} \\
      & [a]_{n} [b]_{n} \equiv [ab]_{n}
    \end{align*}
    making $\mathbb{Z}_{n}$ is a commutative, associative ring with unity. 

    Note that the properties of the operation in $\frac{M}{R}$ inherits all the properties of the addition operation on $M$ that are expressed in the form of identities and inverses, along with the existence of the zero identity. 
    \begin{align*}
      0 \in M & \implies [0] \text{ is the additive identity in } \frac{M}{R} \\
      a + (-a) = 0 & \implies [a] + [-a] = [0] \\
      1 \in M & \implies [1] \text{ is the multiplicative identity in } \frac{M}{R}
    \end{align*}

    \begin{example}
      In $\mathbb{Z}_{5}$, the elements $[2]$ and $[3]$ are multiplicative inverses of each other since $[2] [3] = [6] = [1]$, and $[4]$ is its own inverse since $[4] [4] = [16] = [1]$. The addition and multiplication tables for $\mathbb{Z}_5$ is shown below. 
    \end{example}

    The ring $\mathbb{Z}_n$ has all the properties of a field except the property of having inverses for all of its nonzero elements. This leads to the following theorem. 

    \begin{theorem}
      The ring $\mathbb{Z}_{n}$ is a field if and only if $n$ is a prime number. 
    \end{theorem}
    \begin{proof}
      $(\rightarrow)$ Assume that $n$ is composite $\implies n = k l$ for $k, n \in \mathbb{N} \implies k, n \neq 0$, but 
      \begin{equation}
        [k]_n [l]_n = [k l]_n = [n]_n = 0
      \end{equation}
      meaning that $\mathbb{Z}_n$ contains $0$ divisors and is not a field. The contrapositive of this states $(\rightarrow)$. \\
      $(\leftarrow)$ Given that $n$ is prime, let $[a]_n \neq 0$, i.e. $[a]_n \neq [0]_n, [1]_n$. The set of $n$ elements 
      \begin{equation}
        [0]_n, [a]_n, [2a]_n, ..., [(n-1)a]_n
      \end{equation}
      are all distinct. Indeed, if $[k a]_n = [l a]_n$, then $[(k-l) a]_n = 0 \implies n = (k-l) a \iff n$ is not prime. Since the elements are distinct, exactly one of them must be $[1]_n$, say $[p a]_n \implies$ the inverse $[p]_n$ exists. 
    \end{proof}

    \begin{corollary}
      For any $n$, $[k]_n$ is invertible in the ring $\mathbb{Z}_n$ if and only if $n$ and $k$ are relatively prime. 
    \end{corollary}

    \begin{definition}
      The \textbf{characteristic} of ring $R$ (or a field $F$), denoted char$(R)$, is the smallest number of times one must successively add the multiplicative identity $1$ to get the additive identity $0$. That is char$(R)$ is the smallest positive number $n$ such that 
      \begin{equation}
        1 + 1 + ... + 1 = 0 
      \end{equation}
      If no such number $n$ exists, then char$(R) = 0$. The characteristic of $\mathbb{Z}_n = n$
    \end{definition}

    Note that the characteristic of the field $\mathbb{Z}_n$ must be prime. 

    \begin{theorem}[Freshman's Dream]
      Given a field $F$ with char$(F) = p$, 
      \begin{equation}
        (a + b)^p = a^p + b^p
      \end{equation}
    \end{theorem}
    \begin{proof}
      We have 
      \begin{equation}
        (a + b)^p = \sum_{k = 0}^p \binom{p}{k} a^{p-k} b^{k}
      \end{equation}
      It is clear that 
      \begin{equation}
        \binom{p}{k} = \frac{p (p-1) ... (p - k+1)}{k!}
      \end{equation}
      is divisible by $p$ for all $k \neq 0, p$, so all the middle terms must cancel out to $0$. 
    \end{proof}

  \subsection{Polynomial Algebra}

    \subsubsection{Construction and Basic Properties}

      \begin{definition}
        A \textbf{polynomial $f$} of $x$ over a ring $R$ is defined as a formal expression 
        \begin{equation}
          f(x) = a_0 + a_1 x^1 + a_2 x^2 + ...  + a_{n-1} x^{n-1} + a_n x^n
        \end{equation}
        where $n$ is a natural number, the coefficients $a_0, a_1, ..., a_n$ are elements of $R$, $x$ is a formal symbol, whose powers $x^i$ are just placeholders for the corresponding coefficients $a_i$ so that the given formal expression is a way to encode the infinite finitary sequence. 
        \begin{equation}
          (a_0, a_1, a_2, ..., a_n, 0, 0, ...)
        \end{equation}
        Two polynomials are equal if and only if the sequences of their corresponding coefficients are equal.
      \end{definition}

      Note that this is really just a fancy way to write a finitary sequence. 

      \begin{definition}
        The set of polynomials with coefficients in the ring $R$ forms itself a ring, called the \textbf{ring of polynomials over $R$}, denoted $R[x]$. Addition on $R[x]$ is defined component-wise, and it suffices, by the distributive law, to define multiplication as
        \begin{equation}
          x^k x^l = x^{k + l}
        \end{equation}
        given that we have chosen $\{x^i\}$ as a basis of $R[x]$. If $R$ is a commutative associative ring (or a field), then $R[x]$ is called the \textbf{polynomial algebra}. From now, we will treat $R[x]$ and $F[x]$ as an algebra with $R$ denoting a commutative associative ring and $F$ denoting a field, respectively. 
      \end{definition}

      Note that the map from $R \longrightarrow R[x]$ sending $r \mapsto r x^0$ is an injective homomorphism of rings, by which $R$ is viewed as a subring of $R[x]$. 

      The ring of polynomials over field $\mathbb{R}$ is denoted $\mathbb{R}[x]$. $R[x]$ is a subalgebra within the algebra of all function of $\mathbb{R}$. 

      However, for certain finite fields, some formally different polynomials may be indistinguishable in terms of mappings. For example, $x$ and $x^2$ are equivalent in the polynomial algebra defined on the domain $\mathbb{Z}_2$.

      \begin{definition}
        The last nonzero coefficient is called the \textbf{leading coefficient}, and the degree of the polynomial $f$, denoted deg$f$, is the index of the leading coefficient. 
      \end{definition}

      \begin{theorem}
      \begin{align}
        \text{deg}(f+g) & \leq \text{max}\{\text{deg}\,f, \text{deg} \,g\} \\
        \text{deg} \,f g & = \text{deg} \,f + \text{deg} \,g
      \end{align}
      \end{theorem}
      \begin{proof}
        Simple when presenting polynomials if form $(1)$. 
      \end{proof}

      \begin{definition}
        The product of two finitary sequences $(a_0, a_1, a_2, ...)$ and $(b_0, b_1, b_2, ...)$ in the ring $F[x]$ is a sequence 
        \begin{equation}
          (c_0, c_1, c_2, ...), \; c_k = \sum_{l = 0}^{k} a_l b_{k-l}
        \end{equation}
        This formula works for infinite (non-finitary) sequences too, allowing us to define a commutative, associative algebra with unity called the \textbf{algebra of formal power series over $F$}, denoted $F[[x]]$. The elements of $F[[x]]$ are written in the form 
        \begin{equation}
          a_0 + a_1 x + a_2 x^2 + a_3 x^3...
        \end{equation}
      \end{definition}

      \begin{theorem}
        If the field $F$ is infinite, then different polynomials in $F[x]$ determine different functions. 
      \end{theorem}

      \begin{theorem}
        For any collection of given values $y_1, y_2, ..., y_n \in F$ at given distinct points $x_1, x_2, ..., x_n \in F$, there exists a unique polynomial $f \in F[x]$ with deg$\, f < n$ such that
        \begin{equation}
          f(x_i) = y_i, \; i = 1, 2, ..., n
        \end{equation}
        This is commonly known as the \textbf{interpolation problem}, and when $n = 2$, this is called \textbf{linear interpolation}. 
      \end{theorem}

      It is usually impossible to divide one polynomial by another in the algebra $F[x]$; the construction of it does not allow us to. However, division \textbf{with remainder} is possible, similarly to the procedure of division with remainder in the ring of integers. 

      \begin{theorem}
        Let $f, g \in F[x]$ and $g \neq 0$. Then, there exists polynomials $q, r$ such that 
        \begin{equation}
          f = q g + r, \; \text{deg}\, r < \text{deg}\, g \text{ (or } r = 0 \text{)}
        \end{equation}
        This procedure of finding such polynomials $q, r$ is called \textbf{division with a remainder}. A polynomial $f$ is divisible by $g$ in $F[x]$ if and only if $r = 0$. 
      \end{theorem}

      \begin{theorem}[Bezout's Theorem]
        Given that one divides (with remainder) polynomial $f$ by $g = x - c$, let the remainder be $r \in F$. That is, 
        \begin{equation}
          f(x) = (x-c) q(x) + r, \; r \in F
        \end{equation}
        This implies that the remainder equals the value of $f$ at point $c$. That is, 
        \begin{equation}
          f(c) = r
        \end{equation}
      \end{theorem}

    \subsubsection{Roots of Polynomials}

      \begin{definition}
        An element $c \in F$ is a \textbf{root} of polynomial $f$ if and only if 
        \begin{equation}
          f(c) = 0
        \end{equation}
      \end{definition}

      \begin{corollary}
        An element $c$ of a field $F$ is a root of polynomial $f$ if and only if $f$ is divisible by $x - c$. 
      \end{corollary}

      \begin{definition}
        A root $c$ of polynomial $f$ is called \textbf{simple} if $f$ is not divisible by $(x-c)^2$ and \textbf{multiple} otherwise. The \textbf{multiplicity} of a root $c$ is the maximum $k$ such that $(x-c)^k$ divides $f$. 
      \end{definition}

      \begin{theorem}
        The number of roots of a polynomial, counted with multiplicity, does not exceed the degree of this polynomial. Furthermore, these numbers are equal if and only if the polynomial is a product of linear factors.
      \end{theorem}

      \begin{definition}
        A \textbf{monic polynomial} is a polynomial with leading coefficient equal to $1$. 
      \end{definition}

      \begin{theorem}[Viete's Formulas]
        Given that a polynomial $f$ factors into linear terms, that is 
        \begin{equation}
          f(x) = a_0 \prod_{i = 1}^{n} (x - c_i), c_i \text{ roots of } f
        \end{equation}
        Then the coefficients of $f$ can be presented with the formulas
        \begin{align*}
          & \sum_{i=1}^n c_i = - \frac{a_1}{a_0} \\
          & \sum_{i_1 < i_2} c_{i_1} c_{i_2} = \frac{a_2}{a_0} \\
          & \sum_{i_1< ...< i_k} \prod_{j = 1}^{k} c_{i_j} = (-1)^k \frac{a_k}{a_0} \\
          & c_1 c_2 c_3 ... c_n = (-1)^n \frac{a_n}{a_0}
        \end{align*}
      \end{theorem}

      \begin{theorem}[Wilson's Theorem]
        Let $n$ be a prime number. Then 
        \begin{equation}
          (n-1)! \equiv -1 \pmod{n}
        \end{equation}
      \end{theorem}

      \begin{definition}
        The \textbf{derivative} of a polynomial is a map $D: \mathbb{R}[x] \longrightarrow \mathbb{R}[x]$ with the following properties:
        \begin{enumerate}
          \item It is linear. 
          \item $D(f g) = (D f) g + f (D g)$. 
          \item $D x = 1$. 
        \end{enumerate}
      \end{definition}

      In fact, there exists a unique map $D: F[x] \longrightarrow F[x]$ satisfying these properties for any field $F$. 

      \begin{proposition}
        If char$F = 0$, then the coefficients of $f \in F[x]$ regarded as a polynomial in $x - c$ can be expressed as 
        \begin{equation}
          b_k = \frac{ f^{(k)} (c)}{k!}
        \end{equation}
        where $f^{(k)}$ is the $k$th derivative of $f$. 
      \end{proposition}
      \begin{proof}
        We make the substitution $ y = x-c$ in the polynomial $f \in F[x]$ and then express it as a polynomial in $y$ 
        \begin{equation}
          f = b_0 + b_1 (x-c) + b_2 (x-c)^2 + ... + b_n (x-c)^n
        \end{equation}
        We differentiate this equation $k$ times and substitute at $x = c$ to get the corresponding values of the coefficients.
      \end{proof}

    \subsubsection{Fundamental Theorem of Algebra of Complex Numbers}

      While we have defined an upper bound for the number of roots for a polynomial, we have not determined whether a polynomial has any roots at all. Fortunately, it is sufficient to extend the field to $\mathbb{C}$ in order to strongly define a lower limit, too. 

      \begin{definition}
        A field $F$ is \textbf{algebraically closed} if every polynomial of positive degree (i.e. non-constant) in $F[x]$ has at least one root in $F$. This is equivalent to saying that every polynomial can be expressed as a product of first degree polynomials.
      \end{definition}

      \begin{proposition}
        A field $F$ is algebraically closed if and only if for each natural number $n$, every endomorphism of $F^n$ (that is, ever linear map from $F^n$ to itself) has at least one eigenvector. 
      \end{proposition}
      \begin{proof}
        An endomorphism of $F^n$ has an eigenvector if and only if its characteristic polynomial has some root. $(\rightarrow)$ So, when $F$ is algebraically closed, every characteristic polynomial, which is an element of $F[x]$, must have a root. $(\leftarrow)$ Assume that every characteristic polynomial has some root, and let $p \in F[x]$. Dividing the polynomial by a scalar doesn't change its roots, so we can assume $p$ to have leading coefficient $1$. If $p(x) = a_0 + a_1 x + ... + x^n$, then we can identify matrix 
        \begin{equation}
          A = \begin{pmatrix}
          0 & 0 & ... & 0 & -a_0 \\
          1 & 0 & ... & 0 & -a_1 \\
          0 & 1 & ... & 0 & -a_2 \\
          ... & ... & ... & ... & ... \\
          0 & 0 & ... & 1 & -a_{n-1}
          \end{pmatrix}
        \end{equation}
        such that the characteristic polynomial of $A$ is $p$. 
      \end{proof}

      \begin{proposition}
        $\mathbb{R}$ is not algebraically closed. 
      \end{proposition}
      \begin{proof}
        $x^2 + 1$ doesn't have any roots in $\mathbb{R}$. 
      \end{proof}

      \begin{theorem}
        Every polynomial of positive degree over field $\mathbb{C}$ has a root. 
      \end{theorem}

      \begin{corollary}
        In the algebra $\mathbb{C}[x]$, every polynomial splits into a product of linear factors. 
      \end{corollary}

      \begin{corollary}
        Every polynomial of degree $n$ over $\mathbb{C}$ has $n$ roots, counted with multiplicities. 
      \end{corollary}

      \begin{corollary}
        $\mathbb{C}$ is algebraically closed. 
      \end{corollary}

    \subsubsection{Roots of Polynomials with Real Coefficients}

      \begin{theorem}
        If $c$ is a complex root of polynomial $f \in \mathbb{R}[x]$, then $\bar{c}$ is also a root of the polynomial. Moreover, $\bar{c}$ has the same multiplicity as $c$. 
      \end{theorem}

      \begin{corollary}
        Every nonzero polynomial in $\mathbb{R}[x]$ factors into a product of linear terms and quadratic terms with negative discriminants. 
      \end{corollary}

      \begin{example}
      \begin{align*}
        x^5 - 1 & = (x-1) \bigg( x - \Big( \cos{\frac{2\pi}{5}} + i \sin{\frac{2\pi}{5}}\Big) \bigg) \bigg( x - \Big( \cos{\frac{2\pi}{5}} - i \sin{\frac{2\pi}{5}}\Big) \bigg) \\
        & \times \bigg( x - \Big( \cos{\frac{4\pi}{5}} + i \sin{\frac{4\pi}{5}}\Big) \bigg) \bigg( x - \Big( \cos{\frac{4\pi}{5}} - i \sin{\frac{4\pi}{5}}\Big) \bigg) \\
        & = (x-1) \bigg( x^2 - \frac{\sqrt{5} - 1}{2} x + 1\bigg) \bigg( x^2 + \frac{\sqrt{5} + 1}{2} x + 1\bigg) 
      \end{align*}
      \end{example}

      \begin{corollary}
        Every polynomial $f \in \mathbb{R}[x]$ of odd degree has at least one real root. 
      \end{corollary}
      \begin{proof}
        This is a direct result of Theorem **. Alternatively, without loss of generality we can assume that the leading coefficient of $f$ is positive. Then
        \begin{equation}
          \lim_{x \rightarrow + \infty} f(x) = + \infty, \; \lim_{x \rightarrow -\infty} f(x) = -\infty
        \end{equation}
        By the intermediate value theorem, there must be some point where $f$ equals $0$. 
      \end{proof}

      \begin{theorem}[Descartes' Theorem]
        The number of positive roots (counted with multiplicities) of a polynomial $f \in \mathbb{R}[x]$ (denote this $N(f)$) does not exceed the number of changes of sign in the sequence of its coefficients (denote this $L(f)$). Additionally, $L(f) \equiv N(f) \pmod{2}$. If all the complex roots of $f$ are real, then $L(f) = N(f)$. 
      \end{theorem}

      Note that if a polynomial has a multiple root but its coefficients are known only approximately (but with any degree of precision), then it is impossible to prove that the multiple roots exists because under any perturbation of the coefficients, however small, it may separate into simple roots or simply cease to exist. This fact leads to the "instability" of the Jordan Normal form because under any perturbation of the elements of a matrix $A$, the change may drastically affect the characteristic polynomial, hence affecting the geometric multiplicities of its eigenvectors. 

    \subsubsection{Factorization in Euclidean Domains}

      Factorization of polynomials over $\mathbb{C}$ into linear factors and polynomials over $\mathbb{R}$ into linear and quadratic factors is similar to the factoring of the integers to prime numbers. In fact, such a factorization exists for polynomials over any field $F$, but their factors can be of any degree. Moreover, there exists no general solution for the factoring of polynomials over any field. 

      \begin{definition}
        A commutative associative ring with unity and without zero divisors is called an \textbf{integral domain}. That is, the product of any two nonzero elements $x, y \in A$ must be nonzero. Integral domains are generalizations of the ring of integers $\mathbb{Z}$ and provide a natural setting for studying divisibility. 
      \end{definition}

      \begin{example}
        $\mathbb{Z}$ and $F[x]$ over field $F$ are integral domains. Any field $F$ is also an integral domain. 
      \end{example}

      \begin{example}
        The quotient ring $\mathbb{Z}_n$ is not an integral domain when $n$ is composite. 
      \end{example}

      \begin{example}
        A product of two nonzero commutative rings with unity $R \times S$ is not an integral domain since $(1,0) \cdot (0, 1) = (0, 0) \in R \times S$. 
      \end{example}

      \begin{example}
        The ring of $n \times n$ matrices over any nonzero ring when $ n \geq 2$ is not an integral domain. Given matrices $A, B$, if the image of $B$ is in the kernel of $A$, then $A B = 0$.
      \end{example}

      \begin{example}
        The ring of continuous functions on the interval is not an integral domain. To see why, notice that given the piecewise functions 
        \begin{equation}
          f (x) = \begin{cases}
          1 - 2x & x \in [0, \frac{1}{2}] \\
          0 & x \in [\frac{1}{2}, 1] 
          \end{cases}, \; \;\;g (x) = \begin{cases}
          0 & x \in [0, \frac{1}{2}] \\
          2x - 1 & x \in [\frac{1}{2}, 1] 
          \end{cases}
        \end{equation}
        $f, g \neq 0$, but $f g = g f = 0$. 
      \end{example}

      We can classify the rings
      \begin{equation}
        \text{Integral Domains} \subset \text{Commutative Rings} \subset \text{Rings}
      \end{equation}

      \begin{proposition}
        An integral domain is a ring that is isomorphic to a subring of a field. 
      \end{proposition}

      \begin{proposition}
        The characteristic of an integral domain is either $0$ or a prime number. 
      \end{proposition}

      \begin{definition}
         An element $r$ of a ring $R$ is \textbf{regular} if the mapping 
         \begin{equation}
           \rho: R \longrightarrow R, \; x \mapsto x r
         \end{equation}
        is injective for all $x \in R$. 
      \end{definition}

      \begin{proposition}
        An integral domain is a commutative associative ring where every element is regular. 
      \end{proposition}

      \begin{definition}
        Let $A$ be an integral domain. An element $a \in A$ is \textbf{divisible} by $b \in A$, denoted $b | a$ if there exists an element $q \in A$ such that $a = q b$. Elements $a$ and $b$ are \textbf{associated}, denoted $a \sim b$ if either of the following equivalent conditions holds
        \begin{enumerate}
            \item $a | b \text{ and } b | a$
            \item $a = c b, \text{ where } c$ is invertible
        \end{enumerate}
        The two conditions are equivalent because $c$ and $c^{-1}$ are both in $A$. 
      \end{definition}

      \begin{definition}
        Let $A$ be an integral domain which is not a field. $A$ is \textbf{Euclidean} if there exists a function 
        \begin{equation}
          N: A \setminus \{ 0 \} \longrightarrow \mathbb{Z}_+
        \end{equation}
        called a \textbf{norm} that satisfies the following conditions. 
        \begin{enumerate}
          \item $N(a b) \geq N(a)$ and the equality holds if and only if $b$ is invertible. 
          \item For any $a, b \in A, \; b \neq 0$, there exist $q, r \in A$ such that $a = q b + r$ with either $r = 0$ or $ N(r) < N(b)$, known as division with remainder. 
        \end{enumerate}
        Uniqueness of $q, r$ is not required in property 2. 
      \end{definition}

      \begin{example}
        The subring of $\mathbb{C}$, defined
        \begin{equation}
          \mathbb{Z}[i] \equiv \{ a + b i \; | \; a, b \in \mathbb{Z} \}
        \end{equation}
        is a Euclidean integral domain with respect to the norm 
        \begin{equation}
          N(c) \equiv a^2 + b^2
        \end{equation}
        since $N(c d) = N(c) N(d)$ and the invertible elements of $\mathbb{Z}[i]$ are $\pm 1, \pm i$. 
      \end{example}

      \begin{example}
        The ring of rational numbers of the form $2^{-n} m, \; n \in \mathbb{Z}_+, m \in \mathbb{Z}$, is a Euclidean domain. To define the norm, we can first assume that $m$ can be prime factorized into the form 
        \begin{equation}
          m = \pm \prod_{i} p_{i}^{k_i}, \; p \text{ prime}
        \end{equation}
        and the norm is defined 
        \begin{equation}
          N(\frac{m}{2^n}) \equiv 1 + \sum_i k_i
        \end{equation}
        We must further show that division with remainder is possible, but we will not show it here. 
      \end{example}

      \begin{definition}
        The \textbf{greatest common divisor} of elements $a$ and $b$ of an integral domain is a common divisor of $a$ and $b$ divisible by all their common divisors. It is denoted GCD$(a, b)$. 
      \end{definition}

      \begin{definition}
        A \textbf{Gaussian integer} is a complex number whose real part and imaginary part are both integers. That is, 
        \begin{equation}
          \mathbb{Z}[i] \equiv \{a + b i \;|\; a, b \in \mathbb{Z} \}
        \end{equation}
      \end{definition}

    \subsubsection{Polynomials in Several Variables}

      \begin{definition}
        A function of real variable $x_1, x_2, ..., x_n$ is called a \textbf{polynomial} if it can be represented as 
        \begin{equation}
          f(x_1, ..., x_n) =  \sum_{k_1, ..., k_n} a_{k_1 ... k_n} x_1^{k_1} x_2^{k_2} ... x_n^{k_n}
        \end{equation}
        where the summation is taken over a finite set of collections $(k_1, ..., k_n)$. The algebra of polynomials in $x_1, x_2, ..., x_n$ over $\mathbb{R}$ is denoted $\mathbb{R}[x_1, x_2, ..., x_n]$. 
      \end{definition}

      \begin{definition}
        More generally, an infinite dimensional polynomial algebra of variables $x_1, ..., x_n$ over field $\mathbb{F}$ is denoted
        \begin{equation}
          \mathbb{F}[x_1, ..., x_n]
        \end{equation}
        Like polynomials of one variable, it can be naturally identified with an abstract multi-dimensional "sequence." It has basis 
        \begin{equation}
          \{e_{k_1 k_2 ... k_n} \;|\; k_1, k_2, ..., k_n \in \mathbb{Z}_+
        \end{equation}
        with addition defined component-wise and the multiplication rule defined with the table
        \begin{equation}
          e_{k_1...k_n} e_{l_1 ... l_n} = e_{k_1 + l_1, k_2 + l_2, ..., k_n + l_n}
        \end{equation}
        Clearly each polynomial in its usual presentation is gotten by the linear mapping
        \begin{equation}
          e_{k_1 ... k_n} \mapsto x_1^{k_1} x_2^{k_2} ... x_n^{k_n}
        \end{equation}
      \end{definition}

      However, note that different polynomials may define the same functions if the field $\mathbb{F}$ is finite, similarly to polynomials with one variable. If $\mathbb{F}$ is infinite, then every polynomial will determine a different function. 

      \begin{definition}
        A polynomial is called \textbf{homogeneous} if degree $d$ if 
        \begin{equation}
          a_{k_1 k_2 ... k_n} = 0 \text{ for } k_1 + k_2 + ... + k_n \neq d
        \end{equation}
        The space of all homogeneous polynomials of fixed degree $d$ forms a finite dimensional subspace in $\mathbb{F}[x_1, ..., x_n]$ with dimension 
        \begin{equation}
          \frac{n(n+1)...(n+d-1)}{d!}
        \end{equation}
        The dimension can be calculated by thinking of the combinatorics problem of having $d$ indistinguishable balls to put into $n$ distinguishable urns. 
      \end{definition}

    \subsubsection{Symmetric Polynomials}

      \begin{definition}
        A polynomial $f \in \mathbb{F}[x_1, ..., x_n]$ is called \textbf{symmetric} if it is invariant under any permutation of the variables $x_i$. 
      \end{definition}

      \begin{example}
        Power sums are symmetric polynomials. 
        \begin{equation}
          p(x_1, x_2, ..., x_n) = \sum_{i=1}^n x_i^k
        \end{equation}
      \end{example}

      \begin{definition}
        An \textbf{elementary symmetric polynomial} is a symmetric polynomial of one of these forms: 
        \begin{align*}
          \sigma_1 & = x_1 + x_2 + ... + x_n \\
          \sigma_2 & = x_1 x_2 + x_1 x_3 + ... + x_{n-1} x_n \\
          ... & = ... \\
          \sigma_k & = \sum_{i_1 < ... < i_k} x_{i_1} x_{i_2} ... x_{i_k} \\
          ... & = ... \\
          \sigma_n & = x_1 x_2 ... x_n
        \end{align*}
      \end{definition}

      The following theorem presents an extremely useful result about the decomposition of symmetric polynomials. 

      \begin{theorem}
        Every symmetric polynomial can be written as a polynomial of elementary symmetric polynomials $\sigma_i$. 
      \end{theorem}

      \begin{example}
        The polynomial 
        \begin{equation}
          f \equiv \sum_{i=1}^n x_i^3
        \end{equation}
        can be expressed as 
        \begin{equation}
          f = \sigma_1^3 - 3 \sigma_1 \sigma 2 + 3 \sigma_3
        \end{equation}
      \end{example}

    \subsubsection{Cubic Equations}

      The well known discriminant of a quadratic equation 
      \begin{equation}
        f(x) = ax^2 + bx + c
      \end{equation}
      is known in the form $\nabla = b^2 - 4ac$. However, we will present it in a slightly different manner. 

      \begin{definition}
        The \textbf{discriminant} $D(\varphi)$ of a quadratic polynomial
        \begin{equation}
          \varphi = a_0 x^2 + a_1 x + a_2 \in \mathbb{C}[x]
        \end{equation}
        with $c_1, c_2 \in \mathbb{C}$ as its roots is defined
        \begin{equation}
          D(\varphi) = a_1^2 - 4 a_0 a_2 = a_0^2 \bigg( \Big(\frac{a_1}{a_0} \Big)^2 - \frac{4 a_2}{a_0} \bigg) = a_0^2 \big( (c_1 + c_2)^2 - 4 c_1 c_2 \big) = a_0^2 (c_1 - c_2)^2
        \end{equation}
        Clearly, the value of $D(\varphi)$ can tell us three things
        \begin{enumerate}
          \item $c_1, c_2 \in \mathbb{R}, c_1 \neq c_2$. Then $c_1 - c_2$ is a nonzero real number and $D(\varphi) > 0$. 
          \item $c_1 = c_2 \in \mathbb{R}$. Then $c_1 - c_2 = 0$ and $D(\varphi) = 0$. 
          \item $c_1, c_2 \in \mathbb{C}, c_1 = \bar{c}_2$. Then, $c_1 - c_2$ is a nonzero strictly imaginary number and $D(\varphi) < 0$. 
        \end{enumerate}
      \end{definition}

      \begin{definition}
        We can generalize this notion of the discriminant to arbitrary polynomials
        \begin{equation}
          \varphi = a_0 x^n + a_1 x^{n-1} + ... + a_{n-1} x + a_n \in \mathbb{F}[x], \; a_0 \neq 0
        \end{equation}
        The discriminant $D(\varphi)$ of the polynomial above is defined
        \begin{equation}
          D(\varphi) \equiv a_0^{2n-2} \prod_{i>j} (c_i - c_j)^2
        \end{equation}
        The $a_0$ term isn't very important in this formula, since it does not affect whether $D(\varphi)$ is positive, negative, or zero. 
      \end{definition}

      \begin{definition}
        A polynomial 
        \begin{equation}
          \varphi = a_0 x^n + a_1 x^{n-1} + ... + a_{n-1} x + a_n \in \mathbb{F}[x], \; a_0 \neq 0
        \end{equation}
        where $a_1 = 0$ is called \textbf{depressed}. A depressed cubic polynomial is of form
        \begin{equation}
          \varphi = x^3 + p x + q
        \end{equation}
      \end{definition}

      \begin{proposition}
        Every monic (leading coefficeint $=1$) polynomial (and non-monic ones) 
        \begin{equation}
          \varphi = x^n + a_1 x^{n-1} + ... + a_{n-1} x + a_n \in \mathbb{F}[x], \; a_0 \neq 0
        \end{equation}
        can be turned into a depressed polynomial with the change of variable
        \begin{equation}
          x = y - \frac{a_1}{n}
        \end{equation}
        to get the polynomial 
        \begin{equation}
          \psi = y^n + b_2 y^{n-2} + ... + b_{n-1} y + b_n
        \end{equation}
      \end{proposition}

      \begin{lemma}
        A cubic polynomial 
        \begin{equation}
          \varphi = a_0 x^3 + a_1 x^2 + a_2 x + a_3 \in \mathbb{R}[x]
        \end{equation}
        with roots $c_1, c_2, c_3 \in \mathbb{C}$ has discriminant
        \begin{equation}
          D(\varphi) \equiv a_0^4 (c_1 - c_2)^2 (c_1 - c_3)^2 (c_2 - c_3)^2
        \end{equation}
        With a bit of evaluation, it can also be expressed in terms of its coefficients as
        \begin{equation}
          D(\varphi) = a_1^2 a_2^2 - 4a_1^3 a_3 - 4a_0 a_2^3 + 18 a_0 a_1 a_2 a_3 - 27 a_0^2 a_3^2
        \end{equation}
        Again, three possibilities can occur (up to reordering of its roots). 
        \begin{enumerate}
            \item $c_1, c_2, c_3$ are distinct real numbers. Then $D(\varphi) > 0$. 
            \item $c_1, c_2, c_3 \in \mathbb{R}, c_1 = c_2$. Then $D(\varphi) = 0$. 
            \item $c_1 \in \mathbb{R}, c_2 = \bar{c}_3 \not\in \mathbb{R}$. Then $D(\varphi) < 0$. 
        \end{enumerate}
        Furthermore, the cubic formula used to find the roots of the polynomial is 
        \begin{equation}
          c_{1, 2, 3} = \sqrt[3]{-\frac{q}{2} + \sqrt{\frac{p^3}{27} + \frac{q^2}{4}}} + \sqrt[3]{-\frac{q}{2} - \sqrt{\frac{p^3}{27} + \frac{q^2}{4}}}
        \end{equation}
        known as \textbf{Cardano's formula}, after the mathematician Gerolamo Cardano. 
      \end{lemma}

  \subsection{Ideals and Quotient Rings}

    \begin{definition}
      For an arbitrary ring $(R,+, \cdot)$, let $(R, +)$ be its additive group. A subset $I$ is called a \textbf{left ideal} of $R$ if it satisfies the two conditions. 
      \begin{enumerate}
        \item $(I, +)$ is a subgroup of $(R, +)$. 
        \item For every $r \in R$ and every $x \in I$ the left product $r \cdot x \in I$. 
      \end{enumerate}
      Similarly, a \textbf{right ideal} $I$ of $R$ satisfies
      \begin{enumerate}
        \item $(I, +)$ is a subgroup of $(R, +)$. 
        \item For every $r \in R$ and every $x \in I$, the right product $r \cdot x \in I$. 
      \end{enumerate}
      Note that left and right modules are equivalence relations defined on a ring. 
    \end{definition}

    A left/right ideal can also be seen as a left/right $R$-submodule of $R$ viewed as an $R$-module. 

    \begin{definition}
      A \textbf{two-sided ideal}, or more simply an \textbf{ideal}, is a left ideal that is also a right idea. 
    \end{definition}

    \begin{proposition}
      Every right or left ideal of a commutative ring is a two sided ideal. 
    \end{proposition}
    \begin{proof}
      Trivial. 
    \end{proof}

    \begin{example}
      The set of even integers $2 \mathbb{Z}$ is an ideal in the ring $\mathbb{Z}$, since the sum of any even integers is even and the product of any even integer with an integer is an even integer. However, the odd integers do not form an ideal. 
    \end{example}

    \begin{example}
      The set of all polynomials with real coefficients which are divisible by the polynomial $x^2 + 1$ is an ideal in the ring of all polynomials. 
    \end{example}

    \begin{example}
      The set of all $n \times n$ matrices whose last row is zero forms a right ideal in the ring of all $n \times n$ matrices. However, it is not a left ideal.

      The set of all $n\times n$ matrices whose last column is zero is a left ideal, but not a right ideal. 
    \end{example}

    \begin{proposition}
      The only ideals that exist in a field $\mathbb{F}$ is $\{0\}$ and $\mathbb{F}$ itself. 
    \end{proposition}
    \begin{proof}
      Given a nonzero element $x \in \mathbb{F}$, every element of $\mathbb{F}$ can be expressed in the form of $a x$ or $x a$ for some $a \in \mathbb{F}$. 
    \end{proof}

    \begin{definition}
      A left ideal generated by a single element $x$ is called the \textbf{principal left ideal generated by $x$} and is denoted $R x$. Principal right ideals are denoted $x R$, and principal (two-sided) ideals are denoted $R x R$. 
    \end{definition}

    \begin{definition}
      A \textbf{principal ideal domain}, also called a \textbf{PID}, is an integral domain in which every ideal is principal (i.e. can be generated by a single element). 

      More generally, a \textbf{principal ideal ring} is a nonzero commutative ring in which every ideal is principal (i.e. can be generated by a single element). 
    \end{definition}

    The distinction is that a principal ideal ring may have zero divisors whereas a principal ideal domain cannot. Principal ideal domains are thus mathematical objects that behave somewhat like the integers. That is, 
    \begin{enumerate}
      \item Any element of a PID has a unique decomposition into prime elements. 
      \item Any two elements of a PID have a greatest common divisor. 
      \item If $x$ and $y$ are elements of a PID without common divisors, then every element of the PID can be written in the form 
        \begin{equation}
          a x + b y
        \end{equation}
    \end{enumerate}

    \begin{proposition}
      Every Euclidean domain is also a principal ideal domain. 
    \end{proposition}

    \begin{example}
      The following are all examples of principal ideal domains. 
      \begin{enumerate}
        \item Any field $\mathbb{F}$. 
        \item The ring of integers $\mathbb{Z}$. 
        \item $\mathbb{F}[x]$, rings of polynomials in one variable with coefficients in a field $\mathbb{F}$. 
        \item Rings of formal power series $\mathbb{F}[[x]]$. 
        \item The ring of Gaussian integers $\mathbb{Z}[i]$. 
      \end{enumerate}
    \end{example}

    It is quite easy to see that a field $\mathbb{F}$ is a PID since the only two possible ideals are $\{0\}$ and $\mathbb{F}$, both of which are principal. For the integers $\mathbb{Z}$, every ideal is of the form $n\mathbb{Z}$, which is principal since it is generated by the integer $n$. The ring of polynomials $\mathbb{F}[x]$ is a PID since we can imagine a minimal polynomial $p$ in each ideal $I$. Every element in $I$ must be divisible by $p$, which means that the entire ideal $I$ can be generated by the minimal polynomial $p$, making $I$ principal. 

  \subsection{The Algebra of Quaternions}

    \begin{definition}
      The \textbf{quaternions} form an algebra of $4$-dimensional vectors over $\mathbb{R}$, with elements of the form
      \begin{equation}
        (a, b, c, d) \equiv a + bi + cj + dk
      \end{equation}
      where $a$ is called the \textbf{scalar portion} and $bi + cj + dk$ is called the \textbf{vector/imaginary portion}. The algebra of quaternions is denoted $\mathbb{H}$, which stands for "Hamilton." $\mathbb{H}$ is a $4$-dimensional associative normed division algebra over $\mathbb{R}$. 
    \end{definition}

    From looking at the multiplication table, we can see that multiplication in $\mathbb{H}$ is not commutative. 
    \begin{center}
      \begin{tabular}{|c|c|c|c|c|}
      \hline
      i & i & -1 & k & -j \\ 
      \hline
      j & j & -k & -1 & i \\ 
      \hline
      k & k & j & -i & -1 \\ 
      \hline
      \end{tabular}
    \end{center}
    Note the identity 
    \begin{equation}
      i^2 = j^2 = k^2 = -1
    \end{equation}
    The algebra of quaternions are in fact the first noncommutative algebra to be discovered! 

    \begin{proposition}
      $\mathbb{H}$ and $\mathbb{C}$ are the only finite-dimensional divisions rings containing $\mathbb{R}$ as a proper subring. 
    \end{proposition}

    \begin{definition}
      The \textbf{quaternion group}, denoted $Q_8$ is a nonabelian group of order $8$, isomorphic to a certain $8$-element subset in $\mathbb{H}$ under multiplication. It's group presentation is 
      \begin{equation}
        Q_8 = \big\langle \bar{e}, i, j, k \;|\; \bar{e}^2 = e, i^2 = j^2 = k^2 = ijk = \bar{e} \big\rangle
      \end{equation}
    \end{definition}

    Going back to the algebra, we can set $\{1, i, j, k\}$ as a basis and define addition and scalar multiplication component-wise, and multiplication (called the \textbf{Hamilton product}) with properties
    \begin{enumerate}
      \item The real quaternion $1$ is the identity element. 
      \item All real quaternions commute with quaternions: $a q = q a$ for all $a \in \mathbb{R}, q \in \mathbb{H}$. 
      \item Every quaternion has an inverse with respect to the Hamilton product. 
        \begin{equation}
          (a + bi + cj + dk)^{-1} = \frac{1}{a^2 + b^2 + c^2 + d^2} \big( a - bi - cj - dk\big)
        \end{equation}
    \end{enumerate}
    Note that property 3 allows $\mathbb{H}$ to be a division algebra. 

    \begin{proposition}[Scalar and Vector Components]
      Let the quaternion be divided up into a scalar and vector part with the bjective mapping $a + bi + cj + dk \mapsto \big(a, (b, c, d)\big)$. 
      \begin{equation}
        q = (r, v), r \in \mathbb{R}, v \in \mathbb{R}^3
      \end{equation}
      Then, the formulas for addition and multiplication are
      \begin{align*}
        q_1 + q_2 & = (r_1, v_1) + (r_2, v_2) = (r_1 + r_2, v_1 + v_2) \\
        q_1 \cdot q_2 & = (r_1, v_1) \cdot (r_2, v_2) = (r_1 r_2 - v_1 \cdot v_2, r_1 v_2 + r_2 v_1 + v_1 \times v_2)
      \end{align*}
      where the $\cdot$ and $\times$ on the right hand side represnts the dot product and cross product, respectively. 
    \end{proposition}

    \begin{definition}
      The conjugate of a quaternion $q = a + bi + cj + dk$ is defined 
      \begin{equation}
        \bar{q}, q^* \equiv a - bi - cj - dk
      \end{equation}
      It has properties
      \begin{enumerate}
        \item $q^{**} = q$
        \item $(q p)^* = p^* q^*$
      \end{enumerate}
      $q^*$ can also be expressed in terms of addition and multiplication. 
      \begin{equation}
        q^* = -\frac{1}{2} \big( q + iqi + jqj + kqk \big)
      \end{equation}
    \end{definition}

    \begin{definition}
      The \textbf{norm} of $q$ is defined
      \begin{equation}
        ||q|| \equiv \sqrt{q^* q} = \sqrt{q q^*} = \sqrt{a^2 + b^2 + c^2 + d^2}
      \end{equation}
      with properties
      \begin{enumerate}
        \item Scaling factor. $||\alpha q|| = |\alpha| ||q||$
        \item Multiplicative. $||p q|| = ||p|| ||q||$
      \end{enumerate}
    \end{definition}

    The norm allows us to define a metric 
    \begin{equation}
      d(p, q) \equiv ||p - q||
    \end{equation}
    This makes $\mathbb{H}$ a metric space, with addition and multiplication continuous on the metric topology. 

    \begin{definition}
      The \textbf{unit quaternion} is defined to be
      \begin{equation}
        U_q = \frac{q}{||q||}
      \end{equation}
    \end{definition}

    \begin{corollary}
      Every quaternion has a polar decomposition
      \begin{equation}
        q = U_q \cdot ||q||
      \end{equation}
      With this, we can redefine the inverse as
      \begin{equation}
        q^{-1} = \frac{q^*}{||q||^2}
      \end{equation}
    \end{corollary}

    \subsubsection{Matrix Representations of Quaternions}

      We can represent $q$ with $2 \times 2$ matrices over $\mathbb{C}$ or $4\times 4 $ matrices over $\mathbb{R}$. 

      \begin{proposition}
        The following representation is an injective homomorphism $\rho: \mathbb{H} \longrightarrow \GL(2, \mathbb{C})$. 
        \begin{equation}
          \rho: a + bi + cj + dk \mapsto \begin{pmatrix}
          a+bi & c+ di \\ -c + di & a - bi
          \end{pmatrix}
        \end{equation}
        It has properties
        \begin{enumerate}
          \item Constraining any two of $b, c, d$ to $0$ produces a representation of the complex numbers. When $c = d = 0$, this is called the \textbf{diagonal representation}. 
          \begin{align*}
            \begin{pmatrix}
            a+bi & 0 \\ 0 & a-bi
            \end{pmatrix},  \begin{pmatrix}
            a & c \\ -c & a
            \end{pmatrix},  \begin{pmatrix}
            a & di \\ di & a
            \end{pmatrix}
          \end{align*}
          \item The norm of a quaternion is the square root of the determinant of its corresponding matrix representation. 
            \begin{equation}
              ||q|| = \sqrt{\det \begin{pmatrix}
              a+bi & c+di \\ -c+di & a-bi
              \end{pmatrix}} = \sqrt{(a^2 + b^2) + (c^2 + d^2)}
            \end{equation}
          \item The conjugate of a quaternion corresponds to the conjugate (Hermitian) transpose of its matrix representation. 
            \begin{equation}
              \rho(q^*) = \rho(q)^H \iff a-bi-cj-dk \mapsto \begin{pmatrix}
              a-bi & -c-di \\ c-di & a+bi
              \end{pmatrix}
            \end{equation}
          \item The restriction of this representation to only unit quaternions leads to an isomorphism between the subgroup of unit quaternions and their corresponding image in SU$(2)$. Topologically, the unit quaternions is the $3$-sphere, so the underlying space SU$(2)$ is also a $3$-sphere. More specifically, 
            \begin{equation}
              \frac{\text{SU}(2)}{2} \simeq \text{SO}(3)
            \end{equation}
        \end{enumerate}
      \end{proposition}

      \begin{proposition}
      The following representation of $\mathbb{H}$ is an injective homomorphism $\rho: \mathbb{H} \longrightarrow \GL(4, \mathbb{R})$. 
      \begin{equation}
        \rho: a+bi+cj+dk \mapsto \begin{pmatrix}
        a&-b&-c&-d \\
        b&a&-d&c\\
        c&d&a&-b\\
        d&-c&b&a
        \end{pmatrix}
      \end{equation}
      or also as
      \begin{equation}
        a \begin{pmatrix}
        1 &0 &0 &0 \\
        0& 1&0&0\\
        0&0&1&0\\
        0&0&0&1
        \end{pmatrix} + b \begin{pmatrix}
        0&-1&0&0\\1&0&0&0\\0&0&0&-1\\0&0&1&0
        \end{pmatrix} + c\begin{pmatrix}
        0&0&-1&0\\0&0&0&1\\1&0&0&0\\0&-1&0&0
        \end{pmatrix} + d \begin{pmatrix}
        0&0&0&-1\\0&0&-1&0\\0&1&0&0\\1&0&0&0
        \end{pmatrix}
      \end{equation}
      It has properties
      \begin{enumerate}
        \item $\rho(q^*) = \rho(q)^T$
        \item The fourth power of the norm is the determinant of the matrix 
          \begin{equation}
            ||q||^4 = \det\big( \rho (q)\big)
          \end{equation}
        \item Similarly, with the $2\times 2$ representation, complex number representations can be produced by restricting $2$ of $b, c, d$ to $0$. 
      \end{enumerate}
      \end{proposition}

      Note that this representation in $\GL(4, \mathbb{R})$ is not unique. There are in fact 48 distinct representation of this form where one of the component matrices represents the scalar part and the other 3 are skew symmetric. 

    \subsubsection{Square Roots of -1}

      In $\mathbb{C}$, there are two numbers, $i$ and $-i$, whose square is $-1$. However, in $\mathbb{H}$, infinitely many square roots of $-1$ exist, forming the unit sphere in $\mathbb{R}^3$. To see this, let $q = a+bi+cj+dk$ be a quaternion, and assume that its square is $-1$. Then this implies that
      \begin{equation}
        a^2 - b^2 -c^2 -d^2 = -1, 2ab = 2ac = 2ad = 0
      \end{equation}
      To satisfy the second equation, either $a=0$ or $b=c=d=0$. The latter is impossible since then $q$ would be real. Therefore, 
      \begin{equation}
        b^2 + c^2 + d^2 = 1
      \end{equation}
      which forms the unit sphere in $\mathbb{R}^3$. 


\section{Determinants and Trace}

  The definition of the determinant is given first and then shown that it has the corresponding properties. We will work backward and construct the determinant from its properties. 

  \begin{definition}
  The determinant of a $n \times n$ matrix $A$, with column vectors $a_1, a_2, ..., a_n$, is a function
  \[\det: \text{Mat}(n, \mathbb{F}) \longrightarrow \mathbb{F}\]
  with the following three properties
  \begin{enumerate}
      \item The determinant of the identity matrix is 1. 
  \[\det{(I)} \equiv \det{(e_1, e_2, ..., e_n)} = 1\]
      \item Interchanging two columns $a_i$ and $a_j$ of $A$ once changes the sign of $\det{A}$. 
  \[\det{(a_1, ..., a_i, ..., a_j, ..., a_n)} = -\det{(a_1, ..., a_j, ..., a_i, ..., a_n)}\]
      \item It is a multilinear function of the $n$ column vectors. 
  \[\det{(a_1, ..., \lambda a_i + \mu a_i^\prime, ... a_n)} = \lambda \det{(a_1, ..., a_i, ... a_n)} + \mu \det{(a_1, ..., a_i^\prime, ... a_n)} \]
  \end{enumerate}
  \end{definition}

  An important way to visualize determinants is by using the linear map visualization introduced before. That is, the determinant is the area of the transformed shaded unit square. 
  \begin{center}
      \includegraphics[scale=0.25]{img/Determinant.PNG}
  \end{center}

  \begin{proposition}
  The column vectors of $A$ are linearly dependent if and only if $\det{A} = 0$. 
  \end{proposition}
  \begin{proof}
  By linearity, it is sufficient to prove that if two column vectors $a_i$ and $a_j$ of a matrix $A$ are equal, then $\det{A} = 0$. This can be easily seen by property (ii) of determinants. 
  \end{proof}

  \begin{theorem}
  \[ \det{\bigg(\prod_i A_i \bigg)} = \prod_i \det{A_i}\]
  \end{theorem}

  \begin{theorem}
  A matrix is invertible if and only if its determinant is nonzero. 
  \end{theorem}
  \begin{proof}
  A matrix is invertible $\iff$ it is nonsingular $\iff$ its columns are linearly independent $\iff$ its determinant is nonzero, by the previous proposition. 
  \end{proof}

  \begin{corollary}
  Given $n \times n$ matrix $A$,
  \[\det{(A^{-1})} = \frac{1}{\det{A}}\]
  \end{corollary}

  \begin{theorem}
  The determinants of similar matrices are equal. 
  \end{theorem}
  \begin{proof}
  Let $A$ and $B$ be similar matrices. Then, there exists an $S$ such that $A = S^{-1} B S$ and 
  \[ \det{(A)} = \det{(S^{-1} B S} = \det{(S^{-1})} \det{(B)} \det{(S)} = \det{B}\]
  \end{proof}

  This theorem implies that the determinant is an intrinsic property of a linear transformation, so it is invariant under a change of basis. That is, choosing different matrix representations of a linear transformation does not change the determinant.  

  \begin{corollary}
  \[\det{(A)} = \det{(A^T)}\]
  \end{corollary}
  \begin{proof}
  $A$ is similar to $A^T$, which will be proven in chapter 6. 
  \end{proof}

  \begin{proposition}
  The properties of the determinant combined with the previous corollary implies that 
  \begin{enumerate}
      \item Adding a scalar multiple of a row/column to another row/column doesn't affect the determinant. 
      \item Interchanging two rows/columns switches the sign of the determinant. 
      \item Multiplying a row/column by $\alpha$ multiplies the determinant by $\alpha$. 
  \end{enumerate}
  \end{proposition}

  \begin{theorem}
  Let $A$ be an $n \times n$ matrix whose first column is $e_1$
  \[A = \begin{pmatrix}
  1&*&*&* \\
  0 &&& \\
  \ldots& & A_{11}& \\
  0&&&
  \end{pmatrix}\]
  where $A_{11}$ is the $(n-1) \times (n-1)$ submatrix of $A$ with entries $a_{i j}, \; i, j > 1$. Given this, 
  \[\det{A} = \det{A_{11}}\]
  \end{theorem}
  \begin{proof}
  Using column reduction, we can see that 
  \[ \det{A} = \det{\begin{pmatrix}
  1&0&0&0 \\
  0 &&& \\
  \ldots& & A_{11}& \\
  0&&&
  \end{pmatrix}}\]
  it is clear that the right hand side is equal to $\det{A_{11}}$ since it behaves exactly like $\det{A_{11}}$ with respect to the three properties. 
  \end{proof}

  \begin{corollary}
  Let $A$ be an upper or a lower triangular matrix. Then the determinant of $A$ is the product of its diagonal entries. That is,  
  \[ \det{A} = \prod_{i} a_{i i}\]
  \end{corollary}
  \begin{proof}
  We apply the previous theorem recursively to satisfy when $A$ is upper triangular. Since $\det{(A)} = \det{(A^T)}$, this fact can be applied to lower triangular matrices too. 
  \end{proof}

  It is once again verified that the three elementary row (and column) operations affect the determinant in the way stated in Proposition 5.5. To elaborate, since $E_1, E_2$, and $E_3$ are all lower triangular, we can compute their determinants easily
  \begin{align*}
      \det{E^1_{\alpha \times i + j}} = 1 \\
      \det{E^2_{i j}} = -1 \\
      \det{E^3_{\alpha \times i}} = \alpha
  \end{align*}
  and multiplying matrix $A$ by elementary matrices $E^1, E^2$, and $E^3$ multiplies the determinant by $1, -1$, and $\alpha$, respectively. 

  We can describe the determinant visually. Given a linear mapping $A: V \longrightarrow V$, we can fix any basis $\{e_1, e_2, ..., e_n\}$ on $V$. Note that these basis vectors do not need to be orthogonal, nor are they restricted to any magnitude. The set of vectors 
  \[\Big\{ \sum_{i=1}^n c_i e_i \; | \; 0 \leq c_i \leq 1, i = 1, 2, ..., n\Big\}\]
  forms an $n$-dimensional parallelepiped in $V$. Let the volume of this parallelepiped be $U$. Let $W$ be the volume of the parallelepiped 
  \[\Big\{ \sum_{i=1}^n c_i A e_i \; | \; 0<c_i<1, i = 1, 2, ..., n\Big\}\]
  which is formed by the transformed basis vectors $\{Ae_1, Ae_2, ..., Ae_n\}$. We can view this latter shape as the image of the first parallelepiped under transformation $A$. Then, 
  \[ \det{A} = W / V \]
  That is, the ratio of the transformed parallelepiped to the original parallelepiped is the determinant. This is consistent with the properties of the determinant. For example, if $A$ is not isomorphic, then the parallelepiped will get "squished" into a lower-dimensional parallelepiped with volume $0$. The fact that we use a ratio between the original and transformed parallelepiped allows this value to be invariant under the basis that we use. 

  Computationally, finding the LUP decomposition of a matrix $A$ is the best known algorithm to compute the determinant of a general $n \times n$ matrix. That is, 
  \[ \det{A} = \det{L} \det{U} \det{P} = \pm \det{U} = \pm \prod_i u_{i i}\]
  since $\det{L} = 1$ and $\det{P} = \pm 1$. 

  There are other methods to compute the determinant. First, we state the simple but useful formula.

  \begin{proposition}
  \[\det{\begin{pmatrix}
  a&b\\c&d 
  \end{pmatrix}} = a d - b c\] 
  \end{proposition}

  \begin{definition}
  Given an $n \times n$ matrix $A$, the $(i j)$th minor of $A$, denoted $A_{i j}$, is the determinant of the $(n-1) \times (n-1)$ matrix formed by removing the $i$th row and $j$ th column from $A$. 
  \end{definition}

  \begin{theorem}[Laplace Expansion]
  Let $A$ be an $n \times n$ matrix and $j$ any index between $1$ and $n$. Then
  \[\det{A} = \sum_i (-1)^{i + j} a_{i j} A_{i j}\]
  that is, the alternating sums of the $ij$th minors multiplied by the $ij$th entries in the $j$th column of $A$. This can be done by choosing an arbitrary $i$th row, which leads to the alternative formula 
  \[\det{A} = \sum_j (-1)^{i + j} a_{i j} A_{i j} \]
  \end{theorem}

  \begin{theorem}[Cramer's Rule]
  Given a system of linear equations in the form $A x = b$ where $A$ is an $n \times n$ matrix, the solutions of this system can be expressed with the formulas 
  \[ x_i = \frac{ \det{A_i}}{\det{A}}\]
  where $\det{A_i}$ is the matrix formed by replacing $a_i$, the $i$th column of $A$, by the column vector $b$. 
  \end{theorem}

  Albeit very computationally heavy, determinants can also be used to calculate the inverse of a matrix. 

  \begin{theorem}
  The inverse matrix $A^{-1}$ of an invertible matrix $A$ has the form 
  \[(A^{-1})_{i j} = (-1)^{i+j} \frac{\det{A_{i j}}}{\det{A}}\]
  \end{theorem}

  \begin{definition}
  The trace of a square matrix $A$, denoted $\Tr{A}$, is the sum of its diagonal entries. 
  \[\Tr(A) = \sum_{i} a_{ii}\]
  \end{definition}

  \begin{proposition}
  \[\Tr(\lambda A + \alpha B) = \lambda \Tr(A) + \alpha \Tr(B)\]
  \end{proposition}
  \begin{proof}
  Obvious if we look at the entries of $A$ and $B$ and see that it is bilinear.
  \end{proof}

  \begin{theorem}[Cyclic Property of the Trace]
  \[\Tr{\bigg(\prod_{i=1}^n A_i\bigg)} = \Tr{\bigg(A_n \prod_{i=1}^{n-1} A_i\bigg)}\]
  \end{theorem}
  \begin{proof}
  We first prove when $m=2$. Given that the subscripts $i j$ denote that $(i,j)$th element of a matrix, observe that
  \begin{align*}
      (AB)_{ij} = \sum_{k} A_{ik} B_{kj} & \implies (AB)_{ii} = \sum_{K} A_{ik} B_{ki} \\
      & \implies \Tr(AB) = \sum_{i} \sum_{k} A_{ik} B_{ki} \\
      & \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;= \sum_{k} \sum_{i} B_{ki} B_{ik} = Tr(BA)
  \end{align*}
  Similarly, for $m=3$
  \begin{align*}
      (ABC)_{ij} = \sum_{k,l} A_{ik} B_{kl} C_{lj} & \implies \Tr(ABC) = \sum_{i,k,l} A_{ik} B_{kl} C_{li} \\ 
      & \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;= \sum_{i,k,l} C_{li} A_{ik} B_{kl} = \Tr(CAB)
  \end{align*}
  And so we can generalize for $m$. 
  \end{proof}

  \begin{corollary}
  The trace is invariant under a change of basis. That is, the trace is an intrinsic property of a linear transformation since it does not change depending on how it is represented. 
  \end{corollary}
  \begin{proof}
  Given that $A$ is similar to $B$. 
  \[\Tr(B) = \Tr(S A S^{-1}) = \Tr(S^{-1} S A) = \Tr(A) \]
  \end{proof}

  \begin{theorem}
  Let $A$ be a $n \times n$ skew-symmetric matrix over $\mathbb{C}$ (or any field of characteristic $\neq 2$). If $n$ is odd, 
  \[\det{A} = 0 \]
  \end{theorem}
  \begin{proof}
  \[\det{A} = \det{A^T} = \det{-A} = (-1)^n \det{A} \implies \det{A} = 0 \]
  \end{proof}

  We can actually conclude something even futher about antisymmetric matrices. 

  \begin{theorem}
  The determinant of an antisymmetric matrix $A$ of even order is the square of a homogeneous polynomial of degree $n/2$ in the entries of $A$. That is, 
  \[\det{A} = P^2\]
  The polynomial $P$ is called the \textbf{Pfaffian}. 
  \end{theorem}

  \begin{definition}
  A \textbf{Vandermonde matrix} is a square matrix whose columns form a geometric progression. That is, let $a_1, a_2, ..., a_n$ be $n$ scalars. Then, $V(a_1, a_2, ..., a_n)$ is the $n \times n$ matrix
  \[\begin{pmatrix}
  1&1&\ldots&1&1 \\
  a_1&a_2&\ldots&a_{n-1}&a_n\\
  \vdots&\vdots&\ddots&\vdots&\vdots\\
  a_1^{n-2}&a_2^{n-2}&\ldots&a_{n-1}^{n-2}&a_n^{n-2}\\
  a_1^{n-1}&a_2^{n-1}&\ldots&a_{n-1}^{n-1}&a_n^{n-1}
  \end{pmatrix}\]
  \end{definition}

  \begin{theorem}
  The determinant of a Vandermonde matrix is
  \[\det{V(a_1, a_2, ..., a_n)} = \prod_{j>i} (a_j - a_i)\]
  \end{theorem}

  A symmetry in the multivariable expression of a determinant can also reveal a symmetry in the matrix.

  \begin{example}[2019 Putnam A1]
  The symmetric polynomial 
  \[ f(x, y, z) = x^3 + y^3 + z^3 - 3 x y z\]
  can be expressed as the determinant of the $3 \times 3$ matrix
  \[\det{\begin{pmatrix}
  x&y&z\\
  z&x&y\\
  y&z&x
  \end{pmatrix}}\]
  \end{example}

\subsection{Matrices in Block Form}

  \begin{theorem}
  Given $2 \times 2$ block matrices
  \[X = \begin{pmatrix}
  A_1&B_1\\C_1&D_1
  \end{pmatrix}, \; \; Y = \begin{pmatrix}
  A_2&B_2\\C_2&D_2
  \end{pmatrix}\]
  We can compute $X Y$ similarly to regular matrix multiplication, treating the blocks as entries. 
  \[ X Y = \begin{pmatrix}
  A_1&B_1\\C_1&D_1
  \end{pmatrix} \begin{pmatrix}
  A_2&B_2\\C_2&D_2
  \end{pmatrix} = \begin{pmatrix}
  A_1 A_2 + B_1 C_2 & A_1 B_2 + B_1 D_2 \\
  C_1 A_2 + D_1 C_2 & C_1 B_2 + D_1 D_2 
  \end{pmatrix}\]
  Furthermore, this process can be done in general for any $m \times n$ block matrix $X$ and $n \times p$ block matrix $Y$. 
  \end{theorem}

  \begin{theorem}
  Given that $I_N, A, B$ are $n \times n$ matrices, define the $(2n) \times (2n)$ matrix 
  \[X = \begin{pmatrix}
  I & 0 \\ A & B
  \end{pmatrix}\]
  Then 
  \[\det{X} = \det{B}\]
  \end{theorem}
  \begin{proof}
  We can perform Gauss elimination to reduce $X$ without affecting the determinant.
  \[\det{\begin{pmatrix}
  I&0\\A&B
  \end{pmatrix}} = \det{
  \begin{pmatrix}
  I&0\\
  0&B
  \end{pmatrix}} = \det{B}\]
  since it satisfies the correct properties for $\det{B}$. 
  \end{proof}

  \begin{corollary}
  \[\det{\begin{pmatrix}
  A&0\\C&D
  \end{pmatrix}} = \det{A} \det{D}\]
  \end{corollary}

  \begin{proof}
  \[ \det{\begin{pmatrix}
  A&0\\C&D 
  \end{pmatrix}} = \det{\begin{pmatrix}
  A&0\\C&I
  \end{pmatrix} \begin{pmatrix}
  I&0\\0&D
  \end{pmatrix}} = \det{\begin{pmatrix}
  A&0\\C&I
  \end{pmatrix}} \det{\begin{pmatrix}
  I&0\\0&D
  \end{pmatrix}}\]
  \end{proof}

  However, 
  \[\det{\begin{pmatrix}
  A&B\\C&D
  \end{pmatrix}} \neq \det{A} \det{D} - \det{B} \det{C}\]

  Rather, we introduce the following theorem

  \begin{theorem}
  \begin{align}
      \det{\begin{pmatrix} A&B\\C&D \end{pmatrix}}  & = \det{(A)} \det{(D - C A^{-1} B)} \\
      & = \det{(D)} \det{(A - B D^{-1} C)}
  \end{align}
  \end{theorem}
  \begin{proof}
  \[\begin{pmatrix} A&B\\C&D\end{pmatrix} = \begin{pmatrix}
  A&0\\C&I\end{pmatrix} \begin{pmatrix}
  I& A^{-1} B \\ 0 & D - C A^{-1} B
  \end{pmatrix}\]
  by similarity, equation $(6)$ is equal to equation $(7)$. 
  \end{proof}

  \begin{definition}
  A \textbf{block diagonal matrix} is a square matrix in block form such that the diagonal blocks are square matrices and all off-diagonal blocks are zero matrices. 
  \[A = \begin{pmatrix}
  A_1&0&\ldots&0\\
  0&A_2&\ldots&0\\
  \vdots&\vdots&\ddots&\vdots\\
  0&0&\ldots&A_k
  \end{pmatrix}\]
  \end{definition}

  \begin{theorem}
  Given a matrix $A$ in block diagonal form, with diagonal blocks $A_1, A_2, ..., A_k$,
  \[\det{A} = \prod_{i=1}^k A_i, \; \; \Tr{A} = \sum_{i=1}^k \Tr{A_i}\]
  Furthermore, $A$ is invertible if and only if all the $A_i$'s are invertible, and 
  \[A^{-1} = \begin{pmatrix}
  A_1&0&\ldots&0\\
  0&A_2&\ldots&0\\
  \vdots&\vdots&\ddots&\vdots\\
  0&0&\ldots&A_k
  \end{pmatrix} = \begin{pmatrix}
  A_1^{-1}&0&\ldots&0\\
  0&A_2^{-1}&\ldots&0\\
  \vdots&\vdots&\ddots&\vdots\\
  0&0&\ldots&A_k^{-1}
  \end{pmatrix}\]
  \end{theorem}
  \begin{proof}
  The results are obvious when performing block multiplication or Gauss Elimination. 
  \end{proof}

\subsection{Dodgson Condensation}

  We already know that the LUP decomposition is an algorithm used to compute the determinant of a general $n \times n$ matrix. We will introduce another, called \textbf{Dodgson condensation}. The algorithm can be described in the following steps.

  \begin{enumerate}
      \item Let $A$ be a given $n \times n$ matrix. Arrange $A$ so that no zeros occur in its interior (this can be done by any combination of elementary row or column operations that would not change the determinant). 
      \item Create an $(n-1) \times (n-1)$ matrix $B$ consisting of the determinants of every $2 \times 2$ submatrix of $A$. Explicitly, 
      \[B = \det{\begin{pmatrix}
      a_{i,j} & a_{i,j+1} \\ a_{i+1,j} & a_{i+1,j+1}
      \end{pmatrix}}\]
      \item With this $(n-1) \times (n-1)$ matrix $B$, perform step $2$ to obtain an $(n-2) \times (n-2)$ matrix $C$. Divide each term in $C$ by the corresponding term in the interior of $A$. 
      \[C_{i,j} = \det{\begin{pmatrix}
      b_{i,j} & b_{i,j+1} \\ b_{i+1,j} & b_{i+1,j+1}
      \end{pmatrix}} \bigg/ a_{i+1,j+1}\]
      \item Let $A = B$ and $B=C$. Repeat step $3$ as necessary until the $1 \times 1$ matrix is found, which is the determinant. 
  \end{enumerate}

  The reason that we do not want $0$s in $A$ is because then in doing step $3$ we may divide by $0$. 

  \begin{example}
  Let us find
  \[\det{\begin{pmatrix}
  -2&-1&-1&-4\\-1&-2&-1&-6\\-1&-1&2&4\\2&1&-3&-8
  \end{pmatrix}}\]
  All of the interior elements are nonzero, so there is no need to rearrange the matrix. We calculate
  \[\begin{pmatrix}
  -2&-1&-1&-4\\-1&-2&-1&-6\\-1&-1&2&4\\2&1&-3&-8
  \end{pmatrix} \rightarrow \begin{pmatrix}
  3&-1&2\\-1&-5&8\\1&1&-4
  \end{pmatrix} \rightarrow \begin{pmatrix}
  -16&2\\4&12
  \end{pmatrix}\]
  With this $2 \times 2$ matrix, we must divide each term by the interior of the original $A$. 
  \[\begin{pmatrix}
  -16/-2 & 2/-1\\4/-1 & 12/2
  \end{pmatrix} = \begin{pmatrix}
  8&-2\\-4&6
  \end{pmatrix}\]
  Calculating this determinant gives $40$, and dividing by the interior of the $3 \times 3$ matrix $(-5)$ gives $\det{A} = 40/-5 = -8$. 
  \end{example}

\subsection{Matrix Calculus}

  There is nothing special about matrix calculus on its own, since matrices are themselves vectors; they can be sufficiently analyzed using vector calculus. Regardless, we will emphasize a few points. Let
  \[A: \mathbb{R} \longrightarrow \text{Mat}(m \times n, \mathbb{R})\]
  be a matrix valued differential function. That is, the $m \times n$ component functions of $A$ is differentiable. Then, just like in calculus, we introduce differentiation rules.
  \begin{align*}
      & \frac{d}{d x} \big( A(t) + B(t)\big) = \frac{d}{d t} A(t) + \frac{d}{d t} B(t) \\
      & \frac{d}{d x} \big( c A(t)\big) = c \frac{d}{d t} A(t)
  \end{align*}
  The scalar multiplication can actually be extended. By linearity (of matrix multiplication), we can say that if $A$ is independent of $t$, then 
  \[\frac{d}{d x} A B (x) = A \frac{d}{d x} B(x)\]
  The linearity of the derivative allows us to state more rules. Given that $v: \mathbb{R}^n \longrightarrow \mathbb{R}$ is a scalar valued function and $l \in (\mathbb{R}^n)^*$, then  
  \[\frac{d}{d x} l \big( v(x) \big) = l \bigg( \frac{d}{d x} v(x) \bigg)\]
  This result can be extended to when $v$ is replaced by matrix valued function $A$ and $l$ is replaced by $\phi: \text{Mat}(m \times n, \mathbb{R}) \longrightarrow \mathbb{R}$. 
  \[\frac{d}{d x} \phi \big( A(x) \big) = \phi \bigg( \frac{d}{d x} A(x) \bigg)\]
  Since the trace is a linear operator, we have the following theorem. 

  \begin{theorem}
  Given a linear function $A: \mathbb{R} \longrightarrow \text{Mat}(n, \mathbb{R})$ with paramater $x$, 
  \[\frac{d}{d x} \Tr{A} = \Tr \bigg( \frac{d}{d x} A \bigg)\]
  Note that $A$ in here really means $A(x)$.
  \end{theorem}

  The product rule of matrix calculus is similar.
  \[\frac{d}{d x} A B = \bigg(\frac{d}{d x} A\bigg) \cdot B + A \cdot \bigg(\frac{d}{d x} B \bigg)\]
  It is also noting that the derivative of the inner product of two vector valued functions $v, w: \mathbb{R} \longrightarrow \mathbb{R}^n$ is 
  \[\frac{d}{d x} \big( v(x), w(x) \big) = \Big( \frac{d}{d x} v(x), w(x) \Big) + \Big( v(x), \frac{d}{d x} w(x) \Big)\]

  \begin{definition}
  A matrix valued function $A$ is \textbf{invertible at a point $x \in \mathbb{R}$} if there exists a function, denoted $A^{-1}$ such that
  \[A(x) A^{-1} (x) = A^{-1}(x) A(x) = I\]
  where $I$ is the identity matrix. If there exists such $A^{-1}$ for all values $x \in \mathbb{R}$, then $A$ is said to be \textbf{invertible}. 
  \end{definition}

  \begin{theorem}
  Let $A$ be a matrix valued function, differentiable and invertible. Then, the function $A^{-1}$ is also differentiable and 
  \[\frac{d}{d x} A^{-1} = - A^{-1} \bigg( \frac{d}{d x} A \bigg) A^{-1}\]
  \end{theorem}
  \begin{proof}
  We derive this using the product rule. 
  \begin{align*}
      0 = \frac{d}{d x} I & = \frac{d}{d x} \big( A(x) A^{-1} (x) \big) \\
      & = A(x) \bigg(\frac{d}{d x} A^{-1} (x) \bigg) + \bigg( \frac{d}{d x} A(x) \bigg) A^{-1} (x) \\
      \implies \frac{d}{d x} A^{-1} (x) & = - A^{-1} (x) \bigg( \frac{d}{d x} A(x) \bigg) A^{-1}(x)
  \end{align*}
  \end{proof}
  Note that the chain rule is a rule of differentiaion that applies for scalar valued functions. That is, given $f: V \longrightarrow \mathbb{R}$ and $g: \mathbb{R} \longrightarrow V$ ($V$ vector space), 
  \[\frac{d}{d x} f \circ g (x) = f^\prime \big( g(x) \big) \cdot \frac{d}{d x} g(x)\]
  The $\cdot$ operation in the right hand side is the operation of multiplication in the field $\mathbb{R}$. But given $f: \text{Mat}(n, \mathbb{R}) \longrightarrow \mathbb{R}$ and $A \mathbb{R} \longrightarrow \text{Mat}(n, \mathbb{R})$, multiplication within the algebra of matrices are inherently different than component-wise operations, so the chain rule does not apply (it would apply if matrix multiplication was defined component-wise). 

  \begin{example}
  Let $f(A) \equiv A^2$, and let $A$ be a matrix valued function. Then, 
  \begin{align*}
      \frac{d}{d x} f \circ A(x) = \frac{d}{d x} \big( A(t) \big)^2 & = \bigg( \frac{d}{d x} A(x) \bigg) \cdot A(x) + A(x) \cdot \bigg( \frac{d}{d x} A(x) \bigg) \\
      & \neq 2 A(x) \cdot \frac{d}{d x} A(x)
  \end{align*}
  since matrix multiplication is in general not commutative. 
  \end{example}

  \begin{proposition}
  \[\frac{d}{d x} A^k = A^\prime A^{k-1} + A A^\prime A^{k-2} + ... + A^{k-2} A^\prime A + A^{k-1} A^\prime\]
  \end{proposition}
  \begin{proof}
  We inductively apply the product rule
  \[\frac{d}{d x} A^k = A^\prime A^{k-1} + A \frac{d}{d x} A^{k-1}\]
  \end{proof}

  \begin{corollary}
  Given any polynomial $p$ with $A$ a differentiable, square matrix valued function, if $A$ and $A^\prime$ commute, then 
  \[\frac{d}{d x} p(A) = p^\prime (A) A^\prime\]
  \end{corollary}
  \begin{proof}
  We can completely define differentiation over the vector space of polynomials with the formula
  \[\frac{d}{d x} A^k = k A^{k-1} A^\prime \; \forall k \in \mathbb{N}\]
  \end{proof}

  \begin{corollary}
  Given polynomial $p$ with $A$ a differentiable, square matrix valued function, 
  \[\frac{d}{d x} \Tr{p(A)} = \Tr{\big( p^\prime(A) \cdot A^\prime \big)}\]
  \end{corollary}
  \begin{proof}
  Use the cyclic trace property.
  \end{proof}

  \begin{definition}
  The \textbf{exponential map} is defined
  \[\text{exp}: \text{Mat}(n, \mathbb{C}) \longrightarrow \text{Mat}(n, \mathbb{C})\]
  where 
  \[e^A = I + A + \frac{1}{2!} A^2 + \frac{1}{3!} A^3 + ... = \sum_{k=0}^\infty \frac{1}{k!} A^k\]
  where $A^0 \equiv I$. This can clearly be extended to when $A$ is a square, matrix valued function. 
  \end{definition}

  This final theorem establishes the connection between the determinant and trace. 

  \begin{theorem}
  Given a differentiable square matrix valued function $A$ such that $A$ is invertible for a certain $x \in \mathbb{R}$, then 
  \[\frac{d}{d x} \log{\det{A}} = \Tr \bigg( A^{-1} \frac{d}{d x} A \bigg)\]
  Where the log mapping is the inverse of the exponential mapping of matrices. 
  \end{theorem}

  \begin{definition}
  The \textbf{commutator} in the algebra of $n \times n$ matrices is defined as 
  \[[A, B] = A B - B A\]
  \end{definition}

  \begin{theorem}
  If $A$ and $B$ are commuting square matrices, then 
  \[e^{A + B} = e^A \, e^B\]
  In general, the solution $C$ to the equation
  \[e^{A} \, e^B = e^C\]
  is given by the \textbf{Baker-Campbell-Hausdorff formula}, defined
  \[C = A + B + \frac{1}{2}[A,B] + \frac{1}{12} [A,[A,B]] - \frac{1}{12} [B,[A,B]] + ...\]
  consisting of terms involving higher commutators of $A$ and $B$. The full series is much too complicated to write, so we ask the reader to be satisfied with what is shown. 
  \end{theorem}

  \begin{corollary}
  \[\Tr{\log{e^A \, e^B}} = \Tr{A} + \Tr{B}\]
  \end{corollary}


\section{Metrics, Norms, and Inner Products} 

  Given a vector space $V$, we can induce different structures on it to allow us to conduct different measurements on it. For example, the endowment of the basis structure on $V$ allows us to represent vector as an $n$-tuple of scalars. Some structures may induce other structures, such as the inner product inducing a norm or a metric inducing a norm. We will begin by defining these structures. It must be further noted that in order to induce such structures on $V$, its base field $\mathbb{F}$ must be ordered. We will treat $\mathbb{F} = \mathbb{C}$ for the following definitions. 

  \begin{definition}[Metric]
    A \textbf{metric} on a vector space $V$ over field $\mathbb{C}$ is a mapping
    \begin{equation}
      d: V \times V \longrightarrow \mathbb{R}
    \end{equation}
    satisfying three properties 
    \begin{enumerate}
      \item $d(x, y) = d(y, x)$
      \item $d(x, y) \geq 0$, with $d(x,y) = 0 \iff x=y$
      \item $d(x, y) + d(y,z) \geq d(x,z)$
    \end{enumerate}
    A metric allows us to define some notion of distance in $V$. A vector space $V$ with a metric is called a \textbf{metric space}, denoted $(V, d)$. 
  \end{definition}

  \begin{definition}[Norm]
    A \textbf{norm} on a vector space $V$ over field $\mathbb{C}$ is a mapping 
    \begin{equation}
      \rho: V \longrightarrow \mathbb{R}
    \end{equation}
    satisfying three properties 
    \begin{enumerate}
      \item $\rho (x) \geq 0$, with $\rho(x) = 0 \iff x = 0$
      \item For $a \in \mathbb{C}$, $\rho (a x) = |a| \rho(x)$ 
      \item $\rho(x + y) \leq \rho(x) + \rho(y)$ 
    \end{enumerate}
    A norm allows us to define some notion of a magnitude or length on each vector in $V$. A vector space $V$ with a norm is called a \textbf{normed space}, denoted $(V, \rho)$. 
  \end{definition}

  \begin{example}[Absolute Value]
    The absolute value function $|\cdot|: \mathbb{C} \longrightarrow \mathbb{R}_+$ is an example of a norm on the 1 dimensional space $\mathbb{C}$. 
  \end{example}

  \begin{example}[Euclidean Norm, $L_2$-Norm]
    The Euclidean norm of a vector $x \equiv (x_1, x_2, ..., x_n)^T \in \mathbb{R}^n$ is defined
    \begin{equation}
      ||x||_2 \equiv \bigg( \sum_{i=1}^n x_i^2 \bigg)^{\frac{1}{2}}
    \end{equation}
    This is the most commonly used norm in $\mathbb{R}^n$. 
  \end{example}

  \begin{example}[Taxicab Norm, Manhattan Norm]
    The Taxicab norm of $x \equiv (x_1, x_2, ..., x_n)^T \in \mathbb{R}^n$ is defined
    \begin{equation}
      ||x||_1 \equiv \sum_{i=1}^n |x_i|
    \end{equation}
  \end{example}

  \begin{example}[Infinity Norm, $L_\infty$-Norm]
    The Infinity norm of vector $x \equiv (x_1, x_2, ..., x_n)^T \in \mathbb{R}^n$ is defined
    \begin{equation}
      ||x||_\infty \equiv \max{\{|x_1|, |x_2|, ..., |x_n|\}}
    \end{equation}
  \end{example}

  \begin{example}[p-norm, $L_p$-Norm]
    Let $p\geq 1$ be a real number. The p-norm of a vector 
    \begin{equation}
      x \equiv (x_1, x_2, ..., x_n)^T \in \mathbb{R}^n
    \end{equation}
    is defined
    \begin{equation}
      ||x||_p \equiv \bigg( \sum_{i=1}^n x_i^p \bigg)^{\frac{1}{p}}
    \end{equation}
    For $0<p<1$, this function could be of some use, but it is not considered a norm since it violates the triangle inequality. When $p = 1$ and $p =2$, the norm is the Taxicab norm and Euclidean norm, respectively, and 
    \begin{equation}
      \lim_{p \rightarrow \infty} ||\cdot||_p = ||\cdot||_\infty
    \end{equation}
  \end{example}

  \begin{definition}[Seminorm]
    A \textbf{seminorm}, or a pseudo-norm, has the same properties except that $\rho(x) = 0$ does not necessarily imply that $x = 0$. That is, nonzero vectors can have norms of $0$. 
  \end{definition}

  \begin{theorem}[Norm Induces Metric]
    Every norm induces a metric in the following way
    \begin{equation}
      d(x, y) \equiv \rho(x-y)
    \end{equation}
    However, a metric does not necessarily induce a norm because the definition
    \begin{equation}
      \rho(x) \equiv d(x, 0)
    \end{equation}
    is not guaranteed to have all properties of the norm. 
  \end{theorem}

  \begin{definition}[Inner Product]
    An \textbf{inner product} on a vector space $V$ over field $\mathbb{C}$ is a mapping 
    \begin{equation}
      (\cdot, \cdot): V \times V \longrightarrow \mathbb{R}
    \end{equation}
    satisfying three properties 
    \begin{enumerate}
      \item First Argument Linearity: $(\lambda x + \mu y, z) = \lambda (x, z) + \mu (y, z)$
      \item Conjugate symmetry: $(x, y) = \bar{(y, x)}$
      \item $(x, y) \geq 0$, with $(x, y) = 0 \iff x = y$
    \end{enumerate}
    An inner product allows us to define some notion of an angle between two vectors in $V$. A vector space $V$ with an inner product is called an \textbf{inner product space}. Note that when the field is $\mathbb{C}$, the inner product is \textbf{sesqui-linear}, that is, linear with respect to the first argument and \textbf{skew linear} with respect to the second. When $\mathbb{R}$, it is bilinear. 
  \end{definition}

  The inner product of a vector space $V$ over $\mathbb{R}$ is an element of $V^* \otimes V^*$. This concept of the metric tensor occurs when studying Riemannian manifolds in general relativity. 

  \begin{definition}[Inner Product Induces Norm]
    An inner product induces a norm in the following way
    \begin{equation}
      ||x|| \equiv \sqrt{(x,x)} 
    \end{equation}
  \end{definition}

  \begin{theorem}[Schwarz Inequality]
    For all $x, y \in V$, 
    \begin{equation}
      |(x, y)| \leq ||x|| ||y||
    \end{equation}
  \end{theorem}

  \begin{example}[Dot Product]
    Given vectors $x, y \in \mathbb{R}^n$, 
    \begin{equation}
      x \cdot y \equiv  \begin{pmatrix}
      x_1\\x_2\\\vdots\\x_n
      \end{pmatrix} \cdot \begin{pmatrix}
      y_1\\y_2\\\vdots\\y_n
      \end{pmatrix} \equiv \sum_{i=1}^n x_i y_i
    \end{equation}
  \end{example}

  \begin{example}[Integral Product]
    Let $C^0[a, b]$ be the space of all continuous real-valued functions defined over the interval $[a,b] \subset \mathbb{R}$. Given $f, g \in C^0[a,b]$, 
    \begin{equation}
      (f, g) \equiv \int_a^b f(x)g(x) d x
    \end{equation}
    is an inner product on $C^0[a, b]$. 
  \end{example}

  \begin{theorem}[Pythagorean Theorem]
    \begin{equation}
      ||x||^2 + ||y||^2 = ||x+y||^2
    \end{equation}
  \end{theorem}

  \begin{theorem}
    \begin{equation}
      ||x|| = \max_{||y||=1} (x, y)
    \end{equation}
  \end{theorem}

  \begin{definition}[Orthogonal Vectors]
    Two vectors $x, y$ of an inner product space are said to be \textbf{orthogonal} if 
    \begin{equation}
      (x, y) = 0
    \end{equation}
  \end{definition}

  Note that the definition of orthogonality is dependent on the definition of the inner product. If the inner product is defined differently, then orthogonality will be defined differently. In the case when the inner product is defined to be the dot product, orthogonality is defined to be the "normal" perpendicularity between vectors. We can further define subspaces to be orthogonal. 

  \begin{definition}[Orthogonal Subspaces]
    Two subspaces $Y, Z$ of inner product space $Z$ are said to be orthogonal to each other if 
    \begin{equation}
      (y, z) = 0 \text{   for every } y \in Y, z \in Z
    \end{equation}
  \end{definition}

  \begin{definition}[Orthogonal Complement]
    Given a subspace $Y$ of inner product space $X$, the \textbf{orthogonal complement} of $Y$, denoted $Y^\perp$, is defined
    \begin{equation}
      \{ x \in X \; | \; (x, y) = 0 \;\;\; \forall y \in Y\}
    \end{equation}
    which is the set of all vectors in $X$ orthogonal to every vector in $Y$. Clearly, $Y \oplus Y^\perp = X$. 
  \end{definition}

  The concept of orthogonality also allows us to define orthogonal projections onto a vector or subspace. 

  \begin{definition}[Orthogonal Projectjion]
    Let $x \in X$ and let $Y$ be a subspace of $X$. Then $x$ can be decomposed into the form $x = y + z, \; y \in Y, z \in Y^\perp$. The \textbf{orthogonal projection} of $x$ onto $Y$ is then defined as 
    \begin{equation}
      \text{proj}_Y (x) = y
    \end{equation}
    Orthogonal projections are linear transformations. 
  \end{definition}

  \begin{theorem}
    Given that $x \in \mathbb{R}^n$ is projected onto a 1-dimensional subspace $Y$. The orthogonal projection of $x$ onto $Y$ can be computed with the formula 
    \begin{equation}
      \text{proj}_Y (x) = \frac{x \cdot y}{||y||^2} y
    \end{equation}
    where $y$ is an arbitrary vector in $Y$ and $\cdot$ is the dot product in $\mathbb{R}^n$. Furthermore, for a $k$-dimensional subspace $Y$, we can calculate the projection by first adding up the projections of $x$ onto a set of basis vectors of $Y$ and then adding them up. That is, given basis $r_1, r_2, ..., r_k$ of $Y$,
    \begin{equation}
        \text{proj}_Y (x) = \sum_{i=1}^k \text{proj}_{r_i} (x) = \sum_{i=1}^k \frac{x \cdot r_i}{||r_i||^2} r_i 
    \end{equation}
  \end{theorem}

  \begin{theorem}[Orthonormal Basis in Hilbert Space]
    Every inner product space has a basis consisting of vectors that are pairwise orthogonal, called an \textbf{orthogonal basis}. Furthermore, each vector in the orthogonal basis can be scaled to have magnitude 1, forming an \textbf{orthonormal basis}.  
  \end{theorem}

  \begin{proof}
    The algorithm used to construct an orthonormal basis is called \textbf{Grahm-Schmidt}. We start off with any basis, not necessarily orthonormal, of $X$, denoted $\{x_1, x_2, ..., x_n\}$. We first assign 
    \begin{equation}
      x_1 = l_1
    \end{equation}
    Then we take $x_2$ and find the orthogonal component (with respect to $l_1$) with the equation
    \begin{equation}
      l_2 = x_2 - \text{proj}_{l_1} (x_2)
    \end{equation}
    This creates an orthogonal basis for span$\{x_1, x_2\}$. Then we take $x_3$ and find the orthogonal component (with respect to span$\{l_1, l_2\}$. 
    \begin{equation}
      l_3 = x_3 - \text{proj}_{l_1} (x_3) - \text{proj}_{l_2} (x_3)
    \end{equation}
    This creates an orthogonal basis for span$\{x_1, x_2, x_3\}$. We repeat this process until we complete the basis of $X$, using the general equation
    \begin{equation}
      l_k = x_k - \sum_{i=1}^{k-1} \text{proj}_{l_k} (x_k) = x_k - \sum_{i=1}^{k-1} \frac{x_k \cdot l_k}{||l_k||^2} l_k, \; k = 1, 2, ..., n
    \end{equation}
    Finally, we take these orthogonal vectors and normalize them to magnitude 1. Note that this algorithm does not produce a unique orthonormal basis. Rather, it is highly un-unique. 
  \end{proof}

  Given that we have an orthonormal basis $\{r_i\}_{i=1}^k$ of subspace $Y$ in $\mathbb{R}^n$, we can more simply define 
  \begin{equation}
    \text{proj}_Y (x) = \sum_{i=1}^k (x \cdot r_i) r_i
  \end{equation}

  \begin{theorem}
    The inner product endowed on $V$ induces a natural isomorphism between $V$ and $V^\ast$. 
  \end{theorem}
  \begin{proof}
    We fix $y \in V$ and simply define the isomorphism to be. 
    \begin{equation}
      l(y) \equiv (x, y)
    \end{equation}
    which defines a bijection between $x \in V$ and $l \in V^*$. 
  \end{proof}

  Note that given vector spaces $U, V$, the set of all linear mappings $A: U \longrightarrow V$ also forms a vector space. More specifically, it is a rank (1,1) tensor product space. This means that we can define similar Euclidean structures on them. The norm of a matrix is worth mentioning. Note that the structures and concepts of metrics, norms, inner products, distances, magnitudes, orthogonality, and basis are not intrinsic properties of the vector space. So, we will not assume the existence of these structures unless otherwise stated or explicitly implied. 


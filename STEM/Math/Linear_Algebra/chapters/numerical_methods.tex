\section{Numerical Methods in Solving Linear Systems}

  In this section, we will concern ourselves with a system of equations with only \textit{one solution}, represented by the matrix equation
  \[A x = b\]
  where $A$ is an invertible square matrix, $b$ some given vector, and $x$ the vector of unknowns to be determined. 

  An algorithm for solving the system takes as inputs the matrix $A$ and vector $b$ and outputs some approximation to the solution $x$. However, with billions of arithmetic operations on top of each other, the errors can accumulate. Algorithms for which this does not happen are said to be \textbf{arithmetically stable}. 

  The use of finite digit arithmetic places an absolute limitation on the accuracy with which the solution can be determined. To demonstrate this, let us imagine a change $\delta b$ being made in the vector $b$, which causes a corresponding change in $x$, denoted $\delta x$. 
  \[A(x + \delta x) = b + \delta b \implies A \delta x = \delta b\]
  To compare the changes in $x$ with the changes in $b$, we define the following variable. 
  \begin{definition}
  The \textbf{relative change in x with the relative change in $b$} is the quantity
  \[\frac{|\delta x|}{|x|} \bigg/ \frac{|\delta b|}{|b|}\]
  where the norm is convenient for the problem (usually a numerical approximation of the Euclidean norm for floating point numbers). We will assume the use of the Euclidean norm from now on. We can rewrite the value as the expression below with the following upper bound, denoted by $\kappa (A)$, called the \textbf{condition number}. 
  \[\frac{|b|}{|x|} \frac{|\delta x|}{|\delta b|} = \frac{|Ax|}{|x|} \frac{|A^{-1} \delta b|}{|\delta b|} \leq |A||A^{-1}| \equiv \kappa (A)\]
  where $|A|$ is the matrix norm of $A$. 
  A high value of this relative change would mean that small perturbations in $b$ would cause large changes in $x$.
  \end{definition}

  Note that $\kappa(A) \geq 1$. Notice also that the higher the condition number $\kappa (A)$, the harder it is to solve the equation $A x = b$, and $\kappa(A) = \infty$ when $A$ is not invertible. For a $k$-digit floating point arithmetic, the relative error in $b$ can be as large as $10^{-k}$, meaning that the relative error in $x$ can be as large as $10^{-k} \kappa (A)$. 

  Let $\beta$ be the largest absolute value of the eigenvalues of $A$ and $\alpha$ as the smallest absolute value of the eigenvalues of $A$. Then 
  \[\beta \leq |A|, \frac{1}{\alpha} \leq |A^{-1}| \implies \frac{|\beta|}{|\alpha|} \leq \kappa(A)\]


  \begin{definition}
  An algorithm that generates an exact solution after a finite number of arithmetic steps is called a \textit{direct method} (e.g. Gauss Elimination). An algorithm that generates successive approximations that converge onto the solution is called an \textit{iterative method}. 
  \end{definition}

  The methods mentioned in this section will be iterative. 
  \begin{definition}
  Let $\{ x_n\}$ be the sequence of approxmations generated by such an algorithm. The deviation of $x_n$ from the true value $x$ is caelled the \textbf{error at the $n$th stage}, denoted by $e_n$. 
  \[e_n \equiv x_n - x\]
  The amount by which the $n$th approximation fails to satisfy the equation $Ax = b$ is called the $n$th residual, denoted by $r_n$. 
  \[r_n \equiv A x_n - b\]
  Error and residual are related by the equation. 
  \[r_n = A e_n\]
  \end{definition}
  Note that since we do not know $x$, the error cannot be calculated, but the residuals can be. We further restrict our scope to solving linear systems in which $A$ is real, positive, and self-adjoint. Clearly, we already know that $|A| = \beta$, and since $A$ is positive, we can conclude that 
  \[|A^{-1}| = \frac{1}{\alpha}\]
  which implies that
  \[\kappa(A) = \frac{\beta}{\alpha}\]

\subsection{Method of Steepest Descent}

  Assume that $n \times n$ matrix $A$ is self-adjoint.
  \begin{theorem}
  The solution of $Ax = b$ minimizes the functional 
  \[E (y) \equiv \frac{1}{2} (y, A y) - (y, b)\]
  where $(\cdot, \cdot)$ is the Euclidean dot product. That is, the solution $x$ is
  \[x = \min \big\{ E(y) \big\} = \min \Big\{ \frac{1}{2} (y, Ay) - (y, b) \Big\}\]
  \end{theorem}
  \begin{proof}
  Add to $E(y)$ a constant, that is a term independent of $y$ to define a new function $F$. 
  \[F(y) \equiv E(y) + \frac{1}{2} (x, b)\]
  Then, by self adjointness of $A$, we can express $F(y)$ as 
  \begin{align*}
      F(y) & = \frac{1}{2} (y, Ay) - (y, b) + \frac{1}{2} (x, b) \\
      & = \frac{1}{2} (y, Ay) - \frac{1}{2} (y, Ax) + \frac{1}{2} (Ax, x) - \frac{1}{2} (Ay, x) \\
      & = \frac{1}{2} \big( y, A(y-x)\big) + \frac{1}{2} \big( A(x-y), x\big) \\
      & = \frac{1}{2} \big( y - x, A(y - x)\big) 
  \end{align*}
  Since $F(x) = 0$ and $F(x) \geq 0$ (since it is an inner product with repsect to $y-x$), $F(y)$, and also $E(y)$, takes a minimum at $y =x$. 
  \end{proof}

  Now to actually compute the value of $x$, we us the method of steepest descent. Note that $E: \mathbb{R}^m \longrightarrow \mathbb{R}$, so we can utilize ordinary calculus on it. The gradient of $E$ can be computed by the formula 
  \[\text{grad}\,E(y) = A y - b\]
  So, if our $n$th approximation is $x_n$, then the $(n+1)$st approximation, $x_{n+1}$, is calculated as
  \[x_{n+1} = x_n - s (Ax_n - b)\]
  where $s$ is the step length in the direction $-$grad$\,E$. By calculating the residual $r_n = A x_n - b$, we can rewrite the above to
  \[x_{n+1} = x_n - s r_n\]
  Rather than keeping $s$ constant, we can actually determine an optimal value of $s$ at the $n$th step, denoted $s_n$, which minimizes $E(x_{n+1})$. This quadratic minimum problem is easily solved, since
  \begin{align*}
      E(x_{n+1}) & = \frac{1}{2} \big(x_n - s r_n, A(x_n - s r_n) \big) - \big( x_n - s r_n, b\big) \\
      & = E(x_n) - s (r_n, r_n) + \frac{1}{2} s^2 (r_n A r_n)
  \end{align*}
  By taking the derivative and computing the value of $s$ where $E (x_{n+1}) = 0$, we see that the minimum is reached when 
  \[s = s_n \equiv \frac{(r_n, r_n)}{(r_n, A r_n)}\]

  \begin{theorem}
  The sequence of approximations $\{x_n\}$, with $s$ optimized to be $s_n$, converges to the solution of $A x = b$. 
  \end{theorem}

  The error bound for this algorithm is 
  \[||e_n||^2 \leq \frac{2}{\alpha} \bigg( 1-\frac{1}{\kappa(A)} \bigg)^n F(x_0)\]
  which shows that the error $e_n$ tends to $0$ in $\mathbb{R}^m$. However, this algorithm has a very slow rate of convergence for large $\kappa(A)$. 

\subsection{Method of Chebyshev Polynomials}

  the disadvantage of the method of steepest descent mentioned in the end of the last subsection renders it quite outdated and obsolete. this next method has a much better error bound that can handle large values of $\kappa$ more efficiently. however, we will need a positive lower bound $m$ for the smallest eigenvalue of $a$ and an upper bound $m$ for the largest eigenvalue. that is, 
  \[m \leq \alpha, \beta \leq m\]
  and all the eigenvalues of $a$ lie in the interval $[m, m]$. it follows that
  \[\kappa = \frac{\beta}{\alpha} < \frac{m}{m}\]
  we generate the same sequence of approximations $\{x_n\}$ by the same recursion formula
  \[x_{n+1} = x_n - s(a x_n - b) \iff x_{n+1} = (i - s_n a) x_n + s_n b\]
  since the solution of $x$ satisfies the formula; that is, since $x = (i - s_n a) x + s_n b$, we subtract this equation from the top to get
  \[e_{n+1} = (i - s_n a) e_n\]
  doing this recursively, we can deduce an explicit formula 
  \[e_n = p_n (a) e_0 = \prod_{n=1}^n (1 - s_n a)\]
  this allows us to estimate the size of $e_n$. 
  \[||e_n|| \leq ||p_n (a)|| ||e_0||\]
  the norm of a self adjoint matrix $a$ is the largest $|a|$, where $a$ is the eigenvalue, and the spectral mapping theorem states that the eigenvalues $p$ of $p_n (a)$ are of the form $p = p_n (a)$, where $a$ is an eigenvalue of $a$. this means that
  \[||a|| \leq \max_{m \leq a \leq m} |a| \implies ||p_n (a)|| \leq \max_{m \leq a \leq m} |p_n (a)|\]
  so, we are left with the bound
  \[||e_n|| \leq ||e_0|| \max_{m \leq a \leq m} |p_n (a)|\]
  to get the best estimate of $e_n$, we have to choose the $s_1, s_2, ..., s_n$ so that the polynomial $p_n$ has a small maximum on the interval $[m, m]$. note that the polynomial $p_n$ satisfies the normalizing condition 
  \[p_n(0) = 1\]
  to find such a polynomial, we must first define chebyshev polynomials. 

  \begin{definition}
  the \textbf{$n$th chebyshev polynomial} $t_n$ is defined for $-1 \leq u \leq 1$ by
  \[t_n (u) = \cos (n \theta), \;\; u = \cos(\theta)\]
  \end{definition}

  \begin{proposition}
  among all polynomials $p_n$ of degree $n$ that satisfy $p_n (0) = 1$, the one that has the smallest maximum on $[m, m]$ is the \textit{rescaled chebyshev polynomial} that rescales values from $[-1, 1]$ to the interval $[m, m]$ while preserving the condition that $p_n (0) = 1$. this polynomial is expressed as
  \[p_n (a) \equiv t_n \bigg( \frac{m+m-2a}{m-m} \bigg) \bigg/ t_n \bigg(\frac{m+m}{m-m} \bigg)\]
  \end{proposition} 
  now, assuming that $m/m \approx \kappa$, 
  \[t_n \bigg( \frac{m+m}{m-m} \bigg) = t_n \bigg(\frac{\frac{m}{m} + 1}{\frac{m}{m}-1} \bigg) \approx t_n \bigg(\frac{\kappa + 1}{\kappa - 1} \bigg)\]
  since $|t_n (u)| \leq 1$ for $|u| \leq 1$, this also implies that
  \[t_n \bigg( \frac{m + m - 2a}{m-m} \bigg) \leq 1\]
  combining this together, we get
  \[||e_n|| \leq ||e_0|| \max_{m \leq a \leq m} |p_n (a)| = ||e_0|| \bigg/ t_n \bigg( \frac{\kappa+1}{\kappa-1} \bigg)\]
  it is a fact that higher order chebyshev polynomials tend to infinity faster once the value reaches out of $[-1, 1]$, meaning that as $n \rightarrow \infty$, $t_n\big( (\kappa+1)/(\kappa-1)\big)$ will also tend to infinity (note that $(\kappa+1)/(\kappa-1)$ is a constant, implying that $e_n$ tends to $0$ as $n$ tends to infinity. the error bound for $e_n$ is given by the following 
  \[||e_n|| \leq 2 \bigg( 1 + \frac{2}{\sqrt{\kappa}} \bigg)^{-n} ||e_0|| \approx 2 \bigg( 1 - \frac{2}{\sqrt{\kappa}} \bigg)^{n} ||e_0||\]
  once again, this confirms that $e_n \rightarrow 0$ as $n \rightarrow \infty$. furthermore, when $\kappa$ is large, the error bound works with $\sqrt{\kappa}$, which is must smaller than $\kappa$ itself. so, $e_n$ converges much faster through this algorithm than through the method of steepest descent. 


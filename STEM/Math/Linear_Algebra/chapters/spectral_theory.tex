\section{Spectral Theory} 

\subsection{Spectral Theory of General Mappings}

  \begin{definition}
  Let $A: V \longrightarrow V$ be a linear transformation over $\mathbb{F}$. If there exists a vector $v \in V$ such that
  \[ A v = \lambda v, \; \lambda \in \mathbb{F}\]
  then $a$ is called an \textbf{eigenvalue} of $A$, and $v$ is an \textbf{eigenvector} of $A$. Clearly, if a basis is realized for $V$ and $A$ is represented as a matrix, $v$ would have a basis representation. However, the value of $\lambda$ is invariant. The set of all eigenvalues 
  \[\lambda(A) \equiv \{ \lambda_1, \lambda_2, ..., \lambda_k\}\]
  is called the \textbf{spectrum} of $A$. 
  \end{definition}

  For a given eigenvalue $\lambda$ and its corresponding eigenvector $v$, it is clear that by linearity, every vector in $\Span v$ is an eigenvector, too. 

  Now that we have defined eigenvalues and eigenvectors, we first provide a visual description of these terms. Given a linear transformation $A: V \longrightarrow V$, we can visualize a certain basis of $V$ such that all the linear transformation $A$ does on that basis is merely extend or contract the basis vectors.

  \begin{definition}
  Given a $n \times n$ matrix $A$, the \textbf{characteristic polynomial} of $A$, denoted $p_A (t)$, is defined
  \[ p_A (t) \equiv \det{(A - t I)}\]
  The mapping $A \mapsto p_A (t)$ can be thought of as a mapping from Mat$(n, \mathbb{F}) \longrightarrow \mathbb{F}[t]$, where Mat$(n, \mathbb{F})$ is the algebra of $n \times n$ matrices over field $\mathbb{F}$, and $\mathbb{F}[t]$ is the polynomial algebra over $\mathbb{F}$. $p_A (t)$ is invariant under matrix similarity. 
  \end{definition}

  The motivation for defining such a polynomial is that it allows us to compute the eigenvalues of $A$. 
  \begin{definition}
  The \textbf{characteristic equation} of $A$ is defined by equating $p_A (t) = 0$. 
  \end{definition}

  \begin{proposition}
  The solutions of the characteristic equation of $A$ (i.e. the roots of $p_A (t)$) is precisely the spectrum of $A$. 
  \end{proposition}

  \begin{proof} $(\rightarrow)$ Let there be a $t = \lambda$ such that $p_A (\lambda) = 0 \iff \det{(A - \lambda I)} = 0$ which is equivalent to saying that ker$(A - \lambda I)$ is nontrivial. There must exist a $v \in $ ker$(A - \lambda I)$, meaning that $(A - \lambda I) v = 0 \iff A v = \lambda v$. By definition, $\lambda$ is an eigenvalue of $A$. \\
  $(\leftarrow)$ This reasoning can be extended in the opposite direction. 
  \end{proof}

  \begin{theorem}
  Eigenvectors of a linear transformation $A$ corresponding to different eigenvalues are linearly independent, but not necessarily orthogonal. It follows that if the characteristic polynomial of a $n \times n$ matrix $A$ has $n$ distinct roots, then $A$ has $n$ linearly independent eigenvectors. 
  \end{theorem}

  \begin{proof}
  Simple, by contradiction.
  \end{proof}

  \begin{example}
  It is clear that the Fibonacci sequence can be produced with matrix multiplication as such 
  \[ \begin{pmatrix}
  a_{n+1} \\ a_n
  \end{pmatrix} = A^n \begin{pmatrix}
  a_1 \\ a_0
  \end{pmatrix} = \begin{pmatrix}
  1&1\\1&0
  \end{pmatrix}^n \begin{pmatrix}
  1\\1
  \end{pmatrix}\]
  Given that 
  \[\lambda_1 = \frac{1+\sqrt{5}}{2}, \; \lambda_2 = \frac{1 - \sqrt{5}}{2}\]
  we can diagonalize $A$ into the form 
  \[A = \begin{pmatrix}
  \frac{1}{\lambda_1 - \lambda_2} & \frac{\lambda_2}{\lambda_2 - \lambda_1} \\
  \frac{1}{\lambda_2 - \lambda_1} & \frac{\lambda_1}{\lambda_1 - \lambda_2}
  \end{pmatrix} \begin{pmatrix}
  \lambda_1 & 0 \\ 0 & \lambda_2 
  \end{pmatrix} \begin{pmatrix}
  \lambda_1 & \lambda_2 \\
  1 & 1
  \end{pmatrix} \implies A^n = S^{-1} \begin{pmatrix}
  \lambda_1^n & 0 \\ 0 & \lambda_2^n 
  \end{pmatrix} S\]
  which implies that after evaluating, we get 
  \[ a_n = \frac{1}{\sqrt{5}} \bigg( \Big(\frac{1+\sqrt{5}}{2}\Big)^n - \Big(\frac{1-\sqrt{5}}{2}\Big)^n \bigg)\]
  This is a surprising result since it also says that the expression above is always an integer for all natural number $n$. 
  \end{example}
  \begin{definition}
  Given a subspace $U_1 \subset U$ and linear transformation $T: U \longrightarrow U$. We say that $U_1$ is \textbf{invariant} under $T$ if 
  \[u \in U_1 \implies T u \in U_1\]
  \end{definition}

  \begin{theorem} 
  Let $a_1, a_2, ..., a_n$ be the eigenvalues of $A$. Then 
  \[\sum_i a_i = \Tr{A}, \;\;\; \prod_i a_i = \det{A}\]
  \end{theorem}

  \begin{proof}
  The mapping $A \mapsto \det{(A - x I)}$ is a mapping from the set of $n \times n$ matrices to the polynomial algebra $\mathbb{F}[x]$. Direct application of the Viete's formulas in $\mathbb{F}[x]$ produces the statement and this result can be extended to the rest of the formulas. 
  \end{proof}

  \begin{theorem}[Spectral Mapping Theorem]
  Let $q$ be any polynomial, $A$ a square matrix with an eigenvalue $a$. Then: 

  i) $q(a)$ is an eigenvalue of $q(A)$. 

  ii) Every eigenvalue $q(A)$ is of the form $q(a)$, where $a$ is an eigenvalue of $A$. 
  \end{theorem}
  \begin{proof} 
  i) Let $h$ be an eigenvector of A with corresponding eigenvalue $a$. 
  \begin{align*}
      Ah = ah & \implies A^{2} h = Aah = aAh = a^{2} h \\
   & \implies A^{n} h = a^{n} h   \\
   & \implies q(A)h = q(a)h \\
   & \implies q(a) \text{ is an eigenvalue of }q(A)
  \end{align*}
  ii) Let $p$ be the eigenvalue of $q(A) \iff \det{\big(q(A) - p I\big)} = 0$. We expand: 
  $$ q(s) - p = c\prod \big(s-r_{i}\big), r_{i} \in \mathbb{C} $$ 
  Replacing the variable $s$ with $A$, we have
  $$ q(A) - pI = c \prod \big(A-r_{i}I\big) $$
  Since $\det{\big( q(A) - pI\big)} = 0$, at least one $r_{i}$, say $r_{k}$ exists such that $\det{\big( A - r_{k} I \big)} = 0 \iff r_{k}$ is an eigenvalue of $A$. Since $q(r_{j})-p = 0$, $p = q(r_{j})$ is an eigenvalue of $q(A)$. 
  \end{proof}

  The following theorem is an equivalent version of the spectral mapping theorem.
  \begin{theorem}
  Let $A$ be a $n \times n$ matrix and let $f$ be a polynomial. If the chracteristic polynomial of $A$ has factorization 
  \[p_A (t) = \prod_{i = 1}^n (t - \lambda_i)\]
  then the characteristic polynomial of the matrix $f(A)$ is given by 
  \[p_{f(a)} (t) = \prod_{i = 1}^n (t - f(\lambda_i))\]
  \end{theorem}

  We can actually create a bound on the spectrum of a square matrix. 

  \begin{theorem}[Gershgorin Circle Theorem]
  Let $A \in $ Mat$(n, \mathbb{C})$ with entries $a_{i j}$. Let $R_i = \sum_{i \neq j} |a_{i j}|$ be the sum of the absolute values of the non-diagonal entries of the $i$th row, and let $D_(a_{i i}, R_i) \subset \mathbb{C}$ be a closed disk with radius $R_i$ centered at $a_{i i}$ in the complex plane, called a \textbf{Gershgorin Disk}. Then every eigenvalue of $A$ lies within the union of all $n$ Gershgorin Disks. That is, 
  \[ \lambda_j (A) \in \bigcup_{i= 1}^{n} D_(a_{i i}, R_i) \subset \mathbb{C}, \text{ for all } j\]
  \end{theorem}

  \begin{proof}
  Let $\lambda$ be an eigenvalue of $A$ with its eigenvector $v = (v_j)$. Scale $v$ by multiplying it by $ \pm 1 / \max{\{|v_j|\}_j}$ to get a vector $x$ with its maximal entry $x_i = 1$ and $|x_j| \leq 1, \; j \neq i$. Then, 
  \[A x = \lambda x \implies \sum_{j} a_{i j} x_j = \lambda x_i = \lambda \implies \sum_{j \neq i} a_{i j} x_j + a_{i i} = \lambda\]
  Applying the triangle inequality, 
  \[| \lambda - a_{i i} | = \bigg| \sum_{j \neq i} a_{i j} x_j\bigg| \leq \sum_{j \neq i} |a_{i j}| |x_j| \leq \sum_{j \neq i} |a_{i j}| = R_i\]
  \end{proof}

  \begin{corollary}
  The eigenvalues of $A$ must also lie within the Gershgorin discs $C_j$ corresponding to the columns of $A$. 
  \end{corollary}

  \begin{proof}
  This is a direct result from the fact that $A$ is similar to $A^T$. Alternatively, we can apply the same process in the proof above to $A^T$.
  \end{proof} 

  If one observes that the off-diagonal entries of $A$ are small in absolute value, it can be concluded that the diagonal entries are "close" to the true eigenvalues of $A$. $A$ is diagonal if and only if the Gershgorin disks are points. 

  \begin{theorem}[Cayley Hamilton]
  Every matrix $A$ satisfies its own characteristic equation. That is, 
  \[ p_{A}(A) = 0\]\
  \end{theorem} 

\subsection{Eigendecompositions and Jordan Normal Form}

  However, the entire concept of matrices are not fully grasped with just eigenvectors. If it were, then linear algebra would be a much simpler matter. To extend our toolkit, we must introduce generalized eigenvectors. From here, we will assume that our field is over $\mathbb{C}$. We use the fact that the field is over $\mathbb{C}$ because it allows us to claim that the characteristic polynomial in $\mathbb{C}[t]$ can be factored into linear components, by the fundamental theorem of algebra. 

  \begin{definition}
  A genuine eigenvector of $A$ satisfies $(A-aI)h = 0$. A \textbf{generalized eigenvector} $f$ satisfies $(A-a I)^{d} f = 0$ for some $d \geq 1$. 
  \end{definition}

  To provide a visual intuition of how generalized eigenvectors transform under $A$, observe that 
  \begin{align}
      (A - aI) h = 0 \text{ and } (A - aI)^2 f = 0 & \implies (A - aI)^f = h \\
      & \implies A f = a f + h, \; Ah = ah \\
      & \implies A^2 f = a A f + A h = a^2 f + 2 a h \\
      & \implies A^N f = A^N f + N a^{N-1} h 
  \end{align}
  This implies that the generalized eigenvector is first scaled by a factor of $a$, similar to a genuine eigenvector, but then a factor of the genuine eigenvector is then added to the scaled generalized one. Note that in higher dimensions of $N$, a greater multiple of $h$ must be added after scaling $f$. 

  This means that given an eigenvalue $\lambda$, there is always at least one genuine eigenvalue associated with $\lambda$. Furthermore, there may be additional generalized eigenvectors also corresponding to $\lambda$. This leads to the following definition

  \begin{definition}
  The subspace formed by the span of the generalized (and genuine) eigenvectors of $\lambda$ form what is called the \textbf{eigenspace associated with $\lambda$}, denoted $E(\lambda)$. 
  \end{definition}

  We can measure the characteristics of the eigenspaces with the following definitions. 

  \begin{definition}
  The \textbf{algebraic multiplicity} of an eigenvalue $\lambda$ is the dimension of its eigenspace. It is precisely
  \[\dim{E(\lambda)}\]  
  In order to compute the algebraic multiplicity of $\lambda_i$ in $A$, we find the maximal value of $d_i$ such that $(t-\lambda)^{d_i}$ divides $p_A (t)$. With this, we can define 
  \[E(\lambda) = \ker{(A - \lambda_i I)^{d_i}}\]
  \end{definition} 

  \begin{theorem}
  Given $A: V \longrightarrow V$ with eigenspaces $E(\lambda_1), E(\lambda_2), ..., E(\lambda_k)$, 
  \[E(\lambda_1) \oplus E(\lambda_2) \oplus ... \oplus E(\lambda_k) = V\]
  That is, every vector $v \in V$ can be uniquely expressed as the sum 
  \[ v = h_1 + h_2 + ... + h_k, \; h_i \in E(\lambda_i)\]
  this is called the \textbf{eigenbasis of $V$}. 
  \end{theorem}
  \begin{proof}
  The definition of algebraic multiplicity implies that each eigenspace is disjoint except at $0$ and that their dimensions sum to $\dim{V}$. 
  \end{proof}

  \begin{definition}
  The \textbf{geometric multiplicity} of an eigenvalue $\lambda$ of a linear transformation $A$ is the dimension of the span of genuine eigenvectors in its eigenspace. It is precisely 
  \[\dim{\ker{(A - \lambda I)}}\]
  Note that since the span of genuine eigenvectors is a subspace of $E(\lambda)$, the geometric multiplicity is always less than or equal to the algebraic multiplicity. 
  \end{definition}

  Now we are ready to introduce the eigendecomposition of a linear mapping $A$.

  \begin{theorem}
  Given a linear mapping $A$ with its eigenvalues $\lambda_1, \ldots, \lambda_k$ and associated eigenspaces $E(\lambda_1), \ldots, E(\lambda_k)$, $A$ maps each eigenspace to itself. That is, 
  \[A\big( E(\lambda_i) \big) \subset E(\lambda_i), \; i = 1, 2, ..., k\]
  \end{theorem}

  \begin{corollary}[Jordan Normal Form]
  Every linear mapping $A: V \longrightarrow V$ can be decomposed into the sum of the linear mappings of each eigenspace $E(\lambda_i)$. That is, it can be expressed in the form 
  \[A: \prod_i E(\lambda_i) \longrightarrow \prod_i E(\lambda_i)\]
  which we can define, given $h_i \in E(\lambda_i)$, 
  \[A(v) = A\bigg(\sum_i h_i \bigg) = \sum_i A(h_i), \; A(h_i) \in E(\lambda_i) \]
  \end{corollary}

  The process of eigendecomposition for a linear mapping $A$ is really just a clever change of basis for the $n \times n$ matrix representation of $A$ over $\mathbb{C}$, where the new basis is now the set of genuine and generalized eigenvectors. The new matrix formed by performing the change of basis on matrix $A$ is called the \textbf{Jordan Normal Form}, or \textbf{Jordan Canonical Form}, of $A$. We will now describe the construction of the JNF of an arbitrary $n \times n$ matrix. 

  It is actually simple. Let the eigenvalues of the matrix $A$ be $\lambda_1, \lambda_2, ..., \lambda_k$, with its associated eigenspaces $E(\lambda_i)$. Let the algebraic multiplicity of eigenspace $E(\lambda_i)$ be $alg_i$. Then, every $n \times n$ matrix over $\mathbb{C}$ has the block form 
  \[ J = \begin{pmatrix}
  A_1&0&0&0\\
  0&A_2&0&0\\
  0&0&...&0\\
  0&0&0&A_k
  \end{pmatrix}\]
  where each block $A_i$ represents the transformation in $E(\lambda_i)$. This means that each $A_i$ must be an $alg_i \times alg_i$ submatrix. The definition of the generalized eigenvectors shown in equation $(11)$ shows that each block must be of form 
  \[A_i = \begin{pmatrix}
  \lambda_i & 1 & 0 & ... & 0\\
  0 &\lambda_i & 1 &...&0 \\
  0&0&\lambda_i&...&0\\
  ...&...&...&...&1\\
  0&0&...&0&\lambda_i
  \end{pmatrix}\]
  With $\lambda_i$'s in the main diagonal and $1$'s in the superdiagonal of $A_i$. The first column of $A$ refers to the transformation of the genuine eigenvector, while the other columns refers to the transformation of the generalized eigenvectors, where $\lambda_i$ refers to the scaling of the $d$th generalized eigenvector and the $1$ refers to the adding of the $(d-1)$th generalized eigenvector to the scaled $d$th vector. If there are no generalized eigenvectors in an eigenspace $E(\lambda_i)$, then $A_i$ is a $1 \times 1$ matrix $( \lambda_i )$. Observe that this form is consistent with our previous theorems, especially the fact that $A$ maps distinct eigenspaces to themselves. 

  Finally, the change of basis is represented through the matrix multiplication. 
  \[J = P^{-1} A P, \; P = \begin{pmatrix}
  |&|&|&| \\ 
  f_1&f_2&...&f_n \\
  |&|&|&|
  \end{pmatrix} \]
  where $f_i$ is the genuine/generalized eigenvectors corresponding to the transformation represented in the $i$th column of $J$. The Jordan Normal Form of a matrix is unique up to the permutations of its diagonal blocks. 

  Notice that the Jordan Normal Form must be an $n \times n$ matrices over $\mathbb{C}$, not $\mathbb{R}$. However, given a matrix $A$ over $\mathbb{R}$, we can construct a similar block diagonal form over $\mathbb{R}$. Since $A$ is real $\implies p_A (t) \in \mathbb{R}[t]$, $\mu \in \mathbb{C}$ is a root of $p_A$ implies that $\bar{\mu}$ is also a root. This means that in the case where $\mu = a \pm b i$ is a pair of complex eigenvectors with eigenvectors $z$ and $\bar{z}$. The associated $2 \times 2$ Jordan block will be of form
  \[\begin{pmatrix}
  a&-b\\ b&a
  \end{pmatrix}\]
  with the associated column vectors in $P$ being
  \[v_1 = \frac{z + \bar{z}}{2}, \; v_2 = \frac{i (z - \bar{z})}{2}\]
  Notice that $z \in \mathbb{C}^n$ is a complex eigenvector belonging to complex eigenvalue $\mu$, and we make the best "approximations" of $z, \bar{z}$ and $\mu, \bar{\mu}$ with the new real vectors $v_1$ and $v_2$. Note that the Jordan block states that
  \[A(v_1) = a v_1 + b v_2, \; A(v_2) = -b v_1 + a v_2\]
  which is true since 
  \begin{align*}
      A(v_1) = A\Big( \frac{z + \bar{z}}{2} \Big) & = \frac{1}{2} \big( A(z) + A(\bar{z}) \big) \\
      & = \frac{1}{2}\big( (a+bi)z + (a-bi) \bar{z} \big) \\
      & = \frac{1}{2} \big( (a)(z + \bar{z}) + (bi) (z - \bar{z})\big) \\
      & = a \frac{z + \bar{z}}{2} + b \frac{i(z-\bar{z}}{2} = a v_1 + b v_2 \\
  \end{align*}
  and 
  \begin{align*}
      A(v_2) = A \Big( \frac{i(z-\bar{z})}{2} \Big) & = \frac{i}{2} \big( A(z) - A(\bar{z}) \big) \\
      & = \frac{i}{2} \big( (a+bi) z - (a-bi) \bar{z}\big) \\
      & = \frac{i}{2} \big( (a) (z - \bar{z}) + (bi)(z+\bar{z})\big) \\
      & = a \frac{i(z-\bar{z})}{2} - b \frac{z + \bar{z}}{2} = a v_2 - b v_1
  \end{align*}
  It suffices to only modify this case for $2 \times 2$ blocks because all complex eigenvalues of real matrices must come in conjugate pairs (but this is not necessarily true for complex matrices, which have characteristic polynomials in $\mathbb{C}[t]$). 

  \begin{corollary}
  The following $2 \times 2$ Jordan block of the form shown below can be turned into the complex Jordan block and vice versa. 
  \[\begin{pmatrix}
  \cos{\theta} & -\sin{\theta} \\
  \sin{\theta} & \cos{\theta} 
  \end{pmatrix} \xleftrightarrow{} \begin{pmatrix}
  e^{i \theta} & 0 \\
  0 & e^{- i \theta}
  \end{pmatrix}\]
  \end{corollary}

  However, there could be bigger Jordan blocks of generalized eigenspaces corresponding to conjugate pairs. Observe the following JNF, with columns (from left to right) corresponding to the transformations $h_1$ (genuine), $k_1$ (generalized), $h_2$ (genuine), and $k_2$ generalized). 
  \[\begin{pmatrix}
  e^{i \theta} & 1 & & \\
  & e^{i \theta} & & \\
  & & e^{-i \theta} & 1 \\
  & & & e^{i- \theta}
  \end{pmatrix}\]
  Using the corollary shown above, we can modify the eigenvalues and eigenvectors into real values and construct the simplest "real form" (assuming $i \neq 0, \pi$) of the matrix 
  \[\begin{pmatrix}
  \cos{\theta} & - \sin{\theta} & 1 & 0 \\
  \sin{\theta} & \cos{\theta} & 0 & 1 \\
  0 & 0 & \cos{\theta} & - \sin{\theta} \\
  0 & 0 & \sin{\theta} & \cos{\theta}
  \end{pmatrix}\]
  where the columns (from left left to right) now correspond to transformation of real eigenvectors
  \[\frac{h_1 + h_2}{2}, \frac{i(h_1 - h_2)}{2}, \frac{k_1 + k_2}{2}, \frac{i(k_1 - k_2)}{2}\]
  Therefore, we can state that the linear transformation represented by the two matrices in their respective bases are equivalent. 

  \begin{example}[Rotation Around Vector]
    \begin{figure}[H]
      \centering 
      \begin{tikzpicture}
        % Rotate the entire picture 30 degrees clockwise
        \begin{scope}[rotate=-30]
          % Define coordinates for the parallelogram
          \coordinate (A) at (-2,-1);
          \coordinate (B) at (2,-1);
          \coordinate (C) at (1,1);
          \coordinate (D) at (-3,1);
          
          % Draw the parallelogram
          \draw (A) -- (B) -- (C) -- (D) -- cycle;
          
          % Draw the normal vector line
          \draw[-stealth, thick] (-0.5,-2) -- (-0.5,2);
          
          % Draw the dashed portion where the line intersects the parallelogram
          \draw[dashed, thick] (-0.5,-0.5) -- (-0.5,0.5);
          
          % Add a dot at intersection point
          \filldraw (-0.5,0) circle (2pt);
          
          % Label the normal vector
          \node at (-0.2,1.5) {$\vec{n}$};
        \end{scope}
      \end{tikzpicture}
      \caption{The linear operator that rotates around a vector $v$ by an angle $\theta$ has an eigendecomposition of the span of $v$ as shown (with eigenvalue $1$) and the 2-dimensional plane (having two complex eigenvalues). } 
      \label{fig:rotation}
    \end{figure}
  \end{example}

  \begin{definition}
  A matrix is \textbf{diagonalizable} if we can perform a change of basis on it to create a diagonal matrix. 
  \end{definition}

  \begin{theorem}
  A matrix is diagonalizable if and only if its algebraic multiplicities is equal to its geometric multiplicities. That is, if the matrix only has genuine eigenvectors. This is also equivalent to saying that all of $A$'s eigenspaces have dimension $1$. 
  \end{theorem}

  It is clear that since eigendecompositions are intrinsic to linear mappings, the JNF of similar matrices are the same. That is, the eigenvalues and the dimensions of the eigenspaces are invariant under a change of basis. 

  \begin{proposition}
  Two matrices are similar if and only if their eigendecompositions are the same. That is, if they have the same eigenvalues and the dimensions of the corresponding eigenspaces are the same. 
  \end{proposition}

  \begin{proof}
  $(\rightarrow)$ $A \sim B \implies A = S^{-1} B S = S^{-1} P^{-1} J P S = (PS)^{-1} J (PS) \implies$ JNF of $A$ and $B$ are the same.  \\
  $(\leftarrow)$ $A$ and $B$ have same JNF $\implies A = P^{-1} J P, B = Q^{-1} J Q \implies J = Q B Q^{-1} \implies A = P^{-1} Q B Q^{-1} P = (Q^{-1} P)^{-1} B (Q^{-1} P) \implies A \sim B$. 
  \end{proof}

  \begin{theorem}
  $A \sim A^T$. 
  \end{theorem}
  \begin{proof}
  By the proposition above, it is sufficient to prove that $A$ and $A^T$ have the same eigendecomposition. Since $(A - \lambda I)^T = A^T - \lambda I$, $\det{(A - \lambda I)} = 0 \iff \det{(A - \lambda I)^T} = \det{(A^T - \lambda I)} \implies $ $A$ and $A^T$ have the same eigenvalues. Similarly, $\big( (A - \lambda I)^d \big)^T = (A^T - \lambda I)^d \implies$ the eigenspaces of $A$ and $A^T$ have the same dimension. 
  \end{proof}


\section{Vector Spaces and Dual Spaces}

  \begin{definition}[Vector Space]
    A \textbf{vector space} $V$ over a field $\mathbb{F}$ (usually $\mathbb{R}$ or $\mathbb{C}$) is a set of vectors that is algebraically closed under the operations: 
    \begin{enumerate}
      \item $+: V \times V \longrightarrow V$
      \item $\times: \mathbb{F} \times V \longrightarrow V$
    \end{enumerate}
    It is also an additive abelian group, with additional axioms. That is, given $\lambda, \mu \in \mathbb{F}$ and $v, u \in V$, 
    \begin{enumerate}
      \item $(\lambda + \mu) v = \lambda v + \mu v$
      \item $\lambda (v + u) = \lambda v + \lambda u$
      \item $(\lambda \mu) v = \lambda (\mu v) = \mu (\lambda v)$ 
    \end{enumerate}
  \end{definition} 

  \begin{definition}[Vector]
    A \textbf{vector} is an element of a vector space. 
  \end{definition}

  \begin{proposition}[No Zero Divisors]
    There are no zero divisors of vector space $V$. That is, 
    \begin{equation}
      \lambda v = 0 \implies \lambda = 0 \text{ or } v = 0
    \end{equation}
  \end{proposition}
  \begin{proof}
    $\lambda v = 0 \implies \lambda v + \lambda v = 0 + \lambda v \implies 2\lambda v = \lambda v \implies (2\lambda - \lambda ) v = 0$. But $\lambda \neq 0$, so $v$ must equal $0$. This leads to a contradiction. 
  \end{proof}

  We now introduce some classic interpretations of vectors. 

  \begin{example}[Vectors as N-Tuples]
    $n$-tuples of elements of a field $\mathbb{F}$, that is, in the form
    \begin{equation}
      (a_1, a_2, ..., a_n)
    \end{equation}
    are elements of a vector field, with vector addition and scalar multiplication defined component-wise. 
  \end{example}

  \begin{example}[Vectors as Arrows]
    The set of all arrows in space, with addition defined by the parallelogram rule and scalar multiplication defined as the stretching/compressing of the arrow from the origin, forms the vector space of arrows. 
  \end{example}

  We define some more vector spaces that are often used. 

  \begin{example}[Polynomials of Finite Degree]
    The set of all polynomials of degree strictly less than $n$ with coefficients in $\mathbb{F}$ defines a vector space over $\mathbb{F}$. 
  \end{example}

  \begin{definition}[Subspace]
    A subset $Y$ of a linear space $X$ is called a \textbf{subspace} if sums and scalar multiples of elements of $Y$ belong to $Y$. Note that $\{ 0 \}, X$ are subspaces of $X$. 
  \end{definition}

  \begin{definition}[Homomorphism, Linear Map]
    Given vector spaces $U, V$ over the same field $\mathbb{F}$, a mapping $f: V \longrightarrow U$ that has properties 
    \begin{equation}
      f(v_1 + v_2) = f(v_1) + f(v_2), \; \; f(c v) = c f(v), (c \in \mathbb{F})
    \end{equation}
    is called a \textbf{homomorphism}. The set of all homomorphisms from $V$ to $U$ is denoted Hom$(V, U)$. If $f$ is bijective, then $f$ is called an \textbf{isomorphism}, and $U$ is said to be \textbf{isomorphic} to $V$, denoted $U \simeq V$. Elements of Hom$(U,U)$, denoted End$(U)$, are called \textbf{endomorphisms} of $U$, and an endomorphism of $U$ that is also an isomorphism is called an \textbf{automorphism}. The set of all automorphisms of $U$ is denoted Aut$(U)$. 
  \end{definition}

\subsection{Basis and Dimension}

  \begin{definition}[Linear Combination]
    A \textbf{linear combination} of $j$ vectors $v_1, v_2, ..., v_j$ of a linear space is a vector of the form 
    \begin{equation}
      c_1 v_1 + c_2 v_2 + c_3 v_3 + ... + c_j v_j, c_1, ..., c_j \in \mathbb{F}
    \end{equation}
  \end{definition}

  \begin{definition}[Span]
    The \textbf{span} of a collection of vectors $v_1, v_2, ..., v_j \in V$ is the set
    \begin{equation}
      \Span \{ v_1, v_2, ..., v_j \} \equiv \{ c_1 v_1 + c_2 v_2 + c_3 v_3 + ... + c_j v_j\; | \; c_1, ..., c_j \in \mathbb{F}\}
    \end{equation}
    That is, $\Span \{v_1, v_2, ..., v_j\}$ is the smallest subspace of $V$ that contains all $v_1, ..., v_j$. 
  \end{definition}

  It clearly follows that $v_1, ..., v_n$ span the whole space $V$ if every vector in $V$ can be expressed as a linear combination of the $v_i$'s. 

  \begin{definition}[Linear Independence]
    Vectors $v_1, ..., v_j$ are \textbf{linearly independent} if 
    \begin{equation}
      c_1 v_1 + c_2 v_2 + c_3 v_3 + ... + c_j v_j = 0 \implies c_1, ..., c_j = 0
    \end{equation}
    They are \textbf{linearly dependent} if there exists nonzero $c_1, ..., c_j$ such that the equality holds true, which is equivalent to saying that there is at least one vector $v_i, 1 \leq i \leq j,$ such that it can be represented as a linear combination of all the other vectors. 
  \end{definition}

  \begin{definition}[Basis]
    A set of linearly independent vectors $v_1, ..., v_n$ that span vector space $V$ is called a \textbf{basis} of $V$. These vectors $v_i$ are called \textbf{basis vectors}. Note that this basis is not unique; it is actually highly un-unique. 
  \end{definition}

  \begin{example}[Standard Basis]
    The basis $e_i$ of $\mathbb{F}^n$ are the vectors with every element equal to $0$ except for the $i$th element, which is equal to $1$. 
  \end{example}

  \begin{proposition}
    Every possible basis of a vector space $V$ has the same number of vectors.
  \end{proposition}

  \begin{proposition}
    Any maximal linearly independent subset $\{ e_1, e_2, ..., e_k\}$ of a set $S$ is a basis of $\Span S$. 
  \end{proposition}

  \begin{definition}[Dimension]
    The number of vectors in a basis of vector space $V$ is called the \textbf{dimension} of $V$, denoted $\dim{V}$. 
  \end{definition}

  \begin{theorem}[Isomorphism to $\mathbb{F}^n$]
    Every $n$-dimensional vector space $V$ over $\mathbb{F}$ is isomorphic to $\mathbb{F}^n$, the set of $n$-tuples of elements in $\mathbb{F}$. 
  \end{theorem}

  \begin{corollary}[Isomorphism Between $n$-Dimensional Vector Spaces]
    Vector spaces of the same field are isomorphic if and only if their dimensions are the same. 
  \end{corollary}

  \begin{example}
    The field of complex numbers $\mathbb{C}$, regarded as a vector space over $\mathbb{R}$, has dimension $2$. The algebra of quaternions $\mathbb{H}$ has dimension $4$. 
  \end{example}

  \begin{definition}[Hyperplane]
    A $(n - 1)$-dimensional subspace of an $n$-dimensional space is called a \textbf{hyperplane}. 
  \end{definition}

  \begin{definition}[Sum of Subspaces]
    The sum of subspaces $U_1, U_2, ..., U_n \subset V$, denoted
    \begin{equation}
      U_1 + U_2 + U_3 + ... + U_n
    \end{equation}
    is called the \textbf{sum} of the subspaces $U_1, ..., U_n$. It is the set of all vectors that can be expressed as the sum of vectors in each of its respective space. That is, 
    \begin{equation}
      \sum_{i=1}^n U_i \equiv \Big\{ \sum_{i = 1}^n u_i \;|\; u_i \in U_i\Big\}
    \end{equation}
  \end{definition}

  \begin{definition}[Direct Sum of Spaces]
    Given subspaces $V_1, V_2, ..., V_n \subset V$ where the intersection between two $V_i$'s are pairwise disjoint, the \textbf{direct sum} of the subspaces is the set of vectors that can be expressed uniquely as the sum of vectors in each of its respective spaces. That is, 
    \begin{equation}
      \bigoplus_{i=1}^n V_i \equiv \Big\{ \sum_{i=1}^n v_i \; | \; v_i \in V_i\Big\}
    \end{equation}
    $V_1 \oplus V_2 \oplus ... \oplus V_n$ is also a vector space. 
  \end{definition}

  The crucial difference between the sum and the direct sum is that the direct sum requires the subspaces to be disjoint except for at the origin, which allows the expression of each vector in $V_1 \oplus ... \oplus V_n$ to be unique. It is also worth noting that the Cartesian product of vector spaces is merely just the set of tuples of vectors that are in each respective space and is \textit{not} a vector space (since addition and multiplication is not defined on that new set). If we define the operations component-wise, then 
  \begin{equation}
    \prod_{i=1}^n V_i = \sum_{i=1}^n V_i
  \end{equation}

  Note that we can also define the direct sum of spaces $U$ and $V$ by their basis. That is, given that the basis for $U$ is $\{e_i\}_{i=1}^n$ and the basis for $V$ is $\{f_j\}_{j=1}^n$, the basis for $U \oplus V$ is
  \begin{equation}
    \{(e_1, 0), (e_2, 0), ..., (e_n, 0), (0, f_1), ..., (0, f_m)\}
  \end{equation}

  \begin{proposition}
    The dimension of the direct sum of vector spaces is 
    \begin{equation}
      \dim{\bigoplus_{i=1}^n V_i} = \sum_{i=1}^n \dim{V_i}
    \end{equation}
  \end{proposition}
  \begin{proof}
    This follows from the basis construction of the direct sum of $V_i$'s. 
  \end{proof}

  \begin{definition}[Congruence Relations on Vector Spaces]
    Given vector space $X$ and subspace $Y$ we say that two vectors $x_1$ and $x_2$ are \textbf{congruent modulo $Y$}, denoted 
    \begin{equation}
      x_1 \equiv x_2 \pmod{Y}
    \end{equation}
    if $x_2 - x_1 \in Y$. This congruence is a \textbf{relation}, meaning that it is symmetric, reflective, and transitive (elaborated in the abstract algebra chapter). The \textbf{congruence classes} $\{ y\}$ is the set of all vectors that are congruent modulo $Y$ to $y$. 
  \end{definition}

  \begin{definition}[Quotient Vector Space]
    The \textbf{quotient space} modulo $Y$, denoted $ X / Y$, is the set of all congruence classes modulo $Y$. We can define addition and scalar multiplication on this set as such 
    \begin{equation}
      \{ x\} + \{ y\} = \{ x + y\}
    \end{equation}
  \end{definition}

  \begin{proposition}[Decomposition into Subspace and Quotient Space]
    Given vector space $X$, $Y$ a subspace of $X$. Then, 
    \begin{equation}
      X \simeq Y \oplus \frac{X}{Y}
    \end{equation}
  \end{proposition}

  Vector spaces over one field can be interpreted as a vector space over another field. This is most common when interpreting complex vector spaces as real ones. For example, given a complex vector space $Z$ with basis $\{z_1, z_2, ..., z_n\}$, every vector can be expressed as
  \begin{equation}
    z = \sum_{j=1}^n c_j z_j, \; c_j \in \mathbb{C}
  \end{equation}
  We can set $c_j = a_j + b_j i$ uniquely, with $a, b \in \mathbb{R}$, and rewrite
  \begin{equation}
    z = \sum_{j=1}^n a_j z_j + b_j (i z_j)
  \end{equation}
  $\implies \{ z_j\} \cup \{ i z_j\}$ forms a basis of $Z$ as a \textit{real vector space}. 

\subsection{Dual Spaces}

  \begin{definition}[Linear Map]
    A \textbf{linear map} is a homomorphism between vector spaces. That is, a linear map $f: X \longrightarrow Y$ has the properties 
    \begin{align*}
      & \forall u, v \in X \; f(u + v) = f(u) + f(v) \\
      & \forall u \in X, c \in \mathbb{F} \; f(c u) = c f(u) 
    \end{align*}
  \end{definition}

  \begin{definition}[Dual Space]
    Given a vector space $V$ over $\mathbb{F}$, the \textbf{dual vector space} $V^*$ is the set of all linear maps that, given a vector in $V$, outputs a scalar in $\mathbb{F}$. That is, 
    \begin{equation}
      V^* \equiv \{ l \text{ linear} \; | \; l: V \longrightarrow \mathbb{F}\}
    \end{equation}
    or equivalently, 
    \begin{equation}
      V^* \equiv \text{Hom}(V, \mathbb{F})
    \end{equation}
    The addition and scalar multiplication of $V^*$ is defined pointwise. That is, given $l, m \in V^*$, 
    \begin{equation}
      (l + m) (x) = l(x) + m(x), \; (c l)(x) = c l(x)
    \end{equation}
  \end{definition}

  \begin{theorem}[Dimensions of Dual of Finite-Dimensional Vector Spaces]
    The dimension of the dual space is equal to that of the original space. 
    \begin{equation}
      \dim{V} = n \implies \dim{V^*} = n
    \end{equation}
  \end{theorem}

  While we initially view elements of $V$ as "things" and elements of $V^*$ as linear functions, this thought is actually erroneous. Given $l \in V^*$, we see that 
  \begin{equation}
    l: V \longrightarrow \mathbb{F}
  \end{equation}
  But since both $V$ and $V^*$ are vector spaces, we can also see that given $x \in V$, $x$ is also a linear function 
  \begin{equation}
    x: V^* \longrightarrow \mathbb{F}, \text{ where } x(l) \equiv l(x)
  \end{equation}
  But this means that $x$ is an element of $V^{**}$ the dual of $V$! This statement is elaborated with the following theorem. 

  \begin{theorem}[Canonical Isomorphisms of Double Duals]
    $V^{**}$ is \textbf{naturally, or canonically, isomorphic} to vector space $V$. However, $V$ is not naturally isomorphic to $V^*$. 
  \end{theorem}
  \begin{proof}
    What we mean by natural is that we do not need to select a basis in either vector space to define the isomorphism. We fix a vector $l \in V^*$, and given $x \in V, \phi \in V^{**}$, we define
    \begin{equation}
      \phi(l) \equiv l(x)
    \end{equation}
    This defines a one-to-one correspondence between $V$ and $V^{**}$. On the contrary, there is no way to define an isomorphism between $V$ and $V^*$ without further structure on $V$. 
  \end{proof}

  It is important to be aware of this \textit{duality} between elements $x \in V$ and $l \in V^*$, and thus we should interpret $x \in V$ as a linear function of $V^*$ and $l \in V^*$ as a linear function of $V$.

  \begin{definition}[Dual Basis]
    Given a basis $\{ e_1, e_2, ..., e_n\}$ of $V$, the \textbf{dual basis} $\{f_1, f_2, ..., f_n\}$ of $V^*$ has vectors satisfying 
    \begin{equation}
      f_j (e_i) = \delta_{i j} = 
      \begin{cases}
      0 & i \neq j \\
      1 & i = j
      \end{cases}
    \end{equation}
    where $\delta_{i j}$ is called the \textbf{Kronecker delta function}. 
  \end{definition}

  \begin{definition}[Annihilator]
    Let $Y$ be a subspace of $X$. Then the set of functions in $X^*$ that vanish on $Y$, that is, satisfy
    \begin{equation}
      l(y) = 0 \text{ for all } y \in Y
    \end{equation}
    is called the \textbf{annihilator} of $Y$, denoted $Y^0$. If $Y = X$, then it is easy to see that $Y^0$ is trivial. 
  \end{definition}

  \begin{theorem}
    Given subspace $Y$ of $X$ 
    \begin{equation}
      Y^0 \simeq (X / Y)^\ast
    \end{equation}
  \end{theorem}
  \begin{proof}
    The isomorphism is defined as such. Given $l \in Y^0$, we define $L \in (Y/X)^\ast$ as 
    \begin{equation}
      L\{x\} \equiv l(x)
    \end{equation}
  \end{proof}

  \begin{corollary}
    \begin{equation}
      \dim Y^0 + \dim Y = \dim X
    \end{equation}
  \end{corollary}

  \begin{corollary}
    \begin{equation}
      Y^{0 0} = Y
    \end{equation}
  \end{corollary}

  \begin{theorem}[Quadrature Formula]
    Let $l$ be an interval on $\mathbb{R}$ containing $t_1, t_2, ..., t_n$ $n$ distinct points. Then, given any polynomial $p$ with degree $< n$, there exist $n$ real numbers $c_1, c_2, ..., c_n$ such that
    \begin{equation}
      \int_l p(t) d t = c_1 p(t_1) + c_2 p(t_2) + ... + c_n p(t_n)
    \end{equation}
    called the \textit{quadrature formula} suffices. That is, the integral of any polynomial over $l$ can be expressed as a linear combination of the polynomials evaluated at $n$ distinct points in $l$. 
  \end{theorem}
  \begin{proof}
    The space of all polynomials with degree $< n$ is an $n$-dimensional vector space, denote it $V$. We define the basis of the dual space $V^*$ as 
    \begin{equation}
      \phi_i (p) \equiv p(t_i), \; i = 1, 2, ..., n
    \end{equation}
    with addition and scalar multiplication defined
    \begin{align*}
      & (\phi + \gamma) (p) \equiv \phi(p) + \gamma(p) \\
      & (c \phi) (p) \equiv c \phi (p) 
    \end{align*}
    We can see that the $\phi$'s are indeed linear since, given $p, q \in \mathbb{R}[t]$
    \begin{align*}
      & \phi_i (p + q) = (p + q) (t_i) = p(t_i) + q(t_i) = \phi_i (p) + \phi_i (q) \\
      & \phi_i (c p) = (c p) (t_i) = c p(t_i) = c \phi_i (p)
    \end{align*}
    We claim that all the $phi_i$'s are linearly independent. Assume that 
    \begin{equation}
      \sum_{i=1}^n c_i \phi_i (p) = \sum_{i=1}^n c_i p(t_i) = 0
    \end{equation}
    Since the $\phi$'s must be linearly independent for every polynomial $p$, set it equal to
    \begin{equation}
      q_k (t) \equiv \prod_{j \neq k} (t - t_j), \; k = 1, 2, ..., n
    \end{equation}
    $p = q_k$ must imply that all $\phi_i (p) = 0$ for all $i \neq k$, which implies that $c_k = 0$ in the linear combination. Repeating this for $k = 1, 2, ..., n$ results in all $c_i = 0$, implying that the $\phi_i$'s form a basis of $V^*$. Clearly, the function of definite integration over $l$ is a linear mapping from $V \longrightarrow \mathbb{R}$, meaning that it is in $V^*$. Therefore, it can be expressed as a linear combination of $\phi_i$'s. 
  \end{proof}


\documentclass{article}
\usepackage[a4paper, top=1in, bottom=1in, left=1in, right=1in]{geometry}
\usepackage[utf8]{inputenc}

\usepackage[english]{babel}
\usepackage{tikz-cd, extarrows, esvect, esint, pgfplots, lipsum, bm, dcolumn}
\usetikzlibrary{arrows}
\usepackage{amsmath, amssymb, amsthm, mathrsfs, mathtools, centernot, hyperref, fancyhdr, lastpage}


\renewcommand{\thispagestyle}[1]{}

\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\Sym}{Sym}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\im}{Im}
\DeclareMathOperator{\Div}{div}
\DeclareMathOperator{\curl}{curl}
\DeclareMathOperator{\GL}{GL}
\DeclareMathOperator{\SL}{SL}
\DeclareMathOperator{\GA}{GA}
\DeclareMathOperator{\std}{std}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Corr}{Corr}
\DeclareMathOperator{\Int}{Int}
\DeclareMathOperator{\Id}{Id}
\DeclareMathOperator{\Lie}{Lie}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\Alt}{Alt}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\conv}{conv}
\DeclareMathOperator{\aff}{aff}
\DeclareMathOperator{\arccot}{arccot}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{example}{Example}[section]
\newtheorem{corollary}{Corollary}[theorem]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\renewcommand{\qed}{\hfill$\blacksquare$}
\renewcommand{\footrulewidth}{0.4pt}% default is 0pt


\begin{document}
\pagestyle{fancy}

\lhead{Multivariate Calculus}
\chead{Muchang Bahng}
\rhead{\date{August 2021}}
\cfoot{\thepage / \pageref{LastPage}}

\title{Multivariate Calculus}
\author{Muchang Bahng}

\maketitle

Coordinate-dependent calculus of vector-valued functions. Note that the entire concept of calculus is dependent on functions that maps elements between metric spaces. To be more rigorous, we really just need a topology to define the following terms shown, but this course will assume that we are working with Euclidean spaces of $\mathbb{R}^n$ with metric topologies. The metric in $\mathbb{R}^n$ will be denoted $||\cdot||$, defined to be the $L2$ metric. 

\section{Differentiation}
\begin{definition}
A sequence $(x_k)$ of vectors in $\mathbb{R}^n$ \textit{converges} to the vector $x$ if 
\[\lim_{k \rightarrow \infty} ||x_k - x|| = 0\]
\end{definition}

Note that this definition of convergence can be defined for any vector in a metric space $(V, ||\cdot||)$. For example, the space of $n \times n$ matrices with the operator norm or the Frobenius norm. Since this specific definition of convergence is limited in the way that the $L2$ norm is dependent on the coordinates of the vector in $\mathbb{R}^n$, the entire concept of coordinate-based vector calculus is also limited. 

The concepts of continuity and derivatives are dependent on how the output of the function changes as we are changing the input. To measure this change in input, we must define a path function. 

\begin{definition}
A \textit{path function} is any function
\[p: \mathbb{R} \longrightarrow \mathbb{R}^n\]
Note that $\im{(p)}$ traces out an oriented path in $\mathbb{R}^n$. Note also that the paramaterization of $p$ may be different even though the image of $p$ may not change. 
\end{definition}

The defining of the path function allows us to construct arbitrary paths traveling through $\mathbb{R}^n$. Now, we can define continuity. 

\begin{definition}
Let $p: \mathbb{R} \longrightarrow \mathbb{R}^n$ be any path function such that $p(t_0) = x_0$, and let $f: \mathbb{R}^n \longrightarrow \mathbb{R}^m$ be a vector-valued function defined on the open set $U \subset \mathbb{R}^n$. $f$ is \textit{continuous} at $x_0 \in U$ if 
\begin{align}
    \lim_{t \rightarrow t_0} ||(f \circ p)(t) - (f \circ p)(t_0) || = \lim_{t \rightarrow t_0} ||(f \circ p)(t) - f(x_0) || = 0
\end{align}
This is equivalently written as 
\begin{equation}
    \lim_{x \rightarrow x_0} ||f(x) - f(x_0)|| = 0
\end{equation}
It is important to introduce equation $(1)$ since this allows the reader to realize that the limits and the actual value of $f$ at $x_0$ should coincide no matter which path we choose in $\mathbb{R}^n$. While equation $(2)$ is more concise, the expression $x \rightarrow x_0$ does not clearly express the arbitrariness in our choice of path. 
\end{definition}

Note that the metric $||\cdot||$ in $(2)$ is the metric of $\mathbb{R}^m$, not $\mathbb{R}^n$. 

\subsection{Derivatives along Paths, Directional Derivatives}
Differentiability must also be defined using paths since the change of $x \in \mathbb{R}^n$ (an element in the domain of $f$) can be modeled using a specific path function. 

\begin{definition}
Let $p: \mathbb{R} \longrightarrow \mathbb{R}^n$ be a path function such that $p(t_0) = x_0$, and let there exist a function $f: \mathbb{R}^n \longrightarrow \mathbb{R}^m$. We say that $f$ is \textit{differentiable} at $x_0$ if, for every $p$, there exists a value $f_p^\prime (x_0)$ such that 
\[\lim_{h \rightarrow 0} \bigg|\bigg| \frac{(f\circ p)(t_0 + h) - (f \circ p) (t_0)}{h} - f_p^\prime (x_0)\bigg|\bigg| = 0\]
This is equivalent to saying that there exists a well-defined linear mapping $T: \mathbb{R}^n \longrightarrow \mathbb{R}^m$ such that 
\[\lim_{x \rightarrow x_0} \frac{|| f(x) - \big( f(x_0) + T x\big)||}{||x - x_0||} = 0\]
This says that there exists an affine linear approximation $L(x) = f(x_0) + T(x)$ of $f$ in the neighborhood $U$ of $x_0$.
\end{definition}

We now give a visual description of this linear approximation. Let us define the \textit{graph of $f$} as the $m$-dimensional surface
\[G \equiv \{(x_1, x_2, ..., x_n, f_1(x), ..., f_m(x)) \; | \; (x_1, ..., x_n) \in \mathbb{R}^n\} \subset \mathbb{R}^n \oplus \mathbb{R}^m\]
We can interpret $\im{L}$ as the $n$-dimensional affine linear subspace embedded in $\mathbb{R}^n \oplus \mathbb{R}^m$, the extended space where the function is graphed in. That is, 
\[\im{L} \equiv \{(x_1,..., x_n, f_1(x_0) + T_1 (x), ..., f_m(x_0) + T_m (x)) \} \; | \; (x_1, ..., x_n) \in \mathbb{R}^n\}\]
In a way, $\im{L}$ is the affine "tangent space" of $G$ at point $x_0$. We now define the derivative of path functions. 

\begin{definition}
Let $p: \mathbb{R} \longrightarrow \mathbb{R}^n$ be a path function with parameter $t$. Let the coordinate representation of $p$ be defined
\[p \equiv (p_1, p_2, ..., p_n)\]
Then, the derivative of $p$ with respect to $t$ is defined
\[p^\prime (t) = (p_1^\prime, p_2^\prime, ..., p_n^\prime)\]
Visually, this outputs a tangent vector that represents the velocity of the particle traveling through $\im{p} \subset \mathbb{R}^n$ as $t$ travels through $\mathbb{R}$. The magnitude of the vector represents the speed of the particle, while the orientation of the vector represents the particle's direction. 
\end{definition}

The definition of the derivative of the path function now allows us to build on top of it the derivative of a general function. 

\begin{definition}
Given that the derivative of $f$ exists at $x_0$, $f^\prime$ cannot be properly defined unless we specify which path function $p$ (fully on the graph $f$ and passing through $x_0$) the particle is traveling through. Therefore, the derivative of the function $f$ through a path $p$ at $x_0$ is defined
\[f_p^\prime (x_0) \equiv \lim_{h \rightarrow 0} \frac{(f\circ p) (t_0 + h) - (f \circ p) (t_0)}{h}\]
\end{definition}

We can describe the derivative visually, too. $p$ defines a path function that describes a particle traveling through $\mathbb{R}^n$. Now, think of this entire path being mapped onto $\mathbb{R}^m$ through $f$, which will draw another path in $\mathbb{R}^m$. In a way, $f$ has "warped" the velocity of the particle. With this new velocity curve, the tangent vector of curve $f(p(t))$ at the point $t_0$ is the derivative of the $f$ at point $x_0$. (The reader may also realize that we have just described the chain rule, too). 
\begin{center}
    \includegraphics[scale=0.25]{Tangent_Vector_Velocity_Curve.PNG}
\end{center}
We can also visualize how the affine linear approximation is constructed from the tangent vectors of each path. Given the graph $G$ of $f$ in $\mathbb{R}^n \oplus \mathbb{R}^m$, the path function $p$ determines a path on the surface $G$ that passes through $(x_0, f(x_0))$ (we are treating $x_0$ as an $n$-tuple and $f(x_0)$ as an $m$-tuple). This path on $G$ is really just a bigger path function defined as
\[t \mapsto \big( p_1(t), p_2(t), ..., p_n(t), f_1(p(t)), ..., f_m(p(t))\big)\]
or more simply, let $\Tilde{p}$ be the mapping
\[t \mapsto \big( p(t), f(p(t)) \big)\]
Now, for all paths $\Tilde{p}$ on $G$ passing through $(x_0, f(x_0))$, the set of tangent vectors of every possible $\Tilde{p}$ from $x_0$ is precisely the affine linear subspace $\im{L}$. In the $2 + 1 = 3$ dimensional case, we have our familiar tangent plane at point $(x, y, f(x, y))$ on the graph of the function $f$ in $\mathbb{R}^2 \oplus \mathbb{R}$. 
\begin{center}
    \includegraphics[scale=0.2]{Tangent_Plane_to_Graph.PNG}
\end{center}

\subsection{Differential Operators, Total Derivatives}
\subsubsection{Tangent Vectors, Spaces, and Bundles}
\begin{definition}[Tangent Vectors, Spaces at a Point in $\mathbb{R}^n$]
We will construct the geometric tangent space of Euclidean space $\mathbb{R}^n$. Denote the set of all possible path functions $p: \mathbb{R} \longrightarrow \mathbb{R}^n$ that passes through $a \in \mathbb{R}^n$ to be $\mathcal{P}_{a}(\mathbb{R}^n)$. We show some of these path functions in $\mathcal{P}_{a} (\mathbb{R}^n)$ below (note that this is merely a set, so there is no operation defined on these paths, as in the case of the fundamental group of a topological space). 
\begin{center}
    \includegraphics[scale=0.4]{Curves_Through_A_Point.png}
\end{center}
We can define a relation on this set: two path functions $q, r \in \mathcal{P}_{a} (\mathbb{R}^n)$ are equivalent if the tangent vectors of $q$ and $r$ at $a$ (including magnitude) are equal. Note that we have already defined the tangent vector to a path function above, so this move is completely valid. 
\begin{center}
    \includegraphics[scale=0.4]{Curves_in_Same_Equivalence_Class.png}
\end{center}
That is, given $q(t_q) = r (t_r) = a$, 
\[q \sim r \iff q^\prime (t_q) = r^\prime (t_r)\]
which implies that given any smooth function $f: \mathbb{R}^n \longrightarrow \mathbb{R}^m$, 
\[(f \circ q)^\prime (a) = (f \circ r)^\prime (a)\]
Note that within each equivalence class, there is exactly one straight-line path that goes through $a$ with a given magnitude. This fact is extremely significant because this allows us to simplify the concept of derivatives defined on arbitrary paths to paths that are simpler in general. It isn't too hard to see that every arbitrary path $p$ is equivalent to some line function with constant velocity. That is, given a path $p$ such that $p(t_0) = a$ and 
\[p^\prime (t_0) = v \in \mathbb{R}^n\]
we can construct the unique straight-line path that is equivalent to $p$ under $\sim$ as 
\[l(t) \equiv a + v (t - t_0)\]
and calculate the derivative of $f$ under the path $l$ to find $f^\prime$ under $p$ at $a$. This "simplification" of $\mathcal{P}_{a}(\mathbb{R}^n)$ to the quotient space $\mathcal{P}_{a} (\mathbb{R}^n)/\sim$ of path functions that draw constant-velocity lines allows us to define directional derivatives. Note that paths that have the same image in $\mathbb{R}^n$ could be in different equivalence classes under $\sim$ if their velocities are different at the point $x_0$!

Now, since every path function in $\mathcal{P}_{a} (\mathbb{R}^n)$ can be simplified to a straight line path represented by a vector protruding from $a$, $\mathcal{P}_{a} (\mathbb{R}^n)$ looks a lot like the vector space $\mathbb{R}^n$. 
\begin{center}
    \includegraphics[scale=0.4]{Curves_Modeled_as_Vectors.png}
\end{center}
We can define vector addition, scalar multiplication, etc. to give this set the structure of a vector space. Therefore, this quotient space with this vector space structure is isomorphic to the geometric tangent space of $f$ at point $a$, denoted $T_a \mathbb{R}^n$. 
\[T_a \mathbb{R}^n \simeq \frac{\mathcal{P}_a (\mathbb{R}^n)}{\sim}\]
The vectors in here are called the \textit{tangent vectors of $T_a \mathbb{R}^n$}. So, if we would like to find the directional derivative of $f$ the point $a$ in direction of vector $v$, (given that $l(t) \equiv a + v (t - t_0)$) we can calculate 
\[\frac{d}{dt} f\big(l(t)\big) \bigg|_{t = t_0} = \frac{d}{dt} f(a + v t) \bigg|_{t = 0} = \lim_{t \rightarrow 0} \frac{f(a + v t) - f(a)}{t}\]
Note that while
\[T_a \mathbb{R}^n \simeq T_b \mathbb{R}^n \simeq \mathbb{R}^n\]
for all $a, b \in \mathbb{R}^n$, these tangent spaces are not equivalent to each other. 
\end{definition}

\begin{theorem}
If $f^\prime (t) = 0$ for all $t \in \mathbb{R}^n$ and all paths $p$, then $f$ is a constant function. 
\end{theorem}

\begin{definition}[Tangent Bundles in $\mathbb{R}^n$]
In $\mathbb{R}^n$, the \textit{fiber bundle} of the tangent spaces $T_x \mathbb{R}^n$ for all $x \in \mathbb{R}^n$ is the disjoint union of them, called the \textit{tangent bundle}. 
\[T \mathbb{R}^n \equiv \bigsqcup_{a \in \mathbb{R}^n} T_a \mathbb{R}^n\]
Its has a dimension of $2n$. In this chapter, this notation is used when we must identify a geometric tangent space $T_x \mathbb{R}^n$ but the point $x$ at which the tangent space is constructed is not specified. Therefore, we refer to \textit{all} of the tangent spaces at once. 
\end{definition}

Another concept that will pop up is the vector field of $\mathbb{R}^n$. A vector field of $\mathbb{R}^n$ is actually not a function $V: \mathbb{R}^n \longrightarrow \mathbb{R}^n$. 

\begin{definition}[Vector Field of Tangent Vectors on $\mathbb{R}^n$]
A vector field $V$ on $\mathbb{R}^n$ is a mapping
\[V: \mathbb{R}^n \longrightarrow T \mathbb{R}^n\]
where $V(x) \in T_x \mathbb{R}^n$. We can interpret this visually since every vector is "attached" to a different point in $\mathbb{R}^n$, which represents its own tangent space. 
\end{definition}

\begin{definition}[Differential Operator]
Let us denote the set of all smoothly continuous functions mapping from $\mathbb{R}^n$ to $\mathbb{R}^m$ as $C^1 (\mathbb{R}^n; \mathbb{R}^m)$. We have seen that in order to find the derivative of a function, we need
\begin{enumerate}
    \item a smooth function $f \in C^1 (\mathbb{R}^n; \mathbb{R}^m)$ 
    \item a point $x_0 \in \mathbb{R}^n$, where the derivative will be evaluated
    \item a geometric tangent vector $v \in T_{x_0} \mathbb{R}^n$ that represents the direction in which we are evaluating the derivative 
\end{enumerate}
In the most abstract sense, we can interpret the differentiation operator as a function $d$ that takes in these three inputs and outputs a vector that represents the rate of change of $f$ at $x_0$ in direction $v$. 
\[d: C^1(\mathbb{R}^n; \mathbb{R}^m) \times \mathbb{R}^n \times T \mathbb{R}^n \longrightarrow \mathbb{R}^m\]
where $T \mathbb{R}^n_x$ is the fiber bundle of all geometric tangent spaces at $\mathbb{R}^n$.  
\begin{center}
    \includegraphics[scale=0.2]{Three_Input_Differential_Operator.PNG}
\end{center}
Note that the differential operator, denoted $d$, is
\begin{enumerate}
    \item linear with respect to the function argument
    \[(d_v (f + g)) (p) = d_v f (p) + d_v g(p), \;\; (d_v (c f)) (p) = c  (d_v f) (p)\]
    \item linear with respect to the directional vector argument
    \[(d_{v + w} f) (p) = (d_v f) (p) + (d_w f) (p), \;\; (d_{cv} f) (p) = c (d_v f) (p)\]
    \item not linear with respect to the point argument
\end{enumerate}
In the most abstract sense, $d$ finds the derivative of any function $f$ when the input, starting at point $x$, moves infinitesimally in the direction of vector $v$ within the domain. The output is a vector that represents the direction the corresponding output vector travels from point $f(x)$. 
\begin{center}
    \includegraphics[scale=0.2]{Abstract_Directional_Derivative.PNG}
\end{center}
$df (x)$ is also called the \textit{total derivative or differential} at $x$. 
\end{definition}

\subsubsection{Differential Operator as Iterated Mappings}
Rather than interpreting $d$ as a mapping that takes in three inputs all at once to return a vector in $\mathbb{R}^m$, we can interpret it as a mapping that takes in some arguments and outputs another function that takes in the remaining arguments. We list some common interpretations, but note that usually, the function argument is given or is inputted first: 
\begin{enumerate}
    \item $d$ takes in two inputs $f \in C^1 (\mathbb{R}^n; \mathbb{R}^m)$ and $v \in T_x \mathbb{R}^n$ and outputs the vector field $d_v f: \mathbb{R}^n \longrightarrow \mathbb{R}^m$. This vector field then takes in a point $x \in \mathbb{R}^n$ and outputs the corresponding vector. 
    \begin{center}
        \includegraphics[scale=0.2]{f_v_x_Interpretation.PNG}
    \end{center}
    \item $d$ takes in function $f \in C^1 (\mathbb{R}^n; \mathbb{R}^m)$ and outputs a function $d f$. $d f$ then takes in a point $x \in \mathbb{R}^n$ and outputs another function $(d f)(x)$. $(d f)(x)$ finally takes in a vector $v \in T_x \mathbb{R}^n$ (note that this does not have to be a fiber bundle, since $x$ has been determined) and outputs the derivative vector in $\mathbb{R}^m$. 
    \begin{center}
        \includegraphics[scale=0.2]{f_x_v_Interpretation.PNG}
    \end{center}
    \item $d$ takes in function $f \in C^1 (\mathbb{R}^n; \mathbb{R}^m)$ and outputs a function $d f$. $d f$ then takes in a vector $v \in T_x \mathbb{R}^n$ and outputs another function $d_v f$. $d_v f$ finally takes in a point $x \in \mathbb{R}^n$ and outputs the derivative vector in $\mathbb{R}^m$. 
    \begin{center}
        \includegraphics[scale=0.2]{f_v_x_Interpretation.PNG}
    \end{center}
    \item This one is quite different from the previous ones, albeit very powerful. We can define the differential operator on $f: \mathbb{R}^n \longrightarrow \mathbb{R}^m$ as a function that takes in a vector field $V$ of $\mathbb{R}^n$ and outputs $df(V)$. $df(V): \mathbb{R}^n \longrightarrow \mathbb{R}^m$ is another function that takes in point $x \in \mathbb{R}^n$ and outputs a derivative vector according to the rule: 
    \[df(V) (x) \equiv (d_{V(x)} f)(x)\]
    This can be visualized as such: 
    \begin{center}
        \includegraphics[scale=0.2]{Diff_Operator_with_Vector_Field.PNG}
    \end{center}
    We can use this interpretation to define partial derivatives as 
    \[\frac{\partial f}{\partial v_i} (x) \equiv (d_{e_i} f)(x) \equiv df (V_{e_i}) (x)\]
    where $V_{e_i}$ is the constant vector field that outputs $e_i$. 
    \item $d$ takes in function $f \in C^1 (\mathbb{R}^n ; \mathbb{R}^m)$ and point $x$ and outputs a function $d f(x): T_x \mathbb{R}^n \longrightarrow \mathbb{R}^m$. This interpretation will be used later when we view $d f(x)$ as an element of the cotangent space at $x$.  
\end{enumerate}

\subsubsection{Cotangent Vectors, Spaces, and Bundles}

We will now focus on real valued functions $f: \mathbb{R}^n \longrightarrow \mathbb{R}$, since it is easier to define certain concepts this way without having to worry about abstract generalizations. Also, every function $f: \mathbb{R}^n \longrightarrow \mathbb{R}^m$ can be decomposed into its component functions $f_i: \mathbb{R}^n \longrightarrow \mathbb{R}$ for $i = 1, 2, \ldots, n$ such that
\[f = \begin{pmatrix}
f_1 \\ f_2 \\ \vdots \\ f_m
\end{pmatrix}\]
so we do not lose any data with this simplified interpretation. We will also denote $C^1 (\mathbb{R}^n; \mathbb{R})$ as just $C^1 (\mathbb{R}^n)$. 

\begin{definition}[Cotangent Vectors, Spaces in at a Point in $\mathbb{R}^n$]
The \textit{cotangent space} of $\mathbb{R}^n$ at point $x$ is the dual of the tangent space $T_x \mathbb{R}^n$. It is denoted
\[T_x^* \mathbb{R}^n \equiv (T_x \mathbb{R}^n)^*\]
Its elements are \textit{cotangent vectors}, which are functionals from $T_x \mathbb{R}^n$ to $\mathbb{R}$. Therefore, for every point $a \in \mathbb{R}^n$, there exists a tangent space $T_a \mathbb{R}^n$ and a cotangent space $T_a^* \mathbb{R}^n$. 
\begin{center}
    \includegraphics[scale=0.2]{Tangent_Cotangent_Space_at_Point.PNG}
\end{center}
\end{definition}

\begin{definition}[Derivative at a Point as a Cotangent Vector]
Let us view the total derivative $d$ with interpretation 5. The total derivative of $f: \mathbb{R}^n \longrightarrow \mathbb{R}$ at point $a$ is a linear functional 
\[df (a): T_a \mathbb{R}^n \longrightarrow \mathbb{R}\]
which, by definition, implies that it is a cotangent vector in $T_a^* \mathbb{R}^n$! 
\begin{center}
    \includegraphics[scale=0.25]{Cotangent_Vector_as_Linear_Operator.PNG}
\end{center}
\end{definition}

To generalize this to a field $df$, we must introduce the cotangent bundle. 

\begin{definition}[Cotangent Bundles in $\mathbb{R}^n$]
The fiber bundle of cotangent spaces is called the \textit{cotangent bundle}. 
\[T^* \mathbb{R}^n \equiv \bigsqcup_{a \in \mathbb{R}^n} T_a^* \mathbb{R}^n\]
\end{definition}

\begin{definition}[Covector Field of Total Derivatives on $\mathbb{R}^n$]
The total derivative of $f: \mathbb{R}^n \longrightarrow \mathbb{R}$ can be interpreted as a covector field 
\[df: \mathbb{R}^n \longrightarrow T^* \mathbb{R}^n = \bigsqcup_{a \in \mathbb{R}^n} T_a^* \mathbb{R}^n\]
\end{definition}

To summarize, we have the following analogous ideas: 
\begin{align*}
    \text{Tangent Vectors at a Point}& \iff \text{Cotangent Vectors/Total Derivatives at a Point} \\
    \text{Tangent Spaces at a Point}& \iff \text{Cotangent Spaces at a Point} \\
    \text{Vector Fields} & \iff \text{Covector Fields/Total Derivatives}
\end{align*}

\subsection{Derivatives with Bases}
This abstraction, although useful, doesn't help us actually calculate these derivatives, so let's step away from it for a moment. Fortunately, we are working in $T_a \mathbb{R}^n \simeq \mathbb{R}^n$ which is already endowed with the structure of the standard orthonormal basis $\{e_i\}_{i=1}^n$. Since every vector $v \in T_a \mathbb{R}^n$ can be represented as a linear combination of the basis vectors, 
\[v = v_1 e_1 + v_2 e_2 + \ldots + v_n e_n\]
by linearity we can find $d_v$ for all $v$ if we can find 
\[d_{e_1}, \; d_{e_2}, \; \ldots, d_{e_n}\]

\begin{definition}[Partial Derivatives]
The differential operator $d_v$ that has eaten the basis vector argument $e_i$ of $\mathbb{R}^n$ is called the \textit{partial derivative}, denoted $d_{e_i}$. 
\[d_{e_i} \equiv \frac{d}{d v_i}\]
Note that when referring to partial derivatives, the basis vector $e_i$ is written in $d_{e_i}$ while the coefficient $v_i$ is written in $d / d v_i$. They are both referring to the same thing, since the former refers to the point following along the vector $e_i$ at a rate of $1$ while the latter refers to the coefficient of $e_i$ increasing at a rate of $1$. If $d_{e_i}$ eats function $f$, $d_{e_i} f$ is called the \textit{partial derivative of $f$}. 
\[d_{e_i} f \equiv \frac{\partial f}{\partial v_i}\]
which can be solved using our familiar differentiation techniques from single variable calculus. If $d_v$ eats both $f$ and point $x$, $(d_v f)(x)$ is called the \textit{partial derivative of $f$ at $x$}. 
\[(d_{e_i} f)(x) \equiv \frac{d}{dt} f(x + e_i t) \bigg|_{t=0} \equiv \frac{\partial f}{\partial v_i} (x)\]
\end{definition}

\subsubsection{Matrix Interpretation of Differentiation}
We will derive the Jacobian in full generality for when $f: \mathbb{R}^n \longrightarrow \mathbb{R}^m$. 

\begin{definition}[Jacobian Matrix]
Given that $v = v_1 e_1 + \ldots + v_n e_n$,
\begin{align*}
    d_v & = v_1 d_{e_1} + v_2 d_{e_2} + \ldots + v_n d_{e_n} \\
    & = v_1 \frac{\partial}{\partial v_1} + v_2 \frac{\partial}{\partial v_2} + \ldots + v_n \frac{\partial}{\partial v_n} 
\end{align*}
We can rewrite this equation in matrix form. 
\[d_v = \begin{pmatrix}| & \ldots & | \\
d_{e_1} & \ldots & d_{e_n} \\
| & \ldots & | \end{pmatrix} \begin{pmatrix}
v_1 \\ \vdots \\ v_n \end{pmatrix} = \begin{pmatrix}| & \ldots & | \\
\frac{\partial}{\partial v_1} & \ldots & \frac{\partial}{\partial v_n} \\
| & \ldots & | \end{pmatrix} \begin{pmatrix}
v_1 \\ \vdots \\ v_n \end{pmatrix}\]
where $v_1, \ldots, v_n$ are the scalar coefficients of $v$ and the matrix encodes all of the partial derivative operators within itself. If we plug in an arbitrary function $f$ to $d_v$, we would get
\[d_v f = \begin{pmatrix}| & \ldots & | \\
d_{e_1} f& \ldots & d_{e_n} f\\
| & \ldots & | \end{pmatrix} \begin{pmatrix}
v_1 \\ \vdots \\ v_n \end{pmatrix} = \begin{pmatrix}| & \ldots & | \\
\frac{\partial f}{\partial v_1} & \ldots & \frac{\partial f}{\partial v_n} \\
| & \ldots & | \end{pmatrix} \begin{pmatrix}
v_1 \\ \vdots \\ v_n \end{pmatrix}\]
In order to decompose this even further to a matrix form, we can also decompose the function $f: \mathbb{R}^n \longrightarrow \mathbb{R}^m$ into its component functions $f_i: \mathbb{R}^n \longrightarrow \mathbb{R}$ for $i = 1, 2, \ldots, m$. So we would have 
\[f = \begin{pmatrix}
f_1 \\f_2 \\ \vdots \\ f_m
\end{pmatrix} \implies d_{e_i} f = \begin{pmatrix}
d_{e_i} f_1 \\ d_{e_i} f_2 \\ \vdots \\ d_{e_i} f_m
\end{pmatrix}\]
and therefore, we can rewrite the above equation of $d_v f$ into 
\[ d_v f = 
\begin{pmatrix}
\frac{\partial f_1}{\partial v_1} & \frac{\partial f_1}{\partial v_2} & \frac{\partial f_1}{\partial v_3} & \ldots & \frac{\partial f_1}{\partial v_n} \\
\frac{\partial f_2}{\partial v_1} & \frac{\partial f_2}{\partial v_2} & \frac{\partial f_2}{\partial v_3} & \ldots & \frac{\partial f_2}{\partial v_n} \\
\frac{\partial f_3}{\partial v_1} & \frac{\partial f_3}{\partial v_2} & \frac{\partial f_3}{\partial v_3} & \ldots & \frac{\partial f_3}{\partial v_n} \\
\vdots & \vdots & \vdots & \ddots & \vdots\\
\frac{\partial f_m}{\partial v_1} & \frac{\partial f_m}{\partial v_2} & \frac{\partial f_m}{\partial v_3} & \ldots & \frac{\partial f_m}{\partial v_n} \\
\end{pmatrix} \begin{pmatrix}
v_1 \\ v_2 \\ \vdots \\ v_{n-1} \\ v_{n}
\end{pmatrix}\]
where the matrix is called the \textit{Jacobian matrix}. Notice how this matrix equation is consistent with the abstract mapping with three inputs.
\begin{enumerate}
    \item the function $f$ is needed to evaluate all the derivatives as elements of the matrix
    \item the point $x$ is needed to evaluate all $n \times m$ expressions $\partial f_i / \partial v_j$. That is, 
    \[d_v f (x) = \begin{pmatrix}
    \frac{\partial f_1}{\partial v_1} & \ldots & \frac{\partial f_1}{\partial v_n} \\
    \vdots & \ddots & \vdots \\
    \frac{\partial f_m}{\partial v_1} & \ldots & \frac{\partial f_m}{\partial v_n} 
    \end{pmatrix} (x) = \begin{pmatrix}
    \frac{\partial f_1}{\partial v_1} (x)& \ldots & \frac{\partial f_1}{\partial v_n} (x) \\
    \vdots & \ddots & \vdots \\
    \frac{\partial f_m}{\partial v_1} (x) & \ldots & \frac{\partial f_m}{\partial v_n} (x) 
    \end{pmatrix}\]
    \item the directional vector $v$ is needed to evaluate the vector $(v_1\;\ldots\;v_n)^T$. 
\end{enumerate}
This matrix allows us to calculate all directional derivatives at every point in all differentiable functions $f$. The entries of $d_v f$ are the "building blocks" that generate directional derivatives at the point $x$.  
\end{definition}

\begin{theorem}[Chain Rule]
Let $f: \mathbb{R}^n \longrightarrow \mathbb{R}^m$ and $g: \mathbb{R}^p \longrightarrow \mathbb{R}^n$ be two functions such that $f \circ g: \mathbb{R}^p \longrightarrow \mathbb{R}^m$ is defined. Suppose $g$ is differentiable at $x_0 \in \mathbb{R}^p$ and $f$ is differentiable at $y_0 = g(x_0) \in \mathbb{R}^n$. Then $f \circ g$ is differentiable at $x_0$ and 
\[D (f \circ g) (x_0) = D f (y_0) \cdot D g(x_0)\]
where the right hand side is the matrix product of real $m \times n$ matrix $D f(y_0)$ and $n \times p$ matrix $D g (x_0)$. 
\end{theorem}

Therefore, given the composition of function $f \circ g$, we have two methods of finding the derivative matrix of $f \circ g$ at point $x_0$. First is to explicitly compute $f \circ g$ and find its $m \times p$ derivative matrix $D (f \circ g)$, and plug in $x_0$ to get $D(f\circ g)(x_0)$.

The second way is to use the chain rule to find the individual derivative matrices $D f\big( g(x_0)\big) = D f(y_0)$ and $D g(x_0)$ and multiply them together to get the derivative matrix $D (f \circ g) (x_0)$. 

\begin{theorem}[Product, Quotient Rules]
Given that $f, g: \mathbb{R}^n \longrightarrow \mathbb{R}$ are differentiable at $x_0$. Then given that $h(x) \equiv f(x) g(x)$ for all $x$, 
\[D h(x_0) = g(x_0) D f(x_0) + f(x_0) D g(x_0)\]
Additionally, given that $g$ never vanishes and letting $k(x) \equiv f(x) / g(x)$ for all $x$, 
\[D k(x_0) = \frac{ g(x_0) D f(x_0) - f(x_0) D g(x_0)}{\big( g(x_0)\big)^2}\]
\end{theorem}

\subsection{Del Operators, Gradients}
We have successfully defined the total derivative of function $f$, denoted $df$, as a covector field 
\[df: \mathbb{R}^n \longrightarrow T^* \mathbb{R}^n\]
on $\mathbb{R}^n$, where $df (a) \in T_a \mathbb{R}^n$. We now define the gradient, which is the vector field mapping $\mathbb{R}^n \longrightarrow T \mathbb{R}^n$ on $\mathbb{R}^n$. 

\begin{definition}[Del Operator]
The \textit{del} operator takes in a function $f: \mathbb{R}^n \longrightarrow \mathbb{R}$ and outputs a vector field. In Cartesian coordinates, it is defined 
\[\nabla \equiv \begin{pmatrix}
\frac{\partial}{\partial x_1} \\ \frac{\partial}{\partial x_2}  \\ \vdots \\ \frac{\partial}{\partial x_n} 
\end{pmatrix}, \;\; \nabla f \equiv \begin{pmatrix}
\frac{\partial f}{\partial x_1} \\ \frac{\partial f}{\partial x_2}  \\ \vdots \\ \frac{\partial f}{\partial x_n} 
\end{pmatrix}\]
\begin{center}
    \includegraphics[scale=0.3]{del_operator_visual.PNG}
\end{center}
\end{definition}

\begin{definition}[Gradient]
The gradient of a function $f: \mathbb{R}^n \longrightarrow \mathbb{R}$, denoted $\nabla f$ or grad$\,f$, is just the del operator done on $f$. It is a vector field on $\mathbb{R}^n$ 
\[\nabla f: \mathbb{R}^n \longrightarrow T \mathbb{R}^n = \bigsqcup_{a \in \mathbb{R}^n} T_a \mathbb{R}^n \]
satisfying one property. The gradient is the unique vector field whose dot product with any vector $v$ at each point $a$ is the directional derivative of $f$ at $p$ along $v$. That is, $\nabla f$ is the vector field satisfying 
\[\nabla f (a) \cdot v = df_v (a) \text{   for all } a \in \mathbb{R}^n\]
where $\cdot$ is the dot product of $T_a \mathbb{R}^n$.  
\end{definition}

Note that $\nabla f$ is a vector field, while $d f$ is a covector field! However, for visual purposes, we can think of the value of the gradient as a vector in the original space $\mathbb{R}^n$, while the value of the derivative at a point can be thought of as a covector on the original space. 
\begin{align*}
    \nabla f: & \mathbb{R}^n \longrightarrow T \mathbb{R}^n = \bigsqcup_{a \in \mathbb{R}^n} T_a \mathbb{R}^n, \;\; \nabla f(a) \in T_a \mathbb{R}^n \\
    df: & \mathbb{R}^n \longrightarrow T^* \mathbb{R}^n = \bigsqcup_{a \in \mathbb{R}^n} T_a^* \mathbb{R}^n, \;\; df(a) \in T_a^* \mathbb{R}^n 
\end{align*}
The following diagram summarizes the relationship between $d$ and $\nabla$ quite nicely. 
\begin{center}
    \includegraphics[scale=0.28]{d_and_nabla_summary.PNG}
\end{center}

\begin{theorem}[Gradient as Direction of Fastest Increase]
Let $f$ be a real-valued function such that $\nabla f(x) \neq 0$. Then, at the point $x$, $\nabla f(x)$ points in the direction along which $f$ is increasing the fastest. Equivalently, $-\nabla f(x)$ points in the direction along which $f$ is decreasing the fastest. 
\end{theorem}
\begin{proof}
Note that this is a coordinate-independent proof. Let us have a tangent vector $v \in T_x \mathbb{R}^n$; since we are only interested in direction, we can normalize $v$ such that $||v|| = 1$. Evaluating it with the total derivative at $x$ gives us $(d_v f) (x)$. But by definition, 
\[(d_v f) (x) = \nabla f(x) \cdot v\]
which means that 
\begin{align*}
    \sup_{||v|| = 1} \{(d_v f)(x)\} & = \sup_{||v||=1} \{\nabla f(x) \cdot v\} \\
    & = \sup_{||v||=1} \{ ||\nabla f(x)|| ||v|| \cos(\theta)\} \\
    & = \sup \{||\nabla f(x)|| \cos(\theta)\} \\
    & = ||\nabla f(x)|| \text{ when } \theta = 0
\end{align*}
Therefore, $v$ must point in the direction of $\nabla f(x)$. 
\end{proof}

Therefore, we can interpret the gradient evaluated at a point as the tangent vector that points in the direction of fastest increase. We can also interpret the gradient $\nabla f$ itself as the vector field that determines some sort of "flow" in the domain $\mathbb{R}^n$. Therefore, if we drop a point in this field, the point will flow through $\mathbb{R}^n$ through a current determined by $\nabla f$ and will eventually end up at a local maximum. 

\subsubsection{Realization in Terms of Bases}
Since we are working in $\mathbb{R}^n$, we can use the isomorphism $T_a \mathbb{R}^n \simeq \mathbb{R}^n \simeq T_a^* \mathbb{R}^n$ to induce a basis in every tangent space and cotangent space. This allows us to write all vectors as $n$-tuples representing the coefficients of the basis vectors within a linear combination. Our familiar notion of representing vectors as  column ($n$-tuple) vectors and covectors as row vectors will be used. 

\begin{definition}[Realization of the Differential and Del Operator]
The differential operator $d$ that eats function $f: \mathbb{R}^n \longrightarrow \mathbb{R}$ and outputs covector field $df$ can be realized as a row vector. The del operator $\nabla$ that eats $f$ and outputs the (gradient) vector field $\nabla f$ is realized as a column vector. 
\[d = \begin{pmatrix}
\frac{\partial}{\partial x_1} & \frac{\partial}{\partial x_2} & \ldots & \frac{\partial}{\partial x_n}
\end{pmatrix}, \;\;\; \nabla = \begin{pmatrix}
\frac{\partial}{\partial x_1} \\ \frac{\partial}{\partial x_2} \\ \vdots \\
\frac{\partial}{\partial x_n} 
\end{pmatrix}\]
\end{definition}

\begin{definition}[Realization of Total Derivative and Gradients]
The realizations of the covector field $d f: \mathbb{R}^n \longrightarrow T^* \mathbb{R}^n$ and the vector field $\nabla f: \mathbb{R}^n \longrightarrow T \mathbb{R}^n$ is 
\[df = \begin{pmatrix}
\frac{\partial f}{\partial x_1} & \frac{\partial f}{\partial x_2} & \ldots & \frac{\partial f}{\partial x_n}
\end{pmatrix}, \;\;\; \nabla f = \begin{pmatrix}
\frac{\partial f}{\partial x_1} \\ \frac{\partial f}{\partial x_2} \\ \vdots \\
\frac{\partial f}{\partial x_n} 
\end{pmatrix}\]
If we feed these mappings the point $a \in \mathbb{R}^n$, this is same as evaluating the vectors as such: 
\[df (a) = \begin{pmatrix}
\frac{\partial f}{\partial x_1} & \ldots & \frac{\partial f}{\partial x_n}
\end{pmatrix} (a) = \begin{pmatrix}
\frac{\partial f}{\partial x_1} (a) & \ldots & \frac{\partial f}{\partial x_n} (a)
\end{pmatrix}, \; \nabla f (a)= \begin{pmatrix}
\frac{\partial f}{\partial x_1} \\ \frac{\partial f}{\partial x_2} \\ \vdots \\
\frac{\partial f}{\partial x_n} 
\end{pmatrix} (a) = \begin{pmatrix}
\frac{\partial f}{\partial x_1} (a)\\ \frac{\partial f}{\partial x_2} (a)\\ \vdots \\
\frac{\partial f}{\partial x_n} (a)
\end{pmatrix}\]
and these resulting vectors $df(a)$ and $\nabla f (a)$ are in their respective cotangent and tangent spaces (not the original domain space $\mathbb{R}^n$!). 
\end{definition}

\subsection{Graphs, Level Surfaces, and Tangent Planes}

\begin{definition}
Let the graph $G$ of $f: \mathbb{R}^n \longrightarrow \mathbb{R}$ be a subset of $\mathbb{R}^n \oplus \mathbb{R}$. Then the \textit{extended level surface of $f$} is the set of points
\[\Tilde{S}_k = \{ (x, k) \in \mathbb{R}^n \oplus \mathbb{R} \; | \; f(x) = k\}\]
We can interpret it as the cross section of the graph $G$ that forms when we intersect $G$ with the affine hyperplane $(0, k) \in \mathbb{R}^n \oplus \mathbb{R}$. The \textit{level surface} of $f$ are the points
\[S_k = \{ x \in \mathbb{R}^n \; | \; f(x) = k\}\]
that exist in $\mathbb{R}^n$, the domain of $f$. Note that there exists a canonical injection between the level surface $S_k$ and extended level surface $\Tilde{S}_k$. That is,
\[\rho: \mathbb{R}^n \longrightarrow \mathbb{R}^n \oplus \mathbb{R}, \; \rho(x) \equiv (x, k) \]
\end{definition}

To define further theorems, we must now introduce the concept of orthogonality between vectors and surfaces, along with tangent planes. 

\begin{definition}
Let there be a surface $S \subset \mathbb{R}^n$ and a vector $v \in \mathbb{R}^n$ protruding from a point $x_0 \in S$. $v$ is \textit{orthogonal}, or \textit{normal}, to $S$ at $x_0$ if for every path function 
\[p: \mathbb{R} \longrightarrow S \subset \mathbb{R}^n\]
such that $x_0 = p(t_0)$, the tangent vector of $p$ at $x_0$ is orthogonal to $v$. That is, 
\[v \cdot p^\prime (t_0) = 0 \text{ for all } p\]
\end{definition}

\begin{definition}
Let there be a surface $S \subset \mathbb{R}^n$ with a normal vector $v(x_0) \neq 0$ at $x_0$. Then, the \textit{tangent plane} of $S$ at $x_0$ is the set of points 
\[\{x \in \mathbb{R}^n \; | \; v(x_0) \cdot (x - x_0) = 0\}\]
\end{definition}
 
\begin{theorem}
Given a function $f: \mathbb{R}^n \longrightarrow \mathbb{R}$ with a point $x_0 \in S_k \subset \mathbb{R}^n$. Then, the gradient vector at $x_0$ is normal to the surface of $S_k$ at $x_0$. 
\end{theorem}
\begin{corollary}
The tangent plane of level set $S_k$ at $x_0$ is defined
\[\{ x \in \mathbb{R}^n \;|\;\triangledown f(x_0) \cdot (x - x_0) = 0\}\]
\end{corollary}

\subsection{Iterated Partial Derivatives}
\begin{definition}[Iterated Derivatives]
Let us refer to interpretation 1 when thinking about the differential operator. Given a function $f: \mathbb{R}^n \longrightarrow \mathbb{R}$ and tangent vector $v \in T \mathbb{R}^n$, we have the scalar (since codomain is $\mathbb{R}$) field
\[d_v f: \mathbb{R}^n \longrightarrow \mathbb{R}\]
that outputs the derivative of $f$ in direction $v$ at an arbitrary input point in $\mathbb{R}^n$. But notice that this scalar field is really just another function $g: \mathbb{R}^n \longrightarrow \mathbb{R}$. So interpreting $d_v f$ as a function $g$ (and assuming smoothness), we can choose another vector $w \in T \mathbb{R}^n$ to find its derivative. 
\[d_w (d_v f): \mathbb{R}^n \longrightarrow \mathbb{R}\]
This is called the \textit{2nd derivative of $f$ in direction $v$ and $w$}. Visually, 
\begin{center}
    \includegraphics[scale=0.25]{Iterated_Derivative_Function_Abstract.jpg}
\end{center}
By assuming smoothness when needed, we can extend this to get
\[d_{v_1} \big( d_{v_2} ( d_{v_3} ( \ldots (d_{v_k} f) \ldots ) \big)\]
called the \textit{kth (iterated) derivative}. 
\end{definition}

\subsubsection{Tensor Fields, Tensor Bundles}

\begin{definition}[Iterated Derivatives at a Point as Tensors]
In the 2nd derivative operator $d_w(d_v f)(x)$ shown previously, let us fix the point $x$ and function $f$ and interpret the rest as arguments. We are left with an operator that must take in 2 tangent vectors $v$ and $w$. That is, 
\[d_\cdot (d_\cdot f) (x): T_x \mathbb{R}^n \times T_x \mathbb{R}^n \longrightarrow \mathbb{R}\]
But since $d d f(x) = (d^2 f)(x)$ is bilinear, this means that
\[(d^2 f) (x) \in T_x^* \mathbb{R}^n \otimes T_x^* \mathbb{R}^n = (T_x^* \mathbb{R}^n)^{\otimes 2}\]
Similarly, the $k$th iterated total derivative of $f$ at point $x \in \mathbb{R}^n$ is  
\[(d^k f)(x) \in (T_x^* \mathbb{R}^n)^{\otimes k}, \;\; (d^k f)(x): \prod_k T_x \mathbb{R}^n \longrightarrow \mathbb{R}\]
\end{definition}

\begin{definition}[Iterated Total Derivatives as Tensor Fields]
We can unfix the point $x$ and view $d^k f$ as a mapping receiving point $x$ and vectors $v_1, \ldots, v_k$ as arguments. Then, $d^k f$ becomes a \textit{tensor field of rank $k$}. 
\[d^k f: \mathbb{R}^n \longrightarrow \bigsqcup_{x \in \mathbb{R}^n} (T^*_x \mathbb{R}^n)^{\otimes k}\]
We can just interpret $d^k f$ as a field that assigns to every point $x \in \mathbb{R}^n$ a $k$-tensor. Upon selecting a point $a$, you are given a $k$-tensor living in $(T_a^* \mathbb{R}^n)^{\otimes k}$. Then, you choose the $k$ vectors $v_1, \ldots, v_k$ that define the direction in which you want to take the iterated $k$th derivative, input them into the tensor $d^k f(a)$ and it will output a real number representing the $k$th iterated derivative you are looking for. 

This is a generalization of $df$ being a covector field that assigns to every point $x$ a cotangent vector. After choosing a point $a$, $df$ outputs the cotangent vector $df(a)$ which in turn eats a tangent vector $v$ that you choose and outputs the regular directional derivative of $f$ in direction $v$ at $a$. 
\end{definition}

Recall that the tensor algebra of vector space $T_x \mathbb{R}^n$ is the direct sum of all the tensor product spaces of $T_x \mathbb{R}^n$. That is, 
\[\mathcal{T} (T_x^* \mathbb{R}^n) \equiv \bigoplus_{i=0}^\infty (T_x^* \mathbb{R}^n)^{\otimes i}\]
Therefore, for consistency we can envelop every rank-k tensor space in the tensor algebra
\[d^k f: \mathbb{R}^n \longrightarrow \bigsqcup_{x\in \mathbb{R}^n} \mathcal{T} (T_x^* \mathbb{R}^n) = T \mathcal{T}(T_x^* \mathbb{R}^n)\]


\subsubsection{Iterated Derivatives with Bases}

\begin{definition}[Iterated Partial Derivatives]
An iterated derivative in which all the tangent vectors are basis vectors $e_i$ of $T \mathbb{R}^n$ are called \textit{partial iterated derivatives}. The $k$th partial derivative of $f$ has form 
\[d_{e_{i_1}} \big(\ldots (d_{e_{i_k}} f)\big) = \frac{\partial^k f}{\partial e_{i_1} \ldots \partial e_{i_k}}\]
The \textit{2nd partial derivative} has form 
\[d_{e_i} (d_{e_j} f) \equiv \frac{\partial}{\partial e_i} \frac{\partial f}{\partial e_j} \equiv \frac{\partial^2 f}{\partial e_i \, \partial e_j}, \;\; i, j = 1, 2, \ldots, n\]
\end{definition}


\begin{definition}[Hessian Matrix as a 2-Tensor Field]
The matrix realization of the tensor field 
\[d^2 f: \mathbb{R}^n \longrightarrow \bigsqcup_{a \in \mathbb{R}^n} (T_a^* \mathbb{R}^n)^{\otimes 2}\]
is the $n \times n$ \textit{Hessian matrix} $H$, which is of the form 
\[(H)_{i j} \equiv \frac{\partial^2 f}{\partial x_i \partial x_j}\]
of partials. Note that even though this matrix looks like it takes in a row vector (left multiplication) and a column vector (right multiplication), it actually takes in \textit{two} row vectors, both in $T_a \mathbb{R}^n$. 
\end{definition}

\begin{theorem}
Given function $f: \mathbb{R}^n \longrightarrow \mathbb{R}$ with existing 2nd derivatives, for all pairs of $1 \leq i, j \leq n$, 
\[\frac{\partial^2 f}{\partial x_i \partial x_j} = \frac{\partial^2 f}{\partial x_j \partial x_i}\]
$\implies$ the Hessian matrix of $f$ is symmetric. 
\end{theorem}
\begin{proof}
By abuse of notation, we let us focus on two variables $x, y$ and ignore the rest. Then, the partial derivatives $f_{xy}$ and $f_{yx}$ at a point $(x_0, y_0)$ can be expressed as double limits: 
\[f_{xy} (x_0, y_0) = \lim_{y \rightarrow y_0} \frac{f_x (x_0, y) - f_x (x_0, y_0)}{y - y_0}\]
We can use the two limit definitions of partial derivatives
\[f_x (x_0, y) = \lim_{x \rightarrow x_0} \frac{f(x, y) - f(x_0, y)}{x-x_0}, \;\;\;\;\; f_x (x_0, y_0) = \lim_{x \rightarrow x_0} \frac{f(x, y_0) - f(x_0, y_0)}{x-x_0}\]
to get the two partials 
\begin{align*}
    f_{xy} (x_0, y_0) & = \lim_{y \rightarrow y_0} \frac{ \lim_{x \rightarrow x_0} \frac{f(x, y) - f(x_0, y)}{x-x_0} - \lim_{y \rightarrow y_0} \frac{f(x, y_0) - f(x_0, y_0)}{x-x_0}}{y - y_0} \\
    & =  \lim_{y \rightarrow y_0} \lim_{x \rightarrow x_0} \bigg( \frac{f(x, y) - f(x_0, y) - f(x, y_0) + f(x_0, y_0)}{(x - x_0) (y - y_0)} \bigg) \\
    f_{yx} (x_0, y_0) & = \lim_{x \rightarrow x_0} \frac{ \lim_{y \rightarrow y_0} \frac{f(x, y) - f(x, y_0)}{y-y_0} - \lim_{y \rightarrow y_0} \frac{f(x_0, y) - f(x_0, y_0)}{y-y_0}}{x - x_0} \\
    & = \lim_{x \rightarrow x_0} \lim_{y \rightarrow y_0} \bigg( \frac{f(x, y) - f(x, y_0) - f(x_0, y) + f(x_0, y_0)}{(y-y_0) (x-x_0)} \bigg)
\end{align*}
We can see that therefore $f_{xy} = f_{yx}$ for all $(x_0, y_0)$. 
\end{proof}

\begin{corollary}
Given function $f: \mathbb{R}^n \longrightarrow \mathbb{R}$, let 
\[(\alpha_1, \alpha_2, ..., \alpha_k)\]
be any $k$ distinct numbers from $\{1, 2, ..., n\}$, and let 
\[p(\alpha_1, \alpha_2, ..., \alpha_k) \equiv (p(\alpha_1), p(\alpha_2), ..., p(\alpha_n))\]
be any permutation of them. Then
\[\frac{\partial^k f}{\partial x_{\alpha_1}...\partial x_{\alpha_k}} = \frac{\partial^k f}{\partial x_{p(\alpha_1)}...\partial x_{p(\alpha_k)}}\]
for all $p$. 
\end{corollary}

\subsection{Linear, Quadratic, and Taylor Approximations}

\subsubsection{First Order Approximation}
We have defined the total derivative of a function $f$ at a point $x_0$, denoted $df (x_0)$ as the cotangent vector living in the cotangent space $T_{x_0}^* \mathbb{R}^n$. We assume that $f$ is a vector-valued function when we geometrically describe the construction of the total derivative as a linear approximation. 

Most functions $f: \mathbb{R}^n \longrightarrow \mathbb{R}^m$ are nonlinear and may behave in all kinds of ways; we can control them by setting the condition that they must be smooth. Now, since these smooth functions are differentiable, we can construct a linear function that best "approximates" $f$. 

To do this, we first choose an origin point $x_0 \in \mathbb{R}^n$ and evaluate $f(x_0)$. Denote the approximation function as $P_{x_0}^1$. Given that we want to evaluate $f(x_0 + v)$ for some small vector $v \in T_{x_0} \mathbb{R}^n$, it turns out that as the point moves from $x_0 \rightarrow x_0 + v$, $f(x_0)$ doesn't move linearly towards $f(x_0 + v)$ (marked by the curved line). 
\begin{center}
    \includegraphics[scale=0.25]{Function_Moving_in_Curvy_way.PNG}
\end{center}
So what is the best way to approximate this? In order to choose the "best" linear approximation to these nonlinear motions, we look at two criterion
\begin{enumerate}
    \item the approximation $P_{x_0}^1 (x)$ must be equal to the actual function $f(x)$ at $x_0$.  
    \item It satisfies
    \[\lim_{x \rightarrow x_0} \frac{||f(x) - P_{x_0}^1 (x)||}{||x - x_0||} = 0\]
    This can be interpreted geometrically by viewing $P_{x_0}^1$ as a tangent affine subspace on the graph of $f$ at $x_0$, where the directional derivatives of $f$ and $P_{x_0}^1$ align at $x_0$ for all directional vectors $v \in T_{x_0} \mathbb{R}^n$. 
\end{enumerate}
Conveniently, we can use the total derivative $d f(x_0)$, which is a linear map, and use it to create the affine linear function  $f(x_0 + v) \approx f(x_0) + (d_v f)(x_0)$. 
\begin{center}
    \includegraphics[scale=0.25]{Affine_Linear_Function_Approximation.PNG}
\end{center}
It can be clearly seen that as the vector moves from $x_0 \rightarrow x_0 + v$, the output vector $f(x_0) + d_v f(x_0)$ moves in a straight line, which is consistent with our claim of it being an affine linear transformation. Note that by abuse of language, we call this a linear approximation, when it is in fact not linear and rather an \textit{affine} linear approximation. 

In order to make this approximation a function of $x = x_0 + v$, we change the above formula to
\[f(x) \approx P_{x_0}^1 (x) \equiv f(x_0) + (d_{(x - x_0)} f) (x_0)\]
which satisfies both conditions 1 and 2. 

\begin{definition}[First-Order Approximation of Vector-Valued $f$ at $x_0$ and its Matrix Realization]
The first order affine approximation of a function $f: \mathbb{R}^n \longrightarrow \mathbb{R}^m$ centered at $x_0$ is 
\[P_{x_0}^1 (x) \equiv f(x_0) + (d_{(x-x_0)} f) (x_0)\]
The matrix realization of this can be expressed with the Jacobian matrix $J f(x_0)$. 
\begin{align*}
    P_{x_0}^1 (x) & \equiv f(x_0) + J f(x_0) (x - x_0) \\
    & = \begin{pmatrix}
    f_1 (x_0) \\ \vdots \\ f_m (x_0) \end{pmatrix} + \begin{pmatrix}
    \frac{\partial f_1}{\partial x_1} (x_0) & \ldots & \frac{\partial f_1}{\partial x_n} (x_0) \\
    \vdots & \ddots & \vdots \\
    \frac{\partial f_m}{\partial x_1} (x_0) & \ldots & \frac{\partial f_m}{\partial x_n} (x_0) 
    \end{pmatrix} \begin{pmatrix}
    x_1 - x_{01} \\ \vdots \\ x_n - x_{0n}
    \end{pmatrix}
\end{align*}
Note that when interpreting $f$ as a vector valued function, the linear term $(df) (x_0)$ is \textit{not} a tensor since it takes
$(df) (x_0): T_{x_0} \mathbb{R}^n \longrightarrow \mathbb{R}^m$. 
\end{definition}

We can interpret it as a tensor when considering scalar-valued functions $f$. 

\begin{definition}[First-Order Approximation of Real-Valued $f$ at $x_0$ and its Matrix Realization]
The first order affine approximation of a real-valued function $f: \mathbb{R}^n \longrightarrow \mathbb{R}$ centered at $x_0$ is 
\[P_{x_0}^1 \equiv f(x_0) + (d_{(x - x_0)} f) (x_0)\]
Where $(df)(x_0)$ is a $1$-tensor taking in tangent vector $v = x - x_0 \in T_{x_0}\mathbb{R}^n$, and $f(x_0)$ is a $0$-tensor. The matrix realization of this can be expressed with the Jacobian (row vector) matrix $J f(x_0)$. 
\begin{align*}
    P_{x_0}^1 (x) & \equiv f(x_0) + Jf (x_0) (x - x_0) \\
    & = f\begin{pmatrix} x_{01} \\ \vdots \\ x_{0n}
    \end{pmatrix} + \begin{pmatrix}
    \frac{\partial f}{\partial x_1} (x_0) & \ldots & \frac{\partial f}{\partial x_n} (x_0) 
    \end{pmatrix} \begin{pmatrix}
    x_1 - x_{01} \\ \vdots \\ x_n - x_{0n}
    \end{pmatrix}
\end{align*}
Note that the Jacobian row matrix of $f: \mathbb{R}^n \longrightarrow \mathbb{R}$ is the realization of the $1$-tensor. In summation form, the approximation expands to
\[P_{x_0}^1 (x) = f(x_0) + \sum_{i=1}^n \bigg(\frac{\partial f}{\partial x_i} (x_0)  \big(x_i - x_{0i}\big)\bigg)\]
\end{definition}

\subsubsection{Second Order Approximation}
We can improve the linear approximation $P_{x_0}^1$ to a quadratic approximation $P_{x_0}^2$ by utilizing higher order derivatives. Using similar logic, given that we know $f(x_0)$, for some small vector $v$ we can approximate $f(x_0 + v)$ as 
\[f(x_0 + v) \approx f(x_0) + (d_v f) (x_0) + \frac{1}{2!}(d^2_{v, v} f)(x_0)\]
Note that this is a quadratic approximation, and therefore has more "flexibility" than the linear function in approximating $f$. 
\begin{center}
    \includegraphics[scale=0.25]{Affine_Quadratic_Function_Approximation.PNG}
\end{center}
We can make this a function of $x = x_0 + v$ and define it as such. 

\begin{definition}[Second-Order Approximation of Vector-Valued $f$ at $x_0$]
The second order affine approximation of a function $f: \mathbb{R}^n \longrightarrow \mathbb{R}^m$ centered at $x_0$ is 
\[P_{x_0}^2 (x) \equiv f(x_0) + (d_{(x - x_0)} f) (x_0) + \frac{1}{2!}(d_{(x - x_0, x - x_0)} f) (x_0)\]
Unfortunately, we cannot write the matrix realization of this polynomial, since this would require higher dimensional analogies of matrices. Rather, we can just interpret $f(x_0)$ as a scalar, and the other terms as 
\begin{align*}
    (df)(x_0): & T_{x_0} \mathbb{R}^n \longrightarrow \mathbb{R}^m \\
    (d^2 f)(x_0): & T_{x_0} \mathbb{R}^n \times T_{x_0} \mathbb{R}^n \longrightarrow \mathbb{R}^m
\end{align*}
which take in tangent vectors $v = x - x_0 \in T_{x_0} \mathbb{R}^n$. 
\end{definition}

We can, however, write down the matrix realization of the second order approximation of a real-valued function $f$.

\begin{definition}[Second-Order Approximation of Real-Valued $f$ at $x_0$]
The second order affine approximation of a function $f: \mathbb{R}^n \longrightarrow \mathbb{R}$ centered at $x_0$ is 
\[P_{x_0}^2 \equiv f(x_0) + (d_{(x - x_0)} f) (x_0) + \frac{1}{2!} (d^2_{(x - x_0, x - x_0)} f) (x_0)\]
where $(d^2 f)(x_0)$ is a 2-tensor (taking in tangent vector $v = x-x_0$), $(d f)(x_0)$ is a 1-tensor, and $f(x_0)$ is a 0-tensor. The matrix realization of this can be expressed with the Jacobian (row vector) matrix $J f(x_0)$ and the $n \times n$ Hessian matrix $H f(x_0)$. 
\begin{align*}
    P_{x_0}^2 (x) & \equiv f(x_0) + Jf(x_0) (x - x_0) + \frac{1}{2!} (x - x_0)^T H f(x_0) (x - x_0) \\
    & = f \begin{pmatrix} x_{01} \\ \vdots \\ x_{0n} \end{pmatrix} + \begin{pmatrix}
    \frac{\partial f}{\partial x_1} (x_0) & \ldots & \frac{\partial f}{\partial x_n} (x_0) \end{pmatrix} \begin{pmatrix}
    x_1 - x_{01} \\ \vdots \\ x_n - x_{0n}
    \end{pmatrix} \\
    & \;\;\;\;\;\;\;\;\;\;\;\; +\frac{1}{2!} \begin{pmatrix}
    x_1 - x_{01} & \ldots & x_n - x_{0n}
    \end{pmatrix} \begin{pmatrix}
    \frac{\partial^2 f}{\partial x_1 \partial x_1} (x_0) & \ldots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
    \vdots & \ddots & \vdots \\
    \frac{\partial^2 f}{\partial x_n \partial x_1} (x_0) & \ldots & \frac{\partial^2 f}{\partial x_n \partial x_n}
    \end{pmatrix} \begin{pmatrix}
    x_1 - x_{01} \\ \vdots \\ x_n - x_{0n}
    \end{pmatrix} 
\end{align*}
In summation form, the multivariate Taylor quadratic expands to
\[P_{x_0}^2 (x) = f(x_0) + \sum_{i=1}^n \bigg(\frac{\partial f}{\partial x_i} (x_0)  \big(x_i - x_{0i}\big)\bigg) + \frac{1}{2!} \sum_{i, j = 1}^n \bigg( \frac{\partial^2 f}{\partial x_i \partial x_j} (x_0) \big(x_i - x_{0i}\big) \big(x_j - x_{0j}\big)\bigg)\]
\end{definition}

Note that the quadratic term is just a $2$-tensor. This is significant because we can notice that every higher order $n$th term (cubic, quartic, etc.) is just a multilinear map that accepts one argument $v$, $n$ times. For example, a quadratic is a $2$-tensor accepting 2 copies of $v \in T_{x_0} \mathbb{R}^n$. 
\begin{center}
    \includegraphics[scale=0.3]{Tensor_As_a_Quadratic.PNG}
\end{center}
Therefore, increasing the vector $v$ by a factor of $2$ would, by linearity of the first argument of $(d^2 f)(x_0)$ increase the output by $2$ and by linearity of the second argument also increase the output by $2$, resulting in a total increase by $4$ times. 

Generalizing this to $(d^k f)(x_0)$, increasing the input vector $v$ by a factor of $\alpha$ would increase the output $(d^k_{(v, \ldots, v)} f)(x_0)$ of the $k$-tensor by $\alpha^k$. 
\begin{center}
    \includegraphics[scale=0.2]{Tensor_as_a_Higher_Order_Term.PNG}
\end{center}

\subsubsection{Higher Order Approximations}
We can extend this approximation to higher order multivariate polynomials. The $k$th order approximation of function $f: \mathbb{R}^n \longrightarrow \mathbb{R}^m$ centered at $x_0$ is 
\[f(x_0 + v) \approx f(x_0) + (d_v f) (x_0) + \frac{1}{2!} (d^2_{v, v} f) (x_0) + \frac{1}{3!}(d^3_{v, v, v} f)(x_0) + \ldots + \frac{1}{k!} (d^k_{v, \ldots , v} f)(x_0)\]
where all the $(d^i f)(x_0)$'s are multilinear mappings (but not tensors, since codomain is not $\mathbb{R}$)
\[(d^i f)(x_0): \prod_i T_{x_0} \mathbb{R}^n \longrightarrow \mathbb{R}^m\]
with respect to $v$. Treating this as a function of $x = x_0 + v$, we get the following definition. 

\begin{definition}[Higher-Order Approximation of Vector-Valued $f$ at $x_0$]
The $k$th order affine approximation of function $f: \mathbb{R}^n \longrightarrow \mathbb{R}^m$ centered at $x_0$ is
\[P_{x_0}^k (x) \equiv f(x_0) + (d_{(x - x_0)} f)(x_0) + \frac{1}{2!} (d^2_{(x-x_0, x-x_0)} f)(x_0) + \ldots \frac{1}{k!} (d^k_{(x - x_0, \ldots, x-x_0)} f)(x_0)\]
Again, this expression encompasses high-dimensional terms that cannot be written in matrices, so it is best to interpret each $(d^i f)(x_0)$'s as multilinear mappings
\begin{align*}
    (df)(x_0): & T_{x_0} \mathbb{R}^n \longrightarrow \mathbb{R}^m \\
    (d^2f)(x_0): & T_{x_0} \mathbb{R}^n \times T_{x_0} \mathbb{R}^n \longrightarrow \mathbb{R}^m \\
    \ldots & \ldots \\
    (d^kf)(x_0): & \prod_k T_{x_0} \mathbb{R}^n \longrightarrow \mathbb{R}^m 
\end{align*}
which take in tangent vectors $v = x - x_0 \in T_{x_0} \mathbb{R}^n$.
\end{definition}

We distinguish a separate definition for real valued functions $f$.

\begin{definition}[Higher-Order Approximation of Real-Valued $f$ at $x_0$]
The $k$th order affine approximation of function $f: \mathbb{R}^n \longrightarrow \mathbb{R}$ centered at $x_0$ is
\[P_{x_0}^k (x) \equiv f(x_0) + (d_{(x - x_0)} f)(x_0) + \frac{1}{2!} (d^2_{(x-x_0, x-x_0)} f)(x_0) + \ldots \frac{1}{k!} (d^k_{(x - x_0, \ldots, x-x_0)} f)(x_0)\]
where $(d^i f)(x_0)$ is a tensor of rank $i$, and the scalar $f(x_0)$ is a $0$-tensor. 
\begin{align*}
    &(df)(x_0) \in T_{x_0}^* \mathbb{R}^n, & &(df)(x_0): T_{x_0} \mathbb{R}^n \longrightarrow \mathbb{R}^m \\
    &(df)(x_0) \in (T_{x_0}^* \mathbb{R}^n)^{\otimes 2}, &&(d^2f)(x_0): T_{x_0} \mathbb{R}^n \times T_{x_0} \mathbb{R}^n \longrightarrow \mathbb{R}^m \\
    &\ldots & &\ldots \\
    &(d^k f)(x_0) \in (T_{x_0}^* \mathbb{R}^n)^{\otimes k}, && (d^kf)(x_0): \prod_k T_{x_0} \mathbb{R}^n \longrightarrow \mathbb{R}^m 
\end{align*}
Since tensors of rank greater than $2$ cannot be represented in matrix notation, we must embrace this abstract Taylor polynomial. 
\end{definition}

\begin{example}
The summation representation of the third order approximation of function $f: \mathbb{R}^n \longrightarrow \mathbb{R}$ centered at $x_0$ is: 
\begin{align*}
    f(x) & \approx f(x_0) + \sum_{i=1}^n \bigg(\frac{\partial f}{\partial x_i} (x_0) \big(v_i\big) \bigg) + \frac{1}{2!} \sum_{i, j = 1}^n \bigg( \frac{\partial^2 f}{\partial x_i \partial x_j} (x_0) \big(v_i\big) \big(v_j\big)\bigg) \\
    & + \frac{1}{3!} \sum_{i, j, k = 1}^n \bigg( \frac{\partial^3 f}{\partial x_i \partial x_j \partial x_k} (x_0) \big(v_i\big) \big(v_j\big) \big(v_k\big) \bigg)
\end{align*}
where $v = x - x_0$, or in components, $v_i = x_i - x_{0i}$ for $i = 1, 2, \ldots n$. 
\end{example}

\subsection{Local/Global Extrema, Lagrange Multipliers}
Note that while the approximation isn't exact, the $n$th-degree approximation of $f$ "mimics" $f$ in the way that the iterated total derivatives, up to the $n$th order, are the same as the iterated partial derivatives of $f$ at the point $x_0$. This allows us to analyze the behavior of the function $f$ up to the $n$th order at $x_0$ by looking only at the components of its $n$th degree Taylor expansion. 

\subsubsection{Local Extrema}
An application of this is to find the local extrema of $f$ using the second total derivative. In order for the extrema to be properly defined, the codomain of $f$ must be ordered, so we will focus on scalar functions $f: \mathbb{R}^n \longrightarrow \mathbb{R}$. 

\begin{definition}[Local Extrema]
Given a function $f: \mathbb{R}^n \longrightarrow \mathbb{R}$, a point $x_0 \in \mathbb{R}^n$ is a \textit{local minimum} if there exists a neighborhood $U$ of $x_0$ such that 
\[f(x) \geq f(x_0) \text{ for every } x \in U\]
Similarly, $x_0$ is a \textit{local maximum} if there exists a neighborhood $U$ of $x_0$ such that 
\[f(x) \leq f(x_0) \text{ for every } x \in U\]
\end{definition}

\begin{theorem}[1st Derivative Test]
If $x_0$ is a local extremum of a smooth function $f: \mathbb{R}^n \longrightarrow \mathbb{R}$, then $d f(x_0) = 0$, the zero covector in the cotangent space $T_{x_0}^* \mathbb{R}^n$. That is, for every tangent vector $v \in T_{x_0} \mathbb{R}^n$, every directional derivative of $f$ through $x_0$ in direction $v$ is $0$. That is, 
\[x_0 \text{ local extremum} \implies \text{total derivative is 0}\]
However, the converse of this theorem is not true. 
\end{theorem}

In order to determine whether a critical point $x_0$ is a relative maximum, minimum, or neither, we use the second derivative test. 

\begin{theorem}[2nd Derivative Test]
Let $x_0$ be a critical point of smooth function $f: \mathbb{R}^n \longrightarrow \mathbb{R}$. That is, $d f(x_0) = 0$. Then, 
\begin{enumerate}
    \item $x_0$ is a local minimum if $d^2 f(x_0)$ is a positive definite linear map (note that $(d^2 f(x_0)$ is technically a 2-tensor with two inputs, but both inputs by definition must be the same)
    \item $x_0$ is a local maximum if $d^2 f(x_0)$ is a negative definite linear map
    \item $x_0$ is a \textit{saddle point} if $d^2 f(x_0)$ is neither positive definite nor negative definite. 
\end{enumerate}
\end{theorem}

Visually, this makes sense since given a critical point $x_0$, the derivative matrix would be $0$, meaning that the 2nd degree Taylor expansion of $f$ near $x_0$ would be in form
\[f(x) \approx f(x_0) + \frac{1}{2} (d^2_{x-x_0, x-x_0)} f)(x_0)\]
If $H f(x_0)$ is positive definite, then by definition 
\[\frac{1}{2} (d^2_{x-x_0, x-x_0)} f)(x_0) > 0\]
for all $x$ near $x_0$, and so $f$ would increase in every direction within the neighborhood of $x_0$. 
\begin{center}
    \includegraphics[scale=0.25]{Local_Minima_Abstract.PNG}
\end{center}
or when imagining functions of two variables $f: \mathbb{R}^2 \longrightarrow \mathbb{R}$, the neighborhood of $x_0$ looks like a paraboloid.
\begin{center}
    \includegraphics[scale=0.25]{Local_Minimia_Paraboloid.PNG}
\end{center}
The logic follows similarly for negative definite map $(d^2 f)(x_0)$. If $(d^2 f)(x_0)$ is neither positive nor negative definite, then $\frac{1}{2} (d^2_{(x-x_0, x-x_0)} f)(x_0)$ could be positive or negative, depending on which direction vector $v = x - x_0$ we choose for computing the directional derivative. Therefore, $f$ will increase for certain $h$ and decrease for other $h$ and is not an extremum. We call this a saddle point since the graph of functions $f: \mathbb{R}^2 \longrightarrow \mathbb{R}$ in $\mathbb{R}^3$ looks like a saddle within the neighborhood of $x_0$. 
\begin{center}
    \includegraphics[scale=0.4]{Saddle.PNG}
\end{center}

\subsubsection{Global Extrema}

\begin{definition}[Global Extrema]
Given $f: A \subset \mathbb{R}^n \longrightarrow \mathbb{R}$, a point $x_0 \in A$ is said to be an \textit{absolute, or global, maximum} if 
\[f(x_0) \geq f(x) \text{ for all } x \in A\]
and a \textit{global minimum} if 
\[f(x_0) \leq f(x) \text{ for all } x \in A\]
\end{definition}

Unfortunately, determining whether a point $x_0$ is a local extremum requires us to define an open neighborhood $U$ around $x_0$ (such that every point $x \in U$ is greater/smaller than $x_0$). This means that we can only determine local extrema within open sets in $\mathbb{R}^n$. Therefore, we must modify our procedure when looking for extrema on functions defined over closed bounded sets. 

We now describe a method of computing global extrema. 

\begin{theorem}[Computing Global Extrema]
Let $f: D \subset \mathbb{R}^n \longrightarrow \mathbb{R}$ be a function defined on a closed and bounded set $D \equiv U \cup \partial U$, where $U$ is open and $\partial U$ is the boundary of $D$. To find the global extrema on $D$, we find all the stationary points of:
\begin{enumerate}
    \item $f$ defined over open $U$
    \item $f$ defined over $\partial U$, which can be done by composing the path functions $p: \mathbb{R}^{n-1} \longrightarrow \partial U$ and $f: \partial U \longrightarrow \mathbb{R}$ and finding the stationary points of $f \circ p$. 
\end{enumerate}
We take all these stationary points and choose the largest to be the global maximum and the smallest to be the global minimum. 
\end{theorem}

\subsubsection{The Method of Lagrange Multipliers}
In many cases we are required to find the local extrema of a function $f: \mathbb{R}^n \longrightarrow \mathbb{R}$ subject to a system of equality constraints (i.e. subject to the condition that one of more equations have to be satisfied exactly by the chosen values of the variables) of the form: 
\[g_1 (x) = 0, g_c (x) = 0, \ldots, g_c (x) = 0 \]
which can be summarized into the constraint $g: \mathbb{R}^n \longrightarrow \mathbb{R}^c$
\[g = \begin{pmatrix}
g_1 \\ \vdots \\ g_c
\end{pmatrix} \implies g(x) = \begin{pmatrix}
g_1 (x) \\ \vdots \\ g_c (x)
\end{pmatrix} = 0\]
Sometimes, these constraints are written as $g(x) = r$ for some vector $r$, but we can just equivalently set the constraint function as $g(x)-r = 0$. In physics, these types of "well-behaved" constraints are known as \textit{holonomic constraints}. Here is an example of a function $f: \mathbb{R}^2 \longrightarrow \mathbb{R}$ constrained to the unit circle, where $g(x, y) = x^2 + y^2 - 1= 0 $.
\begin{center}
    \includegraphics[scale=0.2]{Function_with_Constraints.PNG}
\end{center}
To solve this constraint problem, we use the method of Lagrange multipliers. The basic idea is to convert a constrained problem into a form such that the derivative test of an unconstrained problem can still be applied. The relationship between the gradient of the function and gradients of the constraints rather naturally leads to a reformulation of the original problem, known as the \textit{Lagrangian function}. That is, in order to find the maximum/minimum of $f$ subjected to the equality constraint $g(x) = 0$, we form the Lagrangian function
\[\mathcal{L}(x, \lambda) \equiv f(x) - \lambda g(x)\]
and find the stationary points of $\mathcal{L}$ considered as a function of $x \in \mathbb{R}^n$ and the Lagrange multiplier $\lambda \in \mathbb{R}$. 

The main advantage to this method is that it allows the optimization to be solved without explicit parameterization in terms of the constraints. 

\begin{theorem}[Lagrange Multipliers Theorem]
Let $f: \mathbb{R}^n \longrightarrow \mathbb{R}$ be a smooth function and let $g(x) = 0$, where $g: \mathbb{R}^n \longrightarrow \mathbb{R}^c$, be a system of smooth constraint equations. 
\[g \equiv \begin{pmatrix}
g_1 \\ \vdots \\ g_c
\end{pmatrix}\]
Let $x^*$ be an optimal solution to the optimization problem of maximizing $f(x)$ subject to the constraint $g(x) = 0$ such that $\rank\big(d g(x^*)\big) = c < n$. Note that $d g(x^*): T_{x^*} \mathbb{R}^n \longrightarrow \mathbb{R}^c$ is the linear differential map of $g$ evaluated at $x^*$, but it can equivalently be interpreted as a map
\[d g(x^*): T_{x^*} \mathbb{R}^n \times (\mathbb{R}^c)^* \longrightarrow \mathbb{R}\]
or as a map
\[d g(x^*): (\mathbb{R}^c)^* \longrightarrow T^*_{x^*} \mathbb{R}^n\]
Then, there exists a unique vector $\lambda^* \in (\mathbb{R}^c)^*$ of Lagrange multipliers $\lambda^*_1, \ldots, \lambda^*_c$ such that the two cotangent vectors of $T^*_{x^*} \mathbb{R}^n$ are equal. 
\[d f(x^*) = d g(x^*) (\lambda^*)\]
Or equivalently, that the two tangent vectors (which are the gradient vector fields $\nabla f$ and $\nabla g (\lambda^*)$ evaluated at $x^*$). 
\[\nabla f(x^*) = \nabla g(x^*)(\lambda^*)\]
Conventionally, we use the latter equation comparing the gradients. 
\end{theorem}

\begin{corollary}[Matrix Realization of the Lagrange Multipliers Theorem]
Let $f: \mathbb{R}^n \longrightarrow \mathbb{R}$ be the objective function and let $g: \mathbb{R}^n \longrightarrow \mathbb{R}^c$ be the constraints function with components $g_i$, both smooth. Let the vector $x^*$ be the optimal solution to the optimization problem of maximizing $f(x)$ subject to the constraint $g(x) = 0$ such that $\rank \big(J g(x^*)\big) = c < n$, where $J g(x^*)$ is the Jacobian matrix of partial derivatives of $g$ evaluated at $x^*$. Then, there exists a unique vector $\lambda^*$ such that 
\[J f (x^*) = \lambda^{*T} J g (x^*)\] 
which in matrix terms is: 
\[\begin{pmatrix}
\frac{\partial f}{\partial x_1} (x^*) & \ldots & \frac{\partial f}{\partial x_c} (x^*) \end{pmatrix} = \begin{pmatrix}
\lambda^*_1 & \ldots & \lambda^*_c \end{pmatrix} \begin{pmatrix}
\frac{\partial g_1}{\partial x_1} (x^*)& \ldots & \frac{\partial g_1}{\partial x_n} (x^*)\\
\vdots & \ddots & \vdots \\
\frac{\partial g_c}{\partial x_1} (x^*)& \ldots & \frac{\partial g_c}{\partial x_n}(x^*)
\end{pmatrix}\]
But conventionally, we express things in terms of vectors, so we just take the transpose of everything to get the gradient form: 
\begin{align*}
    \nabla f (x^*) & = \big( J g(x^*)\big)^T \lambda^* \\
    & = \lambda^*_1 \nabla g_1 (x^*) + \lambda^*_2 \nabla g_2 (x^*) + \ldots + \lambda^*_c \nabla g_c (x^*) \\
    & = \sum_{i=1}^c \lambda^*_i \nabla g_c (x^*) 
\end{align*}
which has a matrix realization of 
\begin{align*}
\begin{pmatrix}
\frac{\partial f}{\partial x_1} (x^*) \\ \vdots\\ \frac{\partial f}{\partial x_n} (x^*) \end{pmatrix} & = \begin{pmatrix}
\frac{\partial g_1}{\partial x_1} (x^*)& \ldots & \frac{\partial g_c}{\partial x_1} (x^*)\\
\vdots & \ddots & \vdots \\
\frac{\partial g_1}{\partial x_n} (x^*)& \ldots & \frac{\partial g_c}{\partial x_n}(x^*)
\end{pmatrix} \begin{pmatrix}
\lambda^*_1 \\ \vdots \\ \lambda^*_c \end{pmatrix} \\
& = \lambda^*_1 \begin{pmatrix}
\frac{\partial g_1}{\partial x_1} (x^*) \\ \vdots \\ \frac{\partial g_1}{\partial x_n}(x^*)
\end{pmatrix} + \lambda^*_2 \begin{pmatrix}
\frac{\partial g_2}{\partial x_1} (x^*) \\ \vdots \\ \frac{\partial g_2}{\partial x_n}(x^*)
\end{pmatrix} + \ldots + \lambda^*_c \begin{pmatrix}
\frac{\partial g_c}{\partial x_1} (x^*) \\ \vdots \\ \frac{\partial g_c}{\partial x_n}(x^*)
\end{pmatrix}
\end{align*}
This equation tells us that at any critical points $x^*$ of $f$ evaluated under the equality constraints, the gradient of $f$ at $x^*$ can be expressed as a linear combination of the gradients of the constraints $\nabla g_i (x^*)$ (at $x^*$), with the Lagrange multipliers acting as coefficients. Therefore, finding the critical points $x^*$ of $f$ constrained with $g$ is equivalent to solving the system of equations 
\begin{align*}
    g(x) & = 0 \\
    \nabla f(x^*) & = \big(J g(x^*)\big)^T \lambda^*
\end{align*}
which can be rewritten as
\begin{align*}
    c \text{ constraint equations} & \begin{cases}
    g_1 (x) & = 0 \\
    \ldots & = 0 \\
    g_c (x) & = 0
    \end{cases} \\
    n \text{ Lagranaian equations} & \begin{cases}
   \frac{\partial f}{\partial x_1} (x^*) & = \lambda^*_1 \frac{\partial g_1}{\partial x_1} (x^*) + \lambda^*_2 \frac{\partial g_2}{\partial x_1} (x^*) + \ldots + \lambda^*_c \frac{\partial g_c}{\partial x_1} (x^*) \\
    \ldots & = \ldots \\
    \frac{\partial f}{\partial x_n} (x^*) & = \lambda^*_1 \frac{\partial g_1}{\partial x_n} (x^*) + \lambda^*_2 \frac{\partial g_2}{\partial x_n} (x^*) + \ldots + \lambda^*_c \frac{\partial g_c}{\partial x_n} (x^*) 
    \end{cases}
\end{align*}
\end{corollary}

Let us introduce a visualization for when where is a single constraint $g: \mathbb{R}^n \longrightarrow \mathbb{R}$. From the properties of the gradient, $\nabla f(x_0)$ is orthogonal to the level set of points satisfying $f(x) = f(x_0)$ at point $x_0$. Note that the constraint function $g$ also maps $\mathbb{R}^n \longrightarrow \mathbb{R}$, and so it has its own level surfaces. We can see that the point where the contour line of $g(x) = 0$ tangentially touches the contours of $f$ is the maximum. Since it intersects it tangentially, the gradient vector at that point $\nabla g(x_0)$ is parallel to $\nabla f(x_0)$. 
\begin{center}
    \includegraphics[scale=0.22]{Lagrange_Multiplier_Single_Constraint.PNG}
\end{center}
We can visualize this for multiple constraints as well, where $\nabla f(x_0)$ (the gradient vector of $f$ at $x^*$) can be expressed as a linear combination of $\nabla g_1 (x_0)$ and $\nabla g_2 (x_0)$ (gradient vectors of the constraint functions at $x^*$). 
\begin{center}
    \includegraphics[scale=0.27]{Lagrange_Multiplier_Multiple_Constraints.PNG}
\end{center}

From the properties of the gradient introduced before, $\triangledown f(x_0)$ is orthogonal to the level set of points satisfying $f(x) = c$ at the point $x_0$. But this level set $f(x) = c$ actually intersects the level set determined by $g(x) = c$ at the point $x_0$ and is indistinguishable from each other at $x_0$. This means that $\triangledown g(x_0)$ is normal the level set of $g(x) = c$ at $x_0 \iff $ it is normal to the level set of $f(x) = c$ at $x_0$. But $\triangledown f(x_0)$ is also normal at that point, so $\triangledown f(x_0)$ must be parallel to $\triangledown g(x_0)$. 

\subsection{k-times Continuously Differentiable Functions}
So far, we have thrown around the words continuous and differentiable a lot, but note that continuity is a topological property, while differentiability is a property of functions mapping between Euclidean spaces. More specifically, for $f: \mathbb{R}^n \longrightarrow \mathbb{R}^m$
\begin{enumerate}
    \item $f$ continuous at $x_0$ means that the preimage of every open neighborhood of $f(x_0)$ in $\mathbb{R}^m$ under $f$ is an open neighborhood of $x_0$ in $\mathbb{R}^n$. 
    \item $f$ smooth (i.e. differentiable) at $x_0$ means that there exists a first order approximation $P_{x_0}^1$ centered at $x_0$. In other words, the covector $d f(x_0)$ is well defined for every input tangent vector $v \in T_{x_0} \mathbb{R}^n$. 
\end{enumerate}
We will examine the relationship between these properties. First, it is clear that if a function is differentiable, then its partials exist since they are by definition $d_v f(x_0)$ where $v \in \{e_1, \ldots, e_n\}$. 

\begin{lemma}[Differentiability Implies Existence of Partials]
\[\text{Differentiability } \implies \text{ Existence of Partials}\]
\end{lemma}
We remind the reader that differentiability means that the derivative exists at every point in \textit{every path}. The partials are just one of the few paths out of the infinitely many possibles ones. We can imagine an example by visualizing a surface with a "crinkle" at a point, which may have well-defined partials but upon a certain path, the derivative may not exist at all. By this logic, 
\[\text{Existence of Partials} \centernot\implies \text{Differentiability}\]

\begin{example}

\end{example}

Furthermore, differentiability implies continuity. 

\begin{lemma}[Differentiability Implies Continuity]
\[\text{Differentiability } \implies \text{ Continuity}\]
\end{lemma}

But continuity $\centernot\implies$ differentiability. We show two examples of this case. 

\begin{example}[Simple Continuous but not Differentiable Function]
The function $f: \mathbb{R} \longrightarrow \mathbb{R}, x \mapsto |x|$ is continuous but not differentiable at $x = 0$. 
\end{example}

\begin{example}[Continuous but nowhere Differentiable Function]
The \textit{Weierstrass function} is an example of a function that is continuous everywhere but differentiable nowhere. The function is described as a Fourier series
\[f(x) \equiv \sum_{n=0}^\infty a^n \cos(b^n \pi x)\]
where $0 < a < 1$, $b$ is a positive integer, and 
\[a b > 1 + \frac{3}{2} \pi\]
Like other fractals, this function exhibits self-similarity. 
\begin{center}
    \includegraphics[scale=0.15]{Weierstrass_Function_Fractal.png}
\end{center}
\end{example}

We introduce a powerful theorem that allows us to determine smoothness. 

\begin{theorem}[Differentiability Theorem]
Given function $f: \mathbb{R}^n \longrightarrow \mathbb{R}^m$, if all of its partials exist and are continuous, then $f$ is differentiable. 
\[\text{Continuous Partials } \implies \text{ Differentiability}\]
\end{theorem}
We can also visualize this theorem. Since the partials are continuous, then the tangent subspace, which is determined by the span of the tangent vectors determined by the partials, also changes continuously. This means that given a $C^1$ function $f$, we can choose \textit{any} directional vector $v \in \mathbb{R}^n$ and the graph of $f_v^\prime$ will be well defined. Following this visual, we can interpret the differentiability theorem as: 
\[ f \in C^k (\mathbb{R}^n) \implies f \text{ can be differentiated k times along any k paths everywhere}\]

This theorem gives rise to a nice classification of functions based on their smoothness. 

\begin{definition}[Differentiability Classes]
Given a function $f: \mathbb{R}^n \longrightarrow \mathbb{R}^m$, if $f$ is continuous, it is said to be of \textit{class $C^0$}. If $f$ has continuous partial derivatives, that is, if
\[d_{e_i} f \equiv \frac{\partial f}{\partial x_i} : \mathbb{R}^n \longrightarrow \mathbb{R}^m \text{ for } i = 1, 2, \ldots, n\]
are continuous, then $f$ is said to be of \textit{class $C^1$}, or \textit{$C^1$ differentiable}. If it additionally has continuous second partial derivatives, that is, if
\[d_{(e_i, e_j)} f \equiv \frac{\partial^2 f}{\partial x_i \partial x_j}: \mathbb{R}^n \longrightarrow \mathbb{R}^m \text{ for } i = 1, 2, \ldots, n\]
are continuous, then $f$ is said to be of \textit{class $C^2$}, or \textit{$C^2$ differentiable}. In general, we say that $f$ is of class $C^k$, or \textit{$C^k$ differentiable}, if its first through $k$th partial derivatives are continuous; that is, if 
\[d_{(e_{i_1}, \ldots, e_{i_k})} f \equiv \frac{\partial^k f}{\partial e_{i_1} \ldots \partial e_{i_k}}: \mathbb{R}^n \longrightarrow \mathbb{R}^m \text{ for } i = 1, 2, \ldots, n\]
are continuous, which implies that $f$ can be differentiated $k$ times (i.e. there exists a $k$th order Taylor approximation of $f$).
\end{definition}

\begin{lemma}[Nested $C^k$ Function Spaces]
The set of all real-valued $C^k$-functions defined over $\mathbb{R}^n$ form an infinite-dimensional vector space, denoted $C^k (\mathbb{R}^n)$. Furthermore, this gives rise to the nested space: 
\[C^0(\mathbb{R}^n) \supset C^1 (\mathbb{R}^n) \supset C^2 (\mathbb{R}^n) \supset ... \supset C^k (\mathbb{R}^n) \supset ... \supset C^\infty (\mathbb{R}^n) \]
\end{lemma}

Note that differentiability does not imply continuous partials! 

\begin{example}[Differentiable but Not Continuously Differentiable Function]
The function 
\[g(x) \equiv \begin{cases}
x^2 \sin\Big(\frac{1}{x}\Big) & x \neq 0 \\
0 & x = 0
\end{cases}\]
is differentiable, with derivative 
\[g^\prime (x) \equiv \begin{cases}
- \cos \Big( \frac{1}{x}\Big) + 2 x \sin\Big(\frac{1}{x}\Big) & x \neq 0 \\
0 & x = 0
\end{cases}\]
But because $\cos(\frac{1}{x})$ oscillates at $x \rightarrow 0$, $g^\prime (x)$ is not continuous at $x = 0$. Therefore $g(x)$ is differentiable but not in $C^1(\mathbb{R})$. 
\end{example}

\begin{theorem}[Nested $C^k$ and $\mathcal{D}^k$ Function Spaces]
Let the space of all $k$-times differentiable functions over $\mathbb{R}^n$ be denoted $\mathcal{D}(\mathbb{R}^n)$. Then, 
\[C^0(\mathbb{R}^n) \supset \mathcal{D}^1 (\mathbb{R}^n) \supset C^1 (\mathbb{R}^n) \supset \mathcal{D}^2 (\mathbb{R}^n) \supset C^2 (\mathbb{R}^n) \ldots \mathcal{D}^k (\mathbb{R}^n) \supset C^k (\mathbb{R}^n) \ldots C^\infty (\mathbb{R}^n) \]
\end{theorem}

Note that mathematicans throw around the word "smooth" a lot. Usually, it means one of three things
\begin{enumerate}
    \item it is of class $C^1$ 
    \item it is of class $C^\infty$
    \item it is of class $C^k$, where $k$ is however high it needs to be to satisfy our assumptions. For example, if I say let us differentiate smooth $f$ two times, then I am assuming that $f \in C^2 (\mathbb{R}^n)$. 
\end{enumerate}
Visualizing $C^k$-functions is easy for low orders. A $C^0$ function produces a graph that isn't "ripped" or "punctured," since this is exactly what a discontinuity would look like. A $C^1$ function requires the surface to be smooth in such a way that there is a well defined affine tangent subspace at every point. This means that there cannot be any sharp "points" or "edges" on the graph since a tangent subspace cannot be well defined. 

\subsection{Inverse Function Theorem}
A special case of the general implicit function theorem is the inverse function theorem. It gives sufficient condition for a function to be invertible in a neighborhood of a point in its domain. 

\begin{theorem}[Inverse Function Theorem for Single-Variable $C^1$ Functions]
If $f: \mathbb{R} \longrightarrow \mathbb{R}$ is a $C^1$ (continuously differentiable) function with a nonzero derivative at point $x_0$, then $f$ is invertible in a neighborhood of $x_0$, the inverse is also $C^1$, and the derivative of the inverse function at $y_0 = f(x_0)$ is the reciprocal of the derivative of $f$ at $x_0$. 
\[\big( f^{-1}\big)^\prime (y_0) = \frac{1}{f^\prime (x_0)}\]
This can be visualized easily by looking at the graph of any $C^1$ function. 
\begin{center}
    \includegraphics[scale=0.25]{Inverse_Function_Theorem_One_Variable.PNG}
\end{center}
In high school mathematics, this theorem is informally presented as the \textit{horizontal line test}. 
\end{theorem}

This can be stated in an alternative form: If $f: \mathbb{R} \longrightarrow \mathbb{R}$ is continuous and injective near $x_0$, and differentiable at $x_0$ such that $f^\prime (x_0) \neq 0$, then $f$ is invertible near $x_0$ with an inverse that's similarly continuous and injective, and where the above formula would apply as well. 

\begin{corollary}[Inverse Function Theorem for Single-Variable $C^k$ Functions]
If $f: \mathbb{R} \longrightarrow \mathbb{R}$ is a $C^k$ functions with a nonzero derivative at point $x_0$, then $f$ is invertible in a neighborhood of $x_0$, the inverse is also $C^k$, and the derivative of the inverse function at $y_0 = f(x_0)$ is the reciprocal of the derivative of $f$ at $x_0$. 
\end{corollary}

\begin{theorem}[Inverse Function Theorem for Multivariable Functions and its Matrix Realization]
Let $f: \mathbb{R}^n \longrightarrow \mathbb{R}^n$ be a $C^1$ function defined on an open neighborhood of $x_0$ in the domain. If the total derivative $d_{x_0} f$ (i.e. the Jacobian matrix $J f(x_0)$) at $x_0$ is invertible, an inverse function of $f$ is defined on some neighborhood of $y_0 = f(x_0)$. Given that we are working with a fixed basis, $f$ can be modeled by the set of $n$ equations 
\begin{align*}
    f_1 (x_1, x_2, \ldots, x_n) &= y_1 \\
    \ldots & = \ldots \\
    f_2 (x_1, x_2, \ldots, x_n) &= y_2
\end{align*}
This theorem says that this system of $n$ equations has a unique solution for $x_1, x_2, \ldots, x_n$ in terms of $y_1, \ldots, y_n$, provided that we restrict $x$ and $y$ to small enough neighborhoods of $x_0$ and $y_0$. 

This inverse function $f^{-1}$ is continuously differentiable, and its derivative $d_{y_0} f^{-1}$ (i.e. the Jacobian matrix $J f^{-1} (y_0)$) at $y_0 = f(x_0)$ is the inverse linear map of $d_{x_0} f$. 
\[d_{y_0} f^{-1} = \big( d_{x_0} f \big)^{-1} \iff J f^{-1} (y_0) = \big( J f (x_0)\big)^{-1}\]
\end{theorem}

\begin{example}
Consider the vector-valued function $f: \mathbb{R}^2 \longrightarrow \mathbb{R}^2$ defined by 
\[f(x, y) = \begin{pmatrix}
e^x \cos (y) \\ e^x \sin(y)
\end{pmatrix}\]
The Jacobian matrix is 
\[J f(x, y) = \begin{pmatrix}
e^x \cos(y) & - e^x \sin(y) \\
e^x \sin(y) & e^x \cos(y)
\end{pmatrix} \implies \det J f(x, y) = e^{2x} \cos^2 (y) + e^{2x} \sin^2 (y) = e^{2x}\]
Since the determinant $e^{2x}$ is nonzero everywhere, $J f(x, y)$ is nonsingular. Thus, the theorem guarantees that for every point $x_0 \in \mathbb{R}^2$, there exists a neighborhood about $x_0$ over which $f$ is invertible. However, this does not mean $f$ is invertible over its entire domain: in this case $f$ isn't even injective since it is periodic. 
\end{example}

\subsection{Implicit Function Theorem}
The implicit function theorem is a tool that allows relations between points in $\mathbb{R}^n$ to be converted to functions of several real variables. That is, it states that for sufficiently "nice" points on a surface defined as $f(x) = c$ (where $f:\mathbb{R}^n \longrightarrow \mathbb{R}^m$, we can locally pretend that this surface is a graph of a function. 

That is, let $f: \mathbb{R}^{n+m} \longrightarrow \mathbb{R}^m$ be a $C^1$ function. We can think of $\mathbb{R}^{n+m}$ as the Cartesian product $\mathbb{R}^n \times \mathbb{R}^m$, where a point of this product is written 
\[(x, y) = (x_1, \ldots, x_n, y_1, \ldots, y_m)\]
Starting from the given function $f$, our goal is to construct a function $g: \mathbb{R}^n \longrightarrow \mathbb{R}^m$ whose graph $(x, g(x))$ is precisely the set of all $(x, y)$ such that $f(x, y) = 0$. 



\begin{theorem}[Implicit Function Theorem for 2D, 3D Case]
Let $f: \mathbb{R}^2 \longrightarrow \mathbb{R}$ be a continuously differentiable function and let there be a point $(x_0, y_0) \in \mathbb{R}^2$ such that $f(x_0, y_0) = 0$. If 
\[\frac{\partial f}{\partial y} (x_0, y_0) \neq 0\]
then there is an open neighborhood $U$ around $(x_0, y_0)$ such that we can make $y$ a function of $x$ within $U$ satisfying $f(x, y(x)) = 0$. 

Let $f: \mathbb{R}^3 \longrightarrow \mathbb{R}$ be a continuously differentiable function and let there be a point $(x_0, y_0, z_0) \in \mathbb{R}^3$ such that $f(x_0, y_0, z_0) = 0$. If 
\[\frac{\partial f}{\partial z} (x_0, y_0, z_0) \neq 0\]
then there is an open neighborhood $U$ around $(x_0, y_0, z_0)$ such that we can make $z$ a function of $x$ and $y$ within $U$ satisfying $f(x, y, z(x,y)) = 0$. 
\end{theorem}

\begin{example}
Let $f: \mathbb{R}^2 \longrightarrow \mathbb{R}$ be defined by $f(x, y) = x^2 + y^2 - 1$. The level set at $z = 0$ would be the set of points satisfying 
\[x^2 + y^2 - 1 = 0\]
the unit circle. The derivative of $f$ with respect to $y$ is $0$ at the points $(-1,0)$ and $(1,0)$, meaning that in any neighborhood of these points, we cannot define a function of $y$ with respect to $x$. This is true, indeed, since any such function would fail the vertical line test, which can be seen in the red neighborhood around $(1,0)$. However, the blue neighborhood of the point $(-\sqrt{2}/2, \sqrt{2}/2)$ does indeed define a function of $y$ with respect to $x$ satisfying the vertical line test. 
\begin{center}
\begin{tikzpicture}
    \draw[<->] (-3,0)--(3,0);
    \draw[<->] (0,-3)--(0,3);
    \draw (0,0) circle (2);
    \draw[fill=red] (2,0) circle (0.08);
    \draw[fill=red] (-2,0) circle (0.08);
    \draw[red, thick] (1.732, -1) arc (-30:60:2);
    \draw[dashed, <->] (1.8,-2.5)--(1.8,2.5);
    \draw[fill=blue] (-1.414, 1.414) circle (0.08);
    \draw[blue, thick] (-1, 1.732) arc (120:170:2);
    \draw[dashed,<->] (-1.7,2.5)--(-1.7,-2.5);
\end{tikzpicture}
\end{center}
\end{example}

Note that for the 2D and 3D case, the level surface that we dealt with has a codimension of $1$; that is, the dimension of the manifold generated by the level surface is $n-1$. The special implicit function theorem generalizes cases such as these. 

\begin{definition}[Truncated Jacobian Matrix]
Given a function $f: \mathbb{R}^{n+m} \longrightarrow \mathbb{R}^m$ where the variables are 
\[x_1, \ldots, x_n, y_1, \ldots, y_m\]
the \textit{truncated Jacobian matrix} of $f$ is the $m \times m$ matrix formed by the rightmost $m$ columns of $J  f(x, y)$. That is, the matrix formed by the elements on the right of the vertical line is the truncated Jacobian matrix.
\[J f(x, y) = \left(\begin{array}{ccc|ccc}
   \frac{\partial f_1}{\partial x_1} &\ldots &\frac{\partial f_1}{\partial x_n} & \frac{\partial f_1}{\partial y_1} & \ldots & \frac{\partial f_1}{\partial y_m}\\
   \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
   \frac{\partial f_m}{\partial x_1} &\ldots &\frac{\partial f_m}{\partial x_n} & \frac{\partial f_m}{\partial y_1} & \ldots & \frac{\partial f_m}{\partial y_m}
   \end{array}\right) \mapsto
   \begin{pmatrix}
   \frac{\partial f_1}{\partial y_1} & \ldots & \frac{\partial f_1}{\partial y_m}\\
   \vdots & \ddots & \vdots \\
   \frac{\partial f_m}{\partial y_1} & \ldots & \frac{\partial f_m}{\partial y_m}
   \end{pmatrix} = J f_y \big(x, y)\big)\]
\end{definition}

\begin{theorem}[Special Implicit Function Theorem]
Let $f: \mathbb{R}^{n+1} \longrightarrow \mathbb{R}$ be a continuously differentiable function. We can think of $\mathbb{R}^{n + 1}$ as the Cartesian product $\mathbb{R}^n \times \mathbb{R}$, where an element of this product is $(x, y)$ ($x \in \mathbb{R}^n, y \in \mathbb{R}$). Let us fix a point $(x_0, y_0) = (x_{01}, x_{02}, \ldots, x_{0n}, y_0)$ with $f(x_0, y_0) = 0$. 

If the ($1 \times 1$) \textit{truncated Jacobian matrix} defined with the partial derivatives with respect to the $y$ terms evaluated at $(x_0, y_0)$ 
\[ J f_y (x_0, y_0) \equiv \frac{\partial f}{\partial y} (x_0, y_0)\]
which can also be thought as the matrix truncated
\[J f(x, y) = \left(\begin{array}{ccc|c}
   \frac{\partial f}{\partial x_1} &\ldots &\frac{\partial f}{\partial x_n} & \frac{\partial f}{\partial y}
   \end{array}\right) \mapsto \begin{pmatrix}
   \frac{\partial f}{\partial y}
   \end{pmatrix}\]
is invertible, then there exists an open neighborhood $U \subset \mathbb{R}^n$ containing $x_0$ and a unique $C^1$ function $g: U \longrightarrow \mathbb{R}$ such that $g(x_0) = y_0$, and 
\[f\big( x, g(x)\big) = 0 \text{  for all } x \in U\]
Moreover, the partial derivatives of $g$ in $U$ (represented by its Jacobian matrix) are given by the matrix product
\[J g(x) = - \big( J f_y (x, g(x)) \big)^{-1}  Jf \big(x, g(x)\big)\]
or represented
\[\begin{pmatrix}
\frac{\partial g}{\partial x_1} & \ldots & \frac{\partial g}{\partial x_n} 
\end{pmatrix} = - \begin{pmatrix}
\frac{\partial f}{\partial y} \big(x, g(x)\big)
\end{pmatrix}^{-1} \begin{pmatrix}
\frac{\partial f}{\partial x_1} \big(x, g(x)\big) & \ldots & \frac{\partial f}{\partial x_n} \big(x, g(x)\big)
\end{pmatrix}\]
\end{theorem}

\begin{example}[Circle Example]
Let $n = m = 1$ and 
\[f(x, y) = x^2 + y^2 - 1\]
The matrix of partial derivatives is just a $1 \times 2$ matrix, given by 
\[J f(x_0, y_0) = \begin{pmatrix}
\frac{\partial f}{\partial x} (x_0, y_0) & \frac{\partial f}{\partial y} (x_0, y_0)
\end{pmatrix} = \begin{pmatrix}
2 x_0 & 2 y_0 \end{pmatrix}\]
The truncated Jacobian matrix is just the $1 \times 1$ matrix $(2 y_0)$, which is invertible if and only if $y_0 \neq 0$. By the implicit function theorem we see that we can locally write the circle in the form $y = g(x)$ for all points where $y \neq 0$. For $(\pm 1, 0)$ we cannot (as seen before by the inverse function theorem). But by writing $x$ as a function of $y$ ($x = h(y)$), we can write it at these points. The derivative of this function $g: \mathbb{R} \longrightarrow \mathbb{R}$ can be written using the formula below, where $\frac{\partial f}{\partial y} \big( x, g(x)\big) = 2y_{(x, g(x))} = 2 g(x)$. 
\[\frac{\partial g}{\partial x} = - \big( 2 g(x)\big)^{-1} \big( 2x \big) = - \frac{x}{g(x)}\]
But since the value of $g(x)$ is the $y$ value, we can rewrite this as
\[\frac{dy}{dx} = -\frac{x}{y}\]
and similarly, get the formula for the derivative of $h$ as
\[\frac{dx}{dy} = - \frac{y}{x}\]
\end{example}

\begin{theorem}[General Implicit Function Theorem]
Let $f: \mathbb{R}^{n+m} \longrightarrow \mathbb{R}^m$ be a continuously differentiable function, and let us interpret $\mathbb{R}^{n+m}$ as the Cartesian product $\mathbb{R}^n \times \mathbb{R}^m$, where an element of this product is $(x, y)$ ($x \in \mathbb{R}^n, y \in \mathbb{R}^m$). Let us fix a point $(x_0, y_0) = (x_{01}, \ldots, x_{0n}, y_{01}, \ldots, y_{0m})$, with $f(x_0, y_0) = 0$.

If the $(m \times m)$ truncated Jacobian matrix constructed by truncating the matrix below as such
\[J f = \left(\begin{array}{ccc|ccc}
   \frac{\partial f_1}{\partial x_1} &\ldots &\frac{\partial f_1}{\partial x_n} & \frac{\partial f_1}{\partial y_1} & \ldots & \frac{\partial f_1}{\partial y_m}\\
   \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
   \frac{\partial f_m}{\partial x_1} &\ldots &\frac{\partial f_m}{\partial x_n} & \frac{\partial f_m}{\partial y_1} & \ldots & \frac{\partial f_m}{\partial y_m}
   \end{array}\right) \mapsto
   \begin{pmatrix}
   \frac{\partial f_1}{\partial y_1}& \ldots & \frac{\partial f_1}{\partial y_m}\\
   \vdots & \ddots & \vdots \\
   \frac{\partial f_m}{\partial y_1} & \ldots & \frac{\partial f_m}{\partial y_m}
   \end{pmatrix} (x_0, y_0) = J f_y (x_0, y_0)\]
evaluated at $(x_0, y_0)$ is invertible (i.e. determinant is nonzero), then there exists an open neighborhood $U \subset \mathbb{R}^n$ containing $x_0$ and a unique $C^1$ function $g: U \longrightarrow \mathbb{R}^m$ such that $g(x_0) = y_0$, or in component terms, the smooth functions $k_i$ exist such that
\[y_{0i} = k_i (x_{01}, x_{02}, \ldots, x_{0n}), \;\;\;\;\; i = 1, 2, \ldots, m\]
and 
\[f\big( x, g(x)\big) = 0 \text{ for all } x \in U\]
Moreover, the partial derivatives of $g$ in $U$ (represented by its Jacobian matrix) are given by the matrix product
\[J g(x) = - \big( J f_y (x, g(x))\big)^{-1} J f\big(x, g(x)\big)\]
or equivalently, 
\[\begin{pmatrix}
\frac{\partial g_1}{\partial x_1} & \ldots & \frac{\partial g_1}{\partial x_n} \\
\vdots & \ddots & \vdots \\
\frac{\partial g_m}{\partial x_1} & \ldots & \frac{\partial g_m}{\partial x_n}
\end{pmatrix} = - \begin{pmatrix}
\frac{\partial f_1}{\partial y_1} (x, g(x))& \ldots & \frac{\partial f_1}{\partial y_m} (x, g(x))\\
\vdots & \ddots & \vdots \\
\frac{\partial f_m}{\partial y_1} (x, g(x))& \ldots & \frac{\partial f_m}{\partial y_m}(x, g(x))
\end{pmatrix} ^{-1} \begin{pmatrix}
\frac{\partial f_1}{\partial x_1} & \ldots & \frac{\partial f_1}{\partial x_n}\\
\vdots & \ddots & \vdots \\
\frac{\partial f_m}{\partial x_1} & \ldots & \frac{\partial f_m}{\partial x_n}
\end{pmatrix}(x, g(x))\]
The derivatives may also be computed by implicit differentiation. 
\end{theorem}



\subsection{Divergence and Curl}
\subsubsection{Tensor Fields}
Note that a scalar field is a set of scalars associated with every point in space. Similarly, a vector field is a set of vectors associated with every point in space. Going further, a tensor field is a set of tensors associated with every point in space. It follows that: 
\begin{enumerate}
    \item A scalar field is a rank $0$ tensor field
    \item A vector field is a rank $1$ tensor field
\end{enumerate}
Therefore, a rank $2$ tensor field would be:
\[F: \mathbb{R}^n \longrightarrow \mathbb{R}^n \otimes \mathbb{R}^n\]
where $F(x_0)$ would be a rank $2$ tensor (or a matrix, given that coordinates are well defined). 

\subsubsection{Coordinate Definition of Divergence}
Colloquially, the divergence is an operator $\Div$ that operates on a vector field and produces a scalar field which provides the quantity of the vector field's source at each point. Technically, the divergence represents the volume density of the outward flux of a vector field from an infinitesimal volume around a given point. 

There is a very nice geometric interpretation for divergence. Imagine that the vector field $F$ represents fluid flow in $\mathbb{R}^n$. Divergence is then the "measure" of the net amount of fluid flowing in and out of an infinitesimally small region, labeled at each point. If the net fluid flow is positive (i.e. more fluid is flowing in than out) at point $x_0$, then $\Div{F}(x_0) > 0$. If the net fluid flow is negative (i.e. more fluid is flowing out than in) at point $x_0$, then $\Div{F}(x_0) < 0$. This measure assigns a number to every point in the space (creating a scalar field). Therefore, each point either acts as a "source" of fluid emanating from it or as a "sink" that sucks in more fluid than it puts out. 
\begin{center}
    \includegraphics[scale=0.25]{Divergence_compared_to_Zero.PNG}
\end{center}

\begin{definition}[Divergence in Coordinates]
In Cartesian coordinates, the \textit{divergence} of a $C^1$ vector field $F: \mathbb{R}^n \longrightarrow \mathbb{R}^n$ is defined
\[\Div{F} \equiv \nabla \cdot F \equiv 
\begin{pmatrix}
\frac{\partial}{\partial x_1} \\ \vdots \\ \frac{\partial}{\partial x_n} \end{pmatrix} \cdot \begin{pmatrix} F_1 \\ \vdots F_n
\end{pmatrix} = \sum_{i=1}^n \frac{\partial F_i}{\partial x_i}\]
where $\cdot$ is the Euclidean dot product and $\nabla$ is the del operator. 
\end{definition}

\begin{example}
The divergence of the origin in the left graph is clearly negative since the net flow is out of the point, while the divergence of the origin in the right graph is positive since the net fluid flow is in. 
\begin{center}
\pgfplotsset{width=7cm,compat=1.16}
\begin{tikzpicture}[scale=0.8]
\begin{axis}[view={0}{90}, domain=-4:4]
\addplot3 [blue,-stealth,samples=16,
        quiver={
            u={2*x/pow(x^2 + y^2,1/2)},
            v={2*y/pow(x^2 + y^2,1/2)},
            scale arrows=0.2,
        },
    ] { 1};
\end{axis}
\end{tikzpicture}
\begin{tikzpicture}[scale=0.8]
\begin{axis}[view={0}{90}]
\addplot3 [blue,-stealth,samples=16,
        quiver={
            u={-2*x/pow(x^2 + y^2,1/2)},
            v={-2*y/pow(x^2 + y^2,1/2)},
            scale arrows=0.2,
        },
    ] { 1};
\end{axis}
\end{tikzpicture}
\end{center}
\end{example}

\begin{definition}[Divergence of Tensor Fields]
Let $A$ be a $C^1$ second-order tensor field; that is, it assigns a tensor to every point in Euclidean space, defined as 
\[A = \begin{pmatrix}
A_{11} & A_{12} & A_{13} \\ 
A_{21} & A_{22} & A_{23} \\
A_{31} & A_{32} & A_{33}
\end{pmatrix}\]
the divergence in a Cartesian coordinate system is a first-order tensor field and can be defined 
\[\Div A = \begin{pmatrix}
\frac{\partial A_{11}}{\partial x_1} + \frac{\partial A_{12}}{\partial x_2} + \frac{\partial A_{13}}{\partial x_3} \\
\frac{\partial A_{21}}{\partial x_1} + \frac{\partial A_{22}}{\partial x_2} + \frac{\partial A_{23}}{\partial x_3} \\
\frac{\partial A_{31}}{\partial x_1} + \frac{\partial A_{32}}{\partial x_2} + \frac{\partial A_{33}}{\partial x_3}
\end{pmatrix}\]
\end{definition}

\begin{definition}[Divergence in Cylindrical, Spherical Coordinates]
For vector field $F: \mathbb{R}^3 \longrightarrow \mathbb{R}^3$ expressed in cylindrical coordinates as 
\[F = \begin{pmatrix}
F_r \\ F_\theta \\ F_z
\end{pmatrix}\]
the divergence is
\[\Div F = \nabla \cdot F = \frac{1}{r} \frac{\partial}{\partial r} \big(r F_r \big) + \frac{1}{r} \frac{\partial F_\theta}{\partial \theta} + \frac{\partial F_z}{\partial z}\]
Note that the condition of locality is important, since in general a global cylindrical coordinate system would be inconsistent. 

In spherical coordinates $(r, \theta, \phi)$, the divergence is 
\[\Div F = \nabla \cdot F = \frac{1}{r^2} \frac{\partial}{\partial r} \big( r^2 F_r) + \frac{1}{r \, \sin{\theta}} \frac{\partial}{\partial \theta} \big( \sin{\theta} F_\theta\big) + \frac{1}{r\, \sin{\theta}} \frac{\partial F_\phi}{\partial \phi}\]
\end{definition}

\subsubsection{Coordinate Definition of Curl}
Colloquially, the curl is a vector operator that describes the infinitesimal circulation of a vector field in $3$-dimensional Euclidean space, where the curl at each point is represented by a vector whose length and direction denote the magnitude and axis as the maximum circulation. 

That is, if one drops a twig or a ball with its center of mass at a certain point, the curl measures how much it will spin. In physics, the rotation of a rigid body in 3-dimensions can be described by a vector $\omega$ along the axis of rotation. $\omega$ is called the \textit{angular velocity vector}, with $||\omega||$ denoting the angular speed of the body. The curl of this vector field measured at the center of mass of the body is measured as $2 \omega$. That is, the curl outputs \textit{twice} the angular velocity vector of any rigid body. 

Furthermore, if $\curl{F}(x_0) = 0$, then this indicates that there are no "whirlpools" at $x_0$, meaning that any rigid body placed at $x_0$, while it may travel along a path, will not rotate around its own axis. Such a vector field $F$ with this property is called \textit{irrotational}. 

\begin{definition}
The \textit{curl} of a 3-dimensional $C^k$ vector field $F: \mathbb{R}^3 \longrightarrow \mathbb{R}^3$ is an operator
\[\curl: C^k (\mathbb{R}^3; \mathbb{R}^3) \longrightarrow C^{k-1} (\mathbb{R}^3; \mathbb{R}^3)\]
defined
\[\curl{F} \equiv \nabla \times F \equiv \begin{pmatrix}
\frac{\partial}{\partial x} \\ \frac{\partial}{\partial y}\\ \frac{\partial}{\partial z} \end{pmatrix} \equiv \begin{pmatrix}
\frac{\partial F_3}{\partial y} - \frac{\partial F_2}{\partial z} \\
\frac{\partial F_1}{\partial z} - \frac{\partial F_3}{\partial x} \\
\frac{\partial F_2}{\partial x} - \frac{\partial F_1}{\partial y}
\end{pmatrix}\]
\end{definition}

Note that unlike the gradient and divergence operators, curl does not generalize as simply to other dimensions. 

\begin{definition}
A vector field $F$ is \textit{irrotational} if 
\[\curl{F} = 0\]
\end{definition}
It has been shown that fluid draining from a tub is usually irrotational except for right at the center, which is surprising since the fluid itself is "rotating" around the drain. 

\begin{theorem}
For any $C^2$ vector field $F$, 
\[\Div{\curl{F}} = \nabla\cdot (\nabla \times F) = 0\]
That is, the divergence of any curl is 0. 
\end{theorem}
\begin{proof}
Also proved by equality of mixed partials. 
\end{proof}

\begin{definition}
The \textit{Laplace operator}, or \textit{Laplacian}, of a function $f: \mathbb{R}^n \longrightarrow \mathbb{R}$ is the divergence of the gradient. 
\[\nabla^2 f \equiv \nabla \cdot (\nabla f) \equiv \sum_{i=1}^n \frac{\partial^2 f}{\partial x_i^2}\]
\end{definition}

\subsubsection{Conservative, Solenoidal Vector Fields}
\begin{definition}[Conservative Vector Fields]
A vector field $F: U \subset \mathbb{R}^n \longrightarrow \mathbb{R}^n$ is a \textit{conservative vector field} if and only if there exists a scalar field $f: U \subset \mathbb{R}^n \longrightarrow \mathbb{R}$ such that 
\[F = \nabla f\]
on $U$. 
\end{definition}

Conservative vector fields appear naturally in mechanics: they are vector fields representing forces of physical systems in which energy is conserved. 

\begin{theorem}
Given a $C^2$-function $f: \mathbb{R}^3 \longrightarrow \mathbb{R}$,
\[\nabla \times ( \nabla f) = 0\]
That is, the curl of any gradient vector field is the zero vector. 
\end{theorem}
\begin{proof}
$\nabla \times \nabla f$ can be expanded to
\[\bigg( \frac{\partial^2 f}{\partial y \partial z} - \frac{\partial^2 f}{\partial z \partial y}, \; \frac{\partial^2 f}{\partial z \partial x} - \frac{\partial^2 f}{\partial x \partial z}, \; \frac{\partial^2 f}{\partial x \partial y} - \frac{\partial^2 f}{\partial y \partial x}\bigg) = (0, 0, 0)\]
by equality of mixed partials. 
\end{proof}

\begin{definition}[Solenoidal Vector Fields]
A \textit{solenoidal, or incompressible, vector field} is a vector field $F: \mathbb{R}^n \longrightarrow \mathbb{R}^n$ such that
\[\Div F = \nabla \cdot F = 0\]
at all point in the field. That is, the field has no sources or sinks. 
\end{definition}

\begin{example}
The vector field $F: (x, y) \mapsto (y, -x)$ is solenoidal. 
\begin{center}
    \includegraphics[scale=0.17]{Solenoidal_vector_field.png}
\end{center}
\end{example}

\subsection{Differentials of Functions}
Note that differentials of functions are not to be confused with differential forms. 
\subsubsection{One Variable Functions}
Given a continuous real valued function $f: \mathbb{R} \longrightarrow \mathbb{R}$, let the parameter for the input be labeled $x$ and the output parameter be labeled with the variable $y$. This is conventional, leading to the well known expression 
\[y = f(x)\]
Now, note that even though the function itself is $f$, we can choose to represent its output values with either $f$ or $y$. 

\begin{definition}
A given change in $x$ is denoted $\Delta x$. Given that the input value changes by $\Delta x$ (i.e. the input changes from $x \rightarrow x + \Delta x$), the change in the output is denoted with $\Delta y$. That is,
\[\Delta y \equiv f(x + \Delta x) - f(x)\]
\end{definition}
Note that the behavior of $\Delta y$ is completely independent from $\Delta x$. We cannot assume that, for example, an increase in $x$ (i.e. $\Delta x > 0$) corresponds to an increase in $y$. Note that $\Delta y$ is really a function of two independent variables, which is $x$ (the initial point where the change happens) and $\Delta x$ (how much $x$ is changing). That is,
\[\Delta y (x, \Delta x) \equiv f(x + \Delta x) - f(x)\]

Now we continue to introduce the differential. 

\begin{definition}
An \textit{infinitesimal} is a real number $\epsilon > 0$ such that $\epsilon$ is less than any real positive number. It can also be defined as a positive real number $\epsilon$ such that
\[\epsilon < \frac{1}{n} \text{ for all } n \in \mathbb{N}\]
\end{definition}

Since $f \in C^0 \mathbb{R}$, we can assume that an infinitesimal change in input value $\Delta x$  corresponds to an infinitesimal change in output value $\Delta y$. This infinitesimal change is now denoted $d x$ and $d y$, respectively. Therefore, the instantaneous rate of change of $y$ (or $f$) with respect to the change of $x$ is really just the ratio of the infinitesismal changes of both $x$ and $y$. This is denoted by the fraction 
\[\frac{d y}{d x} \equiv \lim_{\Delta x \rightarrow 0} \frac{ \Delta y}{\Delta x}\]
This is precisely the \textit{Leibniz Notation} of the derivative of a function $f$. Note that the quotient $d y / d x$ is not infinitesimally small, but it is rather a real number. 

\begin{definition}
The \textit{differential} of a function $f$ of a single real variable $x$ is the linear function $d f$ of two independent variables $x$ and $\Delta x$ given by 
\[d y \equiv d f \equiv d f (x, \Delta x) \equiv f^\prime (x) \, \Delta x\]
Note that since $y$ and $f$ both denote the ouput of the function $f$, we can use them iterchangeably in this notation. Note also that $f^\prime (x)$ is just the derivative of $f$ with respect to $x$. Note that since 
\[d x (x, \Delta x) \equiv \Delta x\]
it is conventional to substitute $\Delta x = d x$ to get the equivalent expression 
\[d f \equiv f^\prime (x) \, dx \text{ or } d y \equiv \frac{d y}{d x} d x\]
\end{definition}

Note that the differential of $f$ of a point $x$ is a linear approximation of $f$ at the point $x$ since $d f$ is linear and the error bound $\epsilon$ of the equality 
\[\Delta y = f^\prime (x) \, \Delta x + \epsilon = d f(x) + \epsilon\]
satisfies 
\[\lim_{\Delta x \rightarrow 0} \frac{\epsilon}{\Delta x} = \lim_{\Delta x \rightarrow 0} \frac{ \Delta y - d y}{\Delta x} = 0\]
$\implies d y \approx \Delta y$, where we can make the approximation arbitrarily small by constraining $\Delta x$ to be sufficiently small. Therefore, the differential of a function is known as the \textit{principal (linear) part} in the increment of a function. That is, $d f$ is linear with respect to $d x$, the increment (but not with respect to $x$ itself. Although the error $\epsilon$ may be nonlinear, it tends to $0$ rapidly as $\Delta x$ tends to $0$. 
\\

We can visually compare the secant line and the differential of an arbitrary function. We can plot the graph of $f$ as the set
\[l \equiv \{\big( x, f(x)\big) \in \mathbb{R} \oplus \mathbb{R}\} \subset \mathbb{R}^2\]
and observe the unqiue secant line that connects the two points $\big( x, f(x)\big)$ and $\big(x + \Delta x, y + \Delta y \big) = \big( x + \Delta x, f(x + \Delta x)\big)$. This line is labeled in blue. The differential, on the other hand, is labeled in red. 
\begin{center}
\begin{tikzpicture}[scale=0.7]
\begin{axis}[
    axis lines = left,
    xlabel = $x$,
    ylabel = {$y$},
    xmin=0,
    xmax=4,
    ymin=0,
    ymax=30,]
\addplot [
    domain=0:10,
    samples=100, ]
{x^3 + 5};
\addplot [
    domain=0:10,
    samples=100, 
    color=red]
{6.75*(x-1.5) + 8.375};
\addplot [
    domain=0:10,
    samples=100, 
    color=blue]
{10.6875*(x-1.5) + 8.375};
\coordinate[label={150:$(x, y)$}] (x, y) at (axis cs:1.5,8.375);
\coordinate[label={130:$(x+\Delta x, y+\Delta y)$}] (A) at (axis cs:2.25, 16.3901);
\coordinate[label={-80:$(x + \Delta x, x + d y)$}] (B) at (axis cs:2.25, 13.4375);
\draw (A) -- (B);
\coordinate[label={0:$\epsilon = \Delta y - d y$}] (C) at (axis cs:2.25, 15.3);
\addplot[only marks] table {
1.5 8.375
2.25 16.3901
2.25 13.4375
};
\coordinate[label={-60:$l$}] (D) at (axis cs:3.2, 27);
\coordinate[label={0:$d y$}] (E) at (axis cs:3.5, 21);
\end{axis}
\end{tikzpicture}
\end{center}

\subsubsection{Multivariable Functions}
\begin{definition}
For functions $f: \mathbb{R}^n \longrightarrow \mathbb{R}$ in the form 
\[y = f(x_1, x_2, ..., x_n)\]
the \textit{partial differential} of $y$ (or $f$) with respect to any one of the variables $x_i$ is the principal part of change in $y$ resulting in the change $d x_1$. Therefore, the partial differential is 
\[\frac{\partial f}{\partial x_1} d x_1\]
where $\partial f / \partial x_1$ is the partial derivative of $f$. The sum of all the partial differentials is called the \textit{total differential} of $f$. 
\[d y = \frac{\partial y}{\partial x_1} \,d x_1+ \frac{\partial y}{\partial x_2} \,d x_2 + ... + \frac{\partial y}{\partial x_n} \,d x_n\]
\end{definition}
More precisely, in vector calculus, if $f: \mathbb{R}^n \longrightarrow \mathbb{R}$ is a differentiable function, then by definition of differentiability (i.e. a linear approximation exists), 
\begin{align*}
    y & \equiv f(x_1 + \Delta x_1, x_2 + \Delta x_2, ..., x_n + \Delta x_n) - f(x_1, x_2, ..., x_n) \\
    & \equiv \frac{\partial f}{\partial x_1}\, d x_1 + \frac{\partial f}{\partial x_2} \, d x_2 + ... + \frac{\partial f}{\partial x_n} \, d x_n + \epsilon_1 \Delta x_1 + ... + \epsilon_n \Delta x_n
\end{align*}
where the $\epsilon_i$ can be made arbitrary close to $0$ by constraining $\Delta x_i$ to $0$. Similar to the one variable case,  we can therefore see that $d y \approx \Delta y$.

\section{Integration}

\subsection{Geometric Interpretations of Integration}
The concept of integration in one variable calculus limits the applicability of the operation to finding only areas of functions under curves. We will replace the reader's intuition of integration with the following description. Given a function $f: \mathbb{R}^n \longrightarrow \mathbb{R}$, we interpret it as a scalar field that assigns a "weight" to each point in $\mathbb{R}^n$. Now, given any "shape" $B$ in $\mathbb{R}^n$ that is closed (but not necessarily bounded), an integral can calculate the "weighed" volume of $B$ by cutting $B$ into infinitesimally small points, multiplying them by their respective weights determined by $f$, and then summing up the weighed points. Integrating $B$ in $\mathbb{R}$, $\mathbb{R}^2$, and $\mathbb{R}^3$ with a constant scalar field equal to $1$ is equivalent to finding the length, area, and volume of $B$, respectively. We deconstruct specific types of iterated integrals. 

\subsubsection{Single Integral as Weighed Length or Area}
A single integral is calculated from a function $f: \mathbb{R} \longrightarrow \mathbb{R}$. Given some intervals (or a collection of intervals) $B \subset \mathbb{R}$, the integration notation is familiar to us. 
\[\int_B f(x) \; d x\]
We can interpret this integration in two ways. First, we imagine that the function $f$ is a scalar field in $\mathbb{R}$. Therefore, every point $x$ in $\mathbb{R}$ has a certain real number $f(x)$ associated to it. Therefore, the interval $B \in \mathbb{R}$ now consists of points that now have different densities each (which can be negative). The entire $B$ can now be thought of as a 1-dimensional "rod" in $\mathbb{R}$ that has an uneven distribution of mass determined by $f$. The total mass of the rod $B$ is calculated by the integral. In the diagram below, we use different "thickness" to represent different densities. 
\begin{center}
    \includegraphics[scale=0.3]{Integral_As_Stick_with_Density.PNG}
\end{center}
Secondly, we can visualize the entire graph of $f$ in $\mathbb{R} \oplus \mathbb{R}$. This represents a curve in the $x y$-axis that most beginner calculus students are familiar with. Note that the interval $B$ exists in the x-axis, and in this case, the "weight" of each point $x$ in $B$ is represented as the "height" of the infinitesimally thin bar at $x$. It is easy to see that the weight of the rod at point $x$ and the height of the bar at point $x$ are really the same measure determined by $f$. Therefore, the density distribution in the rod is now modeled as the height of the function at each point. Calculating the integral of this function now calculates the "area" under the curve.

It is important to point out that $B$ does not necessarily need to be a length in the form of $[a,b]$. It can be any union of disjoint lengths, too. However, adding a finite number of single points to $B$ will not affect the integral. It is also customary for $B$ to be closed. 

\subsubsection{Double Integral as Weighed Area or Volume}
The double integral is calculated from function $f: \mathbb{R}^2 \longrightarrow \mathbb{R}$. Given a certain closed subset $B \subset \mathbb{R}^2$, the integration notation is
\[\iint_B f(x, y) \; d A\]

Again, we can interpret double integration in two ways. First, we think of $f$ as a scalar field assigning a density to every point in $\mathbb{R}^2$. Then, the 2-dimensional shape $B$ itself should have a certain density distribution on it determined by $f$. The double integral above then determines the mass of $B$. 

The second way to interpret this is to imagine the 2-dimensional shape $B$ lying in the extended space $\mathbb{R}^2 \oplus \mathbb{R}$. We then model the density distribution as merely the height of the infinitesimally thin bar at each point $x$. Again, the height of this bar at $x$ is precisely its density described in the first interpretation. Therefore, integrating this shape is equivalent to finding the volume of the infinite union of the infinitesimally thin bars at each point in $B$. 
\\

Note again that $B$ need not be one solid region. It can be a union of multiple disjoint ones. However, adding a single point $p$ or a 1-dimensional path $p$ to $B$ will not affect the integral since they have an area of $0$.

\subsubsection{Triple Integral as Weighed Volumes or Hyper-Volumes}
The double integral is calculated from function $f: \mathbb{R}^3 \longrightarrow \mathbb{R}$. Given a certain closed subset $B \subset \mathbb{R}^3$, the integration notation is
\[\iiint_B f(x, y, z) \; d V\]
Following the logic of the previous two examples, the function $f$, interpreted as a scalar field, assigns a scalar at each point $x$ in the solid $B$. Therefore, we can visualize $B$ as a solid, 3-dimensional object in $\mathbb{R}^3$ with a certain density distribution defined by $f$. The total mass of $B$ is therefore determined by the triple integral above.

Following similar logic, we can interpret this integral as the hypervolume of a 4 dimensional object, but this is not often used. 

\subsection{Reduction to Iterated Integrals}
We first state a basic condition of integration. 

\begin{theorem}
Any function $f: \mathbb{R}^n \longrightarrow \mathbb{R}$ that is continuous over a certain region $B \subset \mathbb{R}^n$ can be integrated over $B$. 
\end{theorem}
That is, if $f$ is discontinuous at a certain subset $D \subset B$, then the infinitesimal neighborhoods around each point $d \in D$ is not well defined, since they would always contain two values of $f$ that do not converge to each other at $d$. 

However, there are some discontinuous functions that are in fact integrable. Assuming $B \subset \mathbb{R}^n$ is the region that we are integrating over, 
\begin{enumerate}
    \item Given that there is a subset $N$ in $B$ with volume $0$ over which $f$ is not defined, we can integrate over $B \setminus N$. In the one and two dimensional cases, 
    \[\int_{B \setminus N} f(x) dx \text{ and } \iint_{B\setminus N} f(x) dA\]
    are well-defined. Visually, 
    \begin{center}
        \includegraphics[scale=0.2]{Integrable_Hole_Function.jpg}
    \end{center}
    \item The function is defined for all values in the region, but there is a jump in the value of the function. 
    \begin{center}
        \includegraphics[scale=0.23]{Integrable_Jump_Function.PNG}
    \end{center}
\end{enumerate}
Informally, if we can visualize the Riemann sum converging to a well-defined area as the rectangles get thinner and thinner, then a discontinuous function is integrable. Indeed, all continuous functions (over a bounded set) are integrable since their Riemann sums are well defined. 

\subsubsection{Integration over Intervals, Rectangles, Boxes}
The simplest region that we can integrate over is a single interval 
\[B \equiv [a,b] \subset \mathbb{R}\]
a rectangle 
\[B \equiv [a, b] \times [c,d] \subset \mathbb{R}^2\]
and a box 
\[B \equiv [a,b] \times [c,d] \times [e,f] \subset \mathbb{R}^3\]
Clearly, this extends to integration over any dimension. 
\[B \equiv \prod_{i=1}^n [\alpha_i, \beta_i] \subset \mathbb{R}^n\]

Solving these integrals are quite simple. However, to rigorously define the methodology, we must use the following theorems. 

\begin{theorem}[Cavalieri's Principle]
Let $S$ be a bounded $n$-dimensional solid in $\mathbb{R}^n$ (note that $S$ can be an interval in $\mathbb{R}$). Define an $n-1$ subspace $P$ in $\mathbb{R}^n$ and given the quotient space $\mathbb{R}^n / P$ with elements $P_x$, let 
\[S \subset \bigcap_{a \leq x \leq b} P_x\]
That is, $S$ is "in between" affine subspaces $P_a$ and $P_b$. The cross section of $S$ cut by $P_x$ is the intersection of it with $S$
\[\text{Cross Section at } P_x \equiv P_x \cap S\]
Denote the area of this cross section as $A(x)$. Then, 
\[\text{Volume of } S = \int_a^b A(x) \; d x\]
\end{theorem}
This theorem basically says that the volume of $S$ is the sum of the areas of its infinitesimal cross sections. 
\begin{center}
    \includegraphics[scale=0.27]{Cavalieri_Principle .PNG}
\end{center}
This clearly works for an interval in $\mathbb{R}$, which is computed by the sum of all its "points" (rigorously speaking, infinitesimally thin intervals). The integral works for a shape in $\mathbb{R}^2$, which is computed by the sum of its "line segments" (rigorously speaking, infinitesimally thin rectangles) that add up to the shape. In $\mathbb{R}^3$, the solid is computed by the sum of its cross sections (infinitesimally thin "molded" cylinders). This analogy continues into higher dimensions. 
\\

Given a solid $S \subset \mathbb{R}^n$, it is easy to see that no matter what subspace $P$ we choosethat is, no matter what orientation we choose to "cut" the solid the sum of all of its cross sections should be equal to the true volume of $S$. In the case when $S$ is a box in $\mathbb{R}^n$, Fubini's theorem states that whether we cut $S$ up along the $x_1$-axis, $x_2$-axis, ..., or the $x_n$-axis, the symmetry in volume is always preserved. This theorem is really just a specific case of this general symmetry in volume. 

\begin{theorem}[Fubini's Theorem]
Given a function $f: \mathbb{R}^n \longrightarrow \mathbb{R}$, let 
\[B \equiv \prod_{i=1}^n [\alpha_i, \beta_i]\]
and let 
$p$ be any permutation of the elements $\{x_1, x_2, ..., x_n\}$. Then 
\begin{align*}
    \int_B f \; d V & = \int_{\alpha_n}^{\beta_n} ... \int_{\alpha_1}^{\beta_1} f(x_1,x_2,...,x_n) \; d x_1 ... d x_n \\
    & = \int_{p(\alpha_n)}^{p(\beta_n)} ... \int_{p(\alpha_1)}^{p(\beta_1)} f(x_1,x_2, ..., x_n) \; d p(x_1) ... d p(x_n) 
\end{align*}
In the two dimensional case, we have
\begin{align*}
    \iint_B f \; d A & = \int_c^d \int_a^b f(x, y) \; d x \, d y = \int_a^b \int_c^d f(x, y) \; d y \, d x 
\end{align*}
\begin{center}
    \includegraphics[scale=0.27]{Fubini_Theorem.PNG}
\end{center}
In the three dimensional case, we have
\begin{align*}
    \iiint_B f \; d V & 
    = \int_e^f \int_c^d \int_a^b f(x, y, z) \; d x \, d y \, d z = \int_e^f \int_a^b \int_c^d f(x, y, z) \; d y \, d x \, d z \\
    & = \int_c^d \int_a^b \int_e^f f(x, y, z) \; d z \, d x \, d y = \int_c^d \int_e^f \int_a^b f(x, y, z) \; d x \, d z \, d y \\
    & = \int_a^b \int_e^f \int_c^d f(x, y, z) \; d y \, d z \, d x = \int_a^b \int_c^d \int_e^f f(x, y, z) \; d z \, d y \, d x 
\end{align*}
\end{theorem}

Computation of these integrals is simple. You do the innermost integral first with respect to the corresponding variable, while treating the rest of the variables constant. Evaluating each integral outputs a formula for a higher dimensional cross section of the solid $S$. It is clear that computing iterated integrals is really just doing Cavalieri's principle repeatedly. 

\subsubsection{Integration over Solids Bounded by Curves}
We must first define the different types of \textit{elementary regions} first. 
\begin{definition}
A bounded region $D$ in $\mathbb{R}^n$ is said to be $x_i$-simple if it is bounded by the graphs of two continuous functions $u_1, u_2: \mathbb{R}^{n-1} \longrightarrow \mathbb{R}$ of the variables 
\[x_1, x_2, ..., x_{i-1}, x_{i+1}, ..., x_n\]
That is, $D$ can be expressed in the form 
\[\{ x \in \mathbb{R}^n \; | \; u_1 (x_1,..., x_{i-1}, x_{i+1}, ... , x_n) \leq x_i \leq u_2 (x_1, ..., x_{i-1}, x_{i+1}, ..., x_n)\}\]
If a region is simple in all of its variables, it is simply called \textit{simple}. Note that $n$-dimensional boxes are simple regions. 
\end{definition}

\begin{example}
In $\mathbb{R}^2$, the region on the left graph is an $y$-simple region and the region on the right is a $x$-simple region. \\
\begin{center}
\begin{tikzpicture}[scale=0.8]
    \draw[<->] (-1,0)--(5,0);
    \draw[<->] (0,-1)--(0,5);
    \draw[<->] (6,0)--(12,0);
    \draw[<->] (7,-1)--(7,5);
    \draw plot [smooth] coordinates {(0.6, 1.2) (1,1) (2,1.4) (3,1.3) (4,1.5) (4.3,1.7)};
    \draw plot [smooth] coordinates {(0.6, 3.9) (1,4.1) (2,4) (3,4.3) (4,4.2) (4.3,4.1)};
    \draw[dashed] (0.6,1.2)--(0.6,3.9);
    \draw[dashed] (4.3,1.7)--(4.3,4.1);
    \draw plot [smooth] coordinates {(8.2,0.6) (8,1) (8.4,2) (8.3,3) (8.5,4) (8.7,4.3)};
    \draw plot [smooth] coordinates {(10.9,0.6) (11.1,1) (11,2) (11.3,3) (11.2,4) (11.1,4.3)};
    \draw[dashed] (8.2,0.6)--(10.9,0.6);
    \draw[dashed] (8.7,4.3)--(11.1,4.3);
    \node[below] at (4.8,0) {$x$};
    \node[below] at (11.8,0) {$x$};
    \node[left] at (0,4.8) {$y$};
    \node[left] at (7,4.8) {$y$};
    \node[above] at (3,4.2) {$u_1$};
    \node[above] at (3,1.3) {$u_2$};
    \node[left] at (8.3, 3) {$v_1$};
    \node[right] at (11.3, 3) {$v_2$};
    \draw[fill] (0.6,0) circle (0.05);
    \node[below] at (0.6,0) {$a$};
    \draw[fill] (4.3,0) circle (0.05);
    \node[below left] at (4.3,0) {$b$};
    \draw[fill] (7,0.6) circle (0.05);
    \draw[fill] (7,4.3) circle (0.05);
    \node[left] at (7,0.6) {$c$};
    \node[left] at (7,4.3) {$d$};
\end{tikzpicture}
\end{center}
\end{example}
We now describe the method of calculating double integrals over elementary regions. 
\begin{theorem}
The double integral over a $y$-simple region $D$ bounded by functions $u_1$ and $u_2$ in $\mathbb{R}^2$ and the $x$-values $a$ and $b$ (as shown in the left graph of example 2.1) is
\[\iint_D f(x, y) = \int_a^b \int_{u_2 (x)}^{u_1 (x)} f(x, y) \, dy \, dx\]
The double integral over an $x$-simple region $D$ bounded by functions $v_1$ and $v_2$ in $\mathbb{R}^2$ and the $y$-values $c$ and $d$ (shown in the right of graph of example 2.1) is 
\[\iint_D f(x, y) = \int_c^d \int_{v_2 (y)}^{v_1 (y)} f(x, y) \, dx \, dy\]
\end{theorem}

\begin{example}
Integrating $f(x, y)$ over the unit disk would have the form
\[\int_{-1}^1 \int_{-\sqrt{1-x^2}}^{\sqrt{1-x^2}} f(x,y) \, dy\, dx \text{ or } \int_{-1}^1 \int_{-\sqrt{1-y^2}}^{\sqrt{1-y^2}} f(x,y) \, dx\, dy \]
Note that the unit disk is both $x$ and $y$ simple. 
\end{example}
\subsection{Change of Basis}
Sometimes, integrating a region over a different basis would make the integral computation much more simpler. In this case, we may be able to transform more complicated regions into elementary regions. We first introduce a change of basis in 2 dimensions and then generalize it into higher dimensions. 
\\
Let $\mathbb{R}^2$ have the standard orthonomal basis $e_1, e_2$, commonly known as the $x, y$ basis. Now, let us construct new basis vectors of $\mathbb{R}^2$, denoted $f_1, f_2$ such that $f_1, f_2$ are functions of $e_1, e_2$. Since they are both bases that span $\mathbb{R}^2$, we can equally represent $e_1, e_2$ as functions of $f_1, f_2$. 
\begin{align*}
    &e_1 = g(f_1, f_2)\\
    &e_2 = h(f_1, f_2) 
\end{align*}
Note that this change of basis does not necessarily have to be linear, as in the context of passive transformation in linear algebra. Then, every point $(x,y)$ in the $(e_1, e_2)$-basis can be rewritten as
\begin{align*}
    (x, y) & = x e_1 + y e_2 \\
    & = x \, g(f_1, f_2) + y \, h(f_1, f_2) \\
    & = u f_1 + v f_2
\end{align*}
Note that it is customary to denote $x, y$ as the coefficients in the $e_1, e_2$ basis and $u, v$ as the coefficients in the new $f_1, f_2$ basis. This way, we can not only write $e_1$ and $e_2$ as functions of $f_1$ and $f_2$, but we can also write the coefficents $x, y$ as functions of the coeffiecents $u, v$! That is, 
\begin{align*}
    & x = x(u, v) \\
    & y = y(u, v)
\end{align*}
which is really just a function 
\[B: \mathbb{R}^2 \longrightarrow \mathbb{R}^2, \;\; B(u, v) = \begin{pmatrix} x(u, v) \\ y(u, v) \end{pmatrix}\]
Notice that $B$ changes the $u, v$ coordinates to the $x, y$ coordinates, and $B^{-1}$ changes the $x, y$ coordinates to the $u, v$ coordinates. 
\[B^{-1}: \mathbb{R}^2 \longrightarrow \mathbb{R}^2, \;\; B^{-1} (x, y) = \begin{pmatrix} u (x, y) \\ v (x, y) \end{pmatrix}\]
Note that these coefficients actually change \textit{contravariantly}, that is, they change inversely with respect to how the basis vectors are changed. In vector calculus, it is conventional to represent a change of basis with functions that relate the coefficients $x, y$ with $u, v$, rather than the bases $f_1, f_2$ with $e_1, e_2$. 

\begin{theorem}[Integration over Change of Bases in $\mathbb{R}^2$]
Let $\mathbb{R}^2$ have the standard orthonomal basis $e_1, e_2$. Now, let us construct new basis vectors of $\mathbb{R}^2$, denoted $f_1, f_2$ such that the coefficients of the vectors in $\mathbb{R}^2$ are related by the change of basis function 
\[B = \begin{pmatrix} x \\ y \end{pmatrix} \implies B(u, v) = \begin{pmatrix} x(u, v) \\ y(u, v) \end{pmatrix}\]
Given region $D \subset \mathbb{R}^2$ and $S = B(D)$ is the region transformed by $B$, the integral of function $f(x, y)$ over region $D$ can be expressed as 
\[\iint_D f(x, y) \, dA = \iint_S f \big( x(u, v), y(u, v) \big) \, \big| J B(u, v) \big| \, d \bar{A}\]
where $\big| J B(u, v) \big|$ is the determinant of the Jacobian matrix of $B$. Expanding the Facobian determinant gives
\[\big| J B(u, v) \big| = \frac{\partial x}{\partial u} \frac{\partial y}{\partial v} - \frac{\partial x}{\partial v} \frac{\partial y}{\partial u}\]
\end{theorem}

\begin{theorem}[Integration over Change of Bases in $\mathbb{R}^3$]
Given that we have the change of basis function 
\[B: \mathbb{R}^3 \longrightarrow \mathbb{R}^3, \;\;\; B(u, v, w) = \begin{pmatrix} x(u, v, w) \\ y(u, v, w) \\ z(u, v, w) \end{pmatrix}\]
a region $D \in \mathbb{R}^3$ and $S = B(D)$, the region transformed by $B$, the integral of $f(x, y, z)$ over region $D$ can be expressed as 
\[\iiint_D f(x, y, z)\, dV = \iiint_S f\big( x(u, v, w), y(u, v ,w), z(u, v, w) \big) \big| J B (u, v, w)\big| \, d \bar{V}\]
where $\big| J B (u, v, w)\big|$ is the Jacobian determinant of $B$. 
\end{theorem}

\begin{example}
Given a real-valued function $f$ defined over the region $D \subset \mathbb{R}^2$, we can perform a change of basis of the $x, y$ coordinates into polar ones within a new region $S$. The change of basis 
\begin{align*}
    & x = r \cos{\theta} \\
    & y = r \sin{\theta} 
\end{align*}
\begin{center}
\begin{tikzpicture}
    \draw[thick, fill=lightgray] (7.5,2) circle (1.5);
    \draw[<->] (-1,0)--(3,0);
    \draw[<->] (0,-1)--(0,5);
    \draw[thick, fill=lightgray] (0,0) rectangle (2,4);
    \node at (1,2) {$S$};
    \draw[fill] (0,4) circle (0.05);
    \draw[fill] (2,0) circle (0.05);
    \node[below] at (2,0) {$1$};
    \node[left] at (0,4) {$2 \pi$};
    \node[above] at (3,0) {$r$};
    \node[right] at (0,5) {$\theta$};
    \draw[->] (2.5, 2)--(5,2);
    \node[above] at (4,2.5) {$T: (r, \theta) \mapsto$};
    \node[above] at (4,2) {$ (r \cos{\theta}, r \sin{\theta})$};
    \draw[<->] (5.5,2)--(9.5,2);
    \draw[<->] (7.5,0)--(7.5,4);
    \node at (8,2.5) {$D$};
\end{tikzpicture}  
\end{center}
\end{example}

\begin{theorem}[Integration over Change of Bases in $\mathbb{R}^n$]
Let $\mathbb{R}^n$ have the standard orthonormal basis $e_1, e_2, ..., e_n$, and let us construct a new basis $f_1, f_2, ..., f_n$ such that the coefficients of the vectors in $\mathbb{R}^n$ are related with the functions
\[B: \mathbb{R}^n \longrightarrow \mathbb{R}^n, \;\;\;\; B(u_1, u_2, \ldots, u_n) = \begin{pmatrix}
x_1 (u_1, \ldots, u_n) \\x_2 (u_1, \ldots, u_n) \\ \vdots \\ x_n (u_1, \ldots, u_n)
\end{pmatrix}\]
Given that the region $D \subset \mathbb{R}^n$ is transformed into a new region $S = B(D) \subset \mathbb{R}^n$ under this basis transformation, the integral of function $f(x_1, \ldots, x_n)$ over region $D$ can be expressed as 
\[\int_D f(x) \, dH = \int_S f \big( x_1(u), x_2(u), ..., x_n (u) \big) \big| J B(u_1, \ldots, u_n)\big| \, d \bar{H}\]
where the integral on both the left and right hand side represents integration over an $n$-dimensional region, $x$ represents the $n$-tuple $(x_1, \ldots, x_n)$, $u$ represents the $n$-tuple $(u_1, \ldots, u_n)$, and $\big| J B(u_1, \ldots, u_n)\big|$ represents the Jacobian determinant of function $B$. 
\end{theorem}

We now describe some common change of basis formulas for polar, cylindrical, and spherical coordinates. 

\begin{theorem}[Integration in Polar Coordinates]
\[\iint_{D} f(x, y) \, dx \,dy = \iint_S f(r \cos{\theta}, r \sin{\theta}) r \, dr \, d\theta\]
\end{theorem}

\begin{definition}[Cylindrical, Spherical Coordinates]
In $\mathbb{R}^3$, \textit{cylindrical coordinates} have the following relation to rectangular coordinates. 
\begin{align*}
    & x = r \cos{\theta} \\
    & y = r \sin{\theta} \\
    & z = z
\end{align*}
In $\mathbb{R}^3$, \textit{spherical coordinates} have the following relation to rectangular coordinates. 
\begin{align*}
    & x = \rho \sin{\phi} \cos{\theta} \\
    & y = \rho \sin{\phi} \sin{\theta} \\
    & z = \rho \cos{\phi}
\end{align*}
\end{definition}

\begin{corollary}[Integration in Cylindrical Coordinates]
\[\iiint_D f(x, y, z) \, dx \, dy \, dz = \iiint_S f( r \cos{\theta}, r \sin{\theta}, z) r \, dr \, d\theta \, dz\]
\end{corollary}

\begin{corollary}[Integration in Spherical Coordinates]
\[\iiint_D f(x, y, z) \,dx\,dy\,dz = \iiint_S f(\rho \sin{\phi} \cos{\theta}, \rho \sin{\phi} \sin{\theta}, \rho \cos{\phi}) \rho^2 \sin{\theta} \, d\rho \, d\theta \, d\phi\]
\end{corollary}

\begin{example}[Gaussian Integral]
The following is the (un-normalized) probability distribution function of the Gaussian distribution. 
\[\int_{-\infty}^{\infty} e^{-x^2} \, dx = \sqrt{\pi}\]
\end{example}

\subsection{Average Values, Centers of Mass}
\begin{definition}
The \textit{average value} of a function defined over a region $D \subset \mathbb{R}^n$ is 
\[[f]_{av} = \bigg(\int_D f(x) \,d V \bigg) \bigg/ \bigg( \int_D \, d V \bigg) \]
where both integrals represent integration over the $n$-dimensional region $D$. Informally, the integral above represents the infinitesimal sum of all the values of the function $f$ over $D$ and divides it by the hypervolume of $D$ to average it out. More specifically, the average value of $f: \mathbb{R} \longrightarrow \mathbb{R}$ in the interval $[a,b]$ is defined
\[ [f]_{av} = \bigg(\int_a^b f(x) \,d x \bigg) \bigg/ \bigg( \int_a^b \, d x \bigg) = \frac{1}{b-a} \int_a^b f(x) \,d x \]
For a function $f: \mathbb{R}^2 \longrightarrow \mathbb{R}$ over a two dimensional region $D$, we have 
\[[f]_{av} = \bigg(\iint_D f(x, y) \,dx\,dy \bigg) \bigg/ \bigg( \iint_D \, dx\,dy \bigg)\]
For $f: \mathbb{R}^3 \longrightarrow \mathbb{R}$ over a three dimensional region $V$, we have
\[[f]_{av} = \bigg(\iiint_V f(x, y, z) \,dx\,dy\,dz \bigg) \bigg/ \bigg( \iiint_V \, dx\,dy\,dz \bigg)\]
\end{definition}

It is quite easy to get the center of mass of a system of $n$-distinct points in $\mathbb{R}^n$. We can solve each $x_i$ coordinate for the center of mass by averaging out the $x_i$ coordinates scaled by their respective masses. That is, given points $x_1, x_2, ..., x_n$ with respective masses $m_1, ..., m_n$, the center of mass is defined as
\[\bar{x} = \frac{\sum m_i x_i}{\sum m_i}\]

\begin{definition}
Given an $n$-dimensional continuous mass density distribution, denoted $\delta(x)$, defined over a region $D \subset \mathbb{R}^n$, the center of mass of $D$ can be determined through coordinates.
\[\bar{x_i} = \bigg( \int_D x_i \, \delta(x) d V\bigg) \bigg/ \bigg(\int_D \delta(x) d V \bigg), \; i = 1, 2, 3, ..., n\]
Note that $\delta$ must be continuous (in order for it to be integrable). More specifically, the center of mass of a one dimensional interval $I \subset \mathbb{R}$ is
\[\bar{x} = \bigg(\int_I x\, \delta(x) \, dx\bigg) \bigg/ \bigg(\int_I \delta(x) \, dx\bigg)\]
For a two dimensional region (which we can visualize as a "disk" or "plate," the $x$ and $y$ coordinates for the center of mass is
\begin{align*}
    & \bar{x} = \bigg(\iint_D x\, \delta(x, y) \, dx\,dy\bigg) \bigg/ \bigg(\iint_D \delta(x, y) \, dx\,dy \bigg) \\
    & \bar{y} = \bigg(\iint_D y\, \delta(x, y) \, dx\,dy\bigg) \bigg/ \bigg(\iint_D \delta(x, y) \, dx\,dy \bigg)
\end{align*}
For a three dimensional mass, the $x, y, z$ coordinates of the center of mass of volume $V$ can be found with
\begin{align*}
    & \bar{x} = \bigg(\iiint_V x\, \delta(x, y, z) \, dx\,dy\,dz\bigg) \bigg/ \bigg(\iiint_V \delta(x, y, z) \, dx\,dy \,dz\bigg) \\
    & \bar{y} = \bigg(\iiint_V y\, \delta(x, y, z) \, dx\,dy\,dz\bigg) \bigg/ \bigg(\iint_V \delta(x, y, z) \, dx\,dy\,dz \bigg) \\
    & \bar{z} = \bigg(\iiint_V z\, \delta(x, y, z) \, dx\,dy\,dz\bigg) \bigg/ \bigg(\iint_V \delta(x, y, z) \, dx\,dy\,dz \bigg)
\end{align*}
\end{definition}

\subsection{Improper Integrals}
There are generally two types of improper integrals. 
\begin{enumerate}
    \item The region $D$ integrated over is unbounded. 
    \item The function $f$ that is integrated is unbounded within the region $D$.
\end{enumerate}
\subsubsection{Single Variable Improper Integrals}
These types of improper integrals are usually evaluated using a limiting process. When the interval $I$ is unbounded, say $(1, \infty)$, the integral can be evaluated as 
\[\int_1^\infty \frac{1}{x^2} \,dx = \lim_{b \rightarrow \infty} \int_1^b \frac{1}{x^2} \, dx = \lim_{b\rightarrow \infty} \bigg( 1 - \frac{1}{b} \bigg) = 1\]
In case 2, we can add a limit at the point where the function $f$ diverges as such. 
\[\int_0^1 \frac{1}{\sqrt{x}} \, dx = \lim_{a \rightarrow 0} \int_a^1 \frac{1}{\sqrt{x}} \, dx = \lim_{a \rightarrow 0} (2 - 2\sqrt{a}) = 2\]
We now describe how to integrate over a certain path $p$ embedded in a higher dimensional space $\mathbb{R}^n$, possibly with a scalar or vector field $f$. We must first go over oriented paths. 

\subsubsection{Two Variable Improper Integrals}
Extending the previous case, we use a multivariate limiting process in $\mathbb{R}^2$. We will first work with case 2, when $f$ is unbounded within the region $D$. Let us define an elementary region $D$ in $\mathbb{R}^2$; without loss of generality, we will make it $y$-simple, meaning that $D$ can be expressed as
\[D \equiv \{ (x, y) \in \mathbb{R}^2 \; | \; a \leq x \leq b, \; \phi_1 (x) \leq y \leq \phi_2 (x)\}\]
We can actually assume that the region in which $f$ is unbounded lies in the boundary $\partial D$. This is because if it lied in the interior of $D$, we could split $D$ into pieces across a path that intersects this region with divergent values, evaluate the integrals over the pieces separately, and then sum the integrals. For example, in the rectangular region below, let the dashed line represent the values where the function $f$ diverges. Then, we can split the region into two rectangular regions shown in the right. \\
\begin{center}
\begin{tikzpicture}
    \draw (0,0) rectangle (3,2);
    \draw[dashed] rectangle (2,0)--(2,2);
    \draw[->] (3.5,1)--(5,1);
    \draw (8,0)--(6,0)--(6,2)--(8,2);
    \draw[dashed] (8,0)--(8,2);
    \draw[dashed] (9,0)--(9,2);
    \draw (9,0)--(10,0)--(10,2)--(9,2);
\end{tikzpicture}
\end{center}
Therefore, assuming that $f$ is unbounded in $\partial D$, we can construct a new region 
\[D_{\eta, \delta} \equiv \{(x, y) \in \mathbb{R}^2 \; | \; a + \eta \leq x \leq b - \eta, \; \phi_1 (x) + \delta \leq y \leq \phi_2 (x) - \delta\}\]
for some arbitrarily small numbers $\eta, \delta >0$, meaning that the integral (reduced to iterated integrals using Fubini's theorem) 
\[F(\eta, \delta) \equiv \iint_{D_{\eta, \delta}} f(x, y) \, dA = \int_{a + \eta}^{b - \eta} \int_{\phi_1 (x) + \delta}^{\phi_2 (x) - \delta} f(x, y) \, dy\,dx\]
is well defined. 
\begin{center}
\begin{tikzpicture}
    \draw[<->] (-0.5,0)--(5,0);
    \draw[<->] (0,-0.5)--(0,5);
    \draw (1,1.5)--(1,3);
    \draw plot [smooth] coordinates {(1,3) (1.5,3.3) (2,3.1) (3,3.8) (4,4.2) (4.5,4)};
    \draw (4.5, 4)--(4.5,1);
    \draw plot [smooth] coordinates {(1,1.5) (2,1.7) (3, 1.6) (3.7, 1.3) ( 4.5,1)};
    \draw[dashed] (1.3,1.85)--(1.3,2.9);
    \draw[dashed] (4.2, 3.8)--(4.2,1.4);
    \draw[dashed] plot [smooth] coordinates {(1.3,2.9) (1.5,3) (2,2.8) (3,3.5) (4,3.9) (4.2,3.8)};
    \draw[dashed] plot [smooth] coordinates {(1.3,1.85) (2,2) (3, 1.9) (3.7, 1.6) ( 4.2,1.4)};
    \node at (3,2.5) {$D_{\eta, \delta}$};
    \node[below left] at (2,1) {$D$};
    \draw[->] (2,1)--(2.5,1.85);
\end{tikzpicture}
\end{center}
Clearly, the function $F( \eta, \delta)$ is a function of two variables $\eta$ and $\delta$. So, if the limit 
\[\lim_{(\eta, \delta) \rightarrow (0, 0)} F(\eta, \delta)\]
is well defined, then so is the improper integral. For it to exist, the iterated limits must both equal to a well-defined real number $\mathcal{L}$ (and to each other). That is, 
\[\lim_{\eta \rightarrow 0} \lim_{\delta \rightarrow 0} F(\eta, \delta) = \lim_{\delta \rightarrow 0} \lim_{\eta \rightarrow 0} F(\eta, \delta) = \mathcal{L} \implies \lim_{(\eta, \delta) \rightarrow (0,0)} F(\eta, \delta) = \mathcal{L}\]


It is also worthwhile to note that functions unbounded at isolated points can be evaluated using the methods above using a change of basis. Consider the example below. 

\begin{example}
In the unit disk $D \subset \mathbb{R}^2$, let the function $f$ be defined as 
\[f(x, y) \equiv \frac{1}{\sqrt{x^2 + y^2} }\]
Clearly, $f$ is continuous at every point except $0= (0,0)$, meaning that 
\[\iint_{D \setminus \{0\}} f(x, y)\, dA\]
is well-defined. In order to solve the integral over the entire disk, we convert to polar coordinates and evaluate the limit
\[\iint_{D \setminus \{0\}} f(x, y) \, dA = \lim_{\delta \rightarrow 0} \int_{\delta}^1 \int_0^{2 \pi} r \, f( r \cos{\theta}, r \sin{\theta}) \, d\theta \,dr\]
\end{example}
\begin{center}
\begin{tikzpicture}
    \draw[thick, fill=lightgray] (0,0) circle (1.5); 
    \draw[<->] (-2,0)--(2,0);
    \draw[<->] (0,-2)--(0,2);
    \draw[fill=white] (0,0) circle (0.08);
    \draw[->, thick] (2.5,0.5)--(4, 0.5); 
    \draw[<->] (4.5,-1)--(7,-1);
    \draw[<->] (5,-1.5)--(5,2);
    \draw[white, fill=lightgray] (5,-1) rectangle (6.5,1.5);
    \draw[thick] (5,-1)--(6.5,-1)--(6.5,1.5)--(5,1.5);
    \draw[thick, dashed] (5,-1)--(5,1.5);
    \draw[fill] (6.5,-1) circle (0.03);
    \node[below] at (6.5,-1) {$1$};
    \node[left] at (5,1.5) {$2 \pi$};
    \draw[fill] (5, 1.5) circle (0.03);
    \draw[fill] (1.5,0) circle (0.03);
    \draw[fill] (0,1.5) circle (0.03);
    \node[below right] at (1.5,0) {$1$};
    \node[above right] at (0,1.5) {$1$};
\end{tikzpicture}
\end{center}

If we are given an unbounded region $D \subset \mathbb{R}^2$, we can first create a bounded region and expand that region using a limit to cover all of $D$. 

\subsection{Line Integrals}
\begin{definition}[Orientations, Simple Curves, Closed Curves]
A path function $p: [a,b]\subset \mathbb{R} \longrightarrow \mathbb{R}^n$ determines a curve in $\mathbb{R}^n$ with endpoints $p(a)$ and $p(b)$. The direction the curve $p$ takes, that is from $p(a)$ to $p(b)$ in $\mathbb{R}^n$ is called the \textit{orientation} of $p$. A path or a curve with a defined orientation is called an \textit{oriented curve}. 

A \textit{simple curve} $C$ to be the image of an injective piecewise $C^1$ map $c: I \subset \mathbb{R} \longrightarrow \mathbb{R}^3$. Since it is inejctive, it does not intersect itself, and $C$ is piecewise smooth in $\mathbb{R}^n$. If $I = [a,b]$, then $c(a)$ and $c(b)$ are the endpoints of the curve. A simple curve with an orientation is called an \textit{oriented simple curve}. 

A closed curve $C$ is the image of piecewise $C^1$ map $c: [a,b] \longrightarrow \mathbb{R}^n$ such that $c(a) = c(b)$. That is, the endpoints of $C$ are equal. A \textit{simple closed curve} is a closed curve that is injective over the interval $[a,b)$. Note that a closed curve has two possible orientations. 
\end{definition}

If $C$ is an oriented simple curve or an oriented simple closed curve, then we can unambiguously define line integrals along them. 

\begin{definition}
Let $h$ be an injective function that takes $[\alpha,\beta] \subset \mathbb{R}$ to the interval $[a, b] \subset \mathbb{R}$. Given an oriented simple path function $p: [a,b]\subset \mathbb{R} \longrightarrow \mathbb{R}^n$, the composition
\[\rho = p \circ h: [\alpha, \beta] \longrightarrow \mathbb{R}^n\]
is called a \textit{reparamaterization} of $p$. Note that since $h$ is injective, it takes endpoints to endpoints. If $h$ preserves the direction in which the path travels, that is, if 
\[(p \circ h)(\alpha) = a \text{ and } (p \circ h)(\beta) = b\]
then $h$ is \textit{orientation preserving}. If
\[(p \circ h)(\alpha) = b \text{ and } (p \circ h)(\beta) = a\]
then $h$ is \textit{orientation reversing}. Note that a path $c$ having the same image as $p$ does not imply that $c$ is a reparamaterization of $p$, since $c$ may not be injective. 
\begin{center}
    \includegraphics[scale=0.25]{Orientation_Preserving_Reversing.PNG}
\end{center}
\end{definition}

\begin{definition}[Scalar Line Integral]
Let $f: \mathbb{R}^n \longrightarrow \mathbb{R}$, which can be interpreted as a scalar field. Now define a $C^1$ path function 
\[c: [a,b] \subset \mathbb{R} \longrightarrow \mathbb{R}^n \]
such that the composition of functions
\[f \circ c: [a, b] \subset \mathbb{R} \longrightarrow \mathbb{R}^n\]
is continuous. Then, the \textit{path integral}, or \textit{scalar line integral}, of $f$ along the path $c$. is defined
\begin{align*}
    \int_c f \;d s & = \int_a^b f\big(c(t)\big) ||c^\prime (t)|| \;d t \\
    & = \int_a^b f\big( x_1 (t), x_2 (t), ..., x_n (t)\big) ||c^\prime (t)|| \; d t
\end{align*}
If $c(t)$ is only piece-wise $C^1$, we can define the path integral by breaking $[a,b]$ into pieces over with $f\big( c(t)\big) ||c^\prime (t)||$ is continuous and then summing the integrals over the pieces. 
That is, 
\[\int_a^b f\big(c(t)\big) ||c^\prime (t)|| \;d t = \sum_{i = 0}^{n-1} \int_{\alpha_i}^{\alpha_{i+1}} f\big(c(t)\big) ||c^\prime (t)|| \; d t\]
\end{definition}
Note that since $f$ is a scalar-valued function, we can interpret a path integral as the sum of infinitesmal segments of the path $c$ having a weight determined by $f$ at each section. 
If $f$ is a constant function outputting $1$ at every point, then the path integral just outputs the length of the path $c$ in $\mathbb{R}^n$. 
\[L = \int_a^b f\big( c(t)\big) ||c^\prime (t)|| \; d t = \int_a^b ||c^\prime (t)|| \; d t\]

\begin{definition}[Vector Line Integral]
Let $F: \mathbb{R}^n \longrightarrow \mathbb{R}^n$ be a vector field on $\mathbb{R}^n$ that is continuous on the $C^1$ oriented path $c: [a, b] \subset \mathbb{R} \longrightarrow \mathbb{R}^n$. The \textit{line integral} of $F$ along $c$ is defined by the formula 
\[\int_c F \cdot d s = \int_a^b F\big( c(t)\big) \cdot c^\prime (t) \; d t\]
where $\cdot$ represents the dot product of $F$ with $c^\prime$ over the interval $[a,b]$. It is also commonly written in differential notation, 
\[\int_c F \cdot ds = \int_c F \cdot (dx_1, \ldots, d x_n) = \int_c F_1 dx_1 + F_2 dx_2 + \ldots F_n dx_n\]
\begin{center}
    \includegraphics[scale=0.27]{Vector_Line_Integral.PNG}
\end{center}
 Similarly with path integrals, we can also define line integrals as the sum of integrals over piece-wise continuous sections of $c$. That is, given an oriented curve $C$ made up of several oriented component curves $C_i$, $i = 1, 2, ..., k$, we can paramaterize $C$ by paramaterizing the pieces $C_i$'s separately. Thus, we can treat $C = C_1 + ... C_k$ and get
\[\int_C F \cdot d s = \sum_{i = 1}^k \int_{C_i} F \cdot d s\]
Note that a vector line integral is a generalization of scalar line integrals, so any results holding for vector line integrals also holds for their scalar counterpart. 
\end{definition}

\begin{example}[Work]
In mechanics, work $W$ is defined as 
\[W = F \cdot d\]
where $F$ is force and $d$ is displacement. With this knowledge, the reader can easily see that the work done by vector field $F$ on a particle traveling along a path $c$ from time $a$ to time $b$ can be calculated by the line integral
\begin{align*}
    W & = \int_a^b F\big( c(t)\big) \cdot c^\prime (t) \; d t \\
    & = \int_c F_1 dx + F_2 dy + F_3 dz
\end{align*}
\end{example}

\begin{theorem}[Invariance of Path Paramaterizations on Vector Line Integrals]
Let $F$ be a vector field and $f$ be a scalar field, both continuous on the $C^1$ path function $p: [a,b] \longrightarrow \mathbb{R}^n$ and let $q: [\alpha, \beta] \longrightarrow \mathbb{R}^n$ be a reparamaterization of $p$. Then, 
\begin{align*}
    q \text{ is orientation preserving} & \implies \int_p F \cdot d s = \int_q F \cdot d s \\
    q \text{ is orientation reversing} & \implies \int_p F \cdot d s = - \int_q F \cdot d s
\end{align*}
\end{theorem}

\subsubsection{Conservative Vector Fields}
We now introduce a fundamental theorem about line integrals over gradient fields. Recall the fundamental theorem of calculus and it's equivalent form. 

\begin{theorem}[Fundamental Theorem of Single Variable Calculus]
Let function $\nabla g: \mathbb{R} \longrightarrow \mathbb{R}$ be the gradient of the single variable $C^1$ function $g: \mathbb{R} \longrightarrow \mathbb{R}$; that is, $\nabla g$ is a conservative vector field on $\mathbb{R}$. Then, 
\[\int_a^b \nabla g (x) \,dx = g(b) - g(a)\]
Note that in the single variable case, 
\[\frac{d}{dx} g(x) = \nabla g(x)\]
This means that the value of the integral of $\nabla g$ only depends on the value of $g$ at the endpoints of the interval $[a,b]$. 
\end{theorem}

We can extend this to line integrals for functions mapping $\mathbb{R}^n$ to $\mathbb{R}$. 

\begin{theorem}[Invariance of Line Integrals in Conservative Vector Fields]
Given that $F: \mathbb{R}^n \longrightarrow \mathbb{R}^n$ is a $C^1$ conservative vector field with $\nabla f = F$ for $C^2$ function $f: \mathbb{R}^n \longrightarrow \mathbb{R}$ and path function $p: [a,b] \longrightarrow \mathbb{R}^n$ is a piecewise $C^1$ path, then 
\[\int_p F \cdot d s = \int_p \nabla f \cdot d s = f\big(p(b)\big) - f\big(p(a)\big)\]
That is, the line integral of any path in a conservative vector field is dependent on the value of $f$ at the endpoints $p(a)$ and $p(b)$. 
\begin{center}
    \includegraphics[scale=0.2]{Line_Integral_Independence_of_path.PNG}
\end{center}
\end{theorem}

In physics, calculating the work done by a force represented by a vector field requires us to know the path that it travels through. 
\[W = \int_p F \cdot ds\]
However, in many cases $F$ is assumed to be conservative, so it is only necessary that we find the displacement of the particle from its endpoints, resulting in the simplification of the formula.  
\[W = \int_p \nabla f \cdot ds = f\big( p(b)\big) - f \big(p(a)\big)\]

\begin{corollary}[Equivalent Conditions for Vector Field to be Conservative]
The following conditions are equivalent: 
\begin{enumerate}
    \item $F: \mathbb{R}^n \longrightarrow \mathbb{R}^n$ is a conservative vector field. 
    \item The line integral of $F: \mathbb{R}^n \longrightarrow \mathbb{R}^n$ in curve $C$ is path independent; that is, if $C_1$ and $C_2$ are two paramaterizations of $C$, 
    \[\int_{C_1} F \cdot ds = \int_{C_2} F \cdot ds\]
    \item Given that $C$ is a closed loop, the line integral of $F: \mathbb{R}^n \longrightarrow \mathbb{R}^n$ across $C$ is $0$. 
    \[\oint_C F \cdot ds = 0\]
    \item The curl of $F: \mathbb{R}^3 \longrightarrow \mathbb{R}^3$ vanishes
    \[\curl{F} = \nabla \times F = \begin{pmatrix}
    \frac{\partial F}{\partial x} \\\frac{\partial F}{\partial y} \\\frac{\partial F}{\partial z} 
    \end{pmatrix} \times \begin{pmatrix}
    F_1\\F_2\\F_3
    \end{pmatrix}= 0\]
    \item The following partial derivatives of $F: \mathbb{R}^2 \longrightarrow \mathbb{R}^2$ are equal
    \[\frac{\partial F_1}{\partial y} = \frac{\partial F_2}{\partial x}\]
\end{enumerate}
\end{corollary}

We can develop a bit of intuition to determine whether a vector field is conservative or not. If vector field $F$ is conservative, then there exists a smooth scalar field $f$ such that $\nabla f = F$. For each latitude and longitude on a certain map, we can give it an altitude as a function of those coordinates (picture a map with a bunch of hills and valleys). The gradient and thus the vector field is all the vectors that point in the direction of highest ascent. he vector field is all the vectors that point in the direction of highest ascent. Extending the metaphor the path integral is like starting on at a point and climbing the hills and valleys, creating work as you go up a hill (proportional to the steepness and thus the dot product of your motion vector with the gradient vector field in the path integral) and decreasing the work you put in by going down a hill. Since the path is closed, it is like you are going up and down the same amount overall, so the path integral is zero. Following this analogy, the vector field determined by this function (marked as arrows in the $x, y$ plane) is conservative. 
\begin{center}
    \includegraphics[scale=0.28]{Conservative_Vector_Field.jpg}
\end{center}
If we can construct a closed loop around $F$ where the line integral is nonzero, then it means that we have ended up at a "higher" or "lower" (altitude) at the same point. This means that rather than being a certain landscape, there exist different "levels" of values at one point, like a spiraling staircase. For example, look at the solenoidal vector field below, where we can construct a closed loop (a circle going around the origin counterclockwise). There is no "surface" that can be defined such that it contains the solenoid. 
\begin{center}
    \includegraphics[scale=0.28]{Solenoid_nonconservative.jpg}
\end{center}
Clearly, as a particle travels through the vector field along the path, it does positive work while it has zero displacement, and clearly, there exists no function that can output both these values as determined by vector field $F$. 

\begin{theorem}[Helmholtz Decomposition]
Let $F: \mathbb{R}^3 \longrightarrow \mathbb{R}^3$ be a $C^2$ vector field. Then, $F$ can be decomposed into a curl-free component and a divergence-free component. That is, there exists vector fields $A$ and $\Phi$
\[F = - \nabla \cdot \Phi + \nabla \times A\]
\end{theorem}

\subsubsection{Curvature}
\begin{definition}[Curvature at a Point]
Let $c: [a, b] \longrightarrow C \subset \mathbb{R}^3$ be a unit-speed paramaterization of $C$, meaning that $||c^\prime (t)|| = 1$ for all $t \in [a,b]$, and let $p = c(t_0)$ be a point in $C$. The \textit{curvature} $\kappa(p)$ at $p$ is a mapping defined
\[\kappa: C \longrightarrow \mathbb{R}, \;\; \kappa(p) \equiv ||c^{\prime \prime} (t_0)||\]
Notice that since we require a unit speed paramaterization of $C$, we do not need to worry about how a given curve is paramaterized. 
\end{definition}

Since the curvature is defined pointwise for each point in curve $C$, we can integrate over all the curvatures in $C$ to define the total curvature. 

\begin{definition}[Total Curvature]
The \textit{total curvature} of a curve $c: [a,b] \longrightarrow C \subset \mathbb{R}^3$ is the scalar line integral 
\[\int_C \kappa \, ds\]
\end{definition}

We now present an important theorem in differential geometry. 
\begin{theorem}[Fary-Milnor Theorem]
Given a unit speed paramaterization $c: [a,b] \longrightarrow C \subset \mathbb{R}^3$, if $C$ is closed (that is, $c(a)=c(b)$), then 
\[\oint_C \kappa\, ds \geq 2 \pi\]
and equals $2\pi$ only when $C$ is a circle. Furthermore, if $C$ is a closed space curve with 
\[\oint_C \kappa\, ds \leq 4\pi\]
then $C$ is "unknotted." That is, $C$ can be continuously deformed without every intersecting itself into a planar circle. Therefore, for knotted curves $C$, we have
\[\oint_C \kappa \, ds > 4\pi\]
\end{theorem}

\subsection{Surface Integrals}
Surface integrals are the $2$-dimensional analogue, or the double integral version, of line integrals. It is the integration of surfaces. 

\subsubsection{2-Dimensional Paramaterizations of Surfaces}
Just like how we create path functions using a paramaterization function $p: [a, b] \subset \mathbb{R} \longrightarrow \mathbb{R}^n$, we can parameterize surfaces by defining a function 
\[\varphi: D \subset \mathbb{R}^2 \longrightarrow \mathbb{R}^n, \;\;\; \varphi (u, v) \equiv \begin{pmatrix} x_1 (u, v) \\ \vdots \\ x_n (u, v) \end{pmatrix}\]
The surface 
\[S = \varphi(D)\]
corresponding to the function $\varphi$ is its image. If $\varphi$ is differentiable or is of class $C^1$, then we call $S$ a \textit{differentiable} or $C^1$ surface, respectively. 

For those that are familiar with differential geometry, this makes every paramaterized surface a 2-manifold induced by the single homeormophism $\varphi$. In fact, it is more than just locally homeomorphic; it is \textit{globally} homeomorphic. 


\begin{definition}[Tangent Vectors of Surfaces Embedded in $\mathbb{R}^3$]
Given surface paramaterization 
\[\varphi: \mathbb{R}^2 \longrightarrow \mathbb{R}^3, \;\;\; \varphi(u, v) \equiv \begin{pmatrix}
x (u, v) \\ y(u, v) \\ z(u, v) 
\end{pmatrix}\]
it is visually clear that there can be up to two linearly independent tangent vectors at a point on the surface $S$. We can calculate these two vectors by embedding two nonparallel paths in $D \subset \mathbb{R}^2$ and taking the derivative with respect to a point traveling through these paths, which would give us a tangent vector on $S$. To keep things simple, we take the partial derivatives with respect to $u$ and $v$. 
\begin{center}
    \includegraphics[scale=0.28]{Partial_Derivatives_with_respect_to_U_V.PNG}
\end{center}
Clearly, these paths are functions 
\begin{align*}
    \frac{\partial \varphi}{\partial u} \equiv \begin{pmatrix}
     \frac{\partial x}{\partial u} \\ \frac{\partial y}{\partial u} \\ \frac{\partial z}{\partial u}
    \end{pmatrix} : \mathbb{R}^2 \longrightarrow \mathbb{R}^3\\
    \frac{\partial \varphi}{\partial v} \equiv \begin{pmatrix}
     \frac{\partial x}{\partial v} \\ \frac{\partial y}{\partial v} \\ \frac{\partial z}{\partial v}
    \end{pmatrix} : \mathbb{R}^2 \longrightarrow \mathbb{R}^3
\end{align*}
where 
\[\frac{\partial \varphi}{\partial u} (u_0 ,v_0), \; \frac{\partial \varphi}{\partial v} (u_0, v_0)\]
represent two vectors in $\mathbb{R}^3$ that are tangent to $S$ at the point $\varphi(u_0, v_0) \in \mathbb{R}^3$. 
\end{definition}

We must make sure that the surface $S$ is smooth in the sense that (informally) there aren't any wrinkles, points, folds, or self-intersections in such a way that the tangent plane to the surface is not well-defined. 

\begin{definition}[Regular Surfaces]
To formalize this concept, we say that $S$ is \textit{regular}, or \textit{smooth}, at point $(u_0, v_0)$ if
\[\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v} \neq 0\]
where $\times$ is the Euclidean cross product. That is, if the vector that is orthogonal to the two tangent vectors is well defined at a point, the surface is said to be smooth at that point. Note that $\frac{\partial \varphi}{\partial u}$ is parallel to $\frac{\partial \varphi}{\partial v}$ if and only if their cross product is $0$. 
\begin{center}
    \includegraphics[scale=0.3]{Cross_Product_Regular_Surfaces.PNG}
\end{center}
It is quite clear that $(\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v})(u_0, v_0) \neq 0 \implies \frac{\partial \varphi}{\partial u}$ and $\frac{\partial \varphi}{\partial v}$ are linearly independent. This means that an entire span of tangent vectors, i.e. a tangent plane, of the surface $S$ at $\varphi(u_0, v_0)$ exists. 
$S$ is said to be \textit{regular} if it is regular at all points $\varphi(u_0, v_0) \in S$. 
\end{definition}

In fact, the tangent plane at $\varphi(u_0, v_0)$ is the set of points 
\[\{\varphi(u_0, v_0) + \frac{\partial \varphi}{\partial u} (u_0, v_0) c_1 + \frac{\partial \varphi}{\partial v} (u_0, v_0) c_2 \; | \; c_1, c_2 \in \mathbb{R} \}\]
which is precisely the affine tangent plane spanned by $T_u$ and $T_v$. Note also that the vector $T_u \times T_v$, if nonzero, is normal to this plane, which leads to this equivalent definition. 

\begin{definition}[Tangent Planes of Surfaces]
Given a paramaterized surface $\varphi: D \subset \mathbb{R}^2 \longrightarrow \mathbb{R}^3$ that is regular at $\varphi(u_0, v_0)$, the tangent plane of the surface $S$ at $\varphi(u_0, v_0) = (x_0, y_0, z_0)$ is defined
\[\{(x, y, z) \in \mathbb{R}^3 \;|\; (x-x_0, y-y_0, z-z_0) \cdot n = 0\}\]
where $n = (\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v})(u_0, v_0)$. 
\end{definition}

We finally construct the concept of signed areas before defining surface integration. 
We have all the tools we need to calculate surface areas, but remember that integration also covers the concept of \textit{signed areas}, which could be negative. In order to define this, we define the concept of orientation on surfaces. 

\subsubsection{Orientation of Surfaces}

\begin{definition}[Oriented Surfaces]
An \textit{oriented surface} is a two-sided surface with one side specified as the \textit{outside/positive} side and the other side as the \textit{inside/negative} side. Note that an oriented surface is not guaranteed to have two sides (e.g. a Mobius strip). To ensure that there exist two sides, $S$ must be regular. 

Surprisingly, a paramaterization does not have an intrinsic orientation. Rather, we determine the orientation ourselves by choosing a unit vector that generally points towards the outside of the surface $S$. Again, this choice is arbitrary, but it is customary to choose a vector that generally points "out." Either way, the orientation (unit) vector at every point $\varphi(u, v) \in S$, denoted as $n$, is 
\[n\big(\varphi(u, v)\big) = \pm \frac{\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v}}{\big|\big|\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v}\big|\big|}\]
which can be visually calculated using the right hand rule. 
\begin{center}
    \includegraphics[scale=0.23]{Orientation_Unit_Vector.PNG}
\end{center}
\end{definition}

\begin{definition}[Orientation Preserving, Reversing Paramaterizations]
Given an oriented surface $S$ with its positive side determined by the direction of unit vector $n\big( \varphi(u,v)\big)$, the paramaterization $\varphi$ is said to be \textit{orientation preserving} if 
\[n \big( \varphi(u, v)\big) = \frac{\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v}}{\big|\big|\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v}\big|\big|}\]
and \textit{orientation reversing} if
\[n \big( \varphi(u, v)\big) = - \frac{\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v}}{\big|\big|\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v}\big|\big|}\]
\end{definition}

So, to find whether a paramaterization is orientation preserving or reversing, it suffices to find the cross product $T_u \times T_v$ and see if it points in the same direction of the normal vector $n$ (which should have already been determined when deciding the orientation of $S$). 

Given a paramaterization $\varphi$ and an un-oriented surface $S$, we can also just construct $\varphi$ to be orientation-preserving (or reversing) by \textit{defining} the normal vector $n$ to be 
\[n\big( \varphi(u, v)\big) = \frac{\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v}}{\big|\big|\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v}\big|\big|} \;\; \bigg( \text{or } n\big( \varphi(u, v)\big) = - \frac{\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v}}{\big|\big|\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v}\big|\big|} \bigg)\]
So rather than finding out whether a paramaterization $\varphi$ is orientation preserving or reversing by comparing $T_u \times T_v$ with $n$, we have defined $n$ in a way such that $\varphi$ must be orientation preserving (or reversing). We can utilize these tools of paramaterization to now define the surface integral. 

\subsubsection{Scalar, Vector Surface Integrals}

A physical interpretation of a scalar surface integral is the weighted surface area of a certain surface. 

\begin{definition}[Scalar Surface Integrals]
Let $f: \mathbb{R}^3 \longrightarrow \mathbb{R}$ be a $C^1$ scalar field defined on a paramaterized surface $S \subset \mathbb{R}^3$ with paramaterization $\varphi: D \subset \mathbb{R}^2 \longrightarrow \mathbb{R}^3$. That is, $\varphi(D) = S$. We define the integral $f$ over $S$ to be
\begin{align*}
    \iint_S f \; dS & = \iint_S f(x, y, z) \; dS \\
    & = \iint_D f\big( \varphi(u, v)\big) \bigg|\bigg|\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v}\bigg|\bigg| \; du \,dv
\end{align*}
Note that this will require us to transform $f$, a function of $x, y, z$, into the function $f \circ \varphi$ of $u, v$. Additionally, if the paramaterization of the surface $S$ is not defined, then it one must be constructed. It is also clear that if $S$ is a union of surfaces $S_i$, then its surface integral is the sum of the surface integrals of the $S_i$'s. 
\end{definition}

Letting the scalar field $f$ be the constant field equal to $1$, the scalar surface integral measures the surface area of $S$. 
\[A(S) = \iint_S \; dS = \iint_D \Big|\Big|\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v}\Big|\Big| \; du\, dv\]
It is easy to see that the orientation of the paramaterization $\varphi$ does not affect scalar surface integrals, since the sign of the orientation gets nullified by the absolute value sign over $||\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v}||$. 

Its physical interpretation is to measure the rate at which a fluid (determined by a vector field $F$) is crossing a given surface $S$. It also has many applications in electromagnetism. 

\begin{definition}[Vector Surface Integrals]
Let $F$ be a vector field defined on surface $S$, the image of a paramaterized surface $\varphi$. The \textit{surface integral} of $F$ over $S$ is defined below, which is equivalent to summing up the dot product of the vector field and the normal vector to the surface. 
\begin{center}
    \includegraphics[scale=0.3]{Vector_Surface_Integral.jpg}
\end{center}
It can be calculated with the following formulas by converting it into a scalar surface integral where the scalar field is the value of the dot product of the vector field with the normal vectors of the surface. 
\begin{align*}
    \iint_{S} F \cdot d S & = \iint_S (F \cdot n) \; dS \\
    & = \iint_D \Bigg( F\big( \varphi(u, v)\big) \cdot \frac{\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v}}{\Big|\Big|\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v} \Big|\Big|} \Bigg) \, \bigg|\bigg|\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v}\bigg|\bigg|\; du\,dv \\
    & = \iint_D F\big(\varphi(u, v)\big) \cdot \bigg( \frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v}\bigg) \; du\,dv
\end{align*}
\end{definition}

Since we are now talking about vector fields, the orientation of the paramaterization is now significant. Visually, if the orientation of the surface $S$ generally aligns with the vector field $F$, then the integral will be positive (since two vectors $\alpha, \beta$ generally pointing in the same direction implies that $\alpha \cdot \beta > 0$). The orientation of the paramaterization, which is dependent on $\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v}$, determines the direction of the normal vector $n$ (since it is defined to be $(\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v}) / \big|\big|\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v}\big|\big|$. Therefore, changing the orientation of $\varphi$ will reverse the direction of $n$, which will then reverse the sign of the integral since $n$ now points in the opposite direction of the vector field $F$ than it previously did (by reversing the sign of the dot products). This is formalized in the theorem below. 

\begin{theorem}[Invariance of Surface Paramaterizations on Vector Surface Integrals]
Let $S$ be an oriented surface and let $\varphi_1$ and $\varphi_2$ be two regular paramaterizations with $F$ a continuous vector field defined on $S$. Then, assuming $\varphi_1$ is orientation preserving, 
\begin{align*}
    \varphi_2 \text{ is orientation preserving } & \implies \iint_{\varphi_1} F \cdot d S = \iint_{\varphi_2} F \cdot d S \\
    \varphi_2 \text{ is orientation reversing } & \implies - \iint_{\varphi_1} F \cdot d S = \iint_{\varphi_2} F \cdot d S 
\end{align*}
\end{theorem}

\subsubsection{Surface Integrals over Graphs}
Given that we have the graph of a function $g: \mathbb{R}^2 \longrightarrow \mathbb{R}$ rather than a general surface, we can paramaterize it simply as
\[\varphi(u, v) \equiv \big(u, v, g(u, v) \big)\]
\begin{center}
    \includegraphics[scale=0.25]{Paramaterize_Surfaces_as_Graphs.PNG}
\end{center}
This means that 
\[\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v} = 
\begin{pmatrix}
-\frac{\partial g}{\partial u} \\ -\frac{\partial g}{\partial v} \\ 1
\end{pmatrix} \implies \bigg|\bigg|\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v} \bigg|\bigg| = \sqrt{1 + \Big(\frac{\partial g}{\partial u}\Big)^2 + \Big( \frac{\partial g}{\partial v}\Big)^2}\]
So we can simplify the equation for the surface area $S$ of the graph of $g$ over the region $D$ in the $x y$-plane, as 
\begin{align*}
    A(S) & = \iint_S \; d S = \iint_D \bigg|\bigg|\frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v} \bigg|\bigg| \; d A \\
    & = \iint_D \sqrt{1 + \Big(\frac{\partial g}{\partial u}\Big)^2 + \Big( \frac{\partial g}{\partial v}\Big)^2} \; d u \, d v
\end{align*}
With the same $g$, we can find the weighed surface area of $S$ over the scalar function $f: \mathbb{R}^3 \longrightarrow \mathbb{R}$ with the formula
\[\iint_S f \; d S = \iint_D f\big(u, v, g(u, v)\big) \sqrt{1 + \Big(\frac{\partial g}{\partial u}\Big)^2 + \Big( \frac{\partial g}{\partial v}\Big)^2} \; d u \, d v\]
Finally, with the same graph $g$, the surface integral over the vector field $F$ is
\begin{align*}
    \iint_S F \cdot d S & = \iint_D F\big(\varphi(u, v)\big) \cdot \bigg( \frac{\partial \varphi}{\partial u} \times \frac{\partial \varphi}{\partial v}\bigg) \; d u \, d v \\
    & = \iint_D \bigg( F_1(u, v) \Big(- \frac{\partial g}{\partial u}\Big) + F_2 (u, v) \Big( - \frac{\partial g}{\partial v} \Big) + F_3 (u, v) \bigg) \; d u \, d v
\end{align*}

\subsection{Integral Theorems}
Recall the differential notation for writing line integrals. For 2 and 3 dimensions, it is written as
\begin{align*}
    & \int_C F \cdot d s = \int_C F \cdot (d x, d y) = \int_C F_1 \, d x + F_2 \, d y \\
    & \int_C F \cdot d s = \int_C F \cdot (d x, d y, d z) = \int_C F_1 \, d x + F_2 \, d y + F_3 \, d z 
\end{align*}
\subsubsection{Green's Theorem}
Green's Theorem gives the relationship between a line integral around a simple closed curve $C$ and a double integral over the plane region $D$ bounded by $C$. 

\begin{theorem}[Green's Theorem in $\mathbb{R}^2$]
Let there be a 2-dimensional $C^1$ vector field $F$ on $\mathbb{R}^2$ defined on a simple oriented closed piecewise-smooth curve $C$ and its bounded region $D \subset \mathbb{R}^2$ (that is, $C = \partial D$). Let the orientation of the path of $C$ be such that it is traveling \textit{counterclockwise}, i.e. a point traveling through $C$ would see the region $D$ to its \textit{left}, denoted as $C^+$ and the clockwise orientation as $C^-$.  Then, 
\[\oint_{C^+} F_1 \, d x + F_2 \, d y = \iint_D \bigg( \frac{\partial F_2}{\partial x} - \frac{\partial F_1}{\partial y} \bigg) \; d x \, d y\]
By reversing the orientation, it is clear that we have
\[\oint_{C^-} F_1 \, d x + F_2 \, d y = - \iint_D \bigg( \frac{\partial F_2}{\partial x} - \frac{\partial F_1}{\partial y} \bigg) \; d x \, d y\]
Note that this theorem is expressed in terms of the components of the vector field $F$. 
\begin{center}
\begin{tikzpicture}
    \draw[fill=lightgray] plot [smooth cycle] coordinates {(2,3) (3,4) (5,4) (5,3) (3,2)};
    \node [above left] at (4,3) {D};
    \begin{axis}[view={0}{90}]
    \addplot3 [lightgray,-stealth,samples=10,
        quiver={
            u={3*x/pow(x^2 + y^2,1/2)},
            v={-2*y/pow(x^2 + y^2,1/2)},
            scale arrows=0.2,
        },] { 1};
    \end{axis}
    \node at (2,1) {$F = (F_1, F_2)$};
    \node at (5,4.7) {$C^+ = \partial D$};
    \draw (4.3,2.4)--(4.45,2.6)--(4.2,2.6);
\end{tikzpicture}
\end{center}
\end{theorem}

Green's theorem has many applications in physics. For example, in order to solve two-dimensional flow integrals measuring the sum of fluid outflowing from a volume, Green's theorem allows us to calculate the total outflow summed about an enclosing area . 

\begin{corollary}
Let $D$ be a region for which Green's theorem applies with positively oriented boundary $\partial D$. Then, the area of $D$ can be computed with the formula
\[A(D) = \frac{1}{2} \oint_{\partial D} x \, d y - y \, d x\]
\end{corollary}

Green's theorem can be used to determine the area of centroid of plane figures solely by integrating over the perimeter. 

\subsubsection{Stokes' Theorem}
Green's theorem relates line integrals to double integrals. Stokes' theorem generalizes Green's theorem by relating line integrals to surface integrals of 2-dimensional surfaces embedded in $\mathbb{R}^3$. 

\begin{theorem}[Stokes' Theorem]
Let $S$ be an oriented regular surface defined by paramaterization $\varphi: D \subset \mathbb{R}^2 \longrightarrow \mathbb{R}^3$, and let the image of the boundary $\partial D$ under $\varphi$ be the boundary $\partial S$ of $S$. We can interpret $\partial S$ as a path mapping from $\mathbb{R} \longrightarrow S \subset \mathbb{R}^3$. 
\begin{center}
    \includegraphics[scale=0.3]{Boundary_Mapping.PNG}
\end{center}
The orientation unit vector $n$ of $S$ induces the positive orientation of $\partial S$, denoted $\partial S^+$. Visually, if you are walking along the curve with your head is pointing in the same direction as the unit normal vectors while the surface is on the left then you are walking in the positive direction on $\partial S$. 
\begin{center}
    \includegraphics[scale=0.8]{Stokes_Theorem_Orientation.png}
\end{center}
Given that $F$ is a $C^1$ vector field defined on $S$, then
\[\iint_S \curl{F} \cdot dS = \iint_S \big( \nabla \times F \big) \cdot d S = \oint_{\partial S^+} F \cdot d s\]
If $S$ has no boundary, that is, if the image of $p^\prime = \partial S$ is not a simple closed curve, then the integral is $0$. 
\end{theorem}

The above theorem implies that the vector surface integral of a surface without a boundary (i.e. a closed graph, such as a sphere) is always $0$ along the curl of any $C^1$ field. Geometrically, this means that given a closed solid $S$ with field $\nabla \times F$, the rate of flow of the vector field into $S$ is equal to the flow out of $S$. 

\subsubsection{Gauss' Theorem}
The divergence theorem relates the flux of a vector field through a closed surface to the divergence of the field in the volume enclosed. 

\begin{theorem}[Gauss' Divergence Theorem]
Let $V$ be a subset of $\mathbb{R}^3$. Denote by $\partial V$ the oriented closed surface that bounds $V$ (with outward pointing normal orientation vectors), and let $F$ be a $C^1$ vector field defined on a neighborhood of $V$. Then, 
\[\iiint_V \Div{F} \; d V = \iiint_V (\nabla \cdot F) \; d V = \oiint_{\partial V} F \cdot d S = \oiint_{\partial V} (F \cdot n) \; dS\]
where the two left-most integrals are volume integrals, and the two right-most integrals are surface integrals. Intuitively, this makes sense; the volume integrals represent the total of the sources in volume $V$, and the right hand side represents the total flow across the boundary $\partial V$. 
\begin{center}
    \includegraphics[scale=0.35]{Gauss_Theorem_Volume.png}
\end{center}
\end{theorem}


\end{document}

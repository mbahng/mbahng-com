\section{Markov Chains}

  I have an entire set of notes dedicated to stochastic processes, but we talk about it on a basic level here. 

\subsection{Discrete Time Chains}

  \begin{definition}[Markov Chain]
    A \textbf{Markov chain} is a sequence of random variables $\{X_n\}_{n=0}^\infty$, which take values in some set $\mathcal{S}$, called the \textbf{state space} satisfying the \textbf{Markov property}. Since we are working with discrete time chains, we will assume that $\mathbb{S}$ is a countable (and in most cases, finite). Thus, the $X_n$ will all be discrete random variables. We can also think of $X_n$ as a discrete "time" index; that is, $X_n$ is the state of the system at time $n$. Therefore, the sequence of random variables models a system evolving in a random way. 
  \end{definition}

  \begin{definition}[Markov Property]
    A sequence of random variables $\{X_i\}$ satisfies the \textbf{Markov property} if 
    \begin{equation}
      \mathbb{P}(X_{n+1} = y \; | \; X_n = x_n, X_{n-1} = x_{n-1}, ..., X_0 = x_0\} = \mathbb{P}(X_{n+1} = y \; | \; X_n = x_n\}
    \end{equation}
    holds for any choice oc states $y, x_n, x_{n-1}, ..., x_0 \in \mathcal{S}$ and for any $n \geq 1$. 
  \end{definition}

  Colloquially, given that one is at state $X_n = x_n$, knowing all the previous states does not help in predicting $X_{n+1}$. Knowing only the current state is relevant in predicting the next one. We can model this entire system using a matrix. 

  \begin{definition}[Transition Matrix]
    Assuming that the chain is \textit{time-homogeneous}, the \textit{transition probability matrix} $P$ has elements $P_{x y}$ defined
    \begin{equation}
      P_{x y} = P(x, y) = \mathbb{P}(X_1 = y \,|\, X_0 = x) = \mathbb{P}(X_{n+1} = y \,|\, X_n = x)
    \end{equation}
    which is the probability of moving from state $x$ to state $y$ in one step. The time homogeneous condition refers to the last equality; that is, the one-step transition probabilities don't change with the time index $n$. Note that if $\mathcal{S}$ is finite, then $P$ is a $|S| \times |S|$ matrix, and if $\mathcal{S}$ is countably infinite, then $P$ is an infinite-dimensional matrix. The axioms of probability imply that $A^T$ is an entry-wise nonnegative stochastic matrix.
  \end{definition}

  \begin{example}[Random Walks]
    A \textit{random walk} on the integers $\mathcal{S} = \mathbb{Z}$ where a point has equal probability of moving right or left can be modeled with the probability function. 
    \begin{equation}
      P(x, y) = \mathbb{P}(X_{n+1} = y \, | \, X_n = x) = \begin{cases}
      \frac{1}{2} & y = x + 1 \\
      \frac{1}{2} & y = x - 1\\
      0 & otherwise
      \end{cases}
    \end{equation}
    This can be generalized to multiple dimensional random walks on graphs with probability function 
    \begin{equation}
      P(x, y) = \frac{1}{\text{deg}(x)}
    \end{equation}
    where deg$(x)$ is the number of adjacent nodes to node $x$. In this way, the point hops randomly from node to node, and if the graph is connected, then the walker can visit any vertex in the graph. 
  \end{example}

  \begin{example}[Discrete Moran Model]
    Consider a population of size $N$. Each individual is one of two types (say, red or blue). At each time step, the system evolves in the following way: First, one of the individuals is chosen uniformly at random to be eliminated from the population; and another individual is chosen uniformly at random to produce one offspring identical to itself. These two choices are made independently. So, if a red individual is chosen to reproduce, and a blue one is chosen for elimination, then the total number of red particles increases by one and the number of blue particles decreases by one. If a red is chosen for reproduction and a red is chosen for elimination, then there is no net change in the number of reds and blues. Let $X_n$ be the number of red individuals at time $n$. The transition matrix for this chain is
    \begin{equation}
      P_{i j} = \begin{cases}
      \frac{i}{N} \bigg(\frac{N-i}{N} \bigg) & j=i-1, i \neq 0 \\
      \bigg(\frac{N-i}{N} \bigg) \frac{i}{N} & j=i+1, i \neq N \\
      1 - 2 \bigg(\frac{N-i}{N} \bigg) \frac{i}{N} & j = i \\
      0 & \text{otherwise}
      \end{cases}
    \end{equation}
    Note that the states $X_n = 0$ and $X_n = N$ are absorbing states, which represents a phenomenon called \textit{fixation}. 
  \end{example}

  \begin{definition}[Absorbing State]
    A certain state $F$ in the state space $\mathcal{S}$ of a Markov chain is called an \textbf{absorbing state} if
    \begin{equation}
      \mathbb{P}(X_{n+1} = F \; | \; X_n = F) = 1 \iff \mathbb{P}(X_{n+1} \neq F \; | \; X_n = F) = 0
    \end{equation}
  \end{definition}

  \begin{theorem}
    Let there exist a time homogeneous Markov chain with transition probability matrix $P$. Given a probability distribution $\nu_n$ (a row vector) representing the a state of a system at time $t=n$, the probability distribution of which state the system will be at when $t=n+1$ can be calculated by 
    \begin{equation}
      \nu_{n+1} = \nu_n P
    \end{equation}
    The probability distribution of the state of the system at $t=n+k$ can be calculated by summing up all of the possible probabilities that lead to each state at $t=n+k$. It is calculated equivalently as matrix multiplication: 
    \begin{equation}
      \nu_{n+k} = \nu_n P^k
    \end{equation}
  \end{theorem}

  \begin{definition}[Initial Distribution]
    The distribution $\nu$ of a Markov chain at time $t=0$ is called the \textit{initial distribution} for the chain. That is, $\nu$ is the initial distribution if 
    \begin{equation}
      \mathbb{P}(X_0 = x) = \nu(x)
    \end{equation}
  \end{definition}

  \begin{definition}[Stationary Distribution]
    An \textit{invariant distribution}, or \textit{stationary distribution}, is a probability distribution $\pi$ such that 
    \begin{equation}
      \pi P = \pi
    \end{equation}
    This means that 
    \begin{equation}
      \pi P^k = \pi
    \end{equation}
    for all $k \in \mathbb{N}$. We can equivalently call $\pi$ the left eigenvector of matrix $P$ with eigenvalue $1$. If $\pi$ is an invariant distribution for the chain, and $X_0 \sim \pi$, then the distribution of $X_n$ does not change with $n$; it is invariant. Note that this does not mean that $X_n$ is constant; rather, it means that the distribution of $X_n$ is not changing. 
  \end{definition}

  \begin{example}
    Let us have a two node system with nodes labeled $L$ and $R$. That is, $\mathcal{S} = \{L, R\}$. Consider a chain on this state space with transition probability matrix. 
    \begin{equation}
      P = \begin{pmatrix}
      1-a & a \\ b & 1-b 
      \end{pmatrix}
    \end{equation}
    which can be visualized in the following diagram below.
    \begin{center}
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,
                        thick,main node/.style={circle,draw}]
        \node[main node] (R) {R};
        \node[main node] (L) [left of=R] {L};
        \path[every node/.style={font=\sffamily\small}]
        (L) edge [loop left] node {1-a} (L)
            edge [bend left] node {a} (R)
        (R) edge [loop right] node {1-b} (R)
            edge [bend left] node {b} (L);
    \end{tikzpicture}
    \end{center}

    Then, the stationary distribution is 
    \begin{equation}
      \pi = \Big( \frac{b}{a+b}, \frac{a}{a+b} \Big)
    \end{equation}
    Notice that if $a = b = 0$, then this definition is ill-defined, and any probability distribution is invariant since $P = I_2$, the identity matrix. 
  \end{example}

  \begin{definition}[Recurrent]
    A state $x \in \mathcal{S}$ is \textit{recurrent} if
    \begin{equation}
      \mathbb{P}(X_n = n \text{ for some } n \geq 1 \, | \, X_0 = x\} = 1
    \end{equation}
    That is, if the initial state is $x$, the chain has probability $1$ of returning to $x$ at some later time. If a state is not recurrent, then the state is said to be \textit{transient}. That is, if $x$ is transient, there is some positive probability that the chain will never return to $x$. 
  \end{definition}

  \begin{definition}[Communication]
    Two states $x, y \in \mathcal{S}$ are said to \textit{communicate}, denoted $x \leftrightarrow y$, if there are positive integers $n$ and $m$ such that 
    \begin{equation}
      P^{(n)} (x, y) > 0 \text{ and } P^{(m)} (y, x) > 0
    \end{equation}
    That is, there is some positive probability that the chain can go from $x$ to $y$ and from $y$ to $x$ in some number of steps. 
  \end{definition}

  \begin{definition}[Irreducible Chains]
    If all pairs $x, y \in \mathcal{S}$ communicate, then the chain is said to be \textit{irreducible}. If there exists a pair of states that do not communicate, then the chain is said to be \textit{reducible}. 
  \end{definition}

  Note that the notion of communication is an equivalence relation between states. That is, it satisfies the properties. 
  \begin{enumerate}
    \item $x \leftrightarrow x$.
    \item $x \leftrightarrow y \implies y \leftrightarrow x$.
    \item $x \leftrightarrow y, y \leftrightarrow z \implies x \leftrightarrow z$.
  \end{enumerate}
  This relation partitions the state space $\mathcal{S}$ uniquely into transient states and irreducible sub-chains
  \begin{equation}
    \mathcal{S} = T \cup C_1 \cup C_2 \cup ...
  \end{equation}
  More specifically, $T$ is the set of all transient states, and the sets $C_k$ are \textit{closed communication classes}, meaning that
  \begin{enumerate}
    \item For all $x, y \in C_k$, $x \leftrightarrow y$. 
    \item $P(x, z) = 0$ whenever $x \in C_k$ but $z \not\in C_k$. 
  \end{enumerate}
  Note that for all $x, y \not\in T$, $x$ and $y$ communicate if and only if $x$ and $y$ are in the same class $C_k$. Moreover, once the chain reaches one of the sets $C_k$, it cannot leave $C_k$. 

  \begin{definition}[Period]
    For any state $x \in \mathcal{S}$, the \textit{period} of $x$ is defined to be
    \begin{equation}
      d(x) \equiv \gcd \{n \geq 1 \; | \; P^{(n)} (x, x) > 0\}
    \end{equation}
  \end{definition}

  \begin{theorem}
    It follows that if two states $x$ and $y$ communicate, then they must have the same period: $d(x) = d(y)$. It naturally follows that if the chain is irreducible, then all states must have the same period, and we can define the period of the chain to be $d(x)$ for any $x$ we choose.
  \end{theorem}

  \begin{definition}
    If an irreducible chain has period $1$, the chain is said to be \textit{aperiodic}. Otherwise, the chain is \textit{periodic} with period $d > 1$. 
  \end{definition}

  \begin{theorem}
    Suppose $|\mathcal{S}| < \infty$. If the chain is irreducible, then there always exists a unique stationary distribution $\pi$. If the chain is also aperiodic, then for any initial distribution $\nu$, 
    \begin{equation}
      \lim_{k \rightarrow \infty} \nu P^k = \pi
    \end{equation}
    Hence
    \begin{equation}
      \lim_{k \rightarrow \infty} P^{(k)}(x, y) = \pi(y)
    \end{equation}
    for all $x, y \in \mathcal{S}$. Furthermore, for any function $F: \mathcal{S} \longrightarrow \mathbb{R}$, the limit
    \begin{equation}
      \lim_{N \rightarrow \infty} \frac{1}{N} \sum_{n=1}^N F(X_n) = \sum_{x \in \mathcal{S}} F(x)\, \pi(x) = \mathbb{E} \big( F(x) \big)
    \end{equation}
    holds with probability $1$. In particular, the limit does not depend on the initial distribution. 
  \end{theorem}
  \begin{proof}
    The Frobenius Extension to Perron's theorem (Linear Algebra, Theorem 7.31) combined with its applications to stochastic matrices (Linear Algebra, Theorem 7.30) proves this statement. 
  \end{proof}

  \begin{definition}[First Visit]
    For each $x \in \mathcal{S}$, define the \textit{first visit} to $x$ by 
    \begin{equation}
      T_x \equiv \min\{ n \geq 1 \; | \; X_n = x\}
    \end{equation}
    This $T_x$ is an integer-valued random variable. We say $T_x = + \infty$ if $X_n$ never reaches $x$. Then, we define the \textit{mean return time} to $x$ by 
    \begin{equation}
      \mu_x \equiv \mathbb{E}\big( T_x \, | \, X_0 = x)
    \end{equation}
    If $x$ is transient, then $\mu_x = + \infty$, since there is positive probability that $T_x = + \infty$. 
  \end{definition}

  \begin{definition}
    It is possible that $x$ is recurrent while $\mu_x = +\infty$. If this is the case, then $x$ is said to be \textit{null-recurrent}. If $x$ is recurrent and $\mu_x < \infty$, then $x$ is said to be \textit{positive recurrent}. 
  \end{definition}

  \begin{theorem}
    An irreducible chain has a stationary probability distribution $\pi$ if and only if all states are positive recurrent. If a chain is irreducible and all states are positive recurrent, then 
    \begin{equation}
      \pi(x) = \frac{1}{\mu_x}
    \end{equation}
    for all $x \in \mathcal{S}$. $\pi$ is also unique. 
  \end{theorem}

  \subsubsection{Exit Probabilities}

    Suppose a chain is finite and irreducible. Let $a, b \in \mathcal{S}$ be given states, and let us define $h(x)$ to be the probability of hitting $b$ before $a$, given that we start from $x$. 
    \begin{equation}
      h(x) \equiv \mathbb{P} (X_n \text{ reaches } b \text{ before } a \, | \, X_0 = x)
    \end{equation}
    Clearly, $h(b) = 1$ and $h(a) = 0$. By conditioning on the first jump out of $x$, we also have 
    \begin{align*}
      h(x) & = \mathbb{P}(X_n \text{ reaches } b \text{ before } a \, | \, X_0 = x) \\
      & = \sum_{y} \mathbb{P}(X_n \text{ reaches } b \text{ before } a \, | \, X_1 = y, X_0 = x) \, \mathbb{P}(X_1 = y \,|\,X_0 = x) \\
      & = \sum_y \mathbb{P}(X_n \text{ reaches } b \text{ before } a \, | \,X_1 = y, X_0 = x) \, P(x, y) \\
      & = \sum_y \mathbb{P}(X_n \text{ reaches } b \text{ before } a \, | \,X_1 = y) \, P(x, y) \\
      & = \sum_y h(y) \, P(x, y) 
    \end{align*}
    The sum is over all $y \in \mathcal{S}$ for which $P(x, y) \neq 0$. This gives us a linear system of equations to solve for $h$
    \begin{align*}
      & h(x) = \sum_y P(x, y) h(y) \,\, \forall x \in \mathcal{S} \setminus \{a, b\}, \\
      & h(b) = 1, \\
      & h(a) = 0
    \end{align*}

  \subsubsection{Exit Prize}

    Let $B \subset \mathcal{S}$ be some subset of the state space, and let $g: B \longrightarrow \mathbb{R}$ be some function. Consider the function 
    \begin{equation}
      h(x) = \mathbb{E}\big( g(X_\tau) \, |\, X_0 = x \big)
    \end{equation}

    where $\tau = \min\{ n\geq 0 \,|\, X_n \in B\}$ is the first time that the chain reaches some state in the set $B$ (this time is random). We can interpret $g(y)$ as a "prize" that is awarded if the chain first reaches $B$ at state $y$, which means that $h(x)$ is the expected prize, given that $X_0 = x$. If $x \in B$, then $\tau = 0 \implies h(x) = g(x)$. But if $x \not\in B$, then by the same argument as shown in exit probabilities, it is true that $h$ satisfies the linear system of equations

    \begin{align*}
      & h(x) = \sum_g P(x, y)\,h(y), \;\; \forall x \in \mathcal{S} \setminus B, \\
      & h(x) = g(x), \;\; x \in B 
    \end{align*}

    Note that Exit probability system is a special case of the Exit prize system. In the former, we have defined $B = \{a, b\}$ and $g$ defined by $g(a) = 0, g(b) = 1$. 

  \subsubsection{Occupation Times, Absorbing States}

    Suppose that a chain on a finite $\mathcal{S}$ is irreducible. Let $B \subset \mathcal{S}$ be some subset of states and let $A = \mathcal{S} \setminus B$ be the other states. Then for $x \in A$, we wish to know how many steps the chain will take before reaching a state in the set $B$. We define 
    \begin{equation}
      \tau_B = \min\{n \geq 0 \,|\, X_n \in B\}
    \end{equation}
    which represents the first time that $X$ is in $B$, an integer valued random variable. We wish to compute
    \begin{equation}
      h(x) = \mathbb{E}(\tau_B \,|\, X_0 = x)
    \end{equation}
    Clearly, $h(y) = 0$ for all $y \in B$. For $x \in A$, it takes at least one step to reach $B \implies h(x) \geq 1$ for $x \in A$. We condition on the first step from $x$. This leads to the system 
    \begin{align*}
      h(x) = 1 + \sum_{y \in \mathcal{S}} P(x, y) \, \mathbb{E}(\tau_B \,|\, X_1 = y), & \forall x \in A = \mathcal{S} \setminus B
    \end{align*}
    Since the chain is time-homogeneous, this means that
    \begin{align*}
      h(x) = 1 + \sum_{y \in \mathcal{S}} P(x, y) \, h(y), & \forall x \in A 
    \end{align*}
    Since $h(y) = 0$ for all $y \in B$, we now have
    \begin{align*}
      h(x) = 1 + \sum_{y \in A} P(x, y) \, h(y), & \forall x \in A 
    \end{align*}
    To solve this system, let us define $M$ as the $|A| \times |A|$ submatrix of $P$ obtained by keeping only the entries $P(x, y)$ with $x, y \in A$. So, the system can be written as
    \begin{align*}
      h(x) = 1 + \sum_{y \in A} M(x, y) \, h(y), & \forall x \in A
    \end{align*}
    We can solve this system of equations through the equivalent matrix equation
    \begin{equation}
      (I - M) h = 1
    \end{equation}
    where $1 = (1, 1, ..., 1)^T$ is the column vector consisting of all $1$'s. The solution vector is therefore
    \begin{equation}
      h = (I - M)^{-1} 1
    \end{equation}
    So, for a particular $x \in A$, 
    \begin{equation}
      h(x) = \sum_{y \in A} (I - M)^{-1} (x, y)
    \end{equation}

    Alternatively, we can slightly modify the chain to chain $\Tilde{X}_n$ by replacing the transition probability matrix $P$ with another one defined as 
    \begin{equation}
      \Tilde{P}(x, y) = \begin{cases}
      P(x, y) & x \in A, y \in \mathcal{S} \\
      1 & x = y \in B \\
      0 & \text{else}
      \end{cases}
    \end{equation}

    This modification means that all transitions from state in $A$ to any other state are preserved and the only transitions from a state $x \in B$ are self loops. In particular, all transitions from states $x \in B$ to states $y \in A$ are removed. Therefore, under this modified transition matrix, the states in $B$ are absorbing states. The tail sum formula implies that
    \begin{equation}
      \mathbb{E}(\tau_B \,|\, X_0 = x) = \sum_{k=0}^\infty \mathbb{P}(\tau_B > k \,|\, X_0 = x)
    \end{equation}
    Notice that since the chain $X_n$ and $\Tilde{X}_n$ have the same transition rules before hitting a state $B$, we have 
    \begin{equation}
      P^{(k)} (x, y) = \Tilde{P}^{(k)} = M^{(k)}(x, y)
    \end{equation}
    where $M$ is the $|A| \times |A|$ submatrix defined previously. Therefore, putting this all together, we have
    \begin{align*}
      \mathbb{E}(\tau_B \,|\, X_0 = x) & = \sum_{k=0}^\infty \mathbb{P}(\tau_B > k \,|\, X_0 = x) \\
      & = \sum_{k=0}^\infty \mathbb{P}(\Tilde{X}_k \in A \,|\, X_0 = x) \\
      & = \sum_{k=0}^\infty \sum_{y \in A} \Tilde{P}^{(k)} (x, y) \\
      & = \sum_{k=0}^\infty \sum_{y \in A} M^{(k)} (x, y) \\
      & = \sum_{y \in A} \bigg( \sum_{k=0}^\infty M^{(k)} \bigg) (x, y) 
    \end{align*}
    Using a theorem from linear algebra, we can show that if all the eigenvalues of a $d \times d$ matrix $M$ have modulus strictly less than $1$, then $I-M$ is invertible and
    \begin{equation}
      \sum_{k=0}^\infty M^{(k)} = (I - M)^{-1}
    \end{equation}
    where $I$ is the $d \times d$ identity matrix. If $M$ is the $|A| \times |A|$ submatrix described above, one can show that $M$ has his property and that $I - M$ is invertible. Hence, 
    \begin{equation}
      \mathbb{E}(\tau_B \,|\, X_0 = x) = \sum_{y \in A} \bigg( \sum_{k=0}^\infty M^{(k)} \bigg) (x, y) = \sum_{y \in A} (I-M)^{-1} (x, y)
    \end{equation}
    which refers to the $(x, y)$ entry of the matrix $(I - M)^{-1}$. This is indeed consistent with our previous derivation of the formula for $h(x)$, the expected number of steps before the state reaches $B$. 

\subsection{Markov Chain Monte Carlo Algorithms}

  In statistics, Markov chain Monte Carlo (MCMC) methods comprise of a class of algorithms for sampling from a probability distribution by constructing a Markov chain that has the desired distribution as its equilibrium distribution. That way, by recording samples from the chain, one may get better approximations of the actual distribution. 

  Let there exist a state space $\mathcal{S}$ with some probability distribution $\pi(x)$ for every $x \in \mathcal{S}$. Clearly, 
  \begin{equation}
    \sum_{x \in \mathcal{S}} \pi(x) = 1
  \end{equation}
  but the problem is that we do not know that $\pi$ is. We do know, however, another function $f$ that is directly proportional to $\pi$. 
  \begin{equation}
    \pi(x) = \frac{f(x)}{c}, \text{ where } c = \sum_{x \in \mathcal{S}} f(x)
  \end{equation}
  is the normalizing constant. It is often the case that $c$ is unknown and the state space $\mathcal{S}$ is so large that computing $c$ directly is expensive. Therefore, we construct Markov chains that can provide approximations to $\pi$. 

  \subsubsection{Metropolis-Hastings Algorithm}

    This algorithm is useful because it does not require knowledge of the normalizing constant $c$. The algorithm only requires evaluations of 
    \begin{equation}
      \frac{\pi(x)}{\pi(y)} = \frac{f(x)}{f(y)}
    \end{equation}

    We first have the state space $\mathcal{S}$ consisting of all the possible states. We now construct (any) probability transition matrix $q$ for a Markov chain on $\mathcal{S}$. Note that $q$ is a $|\mathcal{S}| \times |\mathcal{S}|$ matrix and $q^T$ is a stochastic matrix. This matrix is constructed by the user and is completely well-defined and known. We start off with any initial state $x_0 \in \mathcal{S}$ and iterate the following 2-steps to construct a Markov chain. 

    \begin{enumerate}
      \item Given a state $X_n = x$, we generate a new state $X_{n+1}$ by first proposing a new state $y \in \mathcal{S}$ with probability $q(x, y)$ (determined from the matrix $q$). 
      \item With this chosen state $y$, we decide whether to accept to reject the proposal. With probability 
      \begin{equation}
        \min \bigg( 1, \frac{\pi(y) \,  q(y, x)}{\pi(x) \, q(x, y)} \bigg)
      \end{equation}
      we accept the proposal and set $X_{n+1} = y$. Otherwise, the proposal is rejected and the new state is the same $X_{n+1} = x$. 
    \end{enumerate}

    Note that there are two levels of randomness here: which state the new state $y$ will be and whether to accept this state to be the next one or not. If step two did not exist (i.e. the probability of accepting the proposal is always $1$), then this would just be a regular Markov chain represented by the matrix $q$. But the addition of step 2 means that while $q$ is used in constructing the discrete chain $X_n$, it is \textit{not} the transition probability matrix of $X_n$. 

    There is also a lot of flexibility on choosing $q$, although the performance of the algorithm (speed of convergence of the distribution of $X_n$ to the stationary distribution) will depend on the choice.

    \begin{proposition}
      For the chain defined by the Metropolis-Hastings algorithm, the distribution $\pi$ is stationary. 
    \end{proposition}
    \begin{proof}
      Let us write in shorthand 
      \begin{equation}
        \alpha(x, y) = \frac{\pi(y)\, q(y, x)}{\pi(x)\, q(x, y)}
      \end{equation}
      First, observe that if $x \neq y$, the transition probability for the chain defined by the algorithm is just
      \begin{equation}
        P(x, y) = q(x, y)\, \min\{1, \alpha(x, y)\}
      \end{equation}
      Next, we claim that for all $x, y \in \mathcal{S}$, 
      \begin{equation}
        \pi(x) P(x, y) = \pi(y) \, P(y, x)
      \end{equation}
      This condition is called \textit{detailed balance}. Assuming that $\alpha(x, y) \leq 1$, it is true that
      \begin{equation}
        \pi(x) P(x, y) = \pi(x) q(x, y) \frac{\pi(y) q(y, x)}{\pi(x) q(x, y)} = \pi(y) q(y, x)
      \end{equation}
      In this case, we also have $\alpha(y, x) = 1 / \alpha(x, y) \leq 1$. So, 
      \begin{equation}
        \pi(y) P(y, x) = \pi(y) q(y, x)
      \end{equation}
      and we have proved what we had claimed. Now, summing over $x$,
      \begin{equation}
        \sum_x \pi(x) P(x, y) = \sum_x \pi(y) P(y, x) = \pi(y) \sum_x P(y, x) = \pi(y)
      \end{equation}
      since $P^T$ is stochastic. 
    \end{proof}

  \subsubsection{Gibbs Sampling}

    Let $\mathcal{A} = \{a_1, ..., a_k\}$ be some finite set. Suppose that the state space 
    \begin{equation}
      \mathcal{S} = \mathcal{A} \times ... \times \mathcal{A} = \mathcal{A}^M
    \end{equation}
    for some $M \in \mathbb{N}$. The following algorithm generates a Markov chain on $\mathcal{S}$ with stationary distribution
    \begin{equation}
      \pi(x) = \frac{f(x_1, x_2, ..., x_M)}{c}, \;\; x = (x_1, x_2, ..., x_M) \in \mathcal{S} 
    \end{equation}
    where $c >0$ is a normalizing constant. Note that $|\mathcal{S}| = k^M$, so computing $c$ may be expensive when $M$ is large. The current state of the chain is denoted 
    \begin{equation}
      X_n = (X_n^1, X_n^2, ..., X_n^M)
    \end{equation}
    We think of $X_n$ as having $M$ components, each component taking values in $\mathcal{A}$. We start off with any initial state $X_0 = (X_0^1, X_0^2, ..., X_0^M)$ and construct a Markov chain by iterating the following two steps. 
    \begin{enumerate}
      \item Given $X_n = (X_n^1, X_n^2, ..., X_n^M)$, we generate the next state $X_{n+1}$ by picking a component index $i \in \{1, ..., M\}$ uniformly at random. 
      \item With this chosen, well-defined $i$, we choose a random $Y^i \in \mathcal{A}$ according to the distribution
      \begin{equation}
        \mathbb{P}(Y^i = a) = \frac{f\big(X_n^1 ,..., X_n^{i-1}, a, X_n^{i+1}, ..., X_n^M\big)}{\sum_{j=1}^k f\big(X_n^1 ,..., X_n^{i-1}, a_j, X_n^{i+1}, ..., X_n^M\big)}, \;\; a \in \{a_1, ..., a_k\}
      \end{equation}
      \item Then, set $X_{n+1} = \big(X_n^1, ..., X_n^{i-1}, Y^i, X_n^{i+1}, ..., X_n^M\big)$. 
    \end{enumerate}
    Note that at each step, only one component of $X_n$ is updated. Observe that the distribution above is also equal to 
    \begin{equation}
      \mathbb{P}(Y^i = a) = \frac{\pi\big(X_n^1 ,..., X_n^{i-1}, a, X_n^{i+1}, ..., X_n^M\big)}{\sum_{j=1}^k \pi \big(X_n^1 ,..., X_n^{i-1}, a_j, X_n^{i+1}, ..., X_n^M\big)}
    \end{equation}
    which is the marginal distribution of the $i$th component, given the values of the other components. 

    \begin{proposition}
      For the chain defined by this algorithm, the distribution $\pi$ is stationary. 
    \end{proposition}
    \begin{proof}
      We verify that the detailed balance condition holds. It is also helpful to note that $P(x, y) \neq 0$ if and only if $x$ and $y$ differ in one coordinate. 
    \end{proof}

\subsection{Continuous Time Markov Chains}

  As the name suggests, in a continuous time Markov chain $X_t$, the time parameter is continuous ($t \geq 0$). As before, the system jumps randomly between states in $\mathcal{S}$, but now the jumps may occur at any time and they occur randomly. This implies that there are \textit{two} sources of randomness:
  \begin{enumerate}
    \item \textit{where} the system jumps and 
    \item \textit{when} the system jumps
  \end{enumerate}

  \begin{definition}[Continuous Time Markov Chain]
    The Markov property in the continuous time case says that for any $s, t \geq 0$ and $y \in \mathcal{S}$, 
    \begin{equation}
      \mathbb{P}(X_{t + s} = y \, | \, X_t) = \mathbb{P}(X_{t+s} = y \, | \, X_r \; \forall 0 \leq r \leq t)
    \end{equation}
    Colloquially, the conditional distribution of $X_{t+s}$ given the history up to time $t$ is the same as the conditional distribution of $X_{t+s}$ given only $X_t$. Thus, if we know the current state at $t$, knowing information about the past doesn't help us better predict the future state $X_{t+s}$. 

    In order for the Markov property to hold, the times between jumps must be exponentially distributed random variables because it is the only density that has the memoryless property. This fact has already been stated in a theorem when covering Poisson arrival processes. This is what makes Exp$(\lambda)$ so important for continuous time Markov chains. 
  \end{definition}

  \begin{lemma}
    Let $T_1, T_2, ..., T_n$ be independent exponential random variables with rates $\lambda_1, \lambda_2, ..., \lambda_n$, respectively. Then the random variable $T \equiv \min\{T_1, T_2, ..., T_n\}$ is
    \begin{equation}
      T \sim \text{Exp}\Big(\sum_{i=1}^n T_i\Big)
    \end{equation}
    Moreover, 
    \begin{equation}
      \mathbb{P}(T_k = \min\{T_1, ..., T_n\}) = \frac{\lambda_k}{\lambda_1 + ... + \lambda_n}
    \end{equation}
  \end{lemma}

  We can interpret the lemma above by imagining that we have $n$ alarm clocks all set simultaneously, which will ring independently at random times. Suppose that clock $k$ will ring after $T_k$ units of time have expired, where $T_k$ is a random variable distributed as Exp$(\lambda_k)$. Then, $T = \min\{T_1, ..., T_n\}$ is the time at which the first ring occurs. 

  \begin{example}
    The simplest and the most important continuous time Markov chains is the Poisson arrival process. The process really has a single parameter $\lambda >0$ (the rate of process) by definition and is integer valued. At each jump time, the process increases by $1$, and the time between jumps are independent, distributed as Exp$(\lambda)$. 

    Notice that when $\lambda$ is large, the arrivals occur more frequently than when $\lambda$ is small, because the expected time between arrivals is $1/\lambda$. The second way we can interpret it is to choose an interval of time $t$ and let $X_t$ be the number of jumps that have occurred up to time $t$. It is a fact that $X_t$ is a integer-valued, Poisson$(\lambda t)$ distribution. That is, 
    \begin{equation}
      \mathbb{P}(X_t = k) = e^{-\lambda t} \frac{(\lambda t)^k}{k!}, \; k = 0, 1, 2, ...
    \end{equation}
    In particular, $\mathbb{E}(X_t) = \lambda t$ and $\Var(X_t) = \lambda t$. 
  \end{example}

\subsection{Branching Processes}

  \begin{definition}[Branching Process]
    A \textit{branching process} is a type of Markov chain modeling a population in which each individual produces a random number of children (possibly $0$) and dies. The state space is $\mathcal{S} = \{0, 1, 2, 3, ...\}$. Furthermore, there is a discrete-time version and a continuous time version of the chain. In the discrete case, the state is $Z_n$, the size of the population at time $n = 0, 1, 2, ...$, and in the continuous case, the state is $Z_t$ for $t \geq 0$. 
  \end{definition}

  \subsubsection{Discrete-time Branching Process}

    In the discrete case, all of the $Z_n$ individuals in the current generation branch at the same time and immediately die. The branching is independent and distributed according to the \textit{offspring distribution} $\{p_k\}_{k=0}^\infty$. Specifically, if $Z_n = m$, then 
    \begin{equation}
      Z_{n+1} = Y_1^n + Y_2^n + ... + Y_m^n
    \end{equation}
    where $Y_i^n$ represents the number of offspring the $i$th individual in the $n$th generation has. All of them are distributed as
    \begin{equation}
      \mathbb{P}(Y_i^n = k) = p_k, \; k = 0, 1, 2, 3, ...
    \end{equation}
    where $p_k$ is the probability that a parent has $k$ children. Note that if $p_0 \neq 0$, then there is positive probability that $Y_i^n = 0$ for all $i$, meaning that the population can go extinct. A sample branching process up to the second generation is shown below. 

    \begin{center}
    \begin{tikzpicture}[scale=0.8]
      \draw[fill] (0,4) circle (0.05);
      \draw[fill] (-2,2) circle (0.05);
      \draw[fill] (0,2) circle (0.05);
      \draw[fill] (2,2) circle (0.05);
      \draw[dashed] (0,4)--(-2,2);
      \draw[dashed] (0,4)--(0,2);
      \draw[dashed] (0,4)--(2,2);
      \draw[fill] (-3,0) circle (0.05);
      \draw[fill] (-1,0) circle (0.05);
      \draw[dashed] (-2,2)--(-3,0);
      \draw[dashed] (-2,2)--(-1,0);
      \draw[dashed] (2,2)--(3,0);
      \draw[dashed] (2,2)--(1,0);
      \draw[dashed] (2,2)--(2,0);
      \draw[fill] (3,0) circle (0.05);
      \draw[fill] (2,0) circle (0.05);
      \draw[fill] (1,0) circle (0.05);
      \node at (5,4) {$Z_0 = 1$};
      \node at (5,2) {$Z_1 = 3$};
      \node at (5,0) {$Z_2 = 5$};
      \draw[->] (4.3,4)--(3.5,4);
      \draw[->] (4.3,2)--(3.5,2);
      \draw[->] (4.3,0)--(3.5,0);
      \node at (-5, 3) {$Y_1^0 = 3$};
      \node at (-5, 1.5) {$Y_1^1 = 2$};
      \node at (-5, 1) {$Y_2^1 = 0$};
      \node at (-5, 0.5) {$Y_3^1 = 3$};
    \end{tikzpicture}
    \end{center}

    Suppose that the mean number of offspring of a single parent is finite. 
    \begin{equation}
      \mu = \mathbb{E}(Y) = \sum_{k=0}^\infty k \, \mathbb{P}(Y = k) = \sum_{k=0}^\infty k \, p_k < \infty
    \end{equation}
    If $Y_1$ and $Y_2$ are two independent, discrete random variables, we can define their convolution and use the fact that $\mathbb{P}(Y_i = k) = p_k$ to get
    \begin{align*}
      \mathbb{P}(Y_1 + Y_2 = k) & = \sum_j \mathbb{P}(Y_1 = k - j) \, \mathbb{P}(Y_2 = j) \\
      & = \sum_{j=0}^\infty p_{k-j} p_j, \;\; k = 0, 1, 2, ...
    \end{align*}
    This is a two-fold convolution of the sequence $\{p_k\}$ with itself, denoted
    \begin{equation}
      p_k^{*2} = \sum_{j=0}^\infty p_{k-j} \, p_j
    \end{equation}
    Extending this, we can find the $m$-fold convolution of the sequence $\{p_j\}$ with itself, represented by the sequence $\{p_j^{*m}\}$, where $p_k^{*m}$ is the $k$th term in this sequence. This gives us
    \begin{equation}
      p_k^{*n+1} = \sum_{j=0}^\infty p_{k-j} \, p_j^{*n}
    \end{equation}
    for all $n \in \mathbb{N}$. Using this, we can write down the transition probabilities for the Markov chain $Z_n$. 
    \begin{equation}
      \mathbb{P}(Z_{n+1} = k \, | \, Z_n = m) = \begin{cases}
      0 & \text{if } m = 0 \\
      p_k^{*m} & \text{if } m \geq 1, k \geq 0
      \end{cases}
    \end{equation}
    where $\mathbb{P}(Z_{n+1} = k \, | \, Z_n = m)$ represents the probability of the $n$th generation consisting of $m$ individuals producing a total of $k$ offspring for the $(n+1)$th generation. Thus, the branching process is completely determined by the distribution of $Z_0$ and the offspring distribution $\{p_k\}_{k=0}^\infty$. 

    \begin{lemma}
      Given this discrete-time branching process, let $\mu$ be the mean of the offspring distribution. Then, 
      \begin{equation}
        \mathbb{E}(Z_n \, | \, Z_0 = 1) = \mu^n
      \end{equation}
      If $\mu > 1$, the mean of $Z_n$ grows exponentially, and if $\mu_1$, the mean of $Z_n$ decreases exponentially. 
    \end{lemma}

  \subsubsection{Continuous-time Branching Process}

    A continuous time branching process $Z_t$ has very similar structure to the discrete time branching process, except that the times between branch events (for each individual) are independent exponentially distributed random variables Exp$(\lambda)$, where the parameter $\lambda> 0$ is the branching rate. It is as though each individual has an independent alarm clock which rings as a time that is Exp$(\lambda)$, independently of all other clocks. So, if there are currently $N$ individuals, then the next alarm will ring at rate $\lambda N$; that is, the time until the next ring is distributed as Exp$(\lambda N)$, since it is the minimum of $N$ independent Exp$(\lambda)$ random variables. When an individual branches (clock rings), that individual produces a random number of offspring, according to the offspring distribution $\{p_k\}$, as before. So, a continuous time branching process has the same geneological structure as the discrete time process, but the times between branch events is randomized. Consequently, whether or not the process eventually goes extinct, depends only on the offspring distribution, not on the branching rate $\lambda$. 

    Let $m_1(t) = \mathbb{E}(Z_t)$ denote the expected population size at time $t$. Then, it is a fact that $m_1(t)$ satisfies the ordinary differential equation
    \begin{equation}
      \frac{d}{d t} m_1 (t) = \lambda(\mu - 1) m_1 (t)
    \end{equation}
    where 
    \begin{equation}
      \mu = \sum_{k=1}^\infty k p_k
    \end{equation}
    is the mean of the offspring distribution. Solving this equation reveals that 
    \begin{equation}
      m_1 (t) = e^{\lambda (\mu-1) t} m_1 (0)
    \end{equation}
    If $\mu > 1$, the mean population size grows exponentially, and if $\mu < 1$, the mean population size decreases exponentially. 

  \subsubsection{Extinction Probability, Generating Functions}

    The expression for the transition probabilities of $Z_n$ (disrete case) is quite difficult to work with. Alternatively, it can be convenient to work with generating functions. 

    \begin{definition}[Generating Function]
      The \textit{generating function} for the offspring distribution is the function 
      \begin{equation}
        G(s) \equiv \sum_{k=0}^\infty p_k \, s^k = \mathbb{E}(s^Y)
      \end{equation}
      where $Y \sim \{p_k\}$ is a random variable representing the number of children produced by a given individual. Note that $G$ is a power series that simply encodes information about the offspring distribution (also a sequence) $\{p_k\}_{k=0}^\infty$. 
    \end{definition}

    \begin{theorem}[Properties]
      Properties of the generating function. 
      \begin{enumerate}
        \item The radius of convergence of $G(s)$ is at least $1$. $G(s)$ defines a continuous function on $|s| \leq 1$. 
        \item On the interval $[0,1]$, $G(s)$ is increasing and convex. If $p_0 + p_1 < 1$, then $G(s)$ is strictly convex for $s \in [0,1]$. 
        \item $G(0) = p_0$. 
        \item $G(1) = 1$. 
        \item $G^\prime(1^-) = \mu$ is the expected number of offspring of a single individual. 
      \end{enumerate}
    \end{theorem}
    \begin{proof}
      We use the fact that 
      \[\sum_{k=0}^\infty p_k = 1 \text{ and } 0 \geq p_k \geq \; \forall k = 0, 1, 2, ...\]
    \end{proof}

    \begin{theorem}
      Suppose that $Z_0 = 1$ and that $p_0 + p_1 < 1$. Then
      \begin{equation}
        \lim_{n \rightarrow \infty} \mathbb{P}(Z_n = 0) = \mathbb{P}(\text{eventual extinction}) = t
      \end{equation}
      where $t \in [0, 1]$ is the smallest non-negative root of the equation $t = G(t)$. If $\mu \leq 1$, then $t = 1$ (clearly, since the population will exponentially decrease on average). If $\mu > 1$, there is a positive probability that the population never goes extinct. 
    \end{theorem}
    \begin{proof}
      Let $t$ be the probability that an individual's descendent family tree goes extinct. That is, $t = \mathbb{P}(Z_n = 0$ for some $n \geq 1 \; | \; Z_0 = 1)$. To derive the equation $t = G(t)$, let us condition on the first generation, with $Y_1$ denoting the number of offspring of the single parent. 
      \begin{align*}
        t & = \mathbb{P}(\text{eventual extinction} \; | \; Z_0 = 1) \\
        & = \sum_{k=0}^\infty \mathbb{P}(\text{eventual extinction} \; | \; Z_0 = 1, Y_1 = k) \, \mathbb{P}(Y_1 = k \; | \; Z_0 = 1) \\
        & = \sum_{k=0}^\infty \mathbb{P}(\text{eventual extinction} \; | \;Z_0 =1, Y_1 = k) \, p_k
      \end{align*}
      That is, given that there are $k$ children of the first individual, the probability that this first individual's descendent family tree will go extinct is equal to the probability that each of the $k$ children's trees go extinct. These $k$ extinction events are independent. Therefore, 
      \begin{equation}
        \mathbb{P}(\text{eventual extinction} \; | \;Z_0 = 1, Y_1 = k) = t^k
      \end{equation}
      which implies that 
      \begin{equation}
        t = \sum_{k=0}^\infty \mathbb{P}(\text{eventual extinction} \; | \; Z_0 = 1, Y_1 = k) \, p_k = \sum_{k=0}^\infty t^k \, p_k = G(t)
      \end{equation}
      Additionally, under the hypothesis that $p_0 + p_1 < 1$, then $G(s)$ is strictly convex on $[0,1]$. Hence if $G^\prime (1) = \mu \leq 1$, the smallest non-negative root of $t = G(t)$ must be $t=1 \implies$ extinction occurs with probability 1. On the other hand, if $G^\prime (1) = \mu > 1$, then the smallest root of $t = G(t)$ occurs in the interval $[0,1)$. 
    \end{proof}

    Note that this result applies to both the discrete time case and the continuous time case. In continuous-time chains, whether or not the population goes extinct does not depend on $\lambda$, the rate at which individuals give birth. The $\lambda$ affects the time at which extinction occurs (if it occurs), but it does not affect the probability that it occurs. However, the extinction probability certainly does depend on the offspring distribution. 

    \begin{definition}[Counting Variable]
      A random variable $X$ is a \textit{counting variable} if it takes values in $\{0, 1, 2, ...\}$. 
    \end{definition}

    Note that generating functions is a mapping from $X$, the set of counting variables (all assumed to be pairwise independent) to the algebra of power series over variable $s$.
    \begin{equation}
      G: X \longrightarrow F[[s]]
    \end{equation}

    \begin{lemma}
      Let $X$ and $Y$ be two independent random counting variables, with generating functions $G_X (s) = \mathbb{E}(s^X)$ and $G_Y (s) = \mathbb{E}(s^Y)$. Then, the generating function for the random variable $Z = X + Y$ is $G_Z(s) = G_X (s) G_Y (s)$. That is, the generating function mapping $G$ is a homomorphism that maps addition to multiplication. In particular, if $X$ and $Y$ are iid, then $G_Z (s) = G_X (s)^2$. 
    \end{lemma}
    \begin{proof}
      Since $X$ and $Y$ are independent, 
      \begin{equation}
        G_Z (s) = \mathbb{E}(s^Z) = \mathbb{E}(s^{X+Y}) = \mathbb{E}(s^X s^Y) = \mathbb{E}(s^X) \mathbb{E}(s^Y) = G_X (s) G_Y (s)
      \end{equation}
    \end{proof}

    Applying this argument iteratively, we get the following lemma. 
    \begin{lemma}
      Let $N \geq 1$ be a fixed positive integer. Let $Y_1, Y_2, ..., Y_N$ be independent, identically distributed random counting variables with generating function $G_Y (s) = \mathbb{E}(s^Y)$. Then, the generating function for the sum $Z = Y_1 + ... + Y_n$ is 
      \begin{equation}
        G_Z (s) = G_Y (s)^N
      \end{equation}
    \end{lemma}

    Now, suppose that $N$ is not fixed, but another random variable. We wish to describe the distribution of the sum of a random number of random variables. 

    \begin{lemma}
      Let $Y_1, Y_2, Y_3, ...$ be a collection of independent, identically distributed random variables with generating function $G_Y (s) = \mathbb{E}(s^Y)$. Let $N$ be a random counting variable, independent of the $Y_i$. Let $N$ have generating function $G_N (s)$. Then the generating function for $Z = Y_1 + Y_2 + ... + Y_N$ is 
      \begin{equation}
        G_Z (s) = G_N \big( G_Y (s) \big)
      \end{equation}
    \end{lemma}
    \begin{proof}
      Just condition on $N = k$ 
      \begin{align*}
        G_Z (s) = \mathbb{E}(s^Z) & = \sum_{k=0}^\infty \mathbb{E}\big( s^Z \,|\, N=k\big) \, \mathbb{P}(N=k) \\
        & = \sum_{k=0}^\infty \mathbb{E}(s^{Y_1 + ... + Y_k} \,|\,N=k) \, \mathbb{P}(N=k) \\
        & = \sum_{k=0}^\infty G_Y (s)^k \, \mathbb{P}(N=k) \\
        & = \mathbb{E}\big( G_Y (s)^N \big) = G_N \big( G_Y (s) \big)
      \end{align*}
    \end{proof}

    \begin{theorem}
      Let $G(s)$ be the generating function for the offspring distribution $G(s) = \sum_{k=0}^\infty p_k s^k$. Suppose that $Z_0 = 1$ and let $G_n (s) = \mathbb{E}(s^{Z_n})$ be the generating function for the random variable $Z_n$. Then, 
      \begin{equation}
        G_{n+m} (s) = G_n \big(G_m (s)\big) = G_m \big( G_n (s) \big)
      \end{equation}
      Hence, 
      \begin{equation}
        G_n (s) = G(G(G(...(G(s))...))) \;\;\;\; \text{n-fold composition}
      \end{equation}
    \end{theorem}

    \begin{example}
      Suppose the offspring distribution is
      \begin{equation}
        p_k = q p^k, \;\; k \geq 0
      \end{equation}
      for some $p \in (0, 1)$, where $q = 1-p$. Thus, the number of children from a given parent is $Y = X - 1$, where $X \sim$ Geom$(q)$. Then, $\mathbb{E}(Y) = \frac{1}{q} - 1 = \frac{p}{q}$. With some computation, this means that
      \begin{equation}
        G(s) = \frac{q}{1- p s}
      \end{equation}
      and $t = \min \{1, \frac{q}{p}\}$. 
    \end{example}


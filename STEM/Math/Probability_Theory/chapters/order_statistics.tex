\section{Order Statistics}

  Let $X_1, X_2, ..., X_n$ be a finite collection of independent, identically distributed random variables. Suppose that they are continuously distributed with density $f$ and CDF $F$. 

  \begin{definition}[Order Statistic]
    Define the random variable $X_{(k)}$ to be the $k$th ranked value, called the \textit{$k$th order statistic}. This means that 
    \begin{equation}
      X_{(1)} = \min\{X_1, X_2, ..., X_n\}, \;\; X_{(n)} = \max\{X_1, X_2, ..., X_n\}
    \end{equation}
    and in general, for any $k \in \{1, 2, ..., n\}$, 
    \begin{equation}
      X_{(k)} = X_j \text{ if } \sum_{l=1}^n \mathbb{I}_{X_l < X_j} = k - 1
    \end{equation}
    which means that exactly $k-1$ of the values of $X_l$ are less than $X_j$. Since $F$ is continuous, 
    \begin{equation}
      X_{(1)} < X_{(2)} < ... < X_{(n)}
    \end{equation}
    holds with probability $1$. This leads us to define the random variable $X_{(k)}$ representing the $k$th order statistic.
    \begin{equation}
      f_{(k)} (y) = \begin{cases} 
        n \, \binom{n-1}{k-1} y^{k-1} (1-y)^{n-k} & y \in (0, 1) \\
      0 & y \not\in (0,1)
      \end{cases}
    \end{equation}
    That is, $X_{(k)}$ has the Beta$(k, n-k_1)$ distribution. 
  \end{definition}

\subsection{Poisson Arrival Process}

  A \textbf{Poisson Arrival Process} with rate $\lambda > 0$ on the interval $[0, \infty)$ is a model for the occurence of some events which may have at any time. We can interpret the process as a collection of random points in $[0, \infty)$ which are the times at which the arrivals occur. 
  \begin{enumerate}
    \item \textit{Interpretation 1}. Set $T_0 = 0$. The arrival times are random variables $0 < T_1 < T_2 < T_3 < ...$ such that the inter-arrival waiting times
    \begin{equation}
      W_k = T_k - T_{k-1}, \;\;\; k \geq 0
    \end{equation}
    have the property that $\{W_k\}_{k=1}^\infty$ are independent Exp$(\lambda)$ random variables. 

    \item \textit{Interpretation 2}. For any interval $I \subset [0, \infty)$, let 
    \begin{equation}
      N_I \equiv \text{ number of arrivals that occur in interval } I
    \end{equation}
    Then, $N_I \sim$ Poisson$(\lambda |I|)$, and for any collection of disjoint intervals $I_1, I_2, ..., I_n$, the random variables 
    \begin{equation}
      \{N_{I_k}\}_{k=1}^n
    \end{equation}
  are independent. 
  \end{enumerate}

  \begin{theorem}
    These two interpretations of the arrival process are equivalent. 
  \end{theorem}
  \begin{proof}
    In the 2nd interpretation, the statement $N_I \sim$ Poisson$(\lambda |I|)$ means that 
    \begin{equation}
      \mathbb{P}(N_I = m) = e^{-\lambda |I|} \frac{(\lambda |I|)^m}{m!}, \;\;\; m = 0, 1, 2, 3, ...
    \end{equation}
    where $|I|$ is the length of interval $I$. From the first perspective, notice that 
    \begin{equation}
      T_k = W_1 + W_2 + ... + W_k
    \end{equation}
    so that the $k$th arrival time $T_k$ is a sum of $k$ independent Exp$(\lambda)$ random variables. Thus, 
    \begin{equation}
      T_k \sim \text{Gamma}(k, \lambda)
    \end{equation}
    and therefore has density
    \begin{equation}
    \lambda e^{-\lambda t} \frac{(\lambda t)^{k-1}}{(k-1)!}, \;\;\; t>0
    \end{equation}
    Note that the arrival times $T_i$ are not independent of each other, but the wait times $W_i$ are indeed independent. 
  \end{proof}

  We can slightly modify this to create a Poisson arrival process over some finite time horizon $[0, L]$. Again, you can do this two ways: 
  \begin{enumerate}
    \item Starting with independent Exp$(\lambda)$ random variables $W_1, W_2, ...$, we define
    \begin{equation}
      T_k = \sum_{i=1}^k W_i
    \end{equation}
    Once you have $T_k > L$, stop. 
    \item We let $N \sim$ Poisson$(\lambda L)$, since we are only working in finite interval $L$. Given $N = n$, let $U_1, U_2, ..., U_n \sim$ Uniform$([0, L])$. These define the arrival times, and let us order them to get
    \begin{equation}
      T_k = U_{(k)}, \;\; k = 1, 2, ..., N
    \end{equation}
    where $U_{(k)}$ is the $k$th ordered point, with $T_1 = \min(U_1, ..., U_N)$. 
  \end{enumerate}

  \begin{lemma}[Memoryless Property]
    The Exp$(\lambda)$ distribution has the property that for all $t, s \geq 0$, 
    \begin{equation}
      \mathbb{P}(W > t + s \; | \; W > t) = \mathbb{P}(W > s)
    \end{equation}
    which is called the \textit{memoryless property}. We can interpret this in the following way. Let $W$ be the time you have to wait for the first arrival. Given that you already waited $t$ units of time, the probability that you have the wait $s$ additional units of time is just the probability that you wait at least $s$ from the beginning. That is, knowing that $t$ units of time have elapsed does not affect the distribution of the remaining waiting time. 
  \end{lemma}

  \begin{theorem}
    Let $W$ be a continuously distributed random variable. Then $W \sim$ Exp$(\lambda)$ for some $\lambda > 0$ if and only if $W$ satisfies the memoryless property. 
  \end{theorem}


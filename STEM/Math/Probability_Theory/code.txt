\documentclass{article}
\usepackage[a4paper, top=1in, bottom=1in, left=1in, right=1in]{geometry}
\usepackage[utf8]{inputenc}

\usepackage[english]{babel}
\usepackage{tikz-cd, extarrows, esvect, esint, pgfplots, lipsum, bm, dcolumn}
\usetikzlibrary{arrows}
\usepackage{amsmath, amssymb, amsthm, mathrsfs, mathtools, centernot, hyperref, fancyhdr, lastpage}


\renewcommand{\thispagestyle}[1]{}

\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\Sym}{Sym}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\std}{std}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Corr}{Corr}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{definition}
\newtheorem{example}{Example}[section]
\newtheorem{corollary}{Corollary}[theorem]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\renewcommand{\qed}{\hfill$\blacksquare$}
\renewcommand{\footrulewidth}{0.4pt}% default is 0pt


\begin{document}
\pagestyle{fancy}

\lhead{Probability Theory}
\chead{Muchang Bahng}
\rhead{\date{January 2023}}
\cfoot{\thepage / \pageref{LastPage}}

\title{Probability Theory}
\author{Muchang Bahng}
\date{January 2023}

\maketitle

An overview of probability using measures. We will denote probability measures defined over $\sigma$-algebras with $\mathbb{P}$ and probability functions defined over some sample space $\Omega$ or $\mathbb{R}$ with $P$ or $p$. 

\section{Probability Spaces}

\begin{definition}[Probability Space]
A \textbf{probability space} is a measure space $(\Omega, \mathcal{F}, \mathbb{P})$ with $\mathbb{P}(\Omega) = 1$. 
\begin{enumerate}
    \item $\Omega$ is called the \textbf{sample space} and an element $\omega \in \Omega$ is called an outcome. 
    \item $\mathcal{F}$ is called the \textbf{event space} and an element $A \in \mathcal{F}$ is called an event. 
    \item The measure of an event $\mathbb{P}(A)$ is called the \textbf{probability} of that event. 
\end{enumerate}
If some measure space $X$ has a finite total measure, we can construct a probability space from it by normalizing the measure. 
\end{definition}

We can think of the sample space $\Omega$ as the set of all conceivable futures and an event $F \in \mathcal{F}$ as some subset of conceivable futures. The probability $\mathbb{P}(\Omega)$ represents our degree of certainty that our future will be contained in such an event. This formulation allows us to talk about discrete and continuous probability distributions at once. But given the same random experiment, we don't need to always have the same sample space. For example, let's have a coin toss. One could be interested in whether it lands heads or tails, which means $\Omega = \{0, 1\}$, but another could be interested in the number of times the coin flips midair, in which $\Omega = \mathbb{N}_0$. We could even be interested in the set of all trajectories of the coin, which would result in a huge space of all trajectories of the flip, or the velocity at which it lands on the table, which would lead to $\Omega = \mathbb{R}^+$. 

\begin{lemma}[Properties of Probability Measures]
Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space. Then, the following properties hold: 
\begin{enumerate}
    \item Complement: If $A$ is an event in $\mathcal{F}$, 
    \[\mathbb{P}(A^C) = 1 - \mathbb{P}(A)\]
    \item Finite Additivity: If $A_1, \ldots, A_n$ are disjoint events, then 
    \[\mathbb{P} \bigg( \bigcup_{i=1}^n A_i \bigg) = \sum_{i=1}^n \mathbb{P}(A_i)\]
    \item Monotonicity: If $A \subset B$, both in $\mathcal{F}$, then 
    \[\mathbb{P}(A) \leq \mathbb{P}(B)\]
    \item Inclusion Exclusion Principle: If $A, B \in \mathcal{F}$, 
    \[\mathbb{P}(A \cup B) = \mathbb{P}(A) + \mathbb{P}(B) - \mathbb{P}(A \cap B)\]
    and by induction, if $A_1, \ldots, A_n \in \mathcal{F}$, then 
    \[\mathbb{P}\bigg( \bigcup_{i=1}^n A_i \bigg) = \sum_{i=1}^n \mathbb{P}(A_i) - \sum_{i < j} \mathbb{P}(A_i \cap A_j) + \sum_{i < j < k} \mathbb{P}(A_i \cap A_j \cap A_k) + \ldots + (-1)^{n-1} \mathbb{P}\bigg( \bigcap_{i=1}^n \mathbb{P}(A_i) \bigg)\]
    \item Continuity of probability: If $A_1, A_2, \ldots \in \mathcal{F}$, then 
    \[\mathbb{P} \bigg( \bigcup_{i=1}^\infty \mathbb{P}(A_i) \bigg) = \lim_{m \rightarrow \infty} \mathbb{P} \bigg( \bigcup_{i=1}^m A_i\bigg)\]
\end{enumerate}
\end{lemma}
\begin{proof}
Listed. 
\begin{enumerate}
    \item Since $A$ and $A^c$ are disjoint, it follows immediately from the axioms that $\mathbb{P}(A) + \mathbb{P}(A^c) = 1$, and since they are all finite, we can get $\mathbb{P}(A^c) = 1 - \mathbb{P}(A)$. 
    \item Since $\mathcal{F}$ is by definition stable under countable union, use can fill a finite union up with empty sets, which are all disjoint, and then by definition the sum of their measures equal the measure of their unions. 
    \item We can use monotonicity to write 
    \[\mathbb{P}(B) = \mathbb{P}(A) + \mathbb{P}(B \setminus A)\]
    and since $\mathbb{P}$ is nonnegative by definition, $\mathbb{P}(B \setminus A) \geq 0$, proving the result.  
    \item Use set theory.
    \item This is a highly nontrivial statement. Don't think of the infinite union $\cup_{i=1}^\infty A_i$ the sequential union of $A_1 \cup A_2 \cup \ldots$. Rather, just think of it as the set of all $\omega$'s that are in at least one of the $A_i$'s. We can use the monotone convergence theorem to prove that the RHS is nondecreasing and bounded. So really we just have to prove equality. This is proved in Prop 2.5. of my measure theory notes, by defining $B_k = A_k - A_{k-1}$ and working with those. 
\end{enumerate}
\end{proof}

\subsection{Discrete Probability Spaces}

\begin{definition}[Discrete Probability Space]
If $\Omega$ is a countable set, then we can take its $\sigma$-algebra $\mathcal{F}$ to be the power set of $\Omega$ and construct the measurable space $(\Omega, 2^\Omega, \mathbb{P})$. From the axioms, for any event $A \in \mathcal{F}$, we have
\[\mathbb{P} (A) = \sum_{\omega \in A} \mathbb{P}(\{\omega\}) \text{ and } \sum_{\omega \in \Omega} \mathbb{P}(\{\omega\}) = 1\]
The greatest $\sigma$-algebra $F = 2^{\Omega}$ describes the complete information. The cases $\mathbb{P}(\{\omega\}) = 0$ is permitted by the definition, but rarely used since such $\omega$ can safely be excluded from the sample space. Therefore, we can define the probability measure $\mathbb{P}$ by simply defining it for all singleton sets $\{\omega\}$. 
\end{definition}

This may be confusing, since for discrete spaces, it looks like we're assigning probabilities to each $\omega \in \Omega$, but we are actually assigning them to singleton \textit{sets}. We should be writing $\mathbb{P}(\{\omega\})$, but sometimes we abuse notation and write $\mathbb{P}(\omega)$. 

\begin{example}
Consider the flip of a fair coin with outcomes either hands or tails. Then, $\Omega = \{H, T\}$. The $\sigma$-algebra $F = 2^{\Omega}$ contains $2^2 = 4$ events: 
\begin{align*}
    \{\} &= \text{Neither heads nor tails} \\
    \{H\} &= \text{Heads} \\
    \{T\} &= \text{Tails} \\
    \{H, T\} &= \text{Either heads or tails}
\end{align*}
That is, $\mathcal{F} = \{\{\}, \{H\}, \{T\}, \{H, T\}\}$. Our probability measure $\mathbb{P}$ is defined
\[\mathbb{P}(f) = \begin{cases}
0 & f = \{\} \\
0.5 & f = \{H\} \\
0.5 & f = \{T\} \\
1 & f = \{H, T\}
\end{cases}\]
\end{example}

Being able to consider the event space as $2^X$ is very nice, since countability of $X$ allows us to avoid the Banach-Tarski paradox. It doesn't matter whether $\mathcal{F} = 2^X$ itself is uncountable or not. 

\begin{example}
A fair coin is tossed 3 times, creating 8 possible outcomes. 
\[\Omega = \{HHH, HHT, HTH, HTT, THH, THT, TTH, TTT\}\]
The complete information is described by the $\sigma$-algebra $\mathcal{F} = 2^{\Omega} = 2^8 = 256$ events, where each of the events is a subset of $\Omega$.  
\end{example}

\begin{example}[Geometric Measure on $\mathbb{N}$]
Let $\Omega = \mathbb{N}$ and $\mathcal{F} = 2^\mathbb{N}$. We can completely define the probability measure by assigning them to singletons $k \in \mathbb{N}$. One such assignment is 
\[\mathbb{P}(\{k\}) = \frac{1}{2^k}\]
or more generally, 
\[\mathbb{P}(\{k\}) = p (1 - p)^{k-1}\]
\end{example}

\begin{example}[Poisson Measure on $\mathbb{N}_0$]
Let $\Omega = \mathbb{N} \cup \{0\}$. Then, $\mathcal{F} = 2^\Omega$ and we can define $\mathbb{P}$ on the singleton sets as 
\[\mathbb{P}(\{k\}) = \frac{e^{-\lambda} \lambda^k}{k!}\]
for any $\lambda > 0$. We can then compute the probability of, say all primes, by taking 
\[\mathbb{P}(\text{primes}) = \sum_{k \text{ prime}} \mathbb{P}(\{k\})\]
which we know to be monotonically increasing and bounded above, so it must converge. Whether this has a closed form solution is another matter. Again, in reality we are assigning probability measures on all $\mathcal{F}$-measurable sets, but just doing it through assignment of measure through singleton sets. 
\end{example}

\subsection{Continuous, Uncountable Sample Spaces}

Now if we move to uncountable outcome spaces, then things are not as nice, which is why we need to machinery of measure theory to study them. Let us try to model a probability measure on $\Omega = [0, 1]$. It is uncountable, and it turns out that $2^\Omega$ has cardinality strictly greater than even the continuum. If we try to model a uniform probability measure $\mathbb{P}$, then for some subset $A \in 2^\Omega$, it should be the case that $\mathbb{P}(A) = \mathbb{P}(A \oplus k)$, where $A \oplus k$ is just some translated version of $A$ still contained within $[0, 1]$. This applies to singleton sets, and it turns out that if we try to assign a nonzero probability measure to any singleton $\{k\}$, then the probability measure of $\Omega$ blows up to infinity, which we can't have. So the only thing we can do is have every singleton have zero probability. Remember that a measure by definition has the \textit{countable additivity} property, which says that 
\[\mu \bigg( \bigsqcup_{k=1}^\infty A_k \bigg) = \sum_{k=1}^\infty \mu(A_k)\]
for all \textit{countable} collections $\{A_k\}$. Summation is not defined for uncountable collections, and so having a probability $0$ on every singleton does not imply that the probability of any uncountable set has is $0$. That is, having $\mathbb{P}(\{k\}) = 0$ for all $k \in [0, 1]$ does not tell you what $\mathbb{P}([0, 1])$ is. So now rather than assigning probabilities to singletons, like we did with discrete sets, the approach is to assign probabilities directly to our event space $\mathcal{F}$. We can do this by directly assigning the Lebesgue measure to the Borel algebra of $[0, 1]$, which has the properties 
\begin{enumerate}
    \item $\mathbb{P}((a, b)) = \mathbb{P}([a, b)) = \mathbb{P}((a, b]) = \mathbb{P}([a, b]) = b - a$
    \item Translation invariance as stated above. 
\end{enumerate}
Over uncountable $\Omega$, we cannot afford to work with $2^\Omega$, since there is an impossibility theorem that says that there is no measure defined on $2^{[0, 1]}$ with the two properties above. Therefore, we must work with a smaller $\sigma$-algebra. Since the subsets of interest are usually intervals (or more generally, open sets), people usually take the Borel $\sigma$-algebra of open intervals on $[0, 1]$. The Lebesgue measure on $\mathbb{R}$ is not a probability measure since it $\lambda(\mathbb{R}) = \infty$, but we can construct a uniform probability measure on any bounded set of $\mathbb{R}$. Usually, these continuous probability spaces are $\mathbb{R}^n$, and we define some measure $\mu$ directly on its $\sigma$-algebra. 

\subsubsection{Mixture Spaces}

\begin{definition}[Atom]
Let $(\Omega, \mathcal{F}, \mathbb{P})$ be uncountable. If for some $\omega \in \Omega$, $\mathbb{P}(\{\omega\}) \neq 0$, then $\omega$ is called an \textbf{atom}. 
\end{definition}

Now, given a general (discrete or continuous, or a combination of both) distribution, the set of all the atoms are an at most countable (maybe empty) set whose probability is the sum of probabilities of all atoms (by countable additivity). That is, given $\omega_1, \omega_2, \ldots$ atoms, 
\[\mathbb{P} \bigg( \bigsqcup_{i=1}^\infty \{\omega_i\} \bigg) = \sum_{i=1}^\infty \mathbb{P}(\{\omega_i\})\]
\begin{enumerate}
    \item If this sum is equal to $1$ then all other points can be safely excluded from the sample space $\Omega$, returning us to the discrete case. 
    \item If this sum is $0$ then we just have some continuous sample space. This means $\mathbb{P}(\{\omega\}) = 0$ for all $\omega \in \Omega$, and so $\Omega$ must be uncountable (since if it was countable, then we should be able to sum the $\mathbb{P}(\{\omega\})$'s to get $1$, but it's $0$). Remember that summation is only defined for at most countable elements. 
    \item If the sum of probabilities of all atoms is strictly between $0$ and $1$, then the probability space decomposes into a discrete, atomic part and a non-atomic, continuous part. 
\end{enumerate}

\subsection{Infinite Coin Toss Model}

Let us try to model a countably infinite sequence of coin tosses with $\Omega = \{0, 1\}^\infty$ and each $\omega \in \Omega$ of form 
\[\omega = \omega_1 \omega_2 \omega_3 \ldots \]
which are infinite binary strings. Since $\Omega$ is uncountable, we can't just work with $2^\Omega$. We should try to assign an appropriate $\sigma$-algebra on $\Omega$ and then define a measure on it. Let's try to describe some sets that allows us to describe the outcome of the first $n$ tosses. That is, $\mathcal{F}_n$ is a collection of subsets of $\Omega$ whose occurrence can be decided by looking at its first $n$ tosses. Note that $\mathcal{F}_n$ is not a $\sigma$-algebra. 

\section{Conditional Probability}

\begin{definition}[Conditional Probability]
Given a measure space $(\Omega, \mathcal{F}, \mathbb{P})$, let $B$ be an event such that $\mathbb{P}(B) > 0$. The \textbf{conditional probability} of $A$ given $B$ is defined 
\[\mathbb{P}(A \mid B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)}\]
\end{definition}

Note that we can't condition on events that have probability $0$, which is why we need the $\mathbb{P}(B) > 0$ condition. If this is the case, it doesn't even make sense to talk about a conditional probability $\mathbb{P}(A \mid B)$. For example, if we take the probability space $[0, 1]$ with its Borel algebra and the Lebesgue measure, then we cannot condition something on the rationals, e.g. $\mathbb{P}(\{\omega < 0.5\} \mid \omega \in \mathbb{Q})$ does not make sense. In fact, doing so can lead to contradictions, one being the \textbf{Borel-Kolmogorov paradox}. 

An extremely useful theorem is that the conditional probability taken as a measure gives us a new viable measure on the same probability space $\Omega$. 

\begin{theorem}
Given probability space $(\Omega, \mathcal{F}, \mathbb{P})$, let $B \in \mathcal{F}$ with $\mathbb{P}(B) > 0$. Then, $\mathbb{P}( \cdot \mid B): \mathcal{F} \longrightarrow [0, 1]$ is a probability measure on $(\Omega, \mathcal{F})$. 
\end{theorem}
\begin{proof}
We prove the properties of a probability measure. 
\begin{enumerate}
    \item The empty set has measure $0$. 
    \[\mathbb{P}(\emptyset \mid B) = \frac{\mathbb{P}( \emptyset \cap B)}{\mathbb{P}(B)} = \frac{\mathbb{P}(\emptyset)}{\mathbb{P}(B)} = \frac{0}{\mathbb{P}(B)} = 0\]
    \item The entire space has measure $1$. 
    \[\mathbb{P}(\Omega \mid B) =  \frac{\mathbb{P}( \Omega \cap B)}{\mathbb{P}(B)} = \frac{\mathbb{P}(B)}{\mathbb{P}(B)} = 1\]
    \item Countable additivity of disjoint events. Let $A_i \in \mathcal{F}$ for $i = 1, 2, \ldots$ which are disjoint. Then, their union is in $\mathcal{F}$ by definition of $\sigma$-algebra. Now, 
    \begin{align*}
        \mathbb{P}\bigg( \bigcup_{i=1}^\infty A_i \bigg| B \bigg) & = \frac{1}{\mathbb{P}(B)} \mathbb{P} \bigg[ \Big( \bigcup_{i=1}^\infty A_i \Big) \cap B \bigg] \\
        & = \frac{1}{\mathbb{P}(B)} \mathbb{P} \bigg[ \bigcup_{i=1}^\infty (A_i \cap B) \bigg] \\
        & = \frac{1}{\mathbb{P}(B)} \sum_{i=1}^\infty \mathbb{P} (A_i \cap B) \\
        & = \sum_{i=1}^\infty \frac{\mathbb{P} (A_i \cap B)}{\mathbb{P}(B)} = \sum_{i=1}^\infty \mathbb{P}(A_i \mid B) 
    \end{align*}
\end{enumerate}
\end{proof}

\begin{theorem}[Partition Rule/Law of Total Probability]
Let $B_i \in \mathcal{F}$ for $i = 1, 2, \ldots$ with $\mathbb{P}(B_i) > 0$ be a partition of $\Omega$ and let $A \in \mathcal{F}$. Then, 
\[\mathbb{P}(A) = \sum_{i=1}^\infty \mathbb{P}(A \mid B_i) \, \mathbb{P}(B_i)\]
\end{theorem}
\begin{proof}
The left hand side is 
\[\mathbb{P}(A) = \mathbb{P}\bigg( \bigcup_{i=1}^\infty (A \cap B_i) \bigg) = \sum_{i=1}^\infty \mathbb{P} (A \cap B_i) = \sum_{i=1}^\infty \mathbb{P} (A \mid B_i) \, \mathbb{P}(B_i)\]
\end{proof}

\begin{corollary}
If $B \in \mathcal{F}$ s.t. $0 < \mathbb{P}(B) < 1$, then 
\[\mathbb{P}(A) = \mathbb{P}(A \mid B) \, \mathbb{P}(B) + \mathbb{P}(A \mid B^c) \, \mathbb{P}(B^c)\]
\end{corollary}

\begin{theorem}[Bayes Rule]
Let $A, B \in \mathcal{F}$. Then, 
\[\mathbb{P}(B \mid A) = \frac{\mathbb{P}(A \mid B) \, \mathbb{P}(B)}{\mathbb{P}(A)}\]
\end{theorem}
\begin{proof}
We know that 
\[\mathbb{P}(A \mid B) = \frac{\mathbb{P} (A \cap B)}{\mathbb{P}(B)} \text{ and } \mathbb{P}(B \mid A) = \frac{\mathbb{P}(B \cap A)}{\mathbb{P}(B)}\]
and so we can write 
\[\mathbb{P} (A \mid B) \, \mathbb{P}(B) = \mathbb{P}(A \cap B) = \mathbb{P}(B \mid A) \, \mathbb{P}(A)\]
\end{proof}

\begin{corollary}
Let $A \in \mathcal{F}$ and $B_i \in \mathcal{F}$ for $i = 1, 2, \ldots$ be a partition of $\Omega$. Then, using the partition rule, 
\[\mathbb{P}(B_i \mid A) = \frac{\mathbb{P}( A \mid B_i) \, \mathbb{P}(B_i)}{\sum_j \mathbb{P}(A \mid B_j) \, \mathbb{P}(B_j)}\]
\end{corollary}

\begin{theorem}[Probability of Intersection]
Let $A_i \in \mathcal{F}$ for $i = 1, 2, \ldots$. Then, 
\[\mathbb{P} \bigg( \bigcap_{i=1}^\infty A_i \bigg) = \mathbb{P}(A_1) \, \prod_{i=2}^\infty \mathbb{P} \Big( A_i \,\Big|\, \bigcap_{j=1}^{i-1} A_j \Big)\]
as long as the conditional probabilities are defined. 
\end{theorem}
\begin{proof}
By continuity of probability (for first step), the left hand side equals 
\[\mathbb{P} \bigg( \bigcap_{i=1}^\infty A_i \bigg) = \lim_{n \rightarrow \infty} \mathbb{P} \bigg( \bigcap_{i=1}^n A_i \bigg) = \lim_{n \rightarrow \infty} \mathbb{P}(A_1) \, \prod_{i=2}^\infty \mathbb{P} \Big( A_i \,\Big|\, \bigcap_{j=1}^{i-1} A_j \Big)\]
\end{proof}

\section{Independence}

\begin{definition}[Independence of $2$ Events]
Given probability space $(\Omega, \mathcal{F}, \mathbb{\mathbb{P}})$, events $A, B \in \mathcal{F}$ are said to be \textbf{independent under $\mathbf{\mathbb{P}}$} if 
\[\mathbb{P}(A \cap B) = \mathbb{P}(A) \, \mathbb{P}(B)\]
This leads to the immediate property that if $\mathbb{P}(B) > 0$, with $A, B$ independent, then 
\[\mathbb{P}(A \mid B) = \mathbb{P}(A)\]
\end{definition}

Note that $A$ and $B$ may be independent under one measure, but not under another measure. The property that $\mathbb{P}(A \mid B) = \mathbb{P}(A)$ is \textit{not} the definition of independence, since it has the more restricting property that $\mathbb{P}(B) > 0$, so only refer to the definition that $\mathbb{P}(A \cap B) = \mathbb{P}(A) \, \mathbb{P}(B)$. This is the true definition of independent events that we should rely on, not the one that says that $A$ and $B$ are independent if "one does not affect the other." This old definition is misleading and false. For example, take the probability space $[0, 1]$, with Borel $\sigma$-algebra, and Lebesgue measure $\mathbb{P} = \lambda$, and let $A = \mathbb{Q}$ and $B = \mathbb{R} \setminus \mathbb{Q}$. Then, contradictory to our old definition, $A$ and $B$ are independent since $\mathbb{P}(A \cap B) = \mathbb{P}(A) \, \mathbb{P}(B) = 0$! By the definition, an event $A$ is independent of itself if $\mathbb{P}(A) = 0$ or $1$ (e.g. $A$ is rationals, irrationals, cantor set, $\emptyset$, $\Omega$, etc.). 

\begin{definition}[Independence of $n$ Events]
Given probability space $(\Omega, \mathcal{F}, \mathbb{P})$, 
\begin{enumerate}
    \item Let us have a finite collection of events $A_1, A_2, \ldots, A_n \in \mathcal{F}$. They are \textbf{independent} if for all nonempty $I_0 \subset \{1, 2, \ldots n\}$, 
    \[\mathbb{P} \bigg( \bigcap_{i \in I_0} A_i \bigg) = \prod_{i \in I_0} \mathbb{P}(A_i)\]
    Note that it is not enough to just prove that 
    \[\mathbb{P}(A_1 \cap \ldots \cap A_n) = \prod_{i=1}^n \mathbb{P}(A_i)\]
    We must verify this for all $2^n$ possible choices (to be precise, we don't need to prove for $I_0 = \emptyset$ and $I_0 = \{A_i\}$), so for $2^n - n - 1$ choices. 
    
    \item Let $\{A_i\}_{i \in I}$ be a collection of events indexed by a possibly uncountable $I$. They are independent if for all nonempty and finite $I_0 \subset I$, we have 
\[\mathbb{P} \bigg( \bigcap_{i \in I_0} A_i \bigg) = \prod_{i \in I_0} \mathbb{P}(A_i)\]
\end{enumerate}
\end{definition}

Now for a definition which is not seen in introductory probability courses. 

\begin{definition}[Sub-$\sigma$-Algebras]
Given a $\sigma$-algebra $\mathcal{F}$, a \textbf{sub-$\boldsymbol{\sigma}$-algebra} of $\mathcal{F}$ is a $\sigma$-algebra $\mathcal{G}$ s.t. $\mathcal{G} \subset \mathcal{F}$. 
\end{definition}

Now when we are trying to compare two $\sigma$-algebras, the measure defined for one may not even be defined on the other. To ensure that a measure is defined on both, it makes sense to take its $\sigma$-algebra and construct two sub-$\sigma$-algebras, which $\mu$ is guaranteed to be defined on. 

\begin{definition}[Independence of $\sigma$-Algebras]
Let us have probability space $(\Omega, \mathcal{F}, \mathbb{P})$. 
\begin{enumerate}
    \item Let $\mathcal{F}_1, \mathcal{F}_2$ be two sub-$\sigma$-algebras of $\mathcal{F}$. $\mathcal{F}_1$ and $\mathcal{F}_2$ are independent if for any $A_1 \in \mathcal{F}_1, A_2 \in \mathcal{F}_2$, $A_1$ and $A_2$ are independent. 
    \item Let $\{ \mathcal{F}_i\}_{i \in I}$ be an arbitrary collection of sub-$\sigma$-algebras of $\mathcal{F}$, indexed by possibly uncountable $I$. Then, they are independent if for any choices of $A_i \in \mathcal{F}_i$ for $i \in I$, $\{A_i\}_{i \in I}$ are independent events. 
\end{enumerate}
\end{definition}

\subsection{Borel-Cantelli Lemmas}

There are many Borel-Cantelli lemmas, and we will introduce the two most famous ones. To understand what these lemmas say, given a sequence $A_1, A_2, \ldots$ of events in $\sigma$-algebra $\mathcal{F}$, we must first understand what the daunting term  
\[\bigcap_{n=1}^\infty \bigcup_{i = n}^\infty A_i\]
means. Now let's try to explain what the intersection of the unions mean. First, remember that $\sigma$-algebras are stable under both countable unions and countable intersections, this is also in $\mathcal{F}$. We can interpret 
\[\bigcap_{n=1}^\infty \bigcup_{i=n}^\infty A_i = \{ A_n \text{ i.o.}\} \]
as the \textit{event that infinitely many $A_n$'s occur}, where i.o. means "infinitely often." To parse this, let's start from the innermost term and call it 
\[B_n = \bigcup_{i=n}^\infty A_i \implies \{A_n \text{ i.o.}\} = \bigcap_{n=1}^\infty B_n\]
$B_n$ is the event that at least one of the $A_n, A_{n+1}, A_{n+2}, \ldots$ occurs, often referred to as the \textit{$n$th tail event}. Now the intersection of all $B_n$'s is the event that \textit{all} $B_n$'s occur. In other words, this is the event that for no matter how big of an $N \in \mathbb{N}$ I choose, there is always at least an event $A_n$ with $n > N$ that occurs. This is shortly summarized as the event that infinitely many $A_n$'s occur. 

\begin{lemma}[1st Borel-Cantelli Lemma]
Given probability space $(\Omega, \mathcal{F}, \mathbb{P})$, if $A_1, A_2, \ldots$ is a sequence of events such that 
\[\sum_{n=1}^\infty \mathbb{P}(A_n) < \infty\]
the almost surely (with probability $1$) only finitely many $A_n$'s will occur. 
\[\mathbb{P} \bigg( \bigcap_{n=1}^\infty \bigcup_{i = n}^\infty A_i \bigg) = 0\]
\end{lemma}
\begin{proof}
Setting $B_n$ as above, we have 
\begin{align*}
    \mathbb{P}\bigg( \bigcap_{n=1}^\infty B_n \bigg) & = \lim_{n \rightarrow \infty} \mathbb{P}(B_n) & (\text{continuity of probability}) \\
    & = \lim_{n \rightarrow \infty} \mathbb{P} \bigg( \bigcup_{i=1}^\infty A_i \bigg) & (\text{substitute } B_i) \\
    & \leq \lim_{n \rightarrow \infty} \sum_{i = n}^\infty \mathbb{P}(A_i) = 0 & (\text{tail sum of convergent series is } 0)
\end{align*}
\end{proof}

The second Borel-Cantelli lemma is like a partial contrapositive to the first lemma, where it starts with the assumption that the sum of the $\mathbb{P}(A_n)$'s are infinite (along with the addition case that they are independent). 

\begin{lemma}[2nd Borel-Cantelli Lemma]
If $A_1, A_2, \ldots$ are independent events such that 
\[\sum_{n=1}^\infty \mathbb{P}(A_n) = \infty,\]
then almost surely (with probability $1$) infinitely many $A_n$'s will occur. That is, 
\[\mathbb{P} \bigg( \bigcap_{n=1}^\infty \bigcup_{i = n}^\infty A_i \bigg) = 1\]
\end{lemma}

The intuition behind this lemma is challenging: We can let $\mathbb{P}(A_n) = P_n$ and interpret the sum as a series of $P_n$'s. Since the series $P_1 + P_2 + \ldots$ is finite, this implies that 
\[\lim_{n \rightarrow \infty} P_n = 0\]
(but not the converse) and going to zero rather fast such that the series is finite. So, you are working with a sequence of events $A_n$ that are becoming more and more unlikely rather fast. The lemma says that beyond a certain point $n_0$, none of the events $A_n$ will occur almost surely. For the second lemma, we can go as far as we like in the sequence of $A_n$'s, up to any $A_{n_0}$, but beyond that there is always an infinite number of $A_n$'s that occur beyond $A_{n_0}$. 


\section{Random Variables}

Random variables are motivated by the following. When you have a random experiment, the experimenter may not be interested in the specific elementary outcomes. So if you have sample space $\Omega$, you may not be concerned about what $\omega \in \Omega$ shows up, but more interested in some numerical function of the elementary outcome. For example, if you toss a coin 10 times, you're not interested in what sequence in $\{0, 1\}^{10}$ shows up, but you may want to just know how many heads came up. In other words, your interest defines a numerical function $X: \Omega \rightarrow \mathbb{R}$. This is useful, since in many cases the sample space $\Omega$ can be extremely complicated (e.g. the sample space of all weather conditions) and the elementary outcomes also complicated, so you may want to know some simpler aspect (e.g. the temperature). 

The name "random variable" is very misleading. It's not random nor a variable. It is a deterministic function $X: (\Omega, \mathcal{F}, \mathbb{P}) \longrightarrow \mathbb{R}$ that assigns numbers to outcomes. The only source of randomness itself is which $\omega \in \Omega$ is chosen. But we can't just choose any function on $\Omega$; they must satisfy the nice property of measurability. Now, to talk about random variables, recall that the definition of a measurable function $f: (X, \mathcal{A}) \longrightarrow \mathbb{R}$ is one where the preimage of every Borel set $B \in \mathcal{B}(\mathbb{R})$ is in $\mathcal{A}$. With a potential measure $\mu$, this allows us to define the Lebesgue integral of $f$. Note that this is also equivalent to the more easily provable fact that the preimage of every half-interval $(-\infty, t)$ is in $\mathcal{A}$. That is, $f^{-1}((-\infty, t]) \in \mathcal{A}$ for all $t \in \mathbb{R}$. 

\begin{definition}[Random Variable]
A \textbf{random variable} $X$ on probability space $(\Omega, \mathcal{F}, \mathbb{P})$ is an $\mathcal{F}$-measurable function $X: (\Omega, \mathcal{F}, \mathbb{P}) \longrightarrow \mathbb{R}$. That is, for every subset $B \in \mathcal{B}(\mathbb{R})$, its preimage \[X^{-1} (B) = \{\omega \in \Omega \mid X(\omega) \in B\} \in \mathcal{F}\]
\end{definition}

The reason we want $X$ to be $\mathcal{F}$-measurable is because now we can define probabilities on Borel sets $B$ of $\mathbb{R}$ by computing the probabilities of the preimage of $B$, which must be $\mathcal{F}$-measurable. In a way, a random variable "pushes forward" the probability measure $\mathbb{P}$, originally defined on $\mathcal{F}$, to $\mathcal{B}(\mathbb{R})$.  

\begin{definition}[Probability Law of Random Variable $X$]
Let $X$ be a random variable on probability space $(\Omega, \mathcal{F}, \mathbb{P})$. The \textbf{probability law of $X$} is a function $\mathbb{P}_X : \mathcal{B}(\mathbb{R}) \longrightarrow [0, 1]$ defined, for each Borel set $B$ of $\mathbb{R}$, as 
\[\mathbb{P}_X (B) \coloneqq \mathbb{P} \big( X^{-1}(B) \big) = \mathbb{P} \big( \{\omega \in \Omega \mid X(\omega) \in B\} \big)\]
Note that $\mathbb{P}$ refers to the probability measure on $\mathcal{F}$, and $\mathbb{P}_X$ refers to the probability law on $\mathcal{B}(\mathbb{R})$. In shorthand, we can write $\mathbb{P}_X = \mathbb{P} \circ X^{-1}$. By abuse of notation, it is generally written
\[\mathbb{P}(X \in B)\] 
It is important to get used to this notation. Whenever we write $\mathbb{P}(X \ldots)$, we are always working in the probability law of $X$. Furthermore, whatever condition we put within the parentheses describes a measurable set. For example, 
\begin{enumerate}
    \item $\mathbb{P}(X = x)$ describes the probability law of $X$ evaluated on the set $\{x\}$. 
    \item $\mathbb{P}(X \leq x)$ describes the probability law of $X$ evaluated on the set $(-\infty, x]$. 
    \item $\mathbb{P}(Y \leq y)$ describes the probability law of $Y$ evaluated on the set $(-\infty, y]$.
    \item $\mathbb{P}(a \leq Y < b)$ describes the probability law of $Y$ evaluated on the set $[a, b)$. 
    \item $\mathbb{P}(Z \in \mathbb{Q})$ describes the probability law of $Z$ evaluated on the set $\mathbb{Q}$. 
\end{enumerate}
\end{definition}

Note that if we are working in a discrete probability space $\Omega$, then we can simply take the $\sigma$-algebra to be $2^\Omega$, and so we can take any function on $\Omega$ as a random variable since its preimage will always be in $2^\Omega$. 

\begin{example}[Coin Flip]
We start off with a coin flip. Let $\Omega$ denote the set of all conceivable futures of this coin flip: all ways it can fly, spin, or bounce before coming to rest. Then, we can define a random variable $X$ that assigns $0$ to all outcomes landing on tails and $1$ to all outcomes landing on heads. For example, there could be two trajectories $\omega_1, \omega_2 \in \Omega$ that the coin could take before landing on heads. Then, $X(\omega_1) = X(\omega_2) = 1$. Then, $\{1\} \in \mathcal{B}(\mathbb{R})$, and 
\[\mathbb{P}[X = 1] = \mathbb{P}[X^{-1}(\{1\})] = \mathbb{P}[\{\omega \in \Omega \mid X(\omega) = 1\}] = \frac{1}{2}\]
\end{example}

\begin{theorem}
Given $(\Omega, \mathcal{F}, \mathbb{P})$ and a random variable $X: \Omega \rightarrow \mathbb{R}$, let us define a probability law $\mathbb{P}_X = \mathbb{P} \circ X^{-1}$. Then, 
\[(\mathbb{R}, \mathcal{B}(\mathbb{R}), \mathbb{P}_X)\]
is a probability space. 
\end{theorem}

This theorem is extremely useful, since in practical applications, one does not consider an abstract $\Omega$ and works immediately in $\mathbb{R}$. Once we have determined our numerical values of interest (heads or tails, number of heads, sum of dice rolls) with our random variable $X$, we can just throw away $(\Omega, \mathcal{F}, \mathbb{P})$ and work directly in probability space $(\mathbb{R}, \mathcal{B}(\mathbb{R}), \mathbb{P}_X)$. 

\subsection{Cumulative Distribution Function}

Now, remember that the Borel algebra $\mathcal{B}(\mathbb{R})$ is generated by the semi-infinite intervals of form $(-\infty, t]$ (for all $t \in \mathbb{R}$), which are considered "nice" Borel sets. So, $\mathbb{P}_X( (-\infty, t])$ is well defined for all $t \in \mathbb{R}$. In fact, this has a name. 

\begin{definition}[Cumulative Distribution Function]
Given $(\Omega, \mathcal{F}, \mathbb{P})$ and a random variable $X: \Omega \rightarrow \mathbb{R}$. Then, the \textbf{cumulative distribution function} of $X$ is defined 
\[F_X (x) =\mathbb{P}\big( \{\omega \in \Omega \mid X(\omega) \leq x\} \big)\]
We can also define this with the probability law $\mathbb{P}_X$ as 
\[F_X (x) = \mathbb{P}_X \big( (-\infty, x] \big) \]]
By abuse of notation, we will write the CDF as $P(X \leq x)$. It satisfies the properties: 
\begin{enumerate}
    \item Limits: 
    \[\lim_{x \rightarrow -\infty} F_X (x) = 0 \text{ and } \lim_{x \rightarrow \infty} F_X (x) = 1\]
    \item Monotonicity: 
    \[x \leq y \implies F_X (x) \leq F_X (y)\]
    \item Right-continuity: For all $x \in \mathbb{R}$
    \[\lim_{\epsilon \rightarrow 0^+} F_X (x + \epsilon) = F_X (x)\]
    So, if there are jumps, the hole can exist as the function approaches a value from the left. 
\end{enumerate}
What is remarkable is that any function satisfying these three properties satisfies these 3 properties gives you a viable CDF (and as shown below, completely determines a unique random variable). 
\end{definition}

So if you give me the probability law for all Borel sets of $\mathbb{R}$, then I can easily define the CDF since $(-\infty, x]$ are also Borel sets. It turns out that if we know \textit{just} the CDF, then since the semi-infinite intervals form a generating class of $\mathcal{B}(\mathbb{R})$, it turns out that we can completely define $\mathbb{P}_X$. The proof of the theorem below is a bit more involved, using $\pi$-systems, but it is good to know. 

\begin{theorem}
The CDF $F_X (\cdot)$ uniquely specifies the probability law $\mathbb{P}_X$ for any random variable $X$. 
\end{theorem}

To summarize, given a probability space $(\Omega, \mathcal{F}, \mathbb{P})$, a random variable just pushes a measure onto the measure space $(\mathbb{R}, \mathcal{B}(\mathbb{R}))$. If we only care about the values of the random variable, then we can forget about $\Omega$ and only look at $(\mathbb{R}, \mathcal{B}(\mathbb{R}), \mathbb{P}_X)$. The CDF on $(\mathbb{R}, \mathcal{B}(\mathbb{R}), \mathbb{P}_X)$ will be well defined since semi-finite intervals are also Borel. If I am just given a CDF $F_X (\cdot)$, then this is enough for me to specify a unique probability measure $\mathbb{P}_X$ on $(\mathbb{R}, \mathcal{B}(\mathbb{R}))$. So although $\mathbb{P}_X$ contains the complete description of the random variable $X$, in practice we will use $F_X$ since it also captures all the information of $X$ and it's easier to work with. 

\begin{definition}[Indicator Random Variable]
Given $(\Omega, \mathcal{F}, \mathbb{P})$, let $A \in \mathcal{F}$ be an event. A useful random variable is the \textbf{indicator random variable} $I_A: \Omega \longrightarrow \mathbb{R}$ defined  
\[I_A (\omega) = \begin{cases} 1 & \text{ if } \omega \in A \\ 0 & \text{ if } \omega \not\in A \end{cases}\]
This is a random variable since the preimages of $\emptyset, \{0\}. \{1\}, \{0, 1\}$ are $\emptyset, A^c, A, \Omega$, which are all $\mathcal{F}$-measurable. The CDF of this function will look like a step function 
\[F_{I_A} (x) = \begin{cases} 0 & \text{ if } x < 0  \\ P(A^c) & \text{ if } 0 \leq x < 1 \\ 1 & \text{ if } 1 \leq x \end{cases}\]
\end{definition}

\subsection{Discrete Random Variables}

You classify random variables based on the nature of the measure $\mathbb{P}_X$ induced on the real line. There are only three fundamental types of measures: discrete, continuous and singular random variables. In fact, a result in measure theory called \textit{Lebesgue's Decomposition Theorem} says that every measure on $\mathbb{R}$ are either one of these 3 or mixtures thereof. We are used to the first two; the third one is very bizzare and has little to no practical applications. 

\begin{definition}[Discrete Random Variable]
Given $(\Omega, \mathcal{F}, \mathbb{P})$, let us have a random variable $X$ that induces a probability law on $(\mathbb{R}, \mathcal{B}(\mathbb{R}))$. $X$ is said to be \textbf{discrete} if there exists a countable set $E \subset \mathbb{R}$ s.t. $\mathbb{P}_X (E) = 1$ (i.e. $E$'s preimage has probability measure $1$). Since $E$ is at most countable, we can enumerate it $E = \{e_1, e_2, \ldots\}$, and by countable additivity of disjoint sets, we have 
\[1 = \mathbb{P}_X (E) = P\bigg( \bigcup_{i=1}^\infty \{e_i\} \bigg) = \sum_{i=1}^\infty \mathbb{P}_X (\{e_i\}) = \sum_{i=1}^\infty P(X = e_i)\]
and for any $B \in \mathcal{B}(\mathbb{R})$,  
\[\mathbb{P}_X (B) = \sum_{x \in E \cap B} P(X = x) \]
Therefore, the entire probability measure is determined by the probabilities of the singleton sets $P(X = e_i)$. Therefore, the function 
\[p_X (x) \coloneqq P(X = x)\]
is called the \textbf{probability mass function} of $X$, and we can compute using the Lebesgue integral, which reduces to the summation: 
\[\mathbb{P}_X (B) = \int_B p_X (x) \, d \mathbb{P}_X = \sum_{x \in E \cap B} p_X (x)\]
\end{definition}

Sometimes, the definition of discrete $X$ involves having a countable image in $\mathbb{R}$, but our definition allows us to have some $B \in \mathcal{B}(\mathbb{R})$ where its preimage is not necessarily the sample space $\Omega$, but a smaller subset of measure $1$. What's nice about the discrete random variable is that the probability mass function $p_X$ completely describes its probability law. The CDF of a discrete probability function will look like an increasing series of steps. If we have $E = \{e_1, e_2, e_3, e_4, e_5\}$, its CDF would look like: 
\begin{center}
    \includegraphics[scale=0.25]{Discrete_CDF.jpg}
\end{center}
If $E$ was countable, then it would have countably infinite discontinuities. Now we'll give some examples of discrete random variables, and in here we'll completely ignore the sample space $\Omega$, since once we have a random variable $X$, we can just work in $(\mathbb{R}, \mathcal{B}(\mathbb{R}), \mathbb{P}_X)$. Remember that we will write $P(X = x)$ as shorthand for $\mathbb{P}_X (\{x\})$. 

\begin{example}[Bernoulli Random Variable]
Given whatever sample space, a Bernoulli random variable $X: \Omega \longrightarrow \mathbb{R}$, denoted $X \sim \mathrm{Bernoulli}(p)$, has $E = \{0, 1\}$ and a PMF defined 
\[p_X (x) = \begin{cases} 1 - p & \text{ if } x = 0 \\
p & \text{ if } x = 1 \end{cases}\]
This PMF completely defines the probability law $\mathbb{P}_X$ on $(\mathbb{R}, \mathcal{B}(\mathbb{R}), \mathbb{P}_X)$, and we can see that $\mathbb{P}_X (E) = \mathbb{P}_X (\{0\}) + \mathbb{P}_X (\{1\}) = P(X = 0) + P(X = 1) = 1$. 
\end{example}

\begin{example}[Uniform Random Variable]
Given a finite set $E = \{e_i\}_{i=1}^n \subset \mathbb{R}$, we define the PMF as 
\[p_X (e_i) = \mathbb{P}(X = e_i) = \frac{1}{n} \; \forall i = 1, 2, \ldots n\]
which induces the probability measure $\mathbb{P}_X (B) = \sum_{x \in E \cap B} p_X (x)$. 
\end{example}

The Bernoulli RV leads to the geometric and binomial random variables. 

\begin{example}[Geometric Random Variable]
Given $E = \mathbb{N}$, we can define the PMF associated with random varibale $X \sim \mathrm{Geometric}(p)$ as 
\[p_X (k) =\mathbb{P}(X = k) = (1 - p)^{k-1} p \text{ for } k \in \mathbb{N}, \; p \in [0, 1]\]
which induces the probability measure $\mathbb{P}_X (B) = \sum_{x \in E \cap B} p_X (x)$. We can interpret this as the number of times you have to (independently) toss a $p$-coin (probability of heads is $p$) until you get a heads. 
\end{example}

\begin{example}[Binomial Random Variable]
We let $E = \mathbb{N}_0$ and define the PMF associated with random variable $X \sim \mathrm{Binomial}(n, p)$ as 
\[p_X (k) = \mathbb{P}(X = k) = {{n}\choose{k}} p^k (1 - p)^{n - k} \text{ for } k \in E, p \in [0, 1]\]
We can interpret this as the number of heads occurring in a sequence of $n$ independent tosses of a $p$-coin. 
\end{example}

\begin{example}[Poisson Random Variable]
We let $E = \mathbb{N}_0$ and define the PMF of $X \sim \mathrm{Poisson}(\lambda)$ as 
\[p_X (k) = \frac{e^{-\lambda} \lambda^k}{k!} \text{ for } k \in E, \; \lambda > 0\]
\end{example}

\subsection{Simple Random Variables}

A slight generalization of a discrete random variable is a simple random variable. Recall that the indicator random variable is a function $\mathbb{I}_A: \Omega \rightarrow \mathbb{R}$ defined 
\[\mathbb{I}_A (\omega) \coloneqq \begin{cases} 1 & \text{ if } \omega \in A \\
0 & \text{ if else } \end{cases}\]
As simple random variable generalizes this into multiple sets that form a partition of $\Omega$. It is analogous to a simple function, introduced in measure theory. 

\begin{definition}[Simple Random Variable]
Let $\{A_i\}_i$ form a partition of probability space $\Omega$. A \textbf{simple random variable} $X$ is a random variable of the form 
\[X (\omega) = \sum_{i} a_i \mathbb{I}_{A_i} (\omega)\]
that assigns value $a_i$ if the input $\omega \in A_i$. 
\end{definition}

\subsection{Continuous Random Variables}

\begin{definition}[Absolutely Continuous Measures]
Let $\mu, \nu$ be measures defined on $(\Omega, \mathcal{F})$. We say that $\nu$ is \textbf{absolutely continuous} w.r.t. $\mu$ if for every $N \in \mathcal{F}$ s.t. $\mu(N) = 0$, we have $\nu(N) = 0$. 
\end{definition}

\begin{definition}[Continuous Random Variable]
A random variable $X$ is \textbf{continuous} if its induced measure $\mathbb{P}_X: (\mathbb{R}, \mathcal{B}(\mathbb{R})) \rightarrow [0, 1]$ is absolutely continuous w.r.t. the Lebesgue measure $\lambda: (\mathbb{R}, \mathcal{B}(\mathbb{R})) \rightarrow \mathbb{R}$, i.e. if for every Borel set $N$ of Lebesgue measure $0$, we have $\mathbb{P}_X (N) = 0$ also. 
\end{definition}

A common misconception is that a random variable $X$ is continuous if the induced measure on every singleton set in $\mathcal{B}(R)$ is $0$, i.e. $\mathbb{P}_X (\{x\}) = 0$ for all $x \in \mathbb{R}$. The definition above implies this since the Lebesgue measure of every singleton set is $0$. 

We introduce a theorem that is useful to know, but we won't prove it. 

\begin{theorem}[Radon-Nikodym Theorem (Special Case)]
Let $X$ be a continuous random variable. Then, there exists a nonnegative measurable function $f_X : \mathbb{R} \longrightarrow [0, \infty)$ s.t. for any $B \in \mathcal{B}(\mathbb{R})$, we have 
\[\mathbb{P}_X (B) = \int_B f_X \, d\lambda\]
where the above is the Lebesgue integral. Note that we must define using the Lebesgue integral because Riemann integral is not compatible with any Borel set. $f_X$ is called the \textbf{probability density function}, aka \textbf{PDF}. Furthermore, we can get $f_X$ from $\mathbb{P}_X$ by taking the \textbf{Radon-Nikodym derivative} (which we will not define now)
\[f_X = \frac{d \mathbb{P}_X}{d \lambda}\]
which basically says that if we have a set of very small Lebesgue measure $d \lambda$ tending to $0$, then its probability measure $\mathbb{P}_X$ will also be very small, and the infinitesimal ratio of these two measures on an arbitrarily small set is $f_X$. Also, note that the integral does not change if the value of $f$ changes on sets of Lebesgue measure $0$, and so there is no unique PDF describing $\mathbb{P}_X$. It is unique up to sets of Lebesgue measure $0$, so when we refer to such a PDF $f_X$, we are really talking about an equivalence class of functions. 
\end{theorem}

This theorem guarantees the existence of some $f_X$ that completely describes the probability law $P_X$! Take a special case of when $B = (-\infty, x])$, and we can define the CDF as 
\[F_X (x) = P_X ((-\infty, x]) = \int_{(-\infty, x]} f_X \, d\lambda\]
If the set of integration is an interval (and the function is continuous a.e.), then the Lebesgue integral and Riemann integral coincides, and we get the familiar formula 
\[F_X (x) = \int_{-\infty}^x f_X (t)\,dt\]
and we can differentiate it to get back the PDF $f_X$ (or more accurately, some function that agrees with $f_X$ a.e.). We can show that the CDF of a continuous random variable $X$
\begin{enumerate}
    \item is absolutely continuous, and 
    \item is differentiable almost everywhere, which means that its PDF will be defined almost everywhere (and we can fill in the undefined points however we want). 
\end{enumerate}
Note that the PDF $f_X$ itself has no interpretation as a probability (indeed, we can change its value at a countable number of points to anything we want). It is only when we integrate it over some Borel set that gives us a probability. 

\begin{example}[Uniform Random Variable]
Let us define the uniform probability measure $P_X$ on $(\mathbb{R}, \mathcal{B}(\mathbb{R}))$ with the CDF 
\[F_X = \begin{cases} 0 & \text{ if } x < 0 \\
x & \text{ if } 0 \leq x \leq 1 \\
1 & \text{ if } 1 < x \end{cases}\]
It is differentiable almost everywhere except for at the two points $x = 0$ and $x = 1$. Therefore, the PDF $f_X$ is defined for all real numbers except $x = 0$ and $x = 1$. But it doesn't matter: we can assign any value $f_X$ we want on $0$ and $1$ since it won't affect the integral of it. In this example, we just set 
\[f_X = \begin{cases} 1 & \text{ if } 0 \leq x \leq 1 \\
0 & \text{ if else} \end{cases} \]
\end{example}

\begin{example}[Exponential Random Variable]
The exponential random variable has the following CDF: 
\[F_X (x) = \begin{cases} 1 - e^{-\lambda x} & \text{ if } x \geq 0 \\ 0 & \text{ if } x < 0 \end{cases} \text{ for } \lambda > 0\]
which is differentiable everywhere except at $x = 0$. Differentiating it (and assigning a convenient value at $x = 0$ $f(0) = \lambda$) gives the PDF 
\[f_X (x) = \begin{cases} \lambda e^{-\lambda x} & \text{ if } x \geq 0 \\ 0 & \text{ if else} \end{cases}\]
The exponential random variable has the \textit{memoryless property}. 
\end{example}

\begin{definition}[Memoryless Property]
A random variable $X$ has the \textbf{memoryless property} if it satisfies for all $t, s \geq 0$ 
\[\mathbb{P}(X > s + t \mid x > t) = \mathbb{P}(X > s)\]
which is just abuse of notation for the following: We know that $(t, \infty)$, $(s, \infty)$, and $(s + t, \infty)$ are all in $\mathcal{B}(\mathbb{R})$ and so they are events. So it really translates to the probability of an outcome landing in $(s + t, \infty)$ given that it lands in $(t, \infty)$ is equal the probability of it landing in $(s, \infty)$. 
\[\mathbb{P}_X \big( (s + t, \infty) \mid (t, \infty) \big) = \frac{\mathbb{P}_X \big( (s + t, \infty) \cap (t, \infty) \big)}{\mathbb{P}_X \big( (t, \infty) \big)} = \frac{\mathbb{P}_X \big( (s + t, \infty) \big)}{\mathbb{P}_X \big( (t, \infty) \big)} = \mathbb{P}_X \big( (s, \infty) \big)\]
\end{definition}

The exponential random variable is memoryless because the LHS just reduces to 
\[\frac{\mathbb{P}_X \big( (s + t, \infty) \big)}{\mathbb{P}_X \big( (t, \infty) \big)} = \frac{1 - F_X (s + t)}{1 - F_X (t)} = \frac{e^{-\lambda(s + t)}}{e^{-\lambda t}} = e^{-\lambda s} = 1 - F_X (s) = \mathbb{P}_X \big( (s, \infty) \big) \]
In fact, the only continuous random variable having the memoryless property is the exponential random variable. 

\begin{example}[Gaussian Random Variable]
The PDF is easier to specify for the Gaussian, so we define the Gaussian RV as having PDF 
\[f_X (x) = \frac{1}{\sigma \sqrt{2 \pi}} \exp \bigg( -\frac{(x - \mu)^2}{2 \sigma^2} \bigg) \text{ for } \mu \in \mathbb{R}, \sigma > 0\]
Note that this PDF decreases very quickly as we get further from $\mu$. The CDF cannot be written in closed form, and we call the CDF of the standard Gaussian the \textbf{error function}: 
\[\mathrm{Erf}(x) = F_X (x) = \int_{-\infty}^x \frac{1}{\sqrt{2 \pi}} e^{- t^2 / 2} \, dt\]
\end{example}

\begin{example}[Cauchy Random Variable (Standardized)]
The Cauchy random variable gives the PDF 
\[f_X (x) = \frac{1}{\pi} \frac{1}{1 + x^2} \text{ for } x \in \mathbb{R}\]
Integrating this gives the inverse tangent, which after scaling it down by $\pi$ satisfies the conditions of the CDF. Note that the Cauchy distribution falls off much more slowly around the mean (at a rate of $\frac{1}{1 + x^2}$, like a power law) than the Gaussian (which is even \textit{faster} than an exponential, it is at the rate of $e^{-x^2}$). If such a PDF falls off at a slow rate, like a power law, then this is called a \textit{heavy-tailed random variable}. 
\end{example}

\begin{example}[Gamma Random Variable]
The PDF associated with random variable $X \sim \mathrm{Gamma}(n, \lambda)$ is defined 
\[f_X(x) = \frac{\lambda^n x^{n-1}}{\Gamma(n)} e^{-\lambda x} \text{ for } x \geq 0\]
where $\Gamma$ is the gamma function, which is an extension of the factorial function to the domain of complex numbers. 
\[\Gamma(x) \coloneqq \int_{0}^\infty z^{x-1} e^{-z}\, dz, \;\;\;\;\; \text{Re}(x) > 0\]
\end{example}

\begin{example}[Beta Random Variable]
The PDF associated with random variable $X \sim \mathrm{Beta}(\alpha, \beta)$, for positive reals $\alpha, \beta$, is defined 
\[f_X (x) \equiv \frac{x^{\alpha-1} \,(1-x)^{\beta-1}}{B(\alpha, \beta)}, \text{ where } B(\alpha, \beta) \equiv \frac{\Gamma(\alpha) \Gamma(\beta)}{\Gamma(\alpha + \beta)}\]
and $\Gamma$ is the Gamma function. 
\end{example}

\subsubsection{Summary of Discrete and Continuous Random Variables}
To summarize, once we have a random variable $X: \Omega \rightarrow \mathbb{R}$, we can throw away the sample space and work in $(\mathbb{R}, \mathcal{B}(\mathbb{R}), \mathbb{P}_X)$ with the induced measure $\mathbb{P}_X$. 
\begin{enumerate}
    \item If $X$ is discrete, then let there be some at most countable set $E = \{e_i\}$ where $P(E) = 1$. it turns out that $\mathbb{P}_X$ can be completely defined by a probability mass function $p_X : \mathbb{R} \rightarrow \mathbb{R}$ defined 
    \[p_X (x) = \mathbb{P}_X (\{x\}).\] 
    Given that we have this PMF , we can define $\mathbb{P}_X$ as such: Given any Borel $B \in \mathcal{B}(\mathbb{R})$, 
    \[\mathbb{P}_X (B) = \sum_{x \in E \cap B} p_X (x)\]

    \item If $X$ is continuous, then the Radon-Nikodym Theorem asserts the existence of a nonnegative probability density function $f_X$ that completely describes the probability law $\mathbb{P}_X$. Given that we have this PDF, we can then define $\mathbb{P}_X$ as such: Given any Borel $B \in \mathcal{B}(\mathbb{R})$, 
    \[\mathbb{P}_X (B) = \int_B f_X \, d\lambda\]
\end{enumerate}
The function that characterizes the probability measure $\mathbb{P}_X$, whether it's the PMF $p_X$ or the PDF $f_X$, is called the \textbf{probability distribution} of $X$. 

\begin{theorem}[$\sigma$-Algebra Generated by Random Variable $X$]
Given probability space $(\Omega, \mathcal{F}, \mathbb{P})$ and a random variable $X$, $P_X$ is a probability measure on $(\mathbb{R}, \mathcal{B}(\mathbb{R}))$. Now, it turns out that the collection of all preimages of Borel sets under $X$ forms a $\sigma$-algebra on $\Omega$. We call it 
\[\sigma(X) \coloneqq \big\{ A \subset \Omega \mid A = X^{-1}(B) \text{ for some } B \in \mathcal{B}(\mathbb{R}) \big\}\]
which is a $\sigma$-algebra of $\Omega$. Since $X$ is a measurable function, every $X^{-1}(B)$ is $\mathcal{F}$-measurable, and so $\sigma(X)$ is a sub-$\sigma$-algebra of $\mathcal{F}$. 
\end{theorem}

This allows us to "simplify" the $\sigma$-algebra $\mathcal{F}$ to the scope of the random variable. That is, let $\Omega$ be the sample space of all trajectories of a coin flip before it comes to rest. If we are just looking at whether it is heads or tails, we can define $X$ to have image $\{0, 1\}$. Then, $\sigma(X)$ will be a sub-$\sigma$-algebra of $\mathcal{F}$ that looks at only the four subsets $\emptyset, \Omega$, the set of all trajectories landing heads, and the set of all trajectories landing tails. This simplifies $\mathcal{F}$ to a scope that we are interested in. 


\section{Several Random Variables}

Now when we consider several random variables, they will all be defined on the same probability space. Given two random variables $X$ and $Y$ on $(\Omega, \mathcal{F}, \mathbb{P})$, they will each induce a probability law $\mathbb{P}_X$ and $\mathbb{P}_Y$ which completely characterizes them. Note that it is the same underlying randomness that is feeding these random variables, and so if I know some information about the value of $X$, then we know something about outcome $\omega$, which can be used to find something about the value of $Y$. To capture this, we can imagine the map $(X, Y) : \Omega \longrightarrow \mathbb{R}^2$ defined $(X, Y)(\omega) \coloneqq (X(\omega), Y(\omega))$. And just like how $X$ induces a measure $P_X$ onto $\mathbb{R}$, we can imagine $(X, Y)$ inducing a measure onto $\mathcal{B}(\mathbb{R}^2)$, which can be generated by all semi-infinite rectangles $(-\infty, x] \times (-\infty, y]$. Ideally, we would want to put a measure $\mathbb{P}_{X, Y}$ on $\mathbb{R}^2$ s.t. 
\[\mathbb{P}_{X, Y}(B) \coloneqq \mathbb{P}((X, Y)^{-1}(B))\]
where $(X, Y)^{-1}(B) = \{ \omega \in \Omega \mid (X(\omega), Y(\omega)) \in B\}$ denotes the preimage of $(X, Y)$. But is $(X, Y)^{-1}(B)$ $\mathcal{F}$-measurable? It turns out that it is. 

\begin{theorem}
Let $f: (X, \mathcal{A}, \mu) \longrightarrow \mathbb{R}^n$ have component functions $f_1, f_2, \ldots, f_n$. Then, $f$ is measurable (i.e. $f^{-1} (B) \in \mathcal{A}$ for all $B \in \mathcal{B}(\mathbb{R}^n)$) if and only if all of its component functions are measurable (i.e. $f_i^{-1} (B) \in \mathcal{A}$ for all $B \in \mathcal{B}(\mathbb{R}^n)$). 
\end{theorem}

From the theorem above, I have a probability law $\mathbb{P}_{X, Y}$ on all Borel sets of $\mathbb{R}^2$, making $(\mathbb{R}^2, \mathcal{B}(\mathbb{R}^2), \mathbb{P}_{X, Y})$ a probability space. Now, since $X$ and $Y$ are both random variables dependent on the same $\omega \in \Omega$, we could expect certain "combinations" of $X$ and $Y$ to be more probable than other combinations. 

\begin{definition}[Joint Probability Law]
Given two random variables $X, Y$ on $(\Omega, \mathcal{F}, \mathbb{P})$, the \textbf{joint random variable} $(X, Y): \Omega \longrightarrow \mathbb{R}^2$ is a measurable function defined 
\[(X, Y) (\omega) \coloneqq (X(\omega), Y(\omega))\]
which induces a \textbf{joint probability law} $\mathbb{P}_{X, Y}: \mathcal{B}(\mathbb{R}^2) \longrightarrow [0, 1]$ defined 
\[\mathbb{P}_{X, Y}(B) \coloneqq \mathbb{P}((X, Y)^{-1}(B)) \; \forall B \in \mathcal{B}(\mathbb{R})\]
of $X, Y$. This law captures everything there is about the interdependence of $X$ and $Y$. 
\end{definition}

Given joint probability law $\mathbb{P}_{X, Y}$, we can get the probability laws of $X$ and $Y$ separately. For example, we can take a specific Borel set of $\mathbb{R}$ representing the outcomes of $X$ and look at every single combination of it with every $Y$. But knowing $\mathbb{P}_X$ and $\mathbb{P}_Y$ is not enough to know the joint $\mathbb{P}_{X, Y}$. 

\begin{definition}[Marginal Probability Law]
Given a joint probability law $\mathbb{P}_{X, Y}$ of $X, Y$, we can get the \textbf{marginal probability law} of $X$ by feeding in Borel sets of form $B \times \mathbb{R} \in \mathcal{B}(\mathbb{R}^2)$. 
\[\mathbb{P}_X (B) = \mathbb{P}_{X, Y} (B \times \mathbb{R})\]
and the marginal probability law of $Y$ as 
\[\mathbb{P}_Y (B) = \mathbb{P}_{X, Y} (\mathbb{R} \times B)\]
\end{definition}

\begin{definition}[Joint Cumulative Distribution Function]
Since sets of the form $(-\infty, x] \times (-\infty, y]$ are Borel in $\mathbb{R}^2$, the \textbf{joint cumulative distribution function} 
\begin{align*}
    F_{X, Y} & \coloneqq \mathbb{P}_{X, Y} \big( (-\infty, x] \times (-\infty, y] \big) \\
    & = \mathbb{P} \big( \{\omega \mid X(\omega) \leq x\} \cap \{ \omega \mid Y(\omega) \leq y\} \big)
\end{align*}
is well-defined. By abuse of notation, we will write $F_{X, Y} (x, y) = \mathbb{P}(X \leq x, Y \leq y)$. The marginal CDFs are defined 
\begin{align*}
    F_X (x) & \coloneqq \mathbb{P}_{X, Y} ((-\infty, x) \times \mathbb{R}) \\
    F_Y (y) & \coloneqq \mathbb{P}_{X, Y} (\mathbb{R} \times (-\infty, y))
\end{align*}
\end{definition}

\begin{lemma}[Properties of Joint CDF]
Some common properties of the joint CDF are as follows: 
\begin{enumerate}
    \item Limits. 
    \[\lim_{(x, y) \rightarrow (+\infty, +\infty)} F_{X, Y} (x, y) = 1 \text{ and } \lim_{(x, y) \rightarrow (-\infty, -\infty)} F_{X, Y} (x, y) = 0\]
    \item Monotonicity. 
    \[x_1 \leq x_2, \; y_1 \leq y_2 \implies F_{X, Y} (x_1, y_1) \leq F_{X, Y}(x_2, y_2)\]
    \item Continuity from above. 
    \[\lim_{\epsilon \rightarrow 0^+} F_{X, Y} (x + \epsilon, y + \epsilon) = F_{X, Y} (x, y) \text{ for all } x, y \in \mathbb{R}\]
    \item Maringal CDFs. 
    \[\lim_{y \rightarrow \infty} F_{X, Y} (x, y) = F_X (x), \;\;\;\; \lim_{x \rightarrow \infty} F_{X, Y} (x, y) = F_Y (y)\]
\end{enumerate}
\end{lemma}

\subsection{Independent Random Variables}

\begin{definition}[Independent Random Variables]
Two random variables $X, Y$ are \textbf{independent} if $\sigma(X)$ and $\sigma(Y)$ are independent $\sigma$-algebras. That is, for any Borel sets $B_1, B_2 \in \mathcal{B}(\mathbb{R})$, the events $X^{-1}(B_1)$ and $Y^{-1}(B_2)$ are independent: 
\[\mathbb{P}\big[ X^{-1}(B_1) \cap Y^{-1}(B_2) \big] = \mathbb{P}(X^{-1}(B_1)) \, \mathbb{P}(Y^{-1}(B_2))\]
or by abusing notation, 
\[\mathbb{P}(X \in B_1, Y \in B_2) = \mathbb{P}(X \in B_1) \, \mathbb{P}(Y \in B_2)\]
\end{definition}

If $X, Y$ are independent, then we can say something about the CDFs 
\[F_{X, Y} (x, y) = F_X (x) \, F_Y (y)\]
In fact, we can say something stronger. 

\begin{theorem}
$X$ and $Y$ are independent RVs if and only if 
\[F_{X, Y} (x, y) = F_X (x) \, F_Y (y)\]
\end{theorem}

Moving onto multiple variables, we can define that $X_1, X_2, \ldots, X_n$ are independent RVs if $\sigma(X_1), \ldots, \sigma(X_n)$ are independent $\sigma$-algebras. 

\subsection{Joint Discrete Random Variables}

\begin{definition}[Joint PMF]
Given discrete random variables $X$ and $Y$, let their countable images be denoted $E_X, E_Y \subset \mathbb{R}$. Then, $E_X \times E_Y$ is also countable, and so the joint random variable $(X, Y)$ is also discrete. This means that we can write for some Borel $B$ of $\mathbb{R}^2$, 
\[\mathbb{P}_{X, Y} (B) = \sum_{(x, y) \in (E_X \times E_Y) \cap B} \mathbb{P}_{X, Y} (\{(x, y)\})\]
and we can define the PMF as $p_{X, Y} (x, y) \coloneqq \mathbb{P}_{X, Y} (\{(x, y)\})$. By abuse of notation, we write $p_{X, Y} (x, y) = \mathbb{P} (X = x, Y = y)$ and write 
\[\mathbb{P}_{X, Y} (B) = \sum_{(x, y) \in (E_X \times E_Y) \cap B} \mathbb{P} (X = x, Y = y)\]
If you give me a joint PMF $p_{X, Y}$, by the definition above this determines the entire probability law of $\mathbb{P}_{X, Y}$. 
\end{definition} 

\begin{definition}[Conditional PMF]
Let $X, Y$ be discrete random variables on $(\Omega, \mathcal{F}, \mathbb{P})$. The \textbf{conditional PMF} of $X$ given $Y = y$ is defined 
\[p_{X \mid Y} (x \mid y) \coloneqq \frac{p_{X, Y} (x, y)}{p_Y (y)} = \frac{\mathbb{P}_{X, Y} (\{x, y\})}{\mathbb{P}_Y (\{y\})}\]
and again by abuse of notation, we can simply write 
\[\mathbb{P}(X = x \mid Y = y) \coloneqq \frac{\mathbb{P}(X = x, Y = y)}{\mathbb{P}(Y = y)}\]
\end{definition}

\begin{theorem}[TFAE]
Let $X$ and $Y$ be discrete random variables. Then, the following are equivalent: 
\begin{enumerate}
    \item $X$ and $Y$ are independent. 
    \item For all $x, y \in \mathbb{R}$, the events $\{X = x\}$ (aka $X^{-1} (\{x\})$) and $\{Y = y\}$ (aka $Y^{-1} (\{y\})$) are independent. That is, 
    \[\mathbb{P} \big[ X^{-1}(\{x\}) \cap Y^{-1}(\{y\}) \big] = \mathbb{P}(X^{-1}(\{x\})) \, \mathbb{P}(Y^{-1}(\{y\}))\]
    \item For all $x, y \in \mathbb{R}$, $p_{X, Y} (x, y) = p_X (x) \cdot p_Y (y)$. 
    \item For all $x, y \in \mathbb{R}$ such that $p_Y (y) > 0$, we have $p_{X \mid Y}(x \mid y) = p_X (x)$. 
\end{enumerate}
\end{theorem}

\subsection{Joint Continuous Random Variables}

\begin{definition}
$X$ and $Y$ are jointly continuous if the joint law $\mathbb{P}_{X, Y}$ is absolutely continuous w.r.t. the Lebesgue measure on $\mathbb{R}^2$ (i.e. a Borel set of Lebesgue measure $0$ must have $P_{X, Y} = 0$ also). 
\end{definition}

However, $X$ and $Y$ continuous does not always imply that $(X, Y)$ are jointly continuous! If we have $X \sim \mathcal{N}(0, 1)$ and $Y = 2 X \sim \mathcal{N}(0, 4)$. Jointly continuous allows us to define a PDF on it. 

\begin{theorem}[Radon-Nikodym Theorem]
If $X$ and $Y$ are jointly continuous RVs, then there exists a measurable $f_{X, Y} : \mathbb{R}^2 \longrightarrow [0, \infty)$ s.t. for any $B \in \mathcal{B}(\mathbb{R}^2)$, we have 
\[\mathbb{P}_{X, Y} (B) = \int_B f_{X, Y} \, d\lambda\]
\end{theorem}

The Radon-Nikodym Theorem guarantees the existence of such $f_{X, Y}$. Taking $B = (-\infty, x] \times (-\infty, y]$, we can define the joint CDF as 
\[F_{X, Y} (x, y) = \mathbb{P}(X \leq x, Y \leq y) \coloneqq \mathbb{P}_{X, Y} \big( (-\infty, x] \times (-\infty, y] \bigg) = \int_{-\infty}^x \int_{-\infty}^y f_{X, Y} (s, t) \, dt \,ds\]


\section{Transformation of Random Variables}

In many applications, it happens that we are interested not in the value of the random variable $X$, but a function of it. That is, given a probability space $(\Omega, \mathcal{F}, \mathbb{P})$, let us have a random variable $X: \Omega \rightarrow \mathbb{R}$. We can then define another function $f: \mathbb{R} \rightarrow \mathbb{R}$ and consider the potential random variable $f \circ X : \Omega \rightarrow \mathbb{R}$. We say potential because we don't know yet whether $f \circ X$ is measurable (i.e. the preimage of every Borel set in $\mathbb{R}$ is in $\mathcal{F}$). This condition suffices if $f$ itself is a measurable function, i.e. for every Borel set $B \in \mathcal{B}(\mathbb{R})$, its preimage $f^{-1} (B)$ is Borel in $\mathbb{R}$, and by measurablility of $X$, its preimage under $X$ is $\mathcal{F}$-measurable, making $f \circ X$ a viable random variable. With this new random variable $f \circ X$, we would now like to answer the question: What is the probability law $\mathbb{P}_{f \circ X}$ of $\mathbb{R}$? 

This also works for joint random variables. Given a joint random variable $(X_1, X_2, \ldots X_n): \Omega \rightarrow \mathbb{R}^n$, we can define a measurable function $f: \mathbb{R}^n \longrightarrow \mathbb{R}$ and define the scalar random variable $f \circ (X_1, \ldots X_n)$ on $\Omega$. But again, we want to find what the CDF of this composition. 

\subsection{Maximum/Minimum of Random Variables}

Let $X_1, X_2, \ldots, X_n$ be random variables of $(\Omega, \mathcal{F}, \mathbb{P})$ with joint CDF $F_{X_1 \ldots X_n} (x_1, \ldots, x_n)$. Let $Y_n = \min (X_1, \ldots, X_n)$ and $Z_n = \max(X_1, \ldots, X_n)$. Note that $Y_n$ and $Z_n$ are also functions of $\Omega$ to $\mathbb{R}$. To prove that they are random variables, we just have to prove that $\min$ and $\max$ are measurable functions from $\mathbb{R}^n$ to $\mathbb{R}$, which we can do by proving that the preimage of all semi-infinite interval $(-\infty, x]$ are Borel in $\mathbb{R}^n$. 
\begin{enumerate}
    \item The preimage of $(-\infty, x]$ under $\max$ is just the set of all $n$-vectors whose max is less than $x$, which is just the semi-infinite cuboid $(-\infty, x]^n \subset \mathbb{R}^n$, which is Borel in $\mathbb{R}^n$. 
    \item The preimage of $(-\infty, x]$ under $\min$ is the set of all $n$-vectors whose min is less than $x$, i.e. at least one element must be less than $x$. But this is just the complement of all vectors that have elements all greater than $x$, which is $\mathbb{R}^n \setminus (x, +\infty)^n \subset \mathbb{R}^n$, which is Borel in $\mathbb{R}^n$. 
\end{enumerate}
Now we must determine the CDF of $Y_n$ and $Z_n$. 
\begin{enumerate}
    \item We have 
    \begin{align*}
        F_{Z_n} (z) & = \mathbb{P}(\{ \omega \mid Z_n (\omega) \leq z \}) \\
        & = \mathbb{P}(\{ \omega \mid X_1 (\omega) \leq z, \ldots, X_n (\omega) \leq z\}) \\
        & = F_{X_1 \ldots X_n} (z, \ldots, z)
    \end{align*}
    where the last equality is describes simply the joint CDF of the joint distribution $(X_1, \ldots, X_n)$. If we assume independence of $X_i$'s, it simplifies out to 
    \[\prod_{i} F_{X_i} (z)\]
    and if iid, then we have $[F_{X} (z) ]^n$, where $X$ is the common distribution. 
    \item For $Y_n$, we work with complements again and have 
    \begin{align*}
        F_{Y_n} (y) & = \mathbb{P}(\{ \omega \mid Y_n (\omega) \leq y \}) \\ 
        & = 1 - \mathbb{P}(\{ \omega \mid Y_n (\omega) > y \}) \\
        & = 1 - \mathbb{P}(\{ \omega \mid X_1 (\omega) > y, \ldots X_n (\omega) > y \}) \\
    \end{align*}
    where $\mathbb{P}(\{ \omega \mid X_1 (\omega) > y, \ldots X_n > y \})$ can be calculated from the joint distribution. If we assume independence of $X_i$, it simplifies out to 
    \[1 - \prod_{i} \mathbb{P}(\{\omega \mid X_i(\omega) > y \}) = 1 - \prod_{i} \big( 1 - F_{X_i} (y) \big)\]
    and if iid, then we have $1 - [1 - F_{X} (y)]^n$. 
\end{enumerate}

\begin{example}[Uniforms]
Let $X_1, X_2$ be iid distributed as $\mathrm{Uniform}[0, 1]$, and let $Z = \max(X_1, X_2)$ with $Y = \min(X_1, X_2)$, i.e. $Z$ is the greater of the two and $Y$ is the lesser. We would expect the PDF of $Z$ to have more mass towards $1$ and the PDF of $Y$ to have more mass towards $0$. Our common CDF is 
\[F_{X} (x) = \begin{cases} 0 & \text{ if } x < 0 \\
x & \text{ if } 0 \leq x \leq 1 \\
1 & \text{ if } 1 < x \end{cases}\]
Let's calculate the CDF of $Z$. 
\begin{align*}
    F_{Z} (z) & = \mathbb{P}(\{\omega \mid Z(\omega) \leq z\}) \\
    & = \mathbb{P}(\{ \omega \mid X_1 (\omega) \leq z, X_2 (\omega) \leq z\}) \\
    & = F_{X_1, X_2} (z, z) \\
    & = [F_{X} (z)]^2 = \begin{cases} 0 & \text{ if } x < 0 \\
x^2 & \text{ if } x \in [0, 1] \\
1 & \text{ if } 1 < x \end{cases}
\end{align*}
This CDF is differentiable everywhere except the two points $0$ and $1$, so we can get the PDF to be $f_Z (z) = 2 z$ for $z \in (0, 1)$ and $0$ otherwise. For the values of $f_Z$ at $0$ and $1$, we can fill it in with anything we want (since the measure of these sets are $0$), so we will just defined $f_Z (0) = 0$ and $f_Z(1) = 2$, getting 
\[f_Z (z) = \begin{cases} 2 z & \text{ if } z \in [0, 1] \\
0 & \text{ if else} \end{cases}\]
Let's calculate the CDF of $Y$. 
\begin{align*}
    F_{Y} (y) & = \mathbb{P}(\{ \omega \mid Y(\omega) \leq y\}) \\
    & = 1 - \mathbb{P}(\{ \omega \mid Y(\omega) > y\}) \\
    & = 1 - \mathbb{P}(\{\omega \mid X_1 (\omega) > y, X_2 (\omega) > y \}) \\
    & = 1 - \mathbb{P}(\{\omega \mid X_1 (\omega) > y\}) \, \mathbb{P}(\{ X_2 (\omega) > y \}) \\ 
    & = 1 - [1 - F_X (y)]^2 = \begin{cases} 0 & \text{ if } y < 0 \\
    1 - (1 - y)^2 & \text{ if } y \in [0, 1] \\
    1 & \text{ if } y > 1 \end{cases} 
\end{align*}
and differentiating it (with setting any values of the PDF at the nondifferentiable points $0$ and $1$) gives 
\[f_Y (y) = \begin{cases} 2 - 2y & \text{ if } y \in [0, 1] \\
0 & \text{ if else} \end{cases} \]
\end{example}

\begin{example}[Exponentials]
Let $X_1, X_2, \ldots, X_n$ be independent exponential random variables with parameters $\lambda_1, \ldots, \lambda_n$, respectively (not identical!). Then, for each $X_i$, its CDF is 
\[F_{X_i} (x) = 1 - e^{-\lambda_i x} \text{ for } x \geq 0\]
and let $Y = \min(X_1, \ldots, X_n)$. Then, we have 
\begin{align*}
    F_Y (y) & = 1 - \prod_{i=1}^n [ 1 - F_{X_i} (y)] \\
    & = 1 - \prod_{i=1}^n e^{-\lambda_i x} \\
    & = 1 - e^{- ( \sum_{i=1}^n \lambda_i ) x}
\end{align*}
which is the CDF of an exponential distribution. So, 
\[Y \sim \mathrm{Exponential}(\lambda_1 + \ldots + \lambda_n)\]
This is nice, since the minimum of a bunch of exponentials is an exponential. However, this is not the case for the maximum. 
\end{example}

This has nice practical applications. For example, recall the memoryless property of the exponential, which nicely models radioactive decay. If we have $n$ elements each decaying at some $\mathrm{Exponential}(\lambda_i)$ rate, then we can model the time at which the first alpha particle will emit amongst all $n$ elements will also be an exponential. These processes where the inter-emission times are exponentials are called Poisson process, which we will discuss later. 

\subsection{Sums of Random Variables}

Now given two random variables $X, Y: \Omega \rightarrow \mathbb{R}$ that each push their own probability laws $\mathbb{P}_X, \mathbb{P}_Y$ onto $\mathbb{R}$, their sum $Z = X + Y$ is also a random variable that pushes its own probability law $\mathbb{P}_Z$. We must actually prove that $Z$ is a random variable, which we can do by proving that the preimage of every $(-\infty, x]$ is $\mathcal{F}$-measurable. Equivalently (by complementation), we must prove that the preimage of every $(x, +\infty)$ (that is, all sets of form $\{ \omega \mid Z(\omega) > z\}$) is $\mathcal{F}$-measurable. Now we can write $z$ as the sum of two numbers $z = q + (z - q)$, where $q \in \mathbb{R}$, and say that 
\[\{ \omega \mid Z(\omega) > z\} = \bigcup_{q \in \mathbb{R}} \{ \omega \mid X (\omega) > q , \; Y(\omega) > z - q\}\]
But using the fact that $\mathbb{Q}$ is dense in $\mathbb{R}$, we can turn this from an uncountable union to a countable union and say 
\begin{align*}
    \{ \omega \mid Z(\omega) > z\} & = \bigcup_{q \in \mathbb{Q}} \{ \omega \mid X (\omega) > q , \; Y(\omega) > z - q\} \\
    & = \bigcup_{q \in \mathbb{Q}} \big( \{\omega \mid X(\omega) > q\} \cap \{ \omega \mid Y(\omega) > z - q\} \big) 
\end{align*}
and since I have a countable union of (an intersection of) these $\mathcal{F}$-measurable sets, $\{ \omega \mid Z(\omega) > z\}$ is $\mathcal{F}$-measurable, and we are done. This equation above even gives us a hint of how to compute the CDF of $Z$. 

\begin{theorem}
Given random variables $X_1, X_2, \ldots, X_n$ of probability space $(\Omega, \mathcal{F}, \mathbb{P})$, 
\begin{enumerate}
    \item $X_1 + \ldots + X_n$ is a random variable.
    \item $X_1 \cdot \ldots \cdot X_n$ is a random variable. 
\end{enumerate}
\end{theorem}

For simplicity, we will only consider jointly discrete or jointly continuous random variables. The probability law $\mathbb{P}_Z$ can be confusing to define, since given some Borel set $B \in \mathcal{B}(\mathbb{R})$, we must now look at the preimage under the \textit{sum} $X + Y$. A simpler way to approach this is to consider the joint distribution $X, Y$ and look at its distribution. This is especially simple to consider for discrete random variables. 

\begin{definition}[Sums of Discrete Random Variables]
Take two discrete random variables $X, Y$ with their joint PMF $p_{X, Y} (x, y)$ and their sum $Z = X + Y$. We can see that the PMF of $Z$ is 
\[p_Z (z) = \sum_{(x, y) \,:\, x + y = z} p_{X, Y} (x, y) = \sum_{x \in \mathcal{X}} p_{X, Y} (x, z - x)\]
which by abuse of notation, we denote
\[\mathbb{P}(Z = z) = \sum_{x \in \mathcal{X}} \mathbb{P}(X = x, Y = z - x) \]
The CDF is very simple, since we just have to sum over all $(x, y)$ such that their sum is less than $z$: 
\[F_Z (z) = \sum_{(x, y) \,:\, x + y \leq z} p_{X, Y} (x, y)\]
which by abuse of notation, we write 
\[\mathbb{P}(Z \leq z) = \sum_{(x, y) \,:\, x + y \leq z} \mathbb{P}(X = x, Y = y) \]
If $X$ and $Y$ are independent, then their joint distribution is the product of their singular distributions, and so we have 
\[p_Z (z) = \sum_x p_X (x) \, p_Y (z - x) \coloneqq p_X \ast p_Y\]
where $p_Z = p_X \ast p_Y$ is called the convolution of $p_X$ and $p_Y$. By abuse of notation, 
\[\mathbb{P}(Z = z) = \sum_{x \in \mathcal{X}} \mathbb{P}(X = x) \, \mathbb{P}(Y = z - x) \]
\end{definition}

\begin{example}[Sums of Poisson RVs]
Let $X_1$ and $X_2$ be independent Poisson random variables with parameters $\lambda_1, \lambda_2 > 0$, and let $Z = X_1 + X_2$. The PMF of each $X_i$ is 
\[p_{X_i} (k) = \frac{e^{-\lambda_i} \lambda_i^k}{k!} \text{ for } k \in \mathbb{Z}\]
and taking the convolution gives the PMF of $Z$: 
\begin{align*}
    p_Z (z) & = (p_{X_1} \ast p_{X_2}) (z) \\
    & = \sum_{k=-\infty}^{+\infty} \frac{e^{-\lambda_1} \lambda_1^k}{k!} \cdot \frac{e^{-\lambda_2} \lambda_2^{z - k}}{(z - k)!} \\
    & = \sum_{k=0}^{z} \frac{e^{-\lambda_1} \lambda_1^k}{k!} \cdot \frac{e^{-\lambda_2} \lambda_2^{z - k}}{(z - k)!} \\ 
    & = \frac{e^{-(\lambda_1 + \lambda_2)}}{z!} \sum_{k=0}^z {{z}\choose{k}} \lambda_1^k \lambda_2^{z - k} \\
    & = \frac{e^{-(\lambda_1 + \lambda_2)} (\lambda_1 + \lambda_2)^z}{z!} 
\end{align*}
for $z \in \mathbb{N}_0$, which is the PMF of another Poisson. So, $Z \sim \mathrm{Poisson}(\lambda_1 + \lambda_2)$. 
\end{example}

This has a nice visualization, since the joint distribution of $X$ and $Y$ over $\mathbb{R}^2$ is being "summed up/integrated" over the diagonals of $\mathbb{R}^2$, i.e. the lines where $x + y = z$ for some $z$, sort of like marginalizing over these diagonals. This creates a new "diagonally marginal distribution" $Z$. 

\begin{definition}[Sums of Continuous Random Variables]
Take two continuous random variables $X, Y$ with their joint PDF $f_{X, Y} (x, y)$ and their sum $Z = X + Y$. To calculate the CDF, we must basically integrate the joint PDF over the borel set $\{(x, y) \in \mathbb{R}^2 \mid x + y \leq z\}$. 
\begin{align*}
    \mathbb{P}(Z \leq z) = F_Z (z) & = \int_{(x, y) \,:\, x + y \leq z} f_{X, Y} (x, y) \,dy\,dx \\
    & = \int_{-\infty}^{+\infty} \int_{-\infty}^{z - x} f(x, y) \,dy \,dx
\end{align*}
We can see that the PDF of $Z$ is 
\[f_{Z} (z) = \int_{\mathbb{R}} f_{X, Y} (x, z - x) \, dx\]
If $X$ and $Y$ are independent, then 
\[f_{Z} (z) = \int_{\mathbb{R}} f_{X} (x) \, f_Y (z - x) \,dx \coloneqq f_X \ast f_Y\]
where $f_Z = f_X \ast f_Y$ is the convolution of $f_X$ and $f_Y$. 
\end{definition}

\begin{definition}[Convolution]
Given two functions $f, g: \mathbb{R} \longrightarrow \mathbb{R}$, the \textbf{convolution} of $f$ and $g$ is a new function $f \ast g$ defined  
\[(f \ast g) (t) \coloneqq \int_\mathbb{R} f(t)\, g(t - \tau) \, d \tau\]
\end{definition}

Usually, when we take convolutions, it is not pretty and even for nice distributions like two Gaussians, convolving them is quite complicated. What we can do is transform them (using Laplace, Fourier, etc.) to make calculations easier and more elegant. 

\begin{example}
Let $X_1$ and $X_2$ be independent exponential with parameters $\lambda_1, \lambda_2$, with individual PDFs $f_{X_i} (x) = \lambda_i e^{-\lambda_i x}$ for $x \geq 0$. Let $Z = X_1 + X_2$. Then, 
\begin{align*}
    f_Z (z) = (f_{X_1} \ast f_{X_2})(z) & = \int_{-\infty}^\infty \lambda_1 e^{-\lambda_1 x} \, \lambda_2 e^{-\lambda_2 (z -x)} \, dx \\
    & = \int_{0}^z \lambda_1 e^{-\lambda_1 x} \, \lambda_2 e^{-\lambda_2 (z -x)} \, dx \\ 
    & = \lambda_1 \lambda_2 e^{-\lambda_2 z} \int_0^z e^{(\lambda_2 - \lambda_1) x}\,dx \\
    & = \begin{cases} \frac{\lambda_1 \lambda_2}{\lambda_2 - \lambda_1} \big( e^{-\lambda_1 z} - e^{-\lambda-2 z} \big) & \text{ if } \lambda_1 \neq \lambda_2 \\
    \lambda^2 z e^{-\lambda z} & \text{ if } \lambda_1 = \lambda_2 = \lambda \end{cases} 
\end{align*}
The distribution for when $\mu_1 = \mu_2$ is called the Erlang distribution, which has many applications, but the other case is an ugly form and not studied very well. 
\end{example}

\begin{theorem}[Sums of Discrete Variables]
Assume that $X$ and $Y$ are independent. 
\begin{enumerate}
    \item $X \sim$ Binomial$(n, p)$, $Y \sim$ Binomial$(m, p)$ $\implies X + Y \sim$ Binomial$(n + m, p)$. 
    \item $X \sim$ Poisson$(\lambda)$, $Y \sim$ Poisson$(\gamma)$ $\implies X + Y \sim$ Poisson$(\lambda + \gamma)$. 
    \item If $X_1, ..., X_n$ are Geometric$(p)$, then $X_1 + ... + X_n$ is NB$(n, p)$. 
\end{enumerate}
\end{theorem}

\begin{theorem}[Sums of Densities]
Assume that $X$ and $Y$ are independent. 
\begin{enumerate}
    \item $X \sim$ Normal$(\mu_1, \sigma_1^2)$, $Y \sim$ Normal$(\mu_2, \sigma_2^2)$ $\implies X + Y \sim$ Normal $(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2)$. 
    \item If $X_1, X_2, ..., X_n$ are Exponential$(\lambda)$, then $X_1 + ... + X_n \sim$ Gamma$(n, \lambda)$.
    \item $X \sim$ Gamma$(n, \lambda)$, $Y \sim$ Gamma$(m, \lambda)$ $\implies X + Y \sim$ Gamma$(n + m, \lambda)$. 
    \item $X \sim$ Gamma $(n, \lambda)$, $Y \sim$ Exponential $(\lambda)$ $\implies X + Y \sim$ Gamma$(n+1, \lambda)$. 
\end{enumerate}
\end{theorem}

\subsection{Sum of Random Number of Random Variables}

Now we consider a random variable where the number of terms we are summing is a random variable. Let $\{X_i\}_i$ be a countable sequence of independent random variables with CDF $F_{X_i}$. Let $N$ be a positive integer-valued random variable with PMF $p_N(n) = \mathbb{P}(N = n)$. Assume that $N$ is independent of $X_i$'s. Now, consider the function 
\[S_N \coloneqq \sum_{i=1}^N X_i\]
To interpret this, consider the sample space $\Omega$. We have all $X_i$'s and $N$ defined on the same $\Omega$. Once $\omega \in \Omega$ realizes, the $\{X_i\}$'s will realize as a sequence of numbers, and $N$ will realize as a positive integer. We simply sum them up according to the rule $S_N$, and by this definition, $S_N$ is a real-valued function on $\Omega$. We first have to prove that $S_N$ is a random variable (since we only know that a \textit{fixed} sum of random variables is a random variable), and then we must find the CDF of $S_N$ $\mathbb{P}(S_N \leq x)$. 

First, note that the realization of $N$ partitions the sample space as 
\[\Omega = \bigsqcup_{n = 1}^\infty \{\omega \mid N(\omega) = n\}\]
Once I have this partition, I can invoke the partition rule and write 
\begin{align*}
    \mathbb{P}(S_N \leq x) & = \sum_{k=1}^\infty \mathbb{P}(S_N \leq x \mid N = k) \, \mathbb{P}(N = k) \\
    & = \sum_{k=1}^\infty \mathbb{P}(S_k \leq x \mid N = k) \, \mathbb{P}(N = k) & (\text{conditioned on } N = k) \\
    & = \sum_{k=1}^\infty \mathbb{P}(S_k \leq x) \, \mathbb{P}(N = k) & (N \text{ is indep. of } X_i \text{s})
\end{align*}
where $\mathbb{P}(N = k)$ is known since we know the PMF of $N$, and the CDFs $\mathbb{P}(S_k \leq x)$ can be computed by computing the deterministic sums and computing their CDF. 

\begin{example}
Let $X_i$'s be iid $\mathrm{Exponential}(\lambda)$, and $N \sim \mathrm{Geometric}(p)$. We know that the deterministic sum of iid exponentials gives an Erlang. So, $S_N = \sum_{i=1}^N X_i$, and its CDF is 
\[\mathbb{P}(S_N \leq x) = \sum_{k=1}^\infty \mathbb{P}(S_k \leq x) \, \mathbb{P}(N = k)\]
where $\mathbb{P}(N = k) = (1 - p)^{k - 1} p$. The PDF of the Erlang is 
\[p_{S_k} (x) = \frac{\lambda^n x^{n-1}}{(n - 1)!} e^{-\lambda x}\]
and doing the brute force calculations gives a clean $S_N \sim \mathrm{Exponential}(\lambda p)$. 
\end{example}

\subsection{General Transformations of Random Variables}

Now we will look at more general transformations that are not just minimum, maximum, deterministic sums, or random sums. Let us have a probability space $(\Omega, \mathcal{F}, \mathbb{P})$, a random variable $X: \Omega \rightarrow \mathbb{R}$, and a measurable function $f: \mathbb{R} \rightarrow \mathbb{R}$. Now given that we know the CDF (and therefore distribution) of $X$, we want to find the CDF of random variable $Y = f(X) = f \circ X$ (which we have established as a random variable already due to measurability of $f$): $F_Y (y) = \mathbb{P}(Y \leq y)$, which is just $\mathbb{P}_Y ((-\infty, y])$ (where $\mathbb{P}_Y$ is the probability law on $Y$). But rather than trying to take the preimage of the entire composite random variable $Y$ and calculating $\mathbb{P}\big( Y^{-1}((-\infty, y]) \big)$ under the probability on $\mathcal{F}$, let's just take the preimage one step at a time. Note that $f^{-1} \big( (-\infty, y] \big) = \{x \in \mathbb{R} \mid f(x) \leq y\}$. We can then write the CDF of $Y$ in terms of the probability law of $X$: 
\begin{align*}
    F_Y (y) & = \mathbb{P}_X \big( f^{-1} ((-\infty, y]) \big) \\
    & = \mathbb{P}_X \big( \{x \in \mathbb{R} \mid f(x) \leq y\} \big) \\
    & = \mathbb{P} \big( X^{-1} \circ f^{-1} ((-\infty, y]) \big) 
\end{align*}
Depending on how complicated $f$ is, this may be easy or not, but conceptually, this is no problem. But theoretically, this is as far as we can go. Let's move onto some examples. 

\begin{example}[Chi-Squared Distribution]
Let $X \sim \mathcal{N}(0, 1)$ and $Y = f(X) = X^2$. Note that $X$ takes values in $(-\infty, +\infty)$ and $Y$ in $[0, +\infty)$. Then, we can write 
\begin{align*}
    F_Y (y) & = \mathbb{P}(Y \leq y) \\ 
    & = \mathbb{P}_Y ( (-\infty, y]) \\
    & = \mathbb{P}_Y (  [0, y]) & (\text{range of } Y) \\
    & = \mathbb{P}_X ( f^{-1} ([0, y]) ) & (\text{work in prob. law of } X) \\
    & = \mathbb{P}_X ( [-\sqrt{y}, \sqrt{y}] ) \\
    & = \int_{-\sqrt{y}}^{\sqrt{y}} f_X (x) \,dx 
\end{align*}
Rewriting this in our abuse of notation notation, we have 
\begin{align*}
    F_Y (y) & = \mathbb{P}(Y \leq y) \\
    & = \mathbb{P}(X^2 \leq y) \\
    & = \mathbb{P}( -\sqrt{y} \leq X \leq \sqrt{y}) \\
    & = 2 \mathbb{P}(0 \leq X \leq \sqrt{y}) & (\text{Symmetry of Gaussian})\\
    & = \frac{2}{\sqrt{2} \pi} \int_0^{\sqrt{y}} e^{-x^2 / 2} \,dx 
\end{align*}
and this is clearly differentiable, since it is written like an integral. Doing so gives the PDF
\[f_Y (y) = \frac{1}{\sqrt{2 \pi y}} e^{-y/2} \text{ for } y \geq 0\]
This describes the PDF of a \textbf{Chi-Squared} distribution. 
\end{example}

\begin{example}[Log-Normal Distribution]
Let $X \sim \mathcal{N}(0, 1)$ and $Y = f(X) = e^X$. Note that the range of $f$ is $(0, +\infty)$. So, 
\begin{align*}
    F_Y (y) & = \mathbb{P}(Y \leq y) \\
    & = \mathbb{P}_Y ((-\infty, y]) \\
    & = \mathbb{P}_Y ( (0, y]) \\ 
    & = \mathbb{P}_X ( f^{-1} ((0, y]) ) \\
    & = \mathbb{P}_X ( (-\infty, \ln{y}] ) \\
    & = \int_{-\infty}^{\ln{y}} f_X (x)\,dx 
\end{align*}
Rewriting this in our abuse of notation notation, we have 
\begin{align*}
    F_Y (y) & = \mathbb{P}(e^X \leq y) \\
    & = \mathbb{P}(X \leq \ln(y)) \\ 
    & = \int_{-\infty}^{\ln(y)} \frac{1}{\sqrt{2} \pi} e^{-x^2/ 2} \,dx 
\end{align*}
We can differentiate this to get 
\[f_Y (y) = \frac{1}{y \sqrt{2 \pi}} e^{-\frac{(ln{y})^2}{2}} \text{ for } y \geq 0\]
This describes the PDF of a \textbf{log-normal} distribution. 
\end{example}

\subsubsection{Jacobian Formula}

We now show a more specific formula under more specific assumptions about the transformation. Suppose $X$ is a \textit{continuous} random variable with density $f_X$ and $g: \mathbb{R} \rightarrow \mathbb{R}$ a monotonic differentiable function. Then, the CDF of the random variable $Y = g(X)$ can be written in the probability law of $X$, which can then by written as an integral by invoking the Radon-Nikodym theorem: 
\begin{align*}
    \mathbb{P}(Y \leq y) & = \mathbb{P}_X (f^{-1} ((-\infty, y]) \\
    & = \int_{x \,:\, g(x) \leq y} f_X (x) \,dx
\end{align*}
Note that we can now talk about the actual inverse $g^{-1}$ since differentaible and monotonic implies invertibility. 
\begin{enumerate}
    \item Assuming $g$ is monotonically increasing, we can use the change of variables $x = g^{-1} (t)$ and $g(x) = t \implies g^\prime (x) \,dx = dt$ to get the above integral as 
    \[\int_{-\infty}^{g^{-1} (y)} f_X (x) \,dx = \int_{-\infty}^t \frac{f_X \big( g^{-1} (t) \big)}{g^\prime \big( g^{-1} (t)\big)} \,dt \]
    but since this is simply the CDF of $Y$, the PDF must equal 
    \[f_Y (y) = \frac{f_X (g^{-1} (y) )}{g^\prime (g^{-1} (t))}\]
    \item If $g$ is monotonically decreasing, we get 
    \[f_Y (y) = \frac{f_X (g^{-1} (y) )}{- g^\prime (g^{-1} (t))}\]
\end{enumerate}
In general, we can consider both cases by putting an absolute value 
\[f_Y (y) = \frac{f_X (g^{-1} (y) )}{|g^\prime (g^{-1} (t))|}\]
and $g^\prime (g^\prime (y))$ is the Jacobian, the same one that we use when we perform a change of variables in integration. 

\begin{example}[Log-Normal Revisited]
Given $X \sim \mathcal{N}(0, 1)$ and $Y = e^X$ (which is monotonically increasing), we can simply plug in the formula to get the PDF: 
\[f_Y (y) = \frac{f_X (g^{-1} (y) )}{|g^\prime (g^{-1} (t))|} = \frac{f_X (\ln{y}) }{ | e^{\ln{y}} |} = \frac{1}{\sqrt{2 \pi} y} e^{-(\ln{y})^2 / 2}\]
for $y > 0$. This domain is important since $\ln{y}$ is only defined for $y > 0$. 
\end{example}

\begin{example}
Given $X \sim \mathcal{N}(0, 1)$ and $Y = f(X) = X^2$, we cannot use the formula since $f$ is not monotonic on the range of $X$, which is $(-\infty, +\infty)$.  
\end{example}

\begin{example}
Given $X \sim \mathrm{Exponential}(\lambda)$ and $Y = f(X) = X^2$, it may seem like the formula is not applicable here, but $f$ \textit{is} monotonic on the range of $X$, which is $[0, + \infty)$. 
\end{example}

However, there is much less chance of error by deriving using first principles, so I would recommend using it always rather than these formulas. 

Let's do the $n$-dimensional version of this. Given random variables $X_1, X_2, \ldots, X_n$ iid random variables with joint density $f_{X_1 \ldots X_n} (x_1, \ldots, x_n)$, we define the transformation $g: \mathbb{R}^n \rightarrow \mathbb{R}^n$ as 
\[\begin{bmatrix} Y_1 \\ \vdots \\ Y_N \end{bmatrix} = \begin{bmatrix} g_1 (X_1) \\ \vdots \\ g_n (X_N) \end{bmatrix}\]
Then, the PDF of $Y$ will be 
\begin{align*}
    f_{Y_1 \ldots Y_n} (y_1, \ldots, y_n) & = f_{X_1 \ldots X_n} \big( \mathbf{g}^{-1} (\mathbf{y}) \big) \cdot | \mathbf{J}(\mathbf{y})| \\
    & = f_{X_1 \ldots X_n} \big( g_1^{-1}(y_1), \ldots, g_n^{-1} (y_n) \big) \cdot | \mathbf{J}(\mathbf{y})| \\
\end{align*}
where 
\[\mathbf{J}(y) = \mathrm{det}\begin{pmatrix} 
\frac{\partial x_1}{\partial y_1} & \ldots & \frac{\partial x_n}{\partial y_1} \\
\vdots & \ddots & \vdots \\ 
\frac{\partial x_1}{\partial y_n} & \ldots & \frac{\partial x_n}{\partial y_n} \end{pmatrix}\]


\section{Expectation and Variance}

Recall that given a measure space $(X, \mathcal{A}, \mu)$ and a function $f: X \longrightarrow \mathbb{R}$, we can take some $A \in \mathcal{A}$ and integrate $f$ over it with the Lebesgue integral 
\[\int_A f \, d\mu\]
If $A = X$, then it is conventional not to write it in the integral at all (so it looks like an indefinite integral). 

\begin{definition}[Expectation]
Given a probability space $(\Omega, \mathcal{F}, \mathbb{P}$ and a random variable $X: \Omega \longrightarrow \mathbb{R}$, the \textbf{expectation} of $X$ is defined 
\[\mathbb{E}[X] \coloneqq \int_\Omega X \, d\mathbb{P}\] 
\end{definition}

\begin{definition}[Expectation of Discrete RV]
If $X$ is a discrete random variable \textit{that takes positive values}, then let $E = \{e_1, e_2, \ldots\}$ denote the set where $\mathbb{P}_X(E) = 1$, and let $E_i = X^{-1} (\{e_i\}) \subset \Omega$. Then, we can see that since $X$ is constantly $e_i$ on $E_i$, 
\[\int_{E_i} X \, d\mathbb{P} = e_i \cdot \mathbb{P}(E_i) = e_i \cdot \mathbb{P}_X (\{e_i\}) = e_i \cdot \mathbb{P}(X = e_i)\]
which implies 
\[\mathbb{E}[X] = \int_\Omega X \, d\mathbb{P} = \sum_{i=1}^\infty \int_{E_i} X \, d\mathbb{P} = \sum_{i=1}^\infty e_i \cdot \mathbb{P}(X = e_i) \] 
If $X$ is discrete RV possibly taking negative values, then let $X = X^+ - X^-$, where $X^+ = \max(X, 0)$ and $X^- = - \min(X, 0)$. Then, we can compute 
\[\mathbb{E}[X] = \mathbb{E}[X^+] - \mathbb{E}[X^-]\]
which is well-defined as long as we don't have "$\infty - \infty$."
\end{definition}

Note that the reason why expectations of the form $\infty - \infty$ are indeterminate is because of the Riemann rearrangement theorem. 

\begin{theorem}[Riemann's Rearragenement Theorem]
Given a series $\sum a_n$ that is conditionally convergent (i.e. converges but not absolutely convergent), the terms can be arranged so that the new series converges to an arbitrary real number, or diverges. 
\end{theorem}

\begin{lemma}[Properties of Expectation]
Let $X$ and $Y$ be random variables with finite expectations. 
\begin{enumerate}
    \item Monotonicity: If $g(x) \geq h(x)$ for all $x \in \mathbb{R}$, then 
    \[\mathbb{E}[g(X)] \geq \mathbb{E}[h(X)]\]
    \item Linearity: For all $a, b, c \in \mathbb{R}$, 
    \[\mathbb{E}[a X + b Y + c] = a \mathbb{E}[X] + b \mathbb{E}[Y] + c\]
\end{enumerate}
\end{lemma}

We now show a widely-used, but nontrivial, theorem. 

\begin{theorem}[Expectation of Independent Events]
Given independent RVs $X$ and $Y$, 
\[\mathbb{E}[X Y] = \mathbb{E}[X] \, \mathbb{E}[Y]\]
\end{theorem}
\begin{proof}
We show only for simple random variables which will give us a start in proving for all random variables in full generality. Let $X$ and $Y$ be simple random variables, i.e. 
\[X = \sum_i a_i \mathbb{I}_{A_i} \text{ and } Y = \sum_j b_j \mathbb{I}_{B_j}\]
Since $\{A_i\}_i$ and $\{B_j\}_j$ are both partitions, $\{A_i \cap B_j\}_{i, j}$ is also a partition, and 
\[X Y = \sum_{i, j} a_i b_j \, \mathbb{I}_{A_i \cap B_j}\]
Its expectation can be expanded out by linearity, and since $\mathbb{E}[ \mathbb{I}_{A} ] = \mathbb{P}(A)$, we have
\begin{align*}
    \mathbb{E}[X Y] & = \sum_{i, j} a_i b_j \, \mathbb{P}(A_i \cap B_j) \\
    & = \sum_{i, j} a_i b_j \, \mathbb{P}(A_i)\, \mathbb{P}(B_j) = \mathbb{E}[X] \, \mathbb{E}[Y]
\end{align*}
Now that we have proved for simple random variables, we can just approximate $X$ from below using simple functions. 
\end{proof}

\begin{theorem}[Tail Sum Formula]
If a discrete random variable $X$ takes values in the non-negative integers $\{0, 1, 2, 3, ...\}$, then 
\[\mathbb{E}(X) = \sum_{k=1}^\infty \mathbb{P}(X \geq k)\]
In any case (continuous or discrete), if $X$ is a non-negative random variable, then 
\[\mathbb{E}(X) = \int_0^\infty \mathbb{P}(X > x) \, dx = \int_0^\infty 1 - F(x) \, dx\]
where $F$ is the CDF of $X$. 
\end{theorem}
\begin{proof}
Suppose that $X$ takes values in $\{0, 1, 2, 3, ...\}$. Then, 
\begin{align*}
    \mathbb{E}(X) & = \sum_{k \geq 1} k \, \mathbb{P}(X=k) \\
    & = \sum_{k\geq 1} \sum_{j=1}^k \mathbb{P}(X = k) \\
    & = \sum_{k \geq 1} \sum_{j=1}^k \mathbb{I}_{j \leq k} \, \mathbb{P}(X=k) \\
    & = \sum_{j=1}^\infty \sum_{k \geq 1} \mathbb{I}_{j \leq k} \, \mathbb{P}(X =k) \\
    & = \sum_{j=1}^\infty \sum_{k \geq j} \mathbb{P}(X=k) \\
    & = \sum_{j=1}^\infty \mathbb{P}(X \geq j)
\end{align*}
\end{proof}

\begin{corollary}
For any $m > 0$ and $\alpha > 0$,  
\[\mathbb{P} \big(|X| > \alpha \big) \leq \frac{1}{\alpha^m} \mathbb{E} \big( |X|^m \big)\]
\end{corollary}

\begin{example}[Geometric RV]
Recall that given $X \sim \mathrm{Geometric}(p)$, we have $\mathbb{P}(X = i) = (1 - p)^{i-1} p$ for $i \geq 1$. So, 
\[\mathbb{E}[X] = \sum_{x=1}^\infty x \, \mathbb{P}(X = x) = \sum_{x=1}^\infty x \, (1 - p)^{i-1} p = \frac{p}{(1 - (1 - p))^2} = \frac{1}{p}\]
\end{example}

\begin{example}[Infinite Expectation]
Let us have discrete random variable s.t. $\mathbb{P}(X = k) = \frac{6}{\pi^2} \frac{1}{k^2}$ for $k \geq 1$. So, 
\[\mathbb{E}[X] = \sum_{k=1}^\infty k \, \mathbb{P}(X = k) = \frac{6}{\pi^2} \sum_{k=1}^\infty \frac{1}{k} = +\infty\]
\end{example}

\begin{example}[Undefined Expectation]
Let $\mathbb{P}(X = k) = \frac{3}{\pi^2} \frac{1}{k^2}$ for $k \in \mathbb{Z}\setminus \{0\}$. The expectation of this can be computed by getting the expectation of all the positive terms and the negative terms. 
\[\mathbb{E}[X] = \mathbb{E}[X^+] - \mathbb{E}[X^{-}] = \sum_{k=1}^\infty k \cdot \frac{3}{\pi^2} \frac{1}{k^2} + \sum_{k=1}^\infty (-k) \cdot \frac{3}{\pi^2} \frac{1}{k^2} = \infty - \infty\]
Note that by the Riemann rearrangement theorem, we can't just say that the expectation is $0$ since the terms "cancel out." We could only do this if the series is absolutely convergent also, which works if $X$ takes positive values only. 
\end{example}

Note that when we compute expectation, what we do it multiply the PMF/PDF by $x$ and sum/integrate over it. The Cauchy distribution is a power function of form $\frac{1}{x^2}$, so if we multiply it by $x$, we have the new $\frac{1}{x}$ which is harmonic and therefore divergent. 

\subsection{Law of the Unconscious Statistician}

Given probability space $(\Omega, \mathcal{F}, \mathbb{P})$ and a random variable $X: \Omega \rightarrow \mathbb{R}$, this induces a probability law $\mathbb{P}_X$ acting as a measure on $\mathbb{R}$. Assume that this probability law $\mathbb{P}_X$ is known. Now introduce a function $g: \mathbb{R} \rightarrow \mathbb{R}$. We can create a new random variable $Y = g \circ X : \Omega \rightarrow \mathbb{R}$ with its own probability law $\mathbb{P}_Y$ on $\mathbb{R}$. Since we already know the probability distribution of $X$, so we can easily get the expected value of $X$ as (in the discrete case) 
\[\mathbb{E}[X] = \sum_{x \in \mathcal{X}} x \cdot \mathbb{P}(X = x)\]
where $\mathcal{X}$ is the support of $X$. But what if we wanted to get the expected value of $Y$? 
\[\mathbb{E}[Y] = \sum_{y \in \mathcal{Y}} y \cdot \mathbb{P}(Y = y) = ?\]
The problem is that we don't know the probability distribution of $Y$. But since we know that all the values of $X$ are transformed by $g$, we are taught to compute it in terms of the probability distribution of $X$. 
\[\mathbb{E}[Y] = \sum_{x \in \mathcal{X}} g(x) \cdot \mathbb{P}(X = x)\]
This "identity" that is often used must actually be treated as a rigorous theorem. 

\begin{theorem}[LOTUS]
Given probability space $(\Omega, \mathcal{F}, \mathbb{P})$, a random variable $X: \Omega \rightarrow \mathbb{R}$, and a function $g: \mathbb{R} \rightarrow \mathbb{R}$, the expectation of $g(X)$ is 
\[\mathbb{E}[g] \coloneqq \mathbb{E}[g(X)] = \int_\mathbb{R} g \, d\mathbb{P}_X \]
\begin{enumerate}
    \item For the discrete case, the above integral simplifies to 
    \[\mathbb{E}[g(X)] = \sum_{x \in \mathcal{X}} g(x) p_X (x)\]
    \item For the continuous case, we have 
    \[\mathbb{E}[g(X)] = \int_\mathbb{R} g \, f_X \, d \lambda\]
    and in particular, 
    \[\mathbb{E}[X] = \int_\mathbb{R} x \, f_X \,d\lambda\]
\end{enumerate}
\end{theorem}

\begin{example}[Expectation of Exponential RV]
The PDF of exponential random variable $X$ is defined $f_X = k e^{-k x}$ for $x \geq 0$. So, 
\[\mathbb{E}[X] = \int_\mathbb{R} x f_X \, d\lambda = \int_0^\infty x k e^{-k x} \, dx = \frac{1}{k}\]
Similarly, if we want the expectation of $X^2$, then we can get 
\[\mathbb{E}[X^2] = \int_\mathbb{R} x^2 f_X \,d\lambda = \int_0^\infty x^2 k e^{-k x} \,dx = \frac{2}{k^2}\]
\end{example} 

\begin{example}[Expectation of Gaussian RV]
The expectation of a Gaussian random variable $X$ is
\[\mathbb{E}[X] = \int_{-\infty}^\infty x \cdot \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{(x - \mu)^2}{2 \sigma^2}} \, dx = \mu\]
\end{example}

\begin{example}[Expectation of One-Sided Cauchy]
If we have $f_X (x) = \frac{2}{\pi} \frac{1}{1 + x^2}$ for $x \geq 0$, then 
\[\mathbb{E}[X] = \int_0^\infty \frac{2}{\pi} \frac{x}{1 + x^2} \,dx\]
and making the substitution $t = \frac{1 + x^2}, \; dt = 2x$, we have 
\[\int_1^\infty \frac{1}{\pi} \frac{1}{t} \,dt = \frac{\ln(t)}{\pi} \bigg|_1^\infty = +\infty\]
\end{example}

\begin{example}[Expectation of Two-Sided Cauchy]
The two-sided Cauchy is just another copy of the one sided into the negatives, so $f_X (x) = \frac{1}{\pi} \frac{1}{1 + x^2}$ for $x \in \mathbb{R}$. The expectation of $X$ should be split up into for positive and negative images, but computing it gives 
\[\mathbb{E}[X] = \mathbb{E}[X^+] - \mathbb{E}[X^-] = \int_0^\infty \frac{1}{\pi} \frac{x}{1 + x^2}\,dx - \int_{-\infty}^0 \frac{1}{\pi} \frac{x}{1 + x^2}\,dx = \infty - \infty\]
and so it is undefined. 
\end{example}

With LOTUS, we can make sense of an extremely important inequality. 

\begin{theorem}[Jensen's Inequality]
If $f$ is a convex function, then $\mathbb{E}[f(X)] \geq f (\mathbb{E}[X])$. 
\end{theorem}

\subsection{Variance}

\begin{definition}[Variance]
Let $X$ be a random variable and suppose $\mathbb{E}[X] < \infty$. The \textbf{variance} of $X$ is defined 
\[\mathrm{Var}[X] = \sigma^2_X \coloneqq \mathbb{E} [ (X - \mathbb{E}[X])^2 ]\]
and $\sigma_X = \sqrt{\mathrm{Var}[X]}$ is called the \textbf{standard deviation}. This is a measure of how much the probability distribution deviates from its mean. We can use linearity of expectation to write 
\begin{align*}
    \mathrm{Var}[X] & = \mathbb{E} \big[ X^2 + \mathbb{E}[X]^2 - 2 X \mathbb{E}[X] \big] \\
    & = \mathbb{E}[X^2] + \mathbb{E}[X]^2 - 2 \mathbb{E}[X] \mathbb{E}[X] \\
    & = \mathbb{E}[X^2] - \mathbb{E}[X]^2
\end{align*}
which is often easier to compute, since it only requires us to compute the expectation of $X$ and $X^2$. Since variance is always nonnegative, we also know that $\mathbb{E}[X^2] \geq \mathbb{E}[X]^2$. The variance is always defined, whether it's finite or $+\infty$. 
\end{definition}

\begin{proposition}
The covariance of a random variable $X$ is $0$ if and only if it constant almost everywhere on $\Omega$. 
\end{proposition}
\begin{proof}
The if part is easy, so let's prove the only if part. Let $\mathbb{E} [ (X - \mathbb{E}(X))^2 ] = 0$. Then, we can think of the function $x \mapsto (x - \mathbb{E}(X))^2$ and write the variance as 
\[\mathrm{Var}[X] = \int_\Omega (X - \mathbb{E}(X))^2 \, d\mathbb{P} = 0\]
But by nonnegativity of the function, we know that $(X - \mathbb{E}[X])^2 = 0$ w/ probability $1$, which implies that $X = \mathbb{E}[X]$ with prob. $1$. 
\end{proof}

\begin{lemma}[Properties of Variance]
Let $X$ and $Y$ be random variables with well-defined variances. 
\begin{enumerate}
    \item Translation Invariance: Given that $X + a$ is a new random variable defined $(X + a)(\omega) = X(\omega) + a$, 
    \[\mathrm{Var}[X] = \mathrm{Var}[X + a]\]
    \item Quadratic Scaling: Given that $aX$ is a new random variable defined $(aX)(\omega) = a\,X(\omega)$, 
    \[\mathrm{Var}[aX] = a^2 \mathrm{Var}[X]\]
\end{enumerate}
\end{lemma}

From the properties of expectation and variance, we can now \textbf{standardize} a random variable $X$. If $X$ is a random variable with mean $\mu = \mathbb{E}(X)$ and variance $\sigma^2 = \Var(X)$, then the random variable 
\[Y = \frac{X - \mu}{\sigma}\]
has mean $\mathbb{E}(Y) = 0$ and variance $\Var(Y) = 1$. 

\begin{example}[Bernoulli]
Given $X \sim \mathrm{Bernoulli}(p)$, we have
\begin{align*}
    \mathbb{E}[X] & = 0 \cdot \mathbb{P}(X = 0) + 1 \cdot \mathbb{P}(X = 1) = p \\
    \mathbb{E}[X^2] & =  0^2 \cdot \mathbb{P}(X = 0) + 1^2 \cdot \mathbb{P}(X = 1) = p
\end{align*}
and so $\mathrm{Var}[X] = p - p^2 = p(1 - p)$. 
\end{example}

\begin{example}[Poisson]
Given $X \sim \mathrm{Poisson}(X)$, then 
\begin{align*}
    \mathbb{E}[X] & = \sum_{k = 0}^\infty k \cdot \frac{e^{-\lambda} \lambda^k}{k!} = \sum_{k = 1}^\infty \cdot \frac{e^{-\lambda} \lambda^k}{(k-1)!} = \lambda \sum_{k = 1}^\infty \cdot \frac{e^{-\lambda} \lambda^{k-1}}{(k-1)!} = \lambda\\
    \mathbb{E}[X] & = \sum_{k=0}^\infty k^2 \cdot \frac{e^{-\lambda} \lambda^k}{k!} = \ldots = \lambda^2 + \lambda
\end{align*}
So $\mathrm{Var}[X] = \lambda^2 + \lambda - \lambda^2 = \lambda$. 
\end{example}

\begin{example}[Uniform]
Let $X \sim \mathrm{Uniform}[a, b]$. Then, 
\begin{align*}
    \mathbb{E}[X] & = \int_\mathbb{R} x f_X \, d\lambda = \int_a^b x \cdot \frac{1}{b - a}\,dx = \frac{a + b}{2} \\
    \mathbb{E}[X^2] & = \int_\mathbb{R} x^2 f_X \,d\lambda = \int_a^b \frac{x^2}{b - a} \,dx = \frac{a^2 + ab + b^2}{3} 
\end{align*}
So $\mathrm{Var}[X] = \ldots = \frac{1}{12} (b - a)^2$. This is consistent with the fact that if we spread out our measure over a wider interval, then the variance will be bigger. 
\end{example}

\begin{example}[Exponential]
Let $X \sim \mathrm{Exp}(\lambda)$. Then, $\mathbb{E}[X] = \frac{1}{\lambda}$ and $\mathbb{E}[X^2] = \frac{2}{\lambda^2}$, so 
\[\mathrm{Var}[X] = \frac{1}{\lambda^2}\]
This is consistent with the fact that if $\lambda$ is greater, then the PDF is much more concentrated at $0$, making the variance small. 
\end{example}

Just like how we explained that computing finiteness or infiniteness of expectation is similar to multiplying the PMF/PDF by $x$ and determining if the series/integral converges or diverges, we can do the same for variance by multiplying the PMF/PDF by $x^2$. For a probability distribution of form $\frac{1}{x^2}$, it diverges if we multiply by $x$ and also diverges if we multiply by $x^2$. But also, we could construct a distribution where the expectation may be finite, but the variance may be infinite. For example, if we have a distribution of form $\frac{1}{x^3}$, multiplying it by $x$ leads to form $\frac{1}{x^2}$, which is finite (so finite expectation), but multiplying by $x^2$ leads to a harmonic, i.e. infinite variance. 

\subsection{Covariance}

The variance is a measure for one random variable $X$, which measures how much it deviates from its mean. Now, the covariance is defined for two random variables and captures how they jointly vary. 

\begin{definition}[Covariance]
The \textbf{covariance} of random variables $X$ and $Y$ is defined as 
\begin{align*}
    \mathrm{Cov}[X, Y] & = \mathbb{E} \big[ (X - \mathbb{E}[X]) (Y - \mathbb{E}[X]) \big] \\
    & = \mathbb{E}[X Y] - \mathbb{E}[X] \, \mathbb{E}[Y]
\end{align*}
where the intermediate expectations are well-defined. $X$ and $Y$ are said to be \textbf{uncorrelated} if 
\[\mathrm{Cov}[X, Y] = 0\]
\end{definition}

The covariance is also easy to interpret. Given two random variables $X$ and $Y$, if whenever $X$ is greater than its expected value $\mathbb{E}[X]$, $Y$ also tends to be greater than $\mathbb{E}[Y]$, then the covariance will be some positive number. If they tend to be on opposite sides of their expected values, then the covariance will be negative. And the degree with which these RVs lie on which side of the expected value determines the magnitude of the covariance. 

\begin{theorem}
If $X$ and $Y$ are independent random variables, then they are uncorrelated, meaning that independence is a stronger condition. 
\end{theorem}

We show an example of why the converse is not true. Consider $X \sim \mathrm{Uniform}[-1, 1]$. We can show that $x$ and $Y = X^2$ are dependent but uncorrelated. It is clearly dependent, but its covariance is 
\begin{align*}
    \mathrm{Cov}(X, Y) & = \mathbb{E}[X Y] - \mathbb{E}[X] \, \mathbb{E}[Y] \\
    & = \mathbb{E}[X^3] - \mathbb{E}[X] \, \mathbb{E}[X^2] \\
    & = \int_{-1}^1 x^3 \cdot 1 \,dx - 0 \cdot \mathbb{E}[X^2] = 0
\end{align*}

\begin{theorem}
If $X$ and $Y$ are two random variables, then 
\[\mathrm{Var}(X + Y) = \mathrm{Var}(X) + \mathrm{Var}(Y) + 2 \mathrm{Cov}(X, Y)\]
and by induction, we can show that 
\[\mathrm{Var}\bigg( \sum_i X_i\bigg) = \sum_{i} \mathrm{Var}(X_i) + \sum_{i, j} \mathrm{Cov}(X_i, X_j) \]
\end{theorem}
\begin{proof}
Simple computation. The LHS expands to 
\begin{align*}
    \mathbb{E}[(X + Y)^2] - \mathbb{E}[X + Y]^2 & = \mathbb{E}[X^2 + 2XY + Y^2] - (\mathbb{E}[X] + \mathbb{E}[Y])^2 \\
    & = \mathbb{E}[X^2] + 2 \mathbb{E}[XY] + \mathbb{E}[Y^2] - \mathbb{E}[X]^2 - 2 \mathbb{E}[X] \mathbb{E}[Y] - \mathbb{E}[Y]^2 \\
    & = \big( \mathbb{E}[X^2] - \mathbb{E}[X]^2 \big) + \big( \mathbb{E}[Y^2] - \mathbb{E}[Y]^2 \big) + 2 \big( \mathbb{E}[XY] - \mathbb{E}[X] \mathbb{E}[Y] \big) \\
    & = \mathrm{Var}(X) + \mathrm{Var}(Y) + 2 \mathrm{Cov}(X, Y) 
\end{align*}
\end{proof}

\subsection{Correlation, Hilbert Space of Random Variables}

\begin{definition}[Correlation Coefficient]
The \textbf{correlation coefficient} of random variables $X$ and $Y$ is defined 
\[\rho_{X, Y} = \mathrm{Corr}(X, Y) \coloneqq \frac{\mathrm{Cov}(X, Y)}{\sigma_X \sigma_Y} = \frac{\mathrm{Cov}(X, Y)}{\sqrt{\mathrm{Var}(X) \, \mathrm{Var}(Y)}}\]
\end{definition}

In some sense the correlation is a scaled version of the covariance. It is scale-invariant, and it is always a number that lies between $-1$ and $1$, making it a nice way to represent the correlation between two variables without having to worry about scale. We can prove this. 

\begin{theorem}[Cauchy-Schwartz]
For any two random variables $X, Y$, we have $|\mathrm{Cov}(X, Y)| \leq \sigma_X \sigma_Y$, or in other words, 
\[-1 \leq \rho_{X, Y} \leq 1\]
Furthermore, whenever $\rho_{X, Y} = 1$ or $-1$, there exists a deterministic relationship between $X$ and $Y$. 
\begin{enumerate}
    \item If $\rho_{X, Y} = 1$, there exists a $a > 0$ s.t. 
    \[Y - \mathbb{E}[Y] = a (X - \mathbb{E}[X])\]
    \item If $\rho_{X, Y} = -1$ there exists a $a < 0$ s.t. 
    \[Y - \mathbb{E}[Y] = a (X - \mathbb{E}[X])\]
\end{enumerate}
This implies that $\Corr(X, Y) = \pm 1$ indicates that the joint distribution of $(X, Y)$ is concentrated on a line in $\mathbb{R}^2$. 
\end{theorem}

The fact that this is called the Cauchy-Schwartz inequality hints at the existence of inner products, norms, and vector spaces. That is, we can treat the random variables $X, Y$ as vectors in the functional space of real-valued maps over $\Omega$. In some sense, $\mathrm{Cov}(X, Y)$ sort-of plays the role of an inner product. 
\begin{enumerate}
    \item It satisfies symmetricity: 
    \[\mathrm{Cov}(X, Y) = \mathbb{E}[X Y] - \mathbb{E}[X] \, \mathbb{E}[Y] =  \mathbb{E}[Y X] - \mathbb{E}[Y] \, \mathbb{E}[X] = \mathrm{Cov}(Y, X)\] 
    
    \item It satisfies binlinearity. It suffices to show only for first argument, since we have symmetricity. 
    \begin{align*}
        \mathrm{Cov}(aX + bY, Z) & = \mathbb{E}[(a X + b Y) Z] - \mathbb{E}[a X + b Y] \, \mathbb{E}[Z] \\
        & = a \mathbb{E}[X Z] + b \mathbb{E}[Y Z] - a \mathbb{E}[X] \mathbb{E}[Z] - b \mathbb{E}[Y] \, \mathbb{E}[Z] \\
        & = a \big( \mathbb{E}[X Z] - \mathbb{E}[X] \mathbb{E}[Z] \big) + a \big( \mathbb{E}[Y Z] - \mathbb{E}[Y] \, \mathbb{E}[Z] \big) \\
        & = a \, \mathrm{Cov}(X, Z) + b \, \mathrm{Cov}(Y, Z)
    \end{align*}

    \item We want the inner product of $X$ with itself to always be greater than $0$, with equality holding iff $X = 0$. Indeed, we have 
    \[\mathrm{Cov}(X, X) = \mathrm{Var}(X) \geq 0\]
    but it is not necessarily true that $\mathrm{Var}(X) = 0 \implies X = 0$. We can say that $X$ is equal to a constant almost everywhere at best. We can solve this problem by looking at the functional subspace of $0$-mean random variables (which is a vector space due to linearity of expectation). So now all random variables $X$ that are $0$ almost everywhere have inner product $0$, so we must add an equivalence class on this subspace that says two $X, Y$ are equivalent if they agree almost everywhere. 
\end{enumerate}
The standard deviation $\sigma_X$ and $\sigma_Y$ act as norms on this quotient subspace of $0$-mean random variables. So the correlation coefficient $\rho_{X, Y}$ can be interpreted as the cosine of the angle between $X$ and $Y$. This now makes our desired space a Hilbert space, and our uncorrelated random variables are like orthogonal vectors. 

\subsection{Conditional Expectation}

It looks like conditional expectation should be in the previous chapter, grouped in with regular expectation, variance, covariance, and correlation. But it is a large field on its own and a lot to digest, and it has very important connections to regression analysis, so we introduce it in a new section. 

Given some joint probability distribution, we can define the conditional expectation as the expectation of a conditional distribution. This construction is very elementary, and we will start with the discrete case. 

\begin{definition}[Discrete Conditional Expectation Given $Y = y$]
Let $X, Y$ be discrete random variables, with joint random variable $(X, Y): \Omega \rightarrow \mathbb{R}^2$ and its joint PMF $p_{X, Y} (x, y)$. Recall that the conditional PMF is 
\[p_{X\mid Y}(x \mid y) \coloneqq \frac{p_{X, Y} (x, y)}{p_Y (y)}\]
The \textbf{conditional expectation} of $X$ given $Y = y$ is 
\[\mathbb{E}[X \mid Y = y] = \sum_{x \in \mathcal{X}} x \, p_{X \mid Y} (x \mid y)\]
\end{definition}

\begin{definition}[Continuous Conditional Expectation Given $Y = y$]
Let $X, Y$ be jointly continuous with joint PDF $f_{X, Y} (x, y)$. Recall that the conditional PDF is 
\[f_{X \mid Y} (x \mid y) \coloneqq \frac{f_{X, Y} (x, y)}{f_Y (y)}\]
The \textbf{conditional expectation} of $X$ given $Y = y$ is 
\[\mathbb{E}[X \mid Y = y] = \int_{x \in \mathbb{R}} x \, f_{X \mid Y} (x \mid y) \, dx\]
Again, we can set $\psi(y) \coloneqq \mathbb{E}[X \mid Y = y]$, which is a function of $y$ and therefore a random variable. Therefore the \textbf{conditional expectation} of $X$ given $Y$ is $\mathbb{E}[X \mid Y]$, which is a $\sigma(Y)$-measurable random variable. 
\end{definition}

As a visual, we can take a "slice" of the joint distribution of some value of $Y$, look at the distribution of $X$ on this slice, and compute its expectation. That is, for every value of $Y = y$, there exists some (conditional) distribution of $X$ with PMF of $p_{X \mid Y} (x \mid y)$ or PDF of $f_{X \mid Y} (x \mid y)$. 
\begin{center}
    \includegraphics[scale=0.3]{conditional_exp.jpg}
\end{center}
So given a value of $Y = y$, we generally know something about $X$ (e.g. if I know humidity, I know something about the temperature) and want to find the best estimate of $X$. This is precisely the conditional expectation $\mathbb{E}[X \mid Y = y]$, and we can interpret this as a regression function $\psi(y) \coloneqq \mathbb{E}[X \mid Y = y]$, which predicts the expected value of $X$ given $Y = y$. We can go even further. Since $\psi$ is a function that takes in $y$ and outputs a number, it is a transformation $\psi: \mathrm{Im}(Y) \subset \mathbb{R} \longrightarrow \mathbb{R}$, and so we can think of $\psi(Y) = \psi \circ Y$ as a random variable itself. 

\begin{definition}[Conditional Expectation]
The conditional expectation is a random variable $\mathbb{E}[Y \mid X]: \Omega \longrightarrow \mathbb{R}$ defined 
\[\mathbb{E}[Y \mid X] (\omega) \coloneqq \mathbb{E}[Y \mid X = X(\omega)] \]
where $\mathbb{E}[Y \mid X = x]$ is the conditional expectation of $Y$ given $X = x$. $\mathbb{E}[Y \mid X = x]$ is just some function of $x$, which we may write in closed form $\psi(x)$. Then, by replacing the variable $x$ with random variable $X$, this expression becomes a random variable $\psi(X)$. That is, once an $\omega \in \Omega$ is realized, $X$ is realized, and then $\psi(X)$ is realized. Therefore, we can view $\psi(X) = \mathbb{E}[Y \mid X]$ as an estimator of $Y$ that only depends on $X$. 
\end{definition}

\begin{example}
Let $f_{X, Y} (x, y) = \frac{1}{x}$ for $0 < y \leq x \leq 1$. Find $\mathbb{E}[Y \mid X]$. We first calculate the marginal density of $X$, which will allow us to calculate the conditional density of $Y$: 
\[f_X (x) = \int_0^x \frac{1}{x} \,dy = \frac{y}{x} \bigg|_0^x = 1 \implies f_{Y \mid X} (y \mid x) = \frac{f_{X, Y} (x, y)}{f_X (x)} = \frac{1/x}{1} = \frac{1}{x} \text{ for } 0 < y \leq x \leq 1\]
Since the conditional density of $Y$ is not dependent on $x$, $Y$ is uniform from $0$ to $x$. Now calculate the expectation: 
\[\mathbb{E}[Y \mid X = x] = \int_0^x y \cdot \frac{1}{x} \,dy = \frac{x}{2}\]
and so the conditional expectation is 
\[\mathbb{E}[Y \mid X] = \frac{1}{2} X\]
\end{example}

\begin{theorem}[Tower Rule]
We know that $\mathbb{E}[Y \mid X]$ is the random variable $\psi(X)$ that is a transformed version of $X$. Then, we have
\[\mathbb{E}[ \mathbb{E}[Y \mid X]] = \mathbb{E}[Y]\]
This is confusing notation due to the iterated expectations, but note that the term on the inside is a transformed random variable of $X$, while the expectation on the outside computes the expectation of this transformed random variable. So, letting $\psi(X) = \mathbb{E}[Y \mid X]$, we can equivalently write 
\[\mathbb{E}[ \psi(X)] = \mathbb{E}[Y]\]
\end{theorem}
\begin{proof}
We can just expand this out. We will do it for the discrete case. 
\begin{align*}
    \mathbb{E}[ \mathbb{E}[Y \mid X]] & = \sum_x p_X (x) \, \underbrace{\mathbb{E}[Y \mid X = x]}_{\psi(x)} \\
    & = \sum_x \bigg( p_X (x) \cdot \sum_y y \cdot p_{Y \mid X} (y \mid x) \bigg) \\
    & = \sum_x \bigg( p_X (x) \cdot \sum_y y \cdot \frac{p_{X, Y} (x, y)}{p_X (x)} \\
    & = \sum_{x, y} y \cdot p_{X, Y} (x, y) \\
    & = \sum_y \bigg( y \cdot \sum_x p_{X, Y} (x, y) \bigg) \\
    & = \sum_y y \cdot p_Y (y) \\
    & = \mathbb{E}[Y]
\end{align*}
\end{proof}

\begin{example}
Consider the random sum of random variables $S_N = \sum_{i=1}^N X_i$, where $X_i$ are iid and $N$ is independent of $X_i$'s. Then, we can use the tower rule to write $\mathbb{E}[S_N] = \mathbb{E}[\mathbb{E}[S_N \mid N]]$. $\mathbb{E}[S_N \mid N]$ is a transformed random varibale of $N$, and to compute its closed form we should just compute $\mathbb{E}[S_N \mid N = n]$ and replace $n$ with $N$. 
\[\mathbb{E}[S_N \mid N = n] = \mathbb{E}[S_n] = \mathbb{E} \bigg( \sum_{i=1}^n X_i \bigg) = n \mathbb{E}[X]\]
remember that $\mathbb{E}[X]$ is just a number, so replacing $n$ with $N$ gives $\mathbb{E}[S_N \mid N] = N \mathbb{E}[X]$, i.e. the random variable $N$ multiplied by $\mathbb{E}[X]$. Therefore, 
\[\mathbb{E}[\mathbb{E}[S_N \mid N]] = \mathbb{E}[N \mathbb{E}[X]] = \mathbb{E}[N] \, \mathbb{E}[X]\]
This makes sense intuitively, since we want to approximate this value by taking the expected value of $X$ and multiplying it by the expected number of summands. 
\end{example}

\section{Transforms}

\subsection{Probability Generating Function (PGF)}

The PGF is only defined for discrete random variable, and is analogous to the Z-transform in singal processing. 

\begin{definition}[Probability Generating Function]
Let $X$ be a discrete random variable taking values in $\mathbb{N}_0$. Then, the \textbf{probability generating function} of $X$ is defined 
\[G_X (z) \coloneqq \mathbb{E}[z^X] = \sum_{i=0}^\infty z^i \, \mathbb{P}(X = i)\]
Now there is the problem of convergence, but we will not pay attention to this technicality for now and just consider the PGF as a tool. 
\end{definition}

\begin{example}[PGF of Poisson]
The random variable $X \sim \mathrm{Poisson}(\lambda)$ has pmf $\mathbb{P}(X = i) = \frac{e^{-\lambda} \lambda^i}{i!}$ for $i \in \{0, 1, \ldots\}$. Then, 
\[G_X (z) = \mathbb{E}[z^X] = \sum_{i=0}^\infty z^i \, \frac{e^{-\lambda} \lambda^i}{i!} = \sum_{i=0}^\infty \frac{e^{-\lambda} (\lambda z)^i}{i!} = e^{\lambda(z - 1)}\]
\end{example}

\begin{example}[PGF of Geometric]
For $X \sim \mathrm{Geometric}(p)$, its PGF is 
\[G_X (z) = \sum_{i=1}^\infty z^i \, (1 - p)^i p = \frac{p z}{1 - z(1 - p)}\]
\end{example}

\begin{lemma}[Properties of PGF]
Given random variable $X$ and its PGF $G_X$, we have the following: 
\begin{enumerate}
    \item Evaluate at $z = 1$: 
    \[G_X (1) = \mathbb{E}[1^X] = \mathbb{E}[1] = 1\]
    \item Derivative at $z = 1$: 
    \[\frac{d G_X (z)}{d z} \bigg|_{z = 1} = \mathbb{E}[X]\]
    \item $k$th derivative at $z = 1$: 
    \[\frac{d^k G_X (z)}{d z^k} \bigg|_{z = 1} = \mathbb{E}[X (X-1) (X-2) \ldots (X-k +1)]\]
    \item Transformation: Given the sum $Z = X + Y$ (where $X, Y$ are independent), rather than computing its convolution, the PGF of $Z$ is simply the product of the PGFs of $X$ and $Y$: 
    \[G_Z (z) = G_X (z) \, G_Y (z)\]
    For example, since a $\mathrm{Poisson}(\lambda)$ random variable has PGF of form $e^{\lambda (z - 1)}$, if we have two Poissons $X$ and $Y$ with parameters $\lambda, \mu$, then we can easily multiply their PGFs to get the PGF of $Z = X + Y$, which is $e^{(\lambda + \mu)(z - 1)}$, which is the PGF of a $\mathrm{Poisson}(\lambda + \mu)$ random variable. 
\end{enumerate}
\end{lemma}

\subsection{Moment Generating Function (MGF)}

\begin{definition}[Moment]
The \textbf{$\mathbf{n}$th (raw) moment} of a random variable $X$ is $\mathbb{E}[X^n]$. Unlike the raw moment, which is calculated around the origin, the \textbf{$\mathbf{n}$th central moment} of $X$ is its moment centered around its mean $\mathbb{E}[(X - \mathbb{E}[X])^n]$. 
\begin{enumerate}
    \item the first moment is the mean $\mathbb{E}[X]$
    \item the second central moment is the variance $\mathbb{E}[(X - \mathbb{E}[X])^2]$ 
    \item the third central moment, divided by $\sigma^3$, is the skew $\frac{1}{\sigma^3} \mathbb{E}[(X - \mathbb{E}[X])^3]$ 
\end{enumerate}
\end{definition}

\begin{definition}[Moment Generating Function (MGF)]
The \textbf{moment generating function} associated with a random variable $X$ is a function $M_X: \mathbb{R} \longrightarrow [0, \infty]$ defined 
\[M_X (s) \coloneqq \mathbb{E}[e^{s X}] \]
It is like an exponential moment. The region of convergence of $M_X$ is the set
\[D_X = \{s \mid M_X (s) < \infty\}\]
We specify them for discrete and continuous random variables: 
\begin{enumerate}
    \item If $X$ is discrete, with PMF $p_X (x)$, then by LOTUS we have 
    \[M_X (s) = \sum_{x \in \mathcal{X}} e^{s x} p_X (x)\]
    \item If $X$ is continuous, with PDF $f_X (x)$, then 
    \[M_X (s) = \int_{\mathbb{R}} e^{s x} f_X (x) \,dx\]
\end{enumerate} 
\end{definition}

Note that the MGF is really a complex-valued function, and results about it require a fair amount of complex analysis, but we will restrict ourselves to the real-values. 

\begin{example}[Exponential RV]
The PDF of $X \sim \mathrm{Exponential}(\mu)$ is $f_X (x) = \mu e^{-\mu x}$ for $x \geq 0$. The MGF is 
\[M_X (s) \coloneqq \int_0^\infty \mu e^{-\mu x} e^{s x} \, dx = \frac{\mu}{\mu - s} \text{ for } s < \mu\]
and if $s > \mu$, then $e^{(s - \mu) x}$ increases and the MGF is $\infty$. 
\end{example}

\begin{example}[Gaussian RV]
The PDF of a standard Gaussian $X$ is $f_X (x) = \frac{1}{\sqrt{2\pi}} e^{-x^2 / 2}$ for $x \in \mathbb{R}$, and the MGF is 
\[M_X (s) = \int_{-\infty}^\infty \frac{1}{\sqrt{2 \pi}} e^{- x^2 / 2} e^{s x} \,dx = e^{s^2 / 2} \int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}} e^{-\frac{(x - s)^2}{2}}\,dx = e^{s^2 / 2}\]
which is valid for all $s \in \mathbb{R}$. 
\end{example}

\begin{example}[Cauchy RV]
If we have $f_X (x) = \frac{1}{\pi} \frac{1}{1 + x^2}$ for $x \in \mathbb{R}$, the MGF is 
\[M_X (s) = \int_{-\infty}^\infty \frac{e^{s x}}{\pi (1 + x^2)} \,dx = \begin{cases} 1 & \text{ if } s = 0 \\
\infty & \text{ if } s > 0 \\ 
\infty & \text{ if } s < 0 \end{cases}\]
So the region of convergence is just $\{0\}$. It is infinity everywhere else since the exponential function grows exponentially as $x \rightarrow \pm \infty$. 
\end{example}

\begin{theorem}
Suppose $M_X (s)$ is finite for all $s \in [-\epsilon, \epsilon]$ for some $\epsilon > 0$. Then, $M_X$ uniquely determines the CDF of $X$. 
\end{theorem}


\subsection{Characteristic Function}

\begin{definition}[Characteristic Function]
Given a random variable $X: \Omega \longrightarrow \mathbb{R}$, the \textbf{characteristic function} is defined to be 
\[\varphi_X (t) = \mathbb{E}[ e^{i t X} ]\]
\end{definition}

\section{Concentration Inequalities}

Concentration inequalities give you probability bounds on random variables taking atypical values. For example, given a random variable with certain mean and variance, the probability of that random variable taking values outside a certain range around the mean is very small. It's called concentration because the probability concentrates around a certain range. 

The basic question here is that we would like to model a random variable $X$ over a probability space $\Omega$ and have some data $X_1, X_2, \ldots, X_n$ iid according to $X$. Let us have a fixed function $f: \mathbb{R}^n \longrightarrow \mathbb{R}$ that transforms the joint random variable $(X_1, \ldots, X_n)$ to create a new scalar RV 
\[f(X_1, \ldots, X_n) = f \circ (X_1, \ldots, X_n) : \Omega \longrightarrow \mathbb{R}\]
$f(X_1, \ldots, X_n)$ is a random variable so it has a mean, denote it $\mathbb{E}[f]$. Then concentration generally refers to the probability that the value of $f$ is at least some distance further from its mean. 
\[\mathbb{P} \big( |f(x) - \mathbb{E}[f] | \geq t \big) \leq \epsilon\]
for some small positive $\epsilon$. Usually, we would like this $\epsilon$ to be an exponentially decaying function of $t$ so that the bound goes down fast. This is what's so great about the Gaussian, which is why we'll introduce it here. 

\begin{theorem}[Gaussian Tail Inequality]
Given $X \sim \mathcal{N}(0, 1)$, the inequality says that the probability of $X$ taking values past a certain $t$ decays exponentially. 
\[\mathbb{P} \big( |X| > t \big) \leq \frac{2 e^{-t^2/2}}{t}\]
If we have $X_1, \ldots, X_n \sim \mathcal{N}(0, 1)$, then 
\[\mathbb{P} \big( |\overline{X}| > t \big) \leq \frac{2}{\sqrt{n} t} e^{-n t^2/2}\]
We can assume that the coefficient is less than $1$ if $n$ is large. The above tells us that this bound exponentially decays with $t$ but also with the number of samples $n$. 
\end{theorem}
\begin{proof}
We can simply check 
\[\phi(s) = \frac{1}{\sqrt{2\pi}} e^{-s^2/2} \implies \phi^\prime (s) = s \, \phi(s)\]
and use this to evaluate
\begin{align*}
    \mathbb{P}(X > t ) & = \int_t^\infty \phi(s) \,ds \\
    & = \int_t^\infty \frac{s}{s} \phi(s) \,ds \\
    & < \frac{1}{t} \int_t^\infty s \phi(s)\,ds \\
    & = \frac{1}{t} \int_t^\infty \phi^\prime (s)\,ds \\
    & = \frac{\phi(t)}{t}
\end{align*}
\end{proof}

Due to the exponential nature of the probability bound, we are extremely confident in getting the majority of our samples from a small interval. If we had taken some distribution like a Cauchy, with PDF of form 
\[f(x) \propto \frac{1}{1 + x^2}\]
Then we see that even though the shape looks like a Gaussian at first glance, the fat tails go down at the rate of $1/x^2$. It turns out that due to this, when we sample numerically, we occasionally get extreme values. 

\begin{theorem}[Markov's Inequality]
If $X$ is a non-negative random variable of finite expectation and $\alpha > 0$, then 
\[\mathbb{P}(X > \alpha) \leq \frac{\mathbb{E}[X]}{\alpha}\]
That is, the probability that $X$ takes a value greater than $\alpha$ is at most the expectation of $X$ divided by $\alpha$. This is meaningful only when $\mathbb{E}[X] < \alpha$, since otherwise the RHS will be greater than $1$.  
\end{theorem}

\begin{proof}
Given any $\alpha > 0$, we can set 
\[X = X \cdot \mathbb{I}_{X \leq \alpha} + X \cdot \mathbb{I}_{X > \alpha}\]
and by linearity, 
\begin{align*}
    \mathbb{E}[X] & = \mathbb{E}[X \cdot \mathbb{I}_{X \leq \alpha} + X \cdot \mathbb{I}_{X > \alpha}] \\
    & \geq \mathbb{E}[ X \cdot \mathbb{I}_{X > \alpha}] \\
    & \geq \alpha \mathbb{E}[\mathbb{I}_{X > \alpha}] \\
    & = \alpha \, \mathbb{P}(X > x) 
\end{align*}
\end{proof}

In other words, the probability that $X > \alpha$ goes down at least as fast as $1/\alpha$. For example, setting $\alpha = 2 \mathbb{E}[X]$, the probability that $X$ takes value that is at least twice its expectation is at most $1/2$. Furthermore, as $X$ gets very large, the probability that it will take a value beyond a large $\alpha$ goes down faster than $1/\alpha$. But this is a very conservative inequality, and usually the probability goes down much faster. 

Markov's inequality is very conservative but very general, too. If we make further assumptions about the random variable $X$, we can often make stronger bounds. Chebyshev's inequality assumes a (possibly negative) random variable with finite variance and states that the probability will go down as $1/x^2$. 

\begin{theorem}[Chebyshev Inequality]
Given (possibly negative) random variable $X$, if $\mathbb{E}[X] = \mu < +\infty$ and $\Var(X) = \sigma^2 < +\infty$, then for all $\alpha > 0$, 
\[\mathbb{P} \big( |X - \mu| > k \sigma \big) \leq \frac{1}{k^2} \iff \mathbb{P}(|X - \mu| > \alpha) \leq \frac{\mathrm{Var}(X)}{\alpha^2}\]
That is, the probability that $X$ takes a value further than $k$ standard deviations away from $\mu$ goes down by $1/k^2$. Therefore, if $\sigma$ is small, then this bound will be small since there is more concentration in the mean. 
\end{theorem}
\begin{proof}
We apply Markov's inequality to the non-negative random variable $|X - \mu|$. 
\[\mathbb{P}(|X - \mu| > \alpha) = \mathbb{P}(|X - \mu|^2 > \alpha^2) \leq \frac{\mathbb{E}(|X - \mu|^2)}{\alpha^2} = \frac{\mathrm{Var}(X)}{\alpha^2}\]
since the numerator on the RHS is the definition of variance. 
\end{proof}

Chebyshev inequality is just Markov's inequality applied to $X^2$ (assuming $0$ mean), and often yields a better bound. But even Chebyshev's inequality turns out to be quite loose, and even this $1/k^2$ is not a very nice bound. We could apply Markov's inequality to higher powers of $X$, e.g. given a random variable $X$, we can apply Markov's inequality to the $k$th power of nonnegative random variable $|X - \mathbb{E}[X]|$: 
\[\mathbb{P} (|X - \mathbb{E}[X] | > \alpha) = \mathbb{P}\big( |X - \mathbb{E}[X] |^k > \alpha^k \big) \leq \frac{\mathbb{E}( |X - \mathbb{E}[X] |^k )}{\alpha^k}\]
The natural culmination of all this is to apply Markov's inequality to $e^X$ (or, for a little flexibility, $e^{t X}$, where $t$ is a constant to be optimized). This gives us an exponential bound on $\mathbb{P}(X > \alpha)$. 

\begin{example}[Gaussian]
For the normal distribution, recall the 67-95-99.7 rule. It is well known that the probability of a random variable taking values within $2$ standard deviations from the mean is 95\%, so the probability that it takes outside is 5\%, or $1/20$, which is less than the $1/2^2 = 1/4$ bound given by Chebyshev. 
\end{example}

\subsection{Chernoff Bound and MGFs}

\begin{theorem}[Chernoff Bound]
Given a (possibly negative) random variable $X$, assume that its moment generating function $M_X (s) = \mathbb{E}[e^{s X}]$ is finite for every $s \in [-\epsilon, \epsilon]$. Then, since $x \mapsto e^{s x}$ is monotonically increasing, we have the identity 
\[\mathbb{P}(X > \alpha) = \mathbb{P}(e^{s X} > e^{s \alpha}) \text{ for } s > 0\]
But since the new random variable $e^{s X}$ is nonnegative, we can now go back to Markov inequality and write 
\[\mathbb{P}(X > \alpha) = \mathbb{P}(e^{s X} > e^{s \alpha}) \geq \frac{\mathbb{E}[e^{s X}]}{e^{s \alpha}} = M_X (s) \, e^{-s \alpha}\]
for $s > 0$ (for identity above to hold) \textit{and} $s \in D_X$ (and it is in domain of convergence). Now, we have an exponentially decaying bound in terms of $\alpha$. We have the freedom to choose $s$, since our bound is in terms of $\alpha$, so we must choose $s$ that minimizes $M_X (s) \, e^{-s \alpha}$. Ultimately, our best bound is 
\[\mathbb{P}(X > \alpha) \leq \inf_{s > 0} M_X (s) \, e^{-s \alpha}\]
After we optimize over $s$ what remains on the RHS is a function of $\alpha$. 
\end{theorem}

Now, we can calculate the MGF of $X$ directly if we knew the distribution of $X$, but we can also get bounds on it given some coarse statistics of $X$. 

\begin{lemma}
Let $X$ be a $0$-mean random variable s.t. $a \leq X \leq b$ with probability $1$. Then for all $t > 0$, 
\[\mathbb{E}[ e^{t X}] \leq e^{t^2 (b - a)^2 / 8}\]
\end{lemma}
\begin{proof}
We can write $x = \lambda a + (1 - \lambda b)$, $0 \leq \lambda \leq 1$, and convexity of the exponential tells us that 
\[e^{tx} \leq \lambda e^{ta} + (1 - \lambda) e^{tb}\]
Plugging in $\lambda = (b - x) / (b - a)$ then gives 
\[e^{tx} \leq \frac{b - x}{b - a} e^{tx} + \frac{x - a}{b - a} e^{tb} \]
Take expectations of both sides, and using linearity of expectation and the fact that $\mathbb{E}[X] = 0$. 
\[\mathbb{E}[e^{tX}] \leq \frac{b - \mathbb{E} X}{b - a} e^{ta} + \frac{\mathbb{E} X - a}{b - a} e^{tb} = \frac{b e^{ta} - a e^{tb}}{b - a} \leq e^{t^2 (b - a)^2 / 8}\]
\end{proof}

\subsection{Hoeffding's Inequality}

Hoeffding's inequality is one of the most important inequalities in concentration of measure. The proof of this inequality involves many useful tricks. 

\begin{theorem}[Hoeffding's Inequality]
Let $X_1, X_2, \ldots, X_n$ be independent (not necessarily identical) random variables s.t. $a_i \leq X_i \leq b_i$ almost surely. Consider the random variable $\overline{X} = \frac{1}{n} (X_1 + \ldots + X_n)$. Then, for all $t > 0$, we have the two inequalities
\begin{align*}
    \mathbb{P}\big( \overline{X} - \mathbb{E}[\overline{X}] \geq t \big) & \leq \exp \bigg( -\frac{2 n^2 t^2}{\sum_{i=1}^n (b_i - a_i)^2} \bigg) \\
    \mathbb{P}\big( \overline{X} - \mathbb{E}[\overline{X}] \leq -t \big) & \leq \exp \bigg( -\frac{2 n^2 t^2}{\sum_{i=1}^n (b_i - a_i)^2} \bigg)
\end{align*}
which can be combined to produce 
\[\mathbb{P}\big( \big| \overline{X} - \mathbb{E}[\overline{X}] \big| \geq t \big) \leq 2 \exp \bigg( -\frac{2 n^2 t^2}{\sum_{i=1}^n (b_i - a_i)^2} \bigg)\]
We can create an equivalent bound on the sum $S_n = X_1 + \ldots + X_n$: 
\begin{align*}
    \mathbb{P}\big(| S_n - \mathbb{E}[S_n] | \geq t\big) & = \mathbb{P}\big( n |\overline{X} - \mathbb{E}[\overline{X}] | \geq t \big) \\
    & = \mathbb{P} \big( |\overline{X} - \mathbb{E}[X] | \geq \frac{t}{n} \big) \\
    & \leq 2 \exp \bigg( -\frac{2 t^2}{\sum_{i=1}^n (b_i - a_i)^2} \bigg) 
\end{align*}
\end{theorem}
\begin{proof}
We will prove just with the case where $X_1, \ldots X_n$ are all bounded by $[a, b]$, which gives 
\[\mathbb{P} \big( |\overline{X} - \mathbb{E}[\overline{X}] | \geq t \big) \leq 2 \exp \bigg( - \frac{2 n t^2}{(b - a)}\bigg)\] 
Now, we can write 
\begin{align*}
    \mathbb{P}(\overline{X}_n > \epsilon ) & = \mathbb{P} \Big( \sum_{i=1}^n X_i \geq n \epsilon \Big) \\
    & = \mathbb{P} \big( e^{t \sum X_i} \geq e^{t n \epsilon} \big) & (\text{Variational Technique}) \\
    & \leq e^{- t n \epsilon} \, \mathbb{E}[e^{t \sum X_i}] & (\text{Markov's Inequality}) \\
    & = e^{-t n \epsilon} \, \big( \mathbb{E}[ e^{t X_i}] \big)^n & (\text{Independence}) \\
    & \leq e^{-t n \epsilon} e^{n \frac{t (b - a)^2}{2}} & (\text{prev. lemma}) 
\end{align*}
The step where we introduce an extra parameter $t$ is called a variational technique, used for optimization, and we can adjust $t$ to make it as small as possible. Taking the derivative of the final expression w.r.t. $t$ and solving for $0$ gives us $t = \frac{4 \epsilon}{(b - a)^2}$, and substituting into the expression gives the bound as 
\[\mathbb{P}(\overline{X}_n > \epsilon ) \leq \exp \bigg(- \frac{2 n \epsilon^2}{(b - a)^2}\]
\end{proof}

By further rearranging, we can write it as 
\[\mathbb{P} \bigg( | \overline{X} - \mathbb{E}[\overline{X}] | \geq t \sqrt{\frac{\sum_{i=1}^n (b_i - a_i)^2}{n^2}} \bigg) \leq 2 \exp(-2t^2)\]
which now looks like our Chebyshev inequality, but without a notion of standard deviation. But note the fact if $a_i \leq X_i \leq b_i$, then $\mathrm{Var}(X_i) \leq (b_i - a_i)^2$ (since $\mathrm{Var}(X_i) = \mathbb{E}[(X_i - \mathbb{E}[X_i])^2] \leq \mathbb{E}[(b_i - a_i)^2]$). So, we have 
\[\mathrm{Var}(\overline{X}) \leq \frac{\sum_{i=1}^n (b_i - a_i)^2}{n^2} \implies \mathbb{P}\bigg( |\overline{X} - \mathbb{E}[\overline{X}] | \geq t \sqrt{\frac{\sum_{i=1}^n (b_i - a_i)^2}{n^2}} \geq \mathrm{Var}(\overline{X}) \bigg) \leq 2\exp(-2t^2)\]
which allows us to interpret Hoeffding's inequality in a more familiar way. It says that the probability that the sample average is more than $t$ standard deviations from its expectation is at most $2 e^{-2t^2}$. 

\begin{corollary}
If $X_1, X_2, \ldots, X_n$ are independent with $\mathbb{P}(a_i \leq X_i \leq b_i) = 1$ and common mean $\mu$, then 
\[\mathbb{P}\bigg[ \big| \overline{X}_n - \mu \big| \leq \sqrt{ \frac{\sum_{i=1}^n (b_i - a_i)^2}{2n^2} \log \Big(\frac{2}{\delta}\Big)} \bigg] \geq 1 - \delta\]
\end{corollary}

\begin{example}[Bernoulli]
Applying Hoeffding's inequality to a sequence of $n$ $p$-coin tosses $X_1, \ldots, X_n \sim \mathrm{Bernoulli}(p)$ gives 
\[\mathbb{P}\big( | \overline{X}_n - p | > \epsilon \big) \leq 2 \exp^{-2 n \epsilon^2}\]
\end{example}

\begin{example}[Mean]
Suppose we have $X_1, X_2, \ldots X_n \sim \mathrm{Bernoulli}(p)$, all iid. Then, by Hoeffding's inequality, the average $\overline{X} = \frac{1}{n} (X_1 + \ldots + X_n)$ is tightly concentrated around $p$. 
\[\mathbb{P} \big( | \overline{X} - p | \geq t \big) \leq 2 e^{-2 n t^2}\]
Note that $b_i - a_i = 1 - 0 = 1$ for all $i$. There is an exponential decay in the probability of the sample mean deviating from its expectation. 
\end{example}

\begin{example}[Hypercube]
Pick a point $X \in [-1, +1]^d$ uniformly at random, i.e. choose iid $X_1, X_2, \ldots, X_d \sim \mathrm{Uniform}[-1, +1]$. The expectation is 
\[\mathbb{E} ||X||^2 = \sum_{i=1}^d \mathbb{E} X_i^2 = \sum_{i=1}^d \int_{-1}^1 x^2 f_X (x) \,dx = \sum_{i=1}^d \int_{-1}^1 \frac{1}{2} x^2 \,dx = \frac{d}{3}\]
Then, it can be shown that $||X|| = $ is tightly concentrated around $\sqrt{d/3}$. We show this again with Hoeffding's inequality by showing the concentration of $||X||^2$ around $d/3$. 
\[\mathbb{P} \bigg( \bigg| ||X||^2 - \frac{d}{3} \bigg| \geq t \bigg) \leq 2 \exp \Big( - \frac{ d t^2}{2} \Big)\]
This tells us that if we choose the uniform random vector $X \in [-1, +1]^d$, the vast majority of our samples will have $||X|| \approx \sqrt{d/3}$. 
\end{example}


Hoeffding's inequality does not use any information about the random variables expect for the fact that they are bounded. If the variance of $X_i$ is small, then we can get a sharper inequality from Bernstein's inequality. 

\begin{theorem}[Bernstein's Inequality]
If $\mathbb{P}(|X_i| \leq c) = 1$ and $\mathbb{E}[X_i] = 0$, set $\overline{X} = \frac{1}{n} \sum_{i=1}^n X_i$. Then, for any $t > 0$, 
\[\mathbb{P} \big( \big| \overline{X} \big| > \epsilon \big) \leq 2 \exp \bigg( - \frac{n \epsilon^2}{2 \sigma^2 + 2 c \epsilon /3} \bigg)\]
where $\sigma^2 = \frac{1}{n} \sum_{i=1}^n \mathrm{Var}(X_i)$. 
\end{theorem}

\subsection{Concentration of Lipshitz Functions}

Observing the Hoeffding bound, one might wonder whether such concentration applies only to averages or sums of random variables. After all, what's so special about averages? It turns out that the relevant feature of the average that yields tight concentration is that it is smooth in the way that if we change the value of one random variable the function does not change dramatically. 

\begin{theorem}[Bounded Difference Inequality]
Let has have independent random variables $X_1, X_2, \ldots, X_n$ and a function $f: \mathbb{R}^n \longrightarrow \mathbb{R}$ that satisfies the \textbf{bounded difference property} that 
\[\big| f(x_1, \ldots, x_k, \ldots, x_n) - f(x_1, \ldots, x_k^\prime, \ldots, x_n) \big| \leq c_k\]
for every $x, x^\prime \in \mathbb{R}^n$. That is, the function changes by at most $c_k$ if its $k$th coordinate is changed. Then, for all $t \geq 0$, we have the concentration inequality: 
\begin{align*}
    \mathbb{P} \big( f(X_1, \ldots, X_n) - \mathbb{E}[ f(X_1, \ldots, X_n)] \geq t \big) & \leq \exp \bigg(- \frac{2t^2}{\sum_{k=1}^n c_k^2} \bigg) \\
    \mathbb{P} \big( f(X_1, \ldots, X_n) - \mathbb{E}[ f(X_1, \ldots, X_n)] \leq -t \big) & \leq \exp \bigg(- \frac{2t^2}{\sum_{k=1}^n c_k^2} \bigg)
\end{align*}
Combining the two gives 
\[\mathbb{P} \big( \big| f(X_1, \ldots, X_n) - \mathbb{E}[ f(X_1, \ldots, X_n)] \geq t \big| \big) \leq \exp \bigg(- \frac{2t^2}{\sum_{k=1}^n c_k^2} \bigg)\]
\end{theorem}

In fact, any smooth function of bounded independent random variables is tightly concentrated around its expectation, and the notion of smoothness is Lipshitz continuity. 

\begin{definition}
A function $f: \mathbb{R}^n \longrightarrow \mathbb{R}$ is $L$-Lipschitz w.r.t. the $l_p$-metric if for all $\mathbf{x}, \mathbf{y} \in \mathbb{R}^n$, 
\[|f(\mathbf{x}) - f(\mathbf{y})| \leq L ||\mathbf{x} - \mathbf{y}||_p\]
\end{definition}

\begin{example}
For $x = (x_1, x_2, \ldots, x_n)$, we define the average $a(x) = \frac{1}{n} (x_1 + \ldots + x_n)$. Then, $a$ is $(1/n)$-Lipschitz w.r.t. the $l_1$ metric, since for any $\mathbf{x}, \mathbf{y}$, 
\begin{align*}
    |a(\mathbf{x}) - a(\mathbf{y})| & = \bigg| \frac{1}{n} \big[ (x_1 - y_1) + \ldots + (x_n - y_n) \big] \bigg| \\
    & = \frac{1}{n} \big( |x_1 - y_1| + \ldots + |x_n - y_n| \big) \\
    & = \frac{1}{n} ||\mathbf{x} - \mathbf{y} ||_1
\end{align*}
\end{example}

It turns out that Hoeffding's bound holds for all Lipschitz functions w.r.t. the $l_1$ metric. 

\begin{theorem}
Suppose $X_1, X_2, \ldots, X_n$ are independent and bounded with $a_i \leq x_i \leq b_i$. Then, for any $f: \mathbb{R}^n \longrightarrow \mathbb{R}$ that is $L$-Lipschitz w.r.t. the $l_1$-metric, we have 
\begin{align*}
    \mathbb{P} [ f \geq \mathbb{E}(f) + t] & = \mathbb{P} [ f - \mathbb{E}(f) \geq t] \leq \exp\bigg(- \frac{2 t^2}{L^2 \sum_{i=1}^n (b_i - a_i)^2} \bigg) \\
    \mathbb{P} [ f \leq \mathbb{E}(f) - t] & = \mathbb{P} [ f - \mathbb{E}(f) \leq -t] \leq \exp\bigg(- \frac{2 t^2}{L^2 \sum_{i=1}^n (b_i - a_i)^2} \bigg)
\end{align*}
and combining these inequalities gives 
\[\mathbb{P} [ |f - \mathbb{E}(f)| \geq t] \leq \exp\bigg(- \frac{2 t^2}{L^2 \sum_{i=1}^n (b_i - a_i)^2} \bigg)\]
\end{theorem}


\section{Convergence of Random Variables}

Unlike convergence of numbers, which is well-defined with respect to some metric or topology, there are many types of convergence of random variables. We must always specify which convergence when talking about them. Remember that a random variable $X$ is just a function from $\Omega$ to $\mathbb{R}$, so we can talk about pointwise convergence. That is, given a sequence of random variables $\{X_n\}$ and some $\omega \in \Omega$, the sequence 
\[X_1(\omega), X_2 (\omega), X_3(\omega), \ldots \]
is simply a sequence of real numbers. If this sequence converges to the real number $X(\omega)$, then $X_n$ converges to $X$ at $\omega$. If this occurs for all $\omega \in \Omega$, then we have sure convergence, and if this happens for an event (a $\mathcal{F}$-measurable subset of $\Omega$) with probability $1$, then we have almost sure convergence. 

\begin{definition}[Sure Convergence of RVs]
The sequence of random variables $\{ X_n\}_{n \in \mathbb{N}}$ is said to \textbf{converge pointwise} or \textbf{converge surely} to $X$ if 
\[X_n (\omega) \rightarrow X (\omega)\]
for every $\omega \in \Omega$. That is, we can choose \textit{any} $\omega \in \Omega$, and the realized sequence $X_1 (\omega), X_2 (\omega), \ldots$ will always converge to $X(\omega)$. We can visualize the function $X$ with a surface defined over $\Omega$ and can imagine the $X_n$'s as surfaces that converges to that of $X$. 
\begin{center}
    \includegraphics[scale=0.3]{sure_convergence.jpg}
\end{center}
\end{definition}

But this definition is too strong of a form of convergence, since in probability we don't care about values over sets of measure $0$. That is, if we have two probability distributions that differ from each other on a set of measure $0$, then they can be considered essentially the same probability distribution. 

\begin{definition}[Almost Sure Convergence of RVs]
The sequence of random variables $\{ X_n\}_{n \in \mathbb{N}}$ is said to \textbf{converge almost surely} or \textbf{converge with probability 1} to $X$ if $X_n (\omega) \rightarrow X (\omega)$ on a subset of probability $1$. That is, 
\[\mathbb{P} \big( \{ \omega \in \Omega \mid \lim_{n \rightarrow \infty} X_n (\omega) = X(\omega) \} \big) = 1\]
Considering small technicalities, it can be shown that this set of $\omega$'s can be considered an event in $\mathcal{F}$. This can be visualized similarly as sure convergence, but now the surfaces don't have to converge on sets of measure $0$. 
\begin{center}
    \includegraphics[scale=0.3]{almost_sure_convergence.jpg}
\end{center}
Crudely put, we just have to look at each $\omega \in \Omega$, see if $X_n (\omega)$ converges to $X(\omega)$ as $n \rightarrow \infty$, and determine if the set of all $\omega$'s that satisfy this have probability $1$. In other words, let us have some experiment with outcome space $\Omega$. With probability $1$, some $\omega \in \Omega$ will be realized, which will realize the sequence of realized random variables
\[X_1 (\omega), X_2 (\omega), X_3 (\omega), \ldots \]
that will converge to $X(\omega)$. Visually, we can imagine selecting a random point in $\Omega$, which will not hit the curve or point (with probability $1$), and in these cases, the sequence of points will converge to $X(\omega)$. 
\begin{center}
    \includegraphics[scale=0.3]{almost_sure_convergence_2.jpg}
\end{center}
\end{definition}

\begin{definition}[Convergence in Probability]
The sequence of random variables $\{ X_i\}_{i \in \mathbb{N}}$ is said to \textbf{converge to $X$ in probability} if for all $\epsilon > 0$, 
\[\lim_{n \rightarrow \infty} \mathbb{P} \big( |X_n - X| > \epsilon \big) = 0\]
To understand what this means, fix an $\epsilon > 0$. Then, $X_1$ may be very far from $X$, meaning that the event $|X_1 - X| > \epsilon$, i.e. the set of all $\omega \in \Omega$ satisfying $|X_1(\omega) - X (\omega)| > \epsilon$ may be a larger portion of $\Omega$. Now, as we increase $n$, this event will become smaller (in the way that it's probability decreases) until it reaches $0$. 
\begin{center}
    \includegraphics[scale=0.3]{convergence_in_probability.jpg}
\end{center}
\end{definition}

\begin{example}
Given $X_n \sim \mathrm{Exponential}(n)$ with $f_{X_n} (x) = n e^{-nx}$, we show that the sequence converges in probability to the $0$ random variable. Given $\epsilon > 0$, we have 
\begin{align*}
    \lim_{n \rightarrow \infty} \mathbb{P}(|X_n - 0| > \epsilon) & = \lim_{n \rightarrow \infty} \mathbb{P}(X_n > \epsilon \cup X_n < -\epsilon) \\
    & = \lim_{n \rightarrow \infty} \mathbb{P}(X_n > \epsilon) \\
    & = \lim_{n \rightarrow \infty} \int_\epsilon^\infty n e^{-nx} \,dx \\
    & = \lim_{n \rightarrow \infty} e^{-n \epsilon} = 0
\end{align*}
We can imagine this since given any small $\epsilon > 0$, we can see that increasing $n$ results in the distribution of $X_n$ to decrease at a faster rate, and thus a bigger portion of the distribution would lie within $\epsilon$ of the $0$ random variable. 
\begin{center}
    \includegraphics[scale=0.3]{convergence_in_prob_exponential.jpg}
\end{center}
\end{example}

\begin{definition}
We say $X_n$ \textbf{converges to $X$ in the $r$th mean} if 
\[\lim_{n \rightarrow \infty} \mathbb{E} \big[ |X_n - X|^r \big] = 0\]
For $r = 2$, $X_n$ is said to converge to $X$ in the \textbf{mean-squared sense}. 
\end{definition}

\begin{definition}[Convergence in Distribution]
We say $X_n$ \textbf{converges to $X$ in distribution} if the CDF of $X_n$ converges pointwise to the CDF of $X$, i.e. 
\[\lim_{n \rightarrow \infty} F_{X_n} (x) = F_X (x)\]
for all $x$ where $F_{X}$ is continuous. 
\end{definition}

So for practical purposes there are 5 notions of convergence that we will work with: 
\begin{enumerate}
    \item Sure convergence: $X_n \xrightarrow{p.w.} X$ 
    \item Almost sure convergence: $X_n \xrightarrow{a.s.} X$ 
    \item Convergence in probability: $X_n \xrightarrow{i.p.} X$ 
    \item Convergence in $r$th mean: $X_n \xrightarrow{rth} X$ (Mean square: $X_n \xrightarrow{m.s.} X$) 
    \item Convergence in distribution: $X_n \xrightarrow{D} X$
\end{enumerate}

\begin{theorem}[Hierarchy of Convergence]
The following implications hold: 
\begin{enumerate}
    \item Pointwise c. $\implies$ almost sure c. $\implies$ c. in probability $\implies$ c. in distribution. 
    \item $r$th mean c. $\implies$ c. in probability $\implies$ c. in distribution. 
\end{enumerate}
\end{theorem}

Trying to understand these relationships can be very hard, so we will take some time to do that, with some examples. First, convergence in distribution is clearly the weakest, since convergence in distribution does not imply that the random variables need be close to each other. Take a look at the random variables $X \sim \mathrm{Bernoulli}(1/2)$ and $Y = 1 - X$. $X$ and $Y$ are both $\mathrm{Bernoulli}(1/2)$ with the same distribution, but they are \textit{not} the same random variable since $X - Y = 1$ always. Therefore, we can think of two random variables that have the same distribution but are not "close" to each other as functions over $\Omega$ that divide it into identical, but differently cut, distributions. 
\begin{center}
    \includegraphics[scale=0.23]{prob_in_distribution.jpg}
\end{center}

\begin{example}[C. in Distribution $\centernot\implies$ C. in Probability]
Let $X_1, X_2, \ldots$ be such that $X_i = X$ for all $i$ where $X \sim \mathrm{Bernoulli}(1/2)$. This does not mean that the $X_i$'s are iid Bernoulli; they are all copies of the same $X$, i.e. forms a constant sequence. Let $Y = 1 - X$. Clearly, $X_n \xrightarrow{D} Y$ since the CDF of every $X_i$ is the same as that of $Y$, but $|X_n - Y| = 1$ for all $n$, so there is no convergence.  
\end{example}

\begin{example}[C. in Distribution $\centernot\implies$ C. in Probability]
Let $X_1, X_2, \ldots \sim \mathcal{N}(0, 1)$ and $Y = -X$. Then, by symmetry of the standard Gaussian, both $X$ and $Y$ have the same CDF, but they are not the same random variable: their signs are opposite. 
\end{example}

\subsection{Convergence in Probability vs Almost Surely} 

Convergence almost surely and convergence with probability are very different. Almost sure convergence has the limit inside the probability, which indicates that we are talking about convergence of a sequence of random variables. On the other hand, convergence in probability has the limit on the outside, which talks about convergence of a sequence of probabilities. But a key point is that almost sure convergence implies convergence in probability. It happens so because there could exist a subset of small probability in $\Omega$ where the $X_n$'s and $X$ need not be close, but the probabilities of them deviating over whole $\Omega$ is small. 

\begin{example}[C. in Probability $\centernot\implies$ C. Almost Surely]
Consider the interval $\Omega = [0, 1]$ and the subsets $A_1 = [0, 0.1], A_2 = [0.1, 0.2], \ldots$, such that at $A_{10} = [0.9, 1.0]$, the size with halve and will go to the left boundary, $A_{11} = [0, 0.05], \ldots$. Then, the sequence of indicator random variables 
\[X_n \coloneqq \mathbb{I}_{A_n}\]
looks like it's converging to the $0$ random variable. Indeed, $X_n \xrightarrow{i.p.} 0$ since the probability that $X_n$ deviates from $0$ by more than some small $\epsilon$ is simply the measure of $A_n$ itself, which decreases to $0$. That is, given some small $\epsilon > 0$, we have 
\[\lim_{n \rightarrow \infty} \mathbb{P} (|X_n - 0| > \epsilon) = \lim_{n \rightarrow \infty} \mathbb{P}(\mathbb{I}_{A_n} > \epsilon ) = \lim_{n \rightarrow \infty} \mathbb{P}(A_n) = 0\]
Now let's show that this doesn't converge almost surely. For \textit{any} outcome $\omega \in \Omega$, the sequence of random variables $X_1(\omega), X_2(\omega), \ldots$ will hit these intervals $A_n$ infinitely many times and will not converge to $0$, since there will always be a $1$ down the sequence. They will occur with decreasing frequency but they will always occur. Therefore, with probability $1$, whatever realized sequence will not converge to the $0$ random variable. 
\end{example}

Here is another standard counterexample. 

\begin{example}[C. in Probability $\centernot\implies$ C. Almost Surely]
Let us take the sequence $X_1, X_2, \ldots$ of independent random variables where $X_n \sim \mathrm{Bernoulli}(1/n)$. That is, 
\[\mathbb{P}(X_n = 1) = \frac{1}{n} \text{ and } \mathbb{P}(X_n = 0) = 1 - \frac{1}{n}\]
So, as $n$ gets large we expect $X_n$ to realize values of $0$ more and more. Showing that $X_n \xrightarrow{i.p.} 0$ is easy, since we can compute for any $\epsilon > 0$
\begin{align*}
    \lim_{n \rightarrow \infty} \mathbb{P}(|X_n - 0| > \epsilon) & = \lim_{n \rightarrow \infty} \mathbb{P}(|X_n| > \epsilon) \\
    & = \lim_{n \rightarrow \infty} \mathbb{P}(X_n = 1) \\
    & = \lim_{n \rightarrow \infty} \frac{1}{n} = 0
\end{align*}
We want to show that this does not converge almost surely to $0$, i.e. there is some set of nonzero measure such that for some $\omega$ in that set, the sequence $X_1 (\omega), X_2(\omega), \ldots$ does not converge to $0$. This can be hard to see at first, but the fact that we have independence and the terms are $\frac{1}{n}$ hints at the Borel-Cantelli lemma. Let $A_n$ be the event that $\{X_n = 1\}$ (i.e. the preimage of the singleton set under $X_n$: $X_n^{-1} ( \{1\})$). Then, the $A_n$'s are independent, and 
\[\sum_{n=1}^\infty \mathbb{P}(A_n) = +\infty \]
By the Borel-Cantelli lemma 2, this implies that almost surely infinitely many $A_n$'s will occur. That is, we can choose as large of an $n$ as we like, go down the sequence until we look at $X_n, X_{n+1}, \ldots$, and we are guaranteed with probability $1$ that at least one of the $X_i$'s after $n$ will realize a $1$. This means that in every realization of $X_1, X_2, \ldots$, we will get a sequence of $0$s and $1$s, but since BCL states that no matter how far down the road you will always get at least another $1$, this sequence does not converge to $0$.  
\end{example}

The commonality between these two examples is that sequence of random variables satisfies convergence in probability as follows: As $n$ increases, $X_n$ is more and more likely to be near $X$ (in the way that $|X_n - X| < \epsilon$ for some $\epsilon > 0$), ultimately satisfying this closeness property with probability $1$ as $n \rightarrow \infty$. For example, we could have 
\begin{align*}
    \mathbb{P}(|X_1 - X| > \epsilon) & = 1 \\
    \mathbb{P}(|X_2 - X| > \epsilon) & = 1/2 \\
    \mathbb{P}(|X_3 - X| > \epsilon) & = 1/3 \\
    \ldots & = \ldots 
\end{align*}
This definitely satisfies convergence in probability, but this leaves open the possibility that $\mathbb{P}(|X_n - X| > \epsilon)$ an infinite number of times, although at infrequent intervals. Therefore, when looking at the sequence 
\[X_1, X_2, X_3, \ldots\]
each random variable \textit{individually} may have less chance of being more than $\epsilon$ away from $X$, but since there is an infinite number of them in the sequence, the sequence \textit{in totality} may contain an infinite number of cases where $|X_n - X| > \epsilon$. Convergence almost surely tells us that we are \textit{guaranteed} (with probability $1$) that this sequence will converge to $X$. That is, we can specify an $N \in \mathbb{N}$ such that $|X_n - X| < \epsilon$ for all $n > N$. 

Let us define some $\epsilon > 0$ and consider a sequence of random variables $\{X_n\}_{n \in \mathbb{N}}$. Given some outcome $\omega \in \Omega$, we will consider it a \textit{success} if $|X_n(\omega) - X(\omega)| < \epsilon$ and \textit{failure} if not. Then, convergence in probability tells us that the probability of failure goes to $0$ as $n$ goes to infinity and therefore we get better and better estimates of $X$. Convergence almost surely is a bit stronger and says that the total number of failures is \textit{finite}. That is, after a certain point $N$, the random variable $X_n$ will \textit{always} estimate $X$ within an error of $\epsilon$ (i.e. such that $|X_n - X| < \epsilon$). But since you don't know when you've exhausted all failures, there is not much of a difference from a practical point of view. 


\subsection{Complete Convergence}

When proving almost sure convergence, we'd ideally just look at all the $\omega \in \Omega$ where $X_n (\omega) \rightarrow X(\omega)$, and if this set has probability measure $1$, then we are done. But this is not very practical, so we use the following theorem, which gives a sufficient condition for $X_n \xrightarrow{a.s.} X$. 

\begin{theorem}
If for all $\epsilon > 0$, 
\[\sum_{n=1}^\infty \mathbb{P}(|X_n - X| > \epsilon ) < \infty\]
then $X_n \xrightarrow{a.s.} X$. This condition is a bit stronger, since not only are we saying that $\mathbb{P}(|X_n - X| > \epsilon)$ tends to $0$ as $n \rightarrow \infty$, but that it goes down fast enough to keep the series convergent. 
\end{theorem}
\begin{proof}
Let the event that $|X_n - X| > \epsilon$ be denoted $A_n (\epsilon)$ (i.e. the preimage of $(\epsilon, \infty)$ under the map $|X_n - X|$, which is a $\mathcal{F}$-measurable set). Since the sum of their probabilities is finite, by the Borel-Cantelli lemma 1, finitely many $A_n(\epsilon)$'s will occur with probability $1$. This means that for any $\epsilon > 0$, $|X_n - X| \leq \epsilon$ for all large enough $n$, meaning that it converges to $0$. 
\end{proof}

\subsection{Laws of Large Numbers}

\begin{theorem}[Weak Law of Large Numbers]
Let $X_1, X_2, ..., X_n$ be a sequence of iid random variables, with finite mean $\mathbb{E}(X)$. Then, the average of the random variables $S_n / n$ converges in probability to $\mathbb{E}[X]$. 
\[\frac{S_n}{n} = \frac{1}{n} \sum_{i=1}^n X_i \xrightarrow{i.p} \mathbb{E}[X]\]
That is, for any $\epsilon > 0$, 
\[\lim_{n \rightarrow \infty} \mathbb{P} \bigg( \bigg| \Big( \frac{1}{n} \sum_{k=1}^n X_k \Big) - \mathbb{E}[X] \bigg| > \epsilon \bigg) = 0\]
\end{theorem}
\begin{proof}
We first do the proof assuming additionally that $X$ has finite variance, so $\mathrm{Var}(X) < \infty$. We will show that the random variable $S_n/n$ converges in mean square to $\mathbb{E}[X]$, which will imply convergence in probability. Note that $\mathbb{E}[S_n / n] = \mathbb{E}[X]$, and 
\begin{align*}
    \lim_{n \rightarrow \infty} \mathbb{E} \bigg[ \bigg| \frac{S_n}{n} - \mathbb{E}[X] \bigg|^2 \bigg] & = \lim_{n \rightarrow \infty} \mathbb{E} \bigg[ \bigg| \frac{S_n}{n} - \mathbb{E}\Big[\frac{S_n}{n}\Big] \bigg|^2 \bigg] \\
    & = \lim_{n \rightarrow \infty} \mathrm{Var}\Big( \frac{S_n}{n} \Big) \\
    & = \lim_{n \rightarrow \infty} \frac{\mathrm{Var}(S_n)}{n^2} \\
    & = \lim_{n \rightarrow \infty} \frac{\mathrm{Var}(X)}{n} = 0
\end{align*}
\end{proof}

\begin{theorem}[Strong Law of Large Numbers]
Let $X_1, X_2, ..., X_n$ be a sequence of iid random variables, with finite mean $\mathbb{E}(X_k)$ and with finite variance. Then, the average of the random variables $S_n / n$ converges almost surely to $\mathbb{E}[X]$. 
\[\frac{S_n}{n} \xrightarrow{a.s.} \mathbb{E}[X]\]
That is, 
\[\mathbb{P} \bigg( \Big\{ \omega \in \Omega \mid \lim_{n \rightarrow \infty} \Big( \frac{1}{n} \sum_{i=1}^n X_i (\omega) \Big) = \mathbb{E}[X] \Big\} \bigg) = 1\]
\end{theorem}

Now let's compare these two laws. They both deal with averages of random variables, i.e. we keep sampling from $X$ and compute the averages $\overline{X}_n$. The weak law states that for a specified large $n$, the average $\overline{X}_n$ is likely to be near $\mathbb{E}[X]$. But it leaves open the possibility that $|\overline{X}_n - \mathbb{E}[X]| > \epsilon$ happens an infinite number of times (although less frequently). So no matter how big of an $n$ we choose, there could always be an $\overline{X}_n$ in the future that fails to satisfy $|\overline{X}_n - \mathbb{E}[X]| > \epsilon$. However, the strong law shows that this almost surely will not occur. That is, with probability $1$, we have for any $\epsilon > 0$ the inequality $|\overline{X}_n - \mathbb{E}[X]| < \epsilon$ for all large enough $n$ greater than a certain $N$. Note that the weak law does not guarantee the existence of such an $N$. 

This result is very useful because it justifies experiments that estimate some value by taking averages. 

\begin{example}[Estimating Speed of Light]
Say that we are conducting an experiment to justify the speed of light, which will have true value $\mu$. The laws of large numbers say that in theory, after obtaining enough data, we can get arbitrarily close to the true speed of light. Choose $\epsilon > 0$ arbitrarily small. We can obtain $n$ estimates $X_1, \ldots, X_n$ of the speed of light and compute the average 
\[\overline{X}_n = \frac{1}{n} \sum_{i=1}^n X_i\] 
As we obtain more data, we can compute $\overline{X}_n$ for each $n = 1, 2, \ldots$. The weak law says that $\mathbb{P}(|\overline{X}_n - \mu| > \epsilon) \rightarrow 0$ as $n \rightarrow \infty$, i.e. the probability of our estimate being off by more than $\epsilon$ goes to $0$ (though it may happen with nonzero probability if we consider the infinite sequence). The strong law says that the number of times $|\overline{X}_n - \mu|$ is greater than $\epsilon$ is finite (with probability $1$), and after a certain point our estimates will perfectly lie within the error $\epsilon$. This gives us considerable confidence in the value $\overline{X}_n$ because it guarantees the existence of some $N \in \mathbb{N}$ s.t. $|\overline{X}_n - \mu| < \epsilon$ for all $n > N$, i.e. the average \textit{never} fails for $n > N$. 
\end{example}

\subsection{Central Limit Theorem}

By the law of large numbers, the sample averages converge almost surely (and therefore converge in probability) to the expected value $\mu$ as $n \rightarrow \infty$. The CLT describes the size and the distributional form of the stochastic fluctuations around $\mu$ during this convergence. That is, it states that as $n$ gets larger, the distribution of the difference $\overline{X}_n - \mu$ approximates a $\mathcal{N}(0, \sigma^2 / n)$ distribution, where $\sigma^2$ is the variance of $X$. 

\begin{theorem}[Central Limit Theorem]
Let $X_1, X_2, X_3, ...$ be a sequence of iid random variables, with mean $\mu = \mathbb{E}(X)$ and with variance $\Var(X) = \sigma^2 < \infty$. Then, the sequence of random variables $\{\overline{X}_n\}_{n \in \mathbb{N}}$ converges in distribution to a Gaussian $\mathcal{N}(\mu, \sigma^2 / n)$. That is, 
\[\overline{X}_n \xrightarrow{D} \mathcal{N}(\mu, \sigma^2 / n)\]
\end{theorem}

Roughly speaking, the law of large numbers says that 
\[\Big( \frac{1}{n} \sum_{k=1}^n X_k \Big) - \mu \approx 0\]
while the CLT involves renormalization. 
\[\sqrt{n} \bigg( \Big( \frac{1}{n} \sum_{k=1}^n X_k \Big) \bigg) \approx N(0, \sigma^2)\]
where $\approx$ means that the distributions are close when $n$ is large. The central limit theorem is essential when performing \textit{Normal approximation} described as such: If a random variable $Y$ is a sum of many iid random variables having certain mean and variance, then the distribution of $Y$ may be close to that of a normal random variable having the same mean and variance as $Y$. Since many random variables have this structure, we can use the CLT to approximate the distribution of a sum of independent random variables (e.g. the binomial distribution as a sum of $n$ Bernoulli distributions). 

It also turns out (?) that we can use CLT to prove the weak law of large numbers, since (roughly speaking) as $n$ increases, the distribution of $\overline{X}_n$ concentrates more and more around $\mu$, and therefore the probability of $|\overline{X}_n - \mu| < \epsilon$ tends to $1$. 

\section{Multivariate Gaussians}

Recall $X \sim \mathcal{N}(\mu, \sigma^2)$ implies that its PDF is 
\[f_X (x) = \frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{(x - \mu)^2}{2 \sigma^2}}\]
Now we will consider a Gaussian random \textit{vector}, which can be considered a vector of random variables  
\[\mathbf{X} = \begin{bmatrix} X_1 \\ X_2 \\ \vdots \\ X_n \end{bmatrix}\]
mapping from $\Omega$ to $\mathbb{R}^n$. It is not merely a vector where every $X_i$ is Gaussian, as we will show later. That is, a joint distribution that has all $n$ marginal distributions Gaussians does not make a multivariate Gaussian. 

This measurable function $\mathbf{X}: \Omega \rightarrow \mathbb{R}^n$ induces a probability law $\mathbb{P}_X$ on $\mathcal{B}(\mathbb{R}^n)$, and the Radon-Nikodym theorem states the existence of a PDF $f_X$ such that $\mathbb{P}_X (B) = \int_B f_X \,d\lambda$.

\subsection{Bivariate Gaussians}
Let us first begin with two-variable Gaussians. 

\begin{definition}[Standard Bivariate Gaussian RV]
A random variable $(X, Y)$ is said to be a \textbf{standard bivariate Gaussian} if its PDF is of form
\[f_{X, Y} (x, y) = \frac{1}{2 \pi \sqrt{1 - \rho^2}} \exp \bigg( -\frac{x^2 - 2 \rho x y + y^2}{2 (1 - \rho^2)} \bigg) \text{ for } \rho \in (-1, 1)\] 
\end{definition}

\begin{proposition}
Given a standard bivariate Gaussian $(X, Y)$, 
\begin{enumerate}
    \item $X$ and $Y$ are marginally distributed as $\mathcal{N}(0, 1)$. That is, if we integrate a variable (say, $x$) out, we will get a univariate standard Gaussian PDF of the other ($y$): 
    \[\int_{-\infty}^\infty \frac{1}{2 \pi \sqrt{1 - \rho^2}} \exp \bigg( -\frac{x^2 - 2 \rho x y + y^2}{2 (1 - \rho^2)}\bigg) \,dx = \frac{1}{\sqrt{2 \pi}} e^{-y^2 / 2}\]
    \item $\rho_{X, Y}$, the correlation coefficient of $X$ and $Y$, is equal to $\rho$. 
    \item The conditional distribution of $X$ given $Y = y$ is $X \mid Y = y \sim \mathcal{N} (\rho y, 1 - \rho^2)$. That is, 
    \[f_{X \mid Y} (x \mid y) = \frac{1}{2 \pi \sqrt{1 - \rho^2}} \exp \bigg( -\frac{x^2 - 2 \rho x y + y^2}{2 (1 - \rho^2)}\bigg) = \frac{1}{ \sqrt{2 \pi (1 - \rho^2)}} \, \exp\bigg( - \frac{(x - (\rho y))^2}{2 (1 - \rho^2)} \bigg)\]
    \item From (3), we can see that the conditional expectation $\mathbb{E}[X \mid Y = y] = \rho y$ since $X \mid Y = y$ has mean at $\rho y$. Therefore, the conditional expectation of $X$ given $Y$ (which is a random variable) is 
    \[\mathbb{E}[X \mid Y] = \rho Y\]
    i.e. $\mathbb{E}[X \mid Y]$ is a linear function of $Y$. 
\end{enumerate}
\end{proposition}

The formula of the general bivariate Gaussian $\mathbf{X} = (X_1, X_2)$ PDF is messy, but we will put it here. 
\[f_{X_1, X_2} (x, y) = \frac{\sigma_1 \sigma_2 \sqrt{1 - \rho^2}} \exp \bigg[ -\frac{1}{2} \bigg( \frac{(x_1 - \mu_1)^2}{\sigma_1^2} + \frac{(x_2 - \mu_2)^2}{\sigma_2^2} - 2 \rho \frac{(x_1 - \mu_1)}{\sigma_1} \frac{(x_2 - \mu_2)}{\sigma_2}\bigg)\bigg]\]
It is cleaner to put it into matrix form. 
\[f_{X_1, X_2} (x_1 , x_2) = \frac{1}{2 \pi \sqrt{\mathrm{det}(\boldsymbol{\Sigma})}} \exp \bigg( - \frac{(\mathbf{x} - \boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu})}{2} \bigg) \]
where 
\[\mathbf{x} = \begin{pmatrix} x_1 \\ x_2 \end{pmatrix}, \;\; \boldsymbol{\mu} = \begin{pmatrix} \mu_1 \\ mu_2 \end{pmatrix} , \;\; \boldsymbol{\Sigma} = \mathbb{E}\big[ (\mathbf{X} - \boldsymbol{\mu}) (\mathbf{X} - \boldsymbol{\mu})^T \big] = \begin{pmatrix} \mathrm{Var}(X_1) & \mathrm{Cov}(X_1, X_2) \\
\mathrm{Cov}(X_1, X_2) & \mathrm{Var}(X_2) \end{pmatrix}\]

Note that visually, $\boldsymbol{\Sigma}$ will determine how much the Gaussian distribution is "stretched" on one way or another. Obviously, the "peak" of the distribution will be $\boldsymbol{\mu}$. If $\boldsymbol{\Sigma} = I$, then we could visualize the Gaussian distribution as being perfectly symmetric. However, if we scale the distribution up to a certain constant (below shown $\boldsymbol{\Sigma} = I$, $\boldsymbol{\Sigma} = 0.61 I$, $\boldsymbol{\Sigma} = 2 I$), we get
\begin{center}
    \includegraphics[scale=0.65]{Gaussian_Distribution.png}
\end{center}

Now we've made a remark before that given a multivariate distribution $\mathbf{X} = (X_1, \ldots, X_n)$, all of its marginal distributions being Gaussian does not mean that $\mathbf{X}$ is a multivariate Gaussian. We give a counterexample. 

\begin{example}
Let $Y_1, Y_2$ be iid random variables distributed according to the PDF 
\[f_Y (y) = \sqrt{\frac{2}{\pi}} e^{-y^2 / 2} \text{ for } y > 0\]
which we can interpret as a one-sided Gaussian. Let $W \sim \mathrm{Bernoulli}(\frac{1}{2})$ be independent of $Y_1$ and $Y_2$. Now, define the random variables 
\[X_1 = W \, Y_1 \text{ and } X_2 = W \, Y_2\]
Now note that $Y_1$ and $Y_2$ are both positive, and since $X_1$ and $X_2$ are both dependent on the same value of $W$, it is either $X_1$ and $X_2$ are both positive or both negative. So, the joint distribution of $X_1, X_2$ will be on only the 1st and 3rd quadrant with no mass on the 2nd and 4th. 
\begin{center}
    \includegraphics[scale=0.23]{not_multi_Gaussian.jpg}
\end{center}
This is clearly not a multivariate Gaussian, even though the marginals are $X_1, X_2 \sim \mathcal{N}(0, 1)$. We could make the degenerate case that $X_1 = X_2$, which would make the image of $(X_1, X_2)$ just the line at $x_1 = x_2$, but we can think of this as a degenerate Gaussian with a singular $\boldsymbol{\Sigma}$. 
\end{example}

\subsection{Multivariate Gaussians}

There are three equivalent definitions of multivariate Gaussians of $n$-variables. 

\begin{definition}[Multivariate Gaussian]
Let us have a vector-valued random variable $\mathbf{X} = (X_1 \ldots X_n)^T \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})$. 
\begin{enumerate}
    \item $\mathbf{X}$ is a \textbf{multivariate Gaussian distribution} with mean $\boldsymbol{\mu} \in \mathbb{R}^n$ and symmetric, positive-definite covariance matrix $\boldsymbol{\Sigma} \in \mathbb{R}^{n \times n}$ if its probability density function is
    \[f_X (x) = \frac{1}{(2\pi)^{n/2} \mathrm{det}(\boldsymbol{\Sigma})^{1/2}} \exp\bigg( -\frac{1}{2} (x-\boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (x - \boldsymbol{\mu})\bigg)\]
    The covariance matrix $\boldsymbol{\Sigma}$ is the $n \times n$ matrix whose $(i, j)$th entry is $\Cov(X_i, X_j)$. That is, for any random vector $\mathbf{X}$ with mean $\boldsymbol{\mu}$, its covariance matrix 
    \[\boldsymbol{\Sigma} = \mathbb{E}\big[ (\mathbf{X} - \boldsymbol{\mu}) (\mathbf{X} - \boldsymbol{\mu})^T \big] = \mathbb{E}[\mathbf{X} \mathbf{X}^T] - \boldsymbol{\mu} \boldsymbol{\mu}^T\]
    is positive definite and symmetric, which implies by the spectral theorem we can break it down into $n$ orthogonal eigenspaces of positive eigenvalues. 

    \item $X$ is a multivariate Gaussian distribution if it can be expressed as 
    \[\mathbf{X} = \mathbf{D} \mathbf{w} + \boldsymbol{\mu}\]
    where $\mathbf{w}$ is a vector of independent $\mathcal{N}(0, 1)$ Gaussians, $\boldsymbol{\mu} \in \mathbb{R}^n$, and $\mathbf{D} \in \mathbb{R}^{n \times n}$. The mean of $\mathbf{X}$ is $\boldsymbol{\mu}$ and its covariance is $\boldsymbol{\Sigma} = \mathbf{D} \mathbf{D}^T$; $\mathbf{D}$ is called the \textbf{standard deviation matrix}. When modeling high-dimensional Gaussians, this way is most computationally feasible. 

    \item $X$ is a multivariate Gaussian distribution if for every $\mathbf{a} \in \mathbb{R}^n$, $\mathbf{a}^T \mathbf{x}$ is a Gaussian RV. This means that if we take $\mathbf{a} = \mathbf{0}$, then the entire $\mathbf{X}$ is constantly $0$, which we will take to be the degenerate Gaussian with mean, variance $0$. 
\end{enumerate}
The $n$ semi-axes of the $(n-1)$-dimensional isocontour ellipsoid formed by an $n$-dimensional Gaussian distribution are precisely the normalized eigenvectors of $\boldsymbol{\Sigma}$ multiplied by their eigenvalues. 
\end{definition}


If we let $\boldsymbol{\Sigma} = \mathbf{I}$, then this means that all the $X_i$'s are pairwise uncorrelated since $\Sigma_{ij} = \Cov (X_i, X_j) = 0$. In general, this does not mean that the $X_i$'s are independent, but for joint Gaussians, this also implies independence! 

\begin{theorem}
Given multivariate Gaussian $\mathbf{X} = (X_1 \ldots X_n)^T \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})$, the $X_i$'s are pairwise independent if and only if they are uncorrelated. 
\end{theorem}
\begin{proof}
We can expand the PDF of $\mathbf{X}$ as 
\begin{align*}
    f_X (x) & = \frac{1}{(2 \pi)^{n/2}} \exp \bigg( -\frac{1}{2} (x - \mu)^T (x - \mu) \bigg) \\
    & = \bigg(\frac{1}{\sqrt{2\pi}} \bigg)^n \exp \bigg( \sum_{i=1}^n -\frac{1}{2} (x_i - \mu_i)^2 \bigg) \\
    & = \prod_{i=1}^n \frac{1}{\sqrt{2\pi}} \exp \bigg( -\frac{1}{2} (x_i - \mu_i)^2 \bigg)
\end{align*}
which is the product of $n$ single-variable Gaussians $X_i$. Therefore this means that independence and uncorrelation are equivalent! 
\end{proof}

Therefore, if the nondiagonal entries of the covariance matrix are all $0$, then we know that the variables are all uncorrelated and therefore independent. 

\section{Order Statistics}
Let $X_1, X_2, ..., X_n$ be a finite collection of independent, identically distributed random variables. Suppose that they are continuously distributed with density $f$ and CDF $F$. 

\begin{definition}
Define the random variable $X_{(k)}$ to be the $k$th ranked value, called the \textit{$k$th order statistic}. This means that 
\[X_{(1)} = \min\{X_1, X_2, ..., X_n\}, \;\; X_{(n)} = \max\{X_1, X_2, ..., X_n\}\]
and in general, for any $k \in \{1, 2, ..., n\}$, 
\[X_{(k)} = X_j \text{ if } \sum_{l=1}^n \mathbb{I}_{X_l < X_j} = k - 1\]
which means that exactly $k-1$ of the values of $X_l$ are less than $X_j$. Since $F$ is continuous, 
\[X_{(1)} < X_{(2)} < ... < X_{(n)}\]
holds with probability $1$. This leads us to define the random variable $X_{(k)}$ representing the $k$th order statistic.
\[f_{(k)} (y) = \begin{cases} 
n \, {{n-1} \choose {k-1}} y^{k-1} (1-y)^{n-k} & y \in (0, 1) \\
0 & y \not\in (0,1)
\end{cases}\]
That is, $X_{(k)}$ has the Beta$(k, n-k_1)$ distribution. 
\end{definition}

\subsection{Poisson Arrival Process}
A \textit{Poisson Arrival Process} with rate $\lambda > 0$ on the interval $[0, \infty)$ is a model for the occurence of some events which may have at any time. We can interpret the process as a collection of random points in $[0, \infty)$ which are the times at which the arrivals occur. 

\textbf{Interpretation 1} Set $T_0 = 0$. The arrival times are random variables $0 < T_1 < T_2 < T_3 < ...$ such that the inter-arrival waiting times
\[W_k = T_k - T_{k-1}, \;\;\; k \geq 0\]
have the property that $\{W_k\}_{k=1}^\infty$ are independent Exp$(\lambda)$ random variables. 
\\
\\
\textbf{Interpretation 2} For any interval $I \subset [0, \infty)$, let
\[N_I \equiv \text{ number of arrivals that occur in interval } I\]
Then, $N_I \sim$ Poisson$(\lambda |I|)$, and for any collection of disjoint intervals $I_1, I_2, ..., I_n$, the random variables 
\[\{N_{I_k}\}_{k=1}^n\]
are independent. 

\begin{theorem}
These two interpretations of the arrival process are equivalent. 
\end{theorem}
\begin{proof}
In the 2nd interpretation, the statement $N_I \sim$ Poisson$(\lambda |I|)$ means that 
\[\mathbb{P}(N_I = m) = e^{-\lambda |I|} \frac{(\lambda |I|)^m}{m!}, \;\;\; m = 0, 1, 2, 3, ...\]
where $|I|$ is the length of interval $I$. From the first perspective, notice that 
\[T_k = W_1 + W_2 + ... + W_k\]
so that the $k$th arrival time $T_k$ is a sum of $k$ independent Exp$(\lambda)$ random variables. Thus, 
\[T_k \sim \text{Gamma}(k, \lambda)\]
and therefore has density
\[ \lambda e^{-\lambda t} \frac{(\lambda t)^{k-1}}{(k-1)!}, \;\;\; t>0\]
Note that the arrival times $T_i$ are not independent of each other, but the wait times $W_i$ are indeed independent. 
\end{proof}

We can slightly modify this to create a Poisson arrival process over some finite time horizon $[0, L]$. Again, you can do this two ways: 
\begin{enumerate}
    \item Starting with independent Exp$(\lambda)$ random variables $W_1, W_2, ...$, we define
    \[T_k = \sum_{i=1}^k W_i\]
    Once you have $T_k > L$, stop. 
    \item We let $N \sim$ Poisson$(\lambda L)$, since we are only working in finite interval $L$. Given $N = n$, let $U_1, U_2, ..., U_n \sim$ Uniform$([0, L])$. These define the arrival times, and let us order them to get
    \[T_k = U_{(k)}, \;\; k = 1, 2, ..., N\]
    where $U_{(k)}$ is the $k$th ordered point, with $T_1 = \min(U_1, ..., U_N)$. 
\end{enumerate}

\begin{lemma}[Memoryless Property]
The Exp$(\lambda)$ distribution has the property that for all $t, s \geq 0$, 
\[\mathbb{P}(W > t + s \; | \; W > t) = \mathbb{P}(W > s)\]
which is called the \textit{memoryless property}. We can interpret this in the following way. Let $W$ be the time you have to wait for the first arrival. Given that you already waited $t$ units of time, the probability that you have the wait $s$ additional units of time is just the probability that you wait at least $s$ from the beginning. That is, knowing that $t$ units of time have elapsed does not affect the distribution of the remaining waiting time. 
\end{lemma}

\begin{theorem}
Let $W$ be a continuously distributed random variable. Then $W \sim$ Exp$(\lambda)$ for some $\lambda > 0$ if and only if $W$ satisfies the memoryless property. 
\end{theorem}


\section{Markov Chains}
\subsection{Discrete Time Chains}
\begin{definition}
A \textit{Markov chain} is a sequence of random variables $\{X_n\}_{n=0}^\infty$, which take values in some set $\mathcal{S}$, called the \textit{state space} satisfying the \textit{Markov property}. Since we are working with discrete time chains, we will assume that $\mathbb{S}$ is a countable (and in most cases, finite). Thus, the $X_n$ will all be discrete random variables. We can also think of $X_n$ as a discrete "time" index; that is, $X_n$ is the state of the system at time $n$. Therefore, the sequence of random variables models a system evolving in a random way. 
\end{definition}

\begin{definition}
A sequence of random variables $\{X_i\}$ satisfies the \textit{Markov property} if 
\[\mathbb{P}(X_{n+1} = y \; | \; X_n = x_n, X_{n-1} = x_{n-1}, ..., X_0 = x_0\} = \mathbb{P}(X_{n+1} = y \; | \; X_n = x_n\}\]
holds for any choice oc states $y, x_n, x_{n-1}, ..., x_0 \in \mathcal{S}$ and for any $n \geq 1$. 
\end{definition}
Colloquially, given that one is at state $X_n = x_n$, knowing all the previous states does not help in predicting $X_{n+1}$. Knowing only the current state is relevant in predicting the next one. We can model this entire system using a matrix. 

\begin{definition}
Assuming that the chain is \textit{time-homogeneous}, the \textit{transition probability matrix} $P$ has elements $P_{x y}$ defined
\[P_{x y} = P(x, y) = \mathbb{P}(X_1 = y \,|\, X_0 = x) = \mathbb{P}(X_{n+1} = y \,|\, X_n = x)\]
which is the probability of moving from state $x$ to state $y$ in one step. The time homogeneous condition refers to the last equality; that is, the one-step transition probabilities don't change with the time index $n$. Note that if $\mathcal{S}$ is finite, then $P$ is a $|S| \times |S|$ matrix, and if $\mathcal{S}$ is countably infinite, then $P$ is an infinite-dimensional matrix. The axioms of probability imply that $A^T$ is an entry-wise nonnegative stochastic matrix.
\end{definition}

\begin{example}[Random Walks]
A \textit{random walk} on the integers $\mathcal{S} = \mathbb{Z}$ where a point has equal probability of moving right or left can be modeled with the probability function. 
\[P(x, y) = \mathbb{P}(X_{n+1} = y \, | \, X_n = x) = \begin{cases}
\frac{1}{2} & y = x + 1 \\
\frac{1}{2} & y = x - 1\\
0 & otherwise
\end{cases}\]
This can be generalized to multiple dimensional random walks on graphs with probability function 
\[P(x, y) = \frac{1}{\text{deg}(x)}\]
where deg$(x)$ is the number of adjacent nodes to node $x$. In this way, the point hops randomly from node to node, and if the graph is connected, then the walker can visit any vertex in the graph. 
\end{example}

\begin{example}[Discrete Moran Model]
Consider a population of size $N$. Each individual is one of two types (say, red or blue). At each time step, the system evolves in the following way: First, one of the individuals is chosen uniformly at random to be eliminated from the population; and another individual is chosen uniformly at random to produce one offspring identical to itself. These two choices are made independently. So, if a red individual is chosen to reproduce, and a blue one is chosen for elimination, then the total number of red particles increases by one and the number of blue particles decreases by one. If a red is chosen for reproduction and a red is chosen for elimination, then there is no net change in the number of reds and blues. Let $X_n$ be the number of red individuals at time $n$. The transition matrix for this chain is
\[P_{i j} = \begin{cases}
\frac{i}{N} \bigg(\frac{N-i}{N} \bigg) & j=i-1, i \neq 0 \\
\bigg(\frac{N-i}{N} \bigg) \frac{i}{N} & j=i+1, i \neq N \\
1 - 2 \bigg(\frac{N-i}{N} \bigg) \frac{i}{N} & j = i \\
0 & \text{otherwise}
\end{cases}\]
Note that the states $X_n = 0$ and $X_n = N$ are absorbing states, which represents a phenomenon called \textit{fixation}. 
\end{example}

\begin{definition}
A certain state $F$ in the state space $\mathcal{S}$ of a Markov chain is called an \textit{absorbing state} if
\[\mathbb{P}(X_{n+1} = F \; | \; X_n = F) = 1 \iff \mathbb{P}(X_{n+1} \neq F \; | \; X_n = F) = 0\]
\end{definition}

\begin{theorem}
Let there exist a time homogeneous Markov chain with transition probability matrix $P$. Given a probability distribution $\nu_n$ (a row vector) representing the a state of a system at time $t=n$, the probability distribution of which state the system will be at when $t=n+1$ can be calculated by 
\[\nu_{n+1} = \nu_n P\]
The probability distribution of the state of the system at $t=n+k$ can be calculated by summing up all of the possible probabilities that lead to each state at $t=n+k$. It is calculated equivalently as matrix multiplication: 
\[\nu_{n+k} = \nu_n P^k\]
\end{theorem}

\begin{definition}
The distribution $\nu$ of a Markov chain at time $t=0$ is called the \textit{initial distribution} for the chain. That is, $\nu$ is the initial distribution if 
\[\mathbb{P}(X_0 = x) = \nu(x)\]
\end{definition}

\begin{definition}
An \textit{invariant distribution}, or \textit{stationary distribution}, is a probability distribution $\pi$ such that 
\[\pi P = \pi\]
This means that 
\[\pi P^k = \pi\]
for all $k \in \mathbb{N}$. We can equivalently call $\pi$ the left eigenvector of matrix $P$ with eigenvalue $1$. If $\pi$ is an invariant distribution for the chain, and $X_0 \sim \pi$, then the distribution of $X_n$ does not change with $n$; it is invariant. Note that this does not mean that $X_n$ is constant; rather, it means that the distribution of $X_n$ is not changing. 
\end{definition}

\begin{example}
Let us have a two node system with nodes labeled $L$ and $R$. That is, $\mathcal{S} = \{L, R\}$. Consider a chain on this state space with transition probability matrix. 
\[P = \begin{pmatrix}
1-a & a \\ b & 1-b 
\end{pmatrix}\]
which can be visualized in the following diagram below.
\begin{center}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,
                    thick,main node/.style={circle,draw}]
    \node[main node] (R) {R};
    \node[main node] (L) [left of=R] {L};
    \path[every node/.style={font=\sffamily\small}]
    (L) edge [loop left] node {1-a} (L)
        edge [bend left] node {a} (R)
    (R) edge [loop right] node {1-b} (R)
        edge [bend left] node {b} (L);
\end{tikzpicture}
\end{center}

Then, the stationary distribution is 
\[\pi = \Big( \frac{b}{a+b}, \frac{a}{a+b} \Big)\]
Notice that if $a = b = 0$, then this definition is ill-defined, and any probability distribution is invariant since $P = I_2$, the identity matrix. 
\end{example}

\begin{definition}
A state $x \in \mathcal{S}$ is \textit{recurrent} if
\[\mathbb{P}(X_n = n \text{ for some } n \geq 1 \, | \, X_0 = x\} = 1\]
That is, if the initial state is $x$, the chain has probability $1$ of returning to $x$ at some later time. If a state is not recurrent, then the state is said to be \textit{transient}. That is, if $x$ is transient, there is some positive probability that the chain will never return to $x$. 
\end{definition}

\begin{definition}
Two states $x, y \in \mathcal{S}$ are said to \textit{communicate}, denoted $x \leftrightarrow y$, if there are positive integers $n$ and $m$ such that 
\[P^{(n)} (x, y) > 0 \text{ and } P^{(m)} (y, x) > 0\]
That is, there is some positive probability that the chain can go from $x$ to $y$ and from $y$ to $x$ in some number of steps. 
\end{definition}

\begin{definition}
If all pairs $x, y \in \mathcal{S}$ communicate, then the chain is said to be \textit{irreducible}. If there exists a pair of states that do not communicate, then the chain is said to be \textit{reducible}. 
\end{definition}

Note that the notion of communication is an equivalence relation between states. That is, it satisfies the properties. 
\begin{enumerate}
    \item $x \leftrightarrow x$.
    \item $x \leftrightarrow y \implies y \leftrightarrow x$.
    \item $x \leftrightarrow y, y \leftrightarrow z \implies x \leftrightarrow z$.
\end{enumerate}
This relation partitions the state space $\mathcal{S}$ uniquely into transient states and irreducible sub-chains
\[\mathcal{S} = T \cup C_1 \cup C_2 \cup ...\]
More specifically, $T$ is the set of all transient states, and the sets $C_k$ are \textit{closed communication classes}, meaning that
\begin{enumerate}
    \item For all $x, y \in C_k$, $x \leftrightarrow y$. 
    \item $P(x, z) = 0$ whenever $x \in C_k$ but $z \not\in C_k$. 
\end{enumerate}
Note that for all $x, y \not\in T$, $x$ and $y$ communicate if and only if $x$ and $y$ are in the same class $C_k$. Moreover, once the chain reaches one of the sets $C_k$, it cannot leave $C_k$. 

\begin{definition}
For any state $x \in \mathcal{S}$, the \textit{period} of $x$ is defined to be
\[d(x) \equiv \gcd \{n \geq 1 \; | \; P^{(n)} (x, x) > 0\}\]
\end{definition}

\begin{theorem}
It follows that if two states $x$ and $y$ communicate, then they must have the same period: $d(x) = d(y)$. It naturally follows that if the chain is irreducible, then all states must have the same period, and we can define the period of the chain to be $d(x)$ for any $x$ we choose.
\end{theorem}

\begin{definition}
If an irreducible chain has period $1$, the chain is said to be \textit{aperiodic}. Otherwise, the chain is \textit{periodic} with period $d > 1$. 
\end{definition}

\begin{theorem}
Suppose $|\mathcal{S}| < \infty$. If the chain is irreducible, then there always exists a unique stationary distribution $\pi$. If the chain is also aperiodic, then for any initial distribution $\nu$, 
\[\lim_{k \rightarrow \infty} \nu P^k = \pi \]
Hence
\[\lim_{k \rightarrow \infty} P^{(k)}(x, y) = \pi(y)\]
for all $x, y \in \mathcal{S}$. Furthermore, for any function $F: \mathcal{S} \longrightarrow \mathbb{R}$, the limit
\[\lim_{N \rightarrow \infty} \frac{1}{N} \sum_{n=1}^N F(X_n) = \sum_{x \in \mathcal{S}} F(x)\, \pi(x) = \mathbb{E} \big( F(x) \big)\]
holds with probability $1$. In particular, the limit does not depend on the initial distribution. 
\end{theorem}
\begin{proof}
The Frobenius Extension to Perron's theorem (Linear Algebra, Theorem 7.31) combined with its applications to stochastic matrices (Linear Algebra, Theorem 7.30) proves this statement. 
\end{proof}

\begin{definition}
For each $x \in \mathcal{S}$, define the \textit{first visit} to $x$ by 
\[T_x \equiv \min\{ n \geq 1 \; | \; X_n = x\}\]
This $T_x$ is an integer-valued random variable. We say $T_x = + \infty$ if $X_n$ never reaches $x$. Then, we define the \textit{mean return time} to $x$ by 
\[\mu_x \equiv \mathbb{E}\big( T_x \, | \, X_0 = x)\]
If $x$ is transient, then $\mu_x = + \infty$, since there is positive probability that $T_x = + \infty$. 
\end{definition}

\begin{definition}
It is possible that $x$ is recurrent while $\mu_x = +\infty$. If this is the case, then $x$ is said to be \textit{null-recurrent}. If $x$ is recurrent and $\mu_x < \infty$, then $x$ is said to be \textit{positive recurrent}. 
\end{definition}

\begin{theorem}
An irreducible chain has a stationary probability distribution $\pi$ if and only if all states are positive recurrent. If a chain is irreducible and all states are positive recurrent, then 
\[\pi(x) = \frac{1}{\mu_x}\]
for all $x \in \mathcal{S}$. $\pi$ is also unique. 
\end{theorem}

\subsubsection{Exit Probabilities}
Suppose a chain is finite and irreducible. Let $a, b \in \mathcal{S}$ be given states, and let us define $h(x)$ to be the probability of hitting $b$ before $a$, given that we start from $x$. 
\[h(x) \equiv \mathbb{P} (X_n \text{ reaches } b \text{ before } a \, | \, X_0 = x)\]
Clearly, $h(b) = 1$ and $h(a) = 0$. By conditioning on the first jump out of $x$, we also have 
\begin{align*}
    h(x) & = \mathbb{P}(X_n \text{ reaches } b \text{ before } a \, | \, X_0 = x) \\
    & = \sum_{y} \mathbb{P}(X_n \text{ reaches } b \text{ before } a \, | \, X_1 = y, X_0 = x) \, \mathbb{P}(X_1 = y \,|\,X_0 = x) \\
    & = \sum_y \mathbb{P}(X_n \text{ reaches } b \text{ before } a \, | \,X_1 = y, X_0 = x) \, P(x, y) \\
    & = \sum_y \mathbb{P}(X_n \text{ reaches } b \text{ before } a \, | \,X_1 = y) \, P(x, y) \\
    & = \sum_y h(y) \, P(x, y) 
\end{align*}
The sum is over all $y \in \mathcal{S}$ for which $P(x, y) \neq 0$. This gives us a linear system of equations to solve for $h$
\begin{align*}
    & h(x) = \sum_y P(x, y) h(y) \,\, \forall x \in \mathcal{S} \setminus \{a, b\}, \\
    & h(b) = 1, \\
    & h(a) = 0
\end{align*}

\subsubsection{Exit Prize}
Let $B \subset \mathcal{S}$ be some subset of the state space, and let $g: B \longrightarrow \mathbb{R}$ be some function. Consider the function 
\[h(x) = \mathbb{E}\big( g(X_\tau) \, |\, X_0 = x \big)\]
where $\tau = \min\{ n\geq 0 \,|\, X_n \in B\}$ is the first time that the chain reaches some state in the set $B$ (this time is random). We can interpret $g(y)$ as a "prize" that is awarded if the chain first reaches $B$ at state $y$, which means that $h(x)$ is the expected prize, given that $X_0 = x$. If $x \in B$, then $\tau = 0 \implies h(x) = g(x)$. But if $x \not\in B$, then by the same argument as shown in exit probabilities, it is true that $h$ satisfies the linear system of equations
\begin{align*}
    & h(x) = \sum_g P(x, y)\,h(y), \;\; \forall x \in \mathcal{S} \setminus B, \\
    & h(x) = g(x), \;\; x \in B 
\end{align*}
Note that Exit probability system is a special case of the Exit prize system. In the former, we have defined $B = \{a, b\}$ and $g$ defined by $g(a) = 0, g(b) = 1$. 

\subsubsection{Occupation Times, Absorbing States}
Suppose that a chain on a finite $\mathcal{S}$ is irreducible. Let $B \subset \mathcal{S}$ be some subset of states and let $A = \mathcal{S} \setminus B$ be the other states. Then for $x \in A$, we wish to know how many steps the chain will take before reaching a state in the set $B$. We define 
\[\tau_B = \min\{n \geq 0 \,|\, X_n \in B\}\]
which represents the first time that $X$ is in $B$, an integer valued random variable. We wish to compute
\[h(x) = \mathbb{E}(\tau_B \,|\, X_0 = x)\]
Clearly, $h(y) = 0$ for all $y \in B$. For $x \in A$, it takes at least one step to reach $B \implies h(x) \geq 1$ for $x \in A$. We condition on the first step from $x$. This leads to the system \begin{align*}
    h(x) = 1 + \sum_{y \in \mathcal{S}} P(x, y) \, \mathbb{E}(\tau_B \,|\, X_1 = y), & \forall x \in A = \mathcal{S} \setminus B
\end{align*}
Since the chain is time-homogeneous, this means that
\begin{align*}
    h(x) = 1 + \sum_{y \in \mathcal{S}} P(x, y) \, h(y), & \forall x \in A 
\end{align*}
Since $h(y) = 0$ for all $y \in B$, we now have
\begin{align*}
    h(x) = 1 + \sum_{y \in A} P(x, y) \, h(y), & \forall x \in A 
\end{align*}
To solve this system, let us define $M$ as the $|A| \times |A|$ submatrix of $P$ obtained by keeping only the entries $P(x, y)$ with $x, y \in A$. So, the system can be written as
\begin{align*}
    h(x) = 1 + \sum_{y \in A} M(x, y) \, h(y), & \forall x \in A
\end{align*}
We can solve this system of equations through the equivalent matrix equation
\[(I - M) h = 1\]
where $1 = (1, 1, ..., 1)^T$ is the column vector consisting of all $1$'s. The solution vector is therefore
\[h = (I - M)^{-1} 1\]
So, for a particular $x \in A$, 
\[h(x) = \sum_{y \in A} (I - M)^{-1} (x, y)\]
\\

Alternatively, we can slightly modify the chain to chain $\Tilde{X}_n$ by replacing the transition probability matrix $P$ with another one defined as 
\[\Tilde{P}(x, y) = \begin{cases}
P(x, y) & x \in A, y \in \mathcal{S} \\
1 & x = y \in B \\
0 & \text{else}
\end{cases}\]
This modification means that all transitions from state in $A$ to any other state are preserved and the only transitions from a state $x \in B$ are self loops. In particular, all transitions from states $x \in B$ to states $y \in A$ are removed. Therefore, under this modified transition matrix, the states in $B$ are absorbing states. The tail sum formula implies that
\[\mathbb{E}(\tau_B \,|\, X_0 = x) = \sum_{k=0}^\infty \mathbb{P}(\tau_B > k \,|\, X_0 = x)\]
Notice that since the chain $X_n$ and $\Tilde{X}_n$ have the same transition rules before hitting a state $B$, we have 
\[P^{(k)} (x, y) = \Tilde{P}^{(k)} = M^{(k)}(x, y)\]
where $M$ is the $|A| \times |A|$ submatrix defined previously. Therefore, putting this all together, we have
\begin{align*}
    \mathbb{E}(\tau_B \,|\, X_0 = x) & = \sum_{k=0}^\infty \mathbb{P}(\tau_B > k \,|\, X_0 = x) \\
    & = \sum_{k=0}^\infty \mathbb{P}(\Tilde{X}_k \in A \,|\, X_0 = x) \\
    & = \sum_{k=0}^\infty \sum_{y \in A} \Tilde{P}^{(k)} (x, y) \\
    & = \sum_{k=0}^\infty \sum_{y \in A} M^{(k)} (x, y) \\
    & = \sum_{y \in A} \bigg( \sum_{k=0}^\infty M^{(k)} \bigg) (x, y) 
\end{align*}
Using a theorem from linear algebra, we can show that if all the eigenvalues of a $d \times d$ matrix $M$ have modulus strictly less than $1$, then $I-M$ is invertible and
\[\sum_{k=0}^\infty M^{(k)} = (I - M)^{-1}\]
where $I$ is the $d \times d$ identity matrix. If $M$ is the $|A| \times |A|$ submatrix described above, one can show that $M$ has his property and that $I - M$ is invertible. Hence, 
\[\mathbb{E}(\tau_B \,|\, X_0 = x) = \sum_{y \in A} \bigg( \sum_{k=0}^\infty M^{(k)} \bigg) (x, y) = \sum_{y \in A} (I-M)^{-1} (x, y)\]
which refers to the $(x, y)$ entry of the matrix $(I - M)^{-1}$. This is indeed consistent with our previous derivation of the formula for $h(x)$, the expected number of steps before the state reaches $B$. 

\subsection{Markov Chain Monte Carlo Algorithms}
In statistics, Markov chain Monte Carlo (MCMC) methods comprise of a class of algorithms for sampling from a probability distribution by constructing a Markov chain that has the desired distribution as its equilibrium distribution. That way, by recording samples from the chain, one may get better approximations of the actual distribution. 

Let there exist a state space $\mathcal{S}$ with some probability distribution $\pi(x)$ for every $x \in \mathcal{S}$. Clearly, 
\[\sum_{x \in \mathcal{S}} \pi(x) = 1\]
but the problem is that we do not know that $\pi$ is. We do know, however, another function $f$ that is directly proportional to $\pi$. 
\[\pi(x) = \frac{f(x)}{c}, \text{ where } c = \sum_{x \in \mathcal{S}} f(x)\]
is the normalizing constant. It is often the case that $c$ is unknown and the state space $\mathcal{S}$ is so large that computing $c$ directly is expensive. Therefore, we construct Markov chains that can provide approximations to $\pi$. 

\subsubsection{Metropolis-Hastings Algorithm}
This algorithm is useful because it does not require knowledge of the normalizing constant $c$. The algorithm only requires evaluations of 
\[\frac{\pi(x)}{\pi(y)} = \frac{f(x)}{f(y)}\]
We first have the state space $\mathcal{S}$ consisting of all the possible states. We now construct (any) probability transition matrix $q$ for a Markov chain on $\mathcal{S}$. Note that $q$ is a $|\mathcal{S}| \times |\mathcal{S}|$ matrix and $q^T$ is a stochastic matrix. This matrix is constructed by the user and is completely well-defined and known. We start off with any initial state $x_0 \in \mathcal{S}$ and iterate the following 2-steps to construct a Markov chain. 
\begin{enumerate}
    \item Given a state $X_n = x$, we generate a new state $X_{n+1}$ by first proposing a new state $y \in \mathcal{S}$ with probability $q(x, y)$ (determined from the matrix $q$). 
    \item With this chosen state $y$, we decide whether to accept to reject the proposal. With probability 
    \[\min \bigg( 1, \frac{\pi(y) \,  q(y, x)}{\pi(x) \, q(x, y)} \bigg)\]
    we accept the proposal and set $X_{n+1} = y$. Otherwise, the proposal is rejected and the new state is the same $X_{n+1} = x$. 
\end{enumerate}
Note that there are two levels of randomness here: which state the new state $y$ will be and whether to accept this state to be the next one or not. If step two did not exist (i.e. the probability of accepting the proposal is always $1$), then this would just be a regular Markov chain represented by the matrix $q$. But the addition of step 2 means that while $q$ is used in constructing the discrete chain $X_n$, it is \textit{not} the transition probability matrix of $X_n$. 

There is also a lot of flexibility on choosing $q$, although the performance of the algorithm (speed of convergence of the distribution of $X_n$ to the stationary distribution) will depend on the choice.

\begin{proposition}
For the chain defined by the Metropolis-Hastings algorithm, the distribution $\pi$ is stationary. 
\end{proposition}
\begin{proof}
Let us write in shorthand 
\[\alpha(x, y) = \frac{\pi(y)\, q(y, x)}{\pi(x)\, q(x, y)}\]
First, observe that if $x \neq y$, the transition probability for the chain defined by the algorithm is just
\[P(x, y) = q(x, y)\, \min\{1, \alpha(x, y)\}\]
Next, we claim that for all $x, y \in \mathcal{S}$, 
\[\pi(x) P(x, y) = \pi(y) \, P(y, x) \]
This condition is called \textit{detailed balance}. Assuming that $\alpha(x, y) \leq 1$, it is true that
\[\pi(x) P(x, y) = \pi(x) q(x, y) \frac{\pi(y) q(y, x)}{\pi(x) q(x, y)} = \pi(y) q(y, x)\]
In this case, we also have $\alpha(y, x) = 1 / \alpha(x, y) \leq 1$. So, 
\[\pi(y) P(y, x) = \pi(y) q(y, x) \]
and we have proved what we had claimed. Now, summing over $x$,
\[\sum_x \pi(x) P(x, y) = \sum_x \pi(y) P(y, x) = \pi(y) \sum_x P(y, x) = \pi(y)\]
since $P^T$ is stochastic. 
\end{proof}

\subsubsection{Gibb's Sampling}
Let $\mathcal{A} = \{a_1, ..., a_k\}$ be some finite set. Suppose that the state space 
\[\mathcal{S} = \mathcal{A} \times ... \times \mathcal{A} = \mathcal{A}^M\]
for some $M \in \mathbb{N}$. The following algorithm generates a Markov chain on $\mathcal{S}$ with stationary distribution
\[\pi(x) = \frac{f(x_1, x_2, ..., x_M)}{c}, \;\; x = (x_1, x_2, ..., x_M) \in \mathcal{S} \]
where $c >0$ is a normalizing constant. Note that $|\mathcal{S}| = k^M$, so computing $c$ may be expensive when $M$ is large. The current state of the chain is denoted 
\[X_n = (X_n^1, X_n^2, ..., X_n^M)\]
We think of $X_n$ as having $M$ components, each component taking values in $\mathcal{A}$. We start off with any initial state $X_0 = (X_0^1, X_0^2, ..., X_0^M)$ and construct a Markov chain by iterating the following two steps. 
\begin{enumerate}
    \item Given $X_n = (X_n^1, X_n^2, ..., X_n^M)$, we generate the next state $X_{n+1}$ by picking a component index $i \in \{1, ..., M\}$ uniformly at random. 
    \item With this chosen, well-defined $i$, we choose a random $Y^i \in \mathcal{A}$ according to the distribution
    \[\mathbb{P}(Y^i = a) = \frac{f\big(X_n^1 ,..., X_n^{i-1}, a, X_n^{i+1}, ..., X_n^M\big)}{\sum_{j=1}^k f\big(X_n^1 ,..., X_n^{i-1}, a_j, X_n^{i+1}, ..., X_n^M\big)}, \;\; a \in \{a_1, ..., a_k\}\]
    \item Then, set $X_{n+1} = \big(X_n^1, ..., X_n^{i-1}, Y^i, X_n^{i+1}, ..., X_n^M\big)$. 
\end{enumerate}
Note that at each step, only one component of $X_n$ is updated. Observe that the distribution above is also equal to 
\[\mathbb{P}(Y^i = a) = \frac{\pi\big(X_n^1 ,..., X_n^{i-1}, a, X_n^{i+1}, ..., X_n^M\big)}{\sum_{j=1}^k \pi \big(X_n^1 ,..., X_n^{i-1}, a_j, X_n^{i+1}, ..., X_n^M\big)}\]
which is the marginal distribution of the $i$th component, given the values of the other components. 

\begin{proposition}
For the chain defined by this algorithm, the distribution $\pi$ is stationary. 
\end{proposition}
\begin{proof}
We verify that the detailed balance condition holds. It is also helpful to note that $P(x, y) \neq 0$ if and only if $x$ and $y$ differ in one coordinate. 
\end{proof}

\subsection{Continuous Time Markov Chains}
As the name suggests, in a continuous time Markov chain $X_t$, the time parameter is continuous ($t \geq 0$). As before, the system jumps randomly between states in $\mathcal{S}$, but now the jumps may occur at any time and they occur randomly. This implies that there are \textit{two} sources of randomness:
\begin{enumerate}
    \item \textit{where} the system jumps and 
    \item \textit{when} the system jumps
\end{enumerate}

\begin{definition}
The Markov property in the continuous time case says that for any $s, t \geq 0$ and $y \in \mathcal{S}$, 
\[\mathbb{P}(X_{t + s} = y \, | \, X_t) = \mathbb{P}(X_{t+s} = y \, | \, X_r \; \forall 0 \leq r \leq t)\]
Colloquially, the conditional distribution of $X_{t+s}$ given the history up to time $t$ is the same as the conditional distribution of $X_{t+s}$ given only $X_t$. Thus, if we know the current state at $t$, knowing information about the past doesn't help us better predict the future state $X_{t+s}$. 
\\

In order for the Markov property to hold, the times between jumps must be exponentially distributed random variables because it is the only density that has the memoryless property. This fact has already been stated in a theorem when covering Poisson arrival processes. This is what makes Exp$(\lambda)$ so important for continuous time Markov chains. 
\end{definition}

\begin{lemma}
Let $T_1, T_2, ..., T_n$ be independent exponential random variables with rates $\lambda_1, \lambda_2, ..., \lambda_n$, respectively. Then the random variable $T \equiv \min\{T_1, T_2, ..., T_n\}$ is
\[T \sim \text{Exp}\Big(\sum_{i=1}^n T_i\Big)\]
Moreover, 
\[\mathbb{P}(T_k = \min\{T_1, ..., T_n\}) = \frac{\lambda_k}{\lambda_1 + ... + \lambda_n}\]
\end{lemma}

We can interpret the lemma above by imagining that we have $n$ alarm clocks all set simultaneously, which will ring independently at random times. Suppose that clock $k$ will ring after $T_k$ units of time have expired, where $T_k$ is a random variable distributed as Exp$(\lambda_k)$. Then, $T = \min\{T_1, ..., T_n\}$ is the time at which the first ring occurs. 

\begin{example}
The simplest and the most important continuous time Markov chains is the Poisson arrival process. The process really has a single parameter $\lambda >0$ (the rate of process) by definition and is integer valued. At each jump time, the process increases by $1$, and the time between jumps are independent, distributed as Exp$(\lambda)$. 

Notice that when $\lambda$ is large, the arrivals occur more frequently than when $\lambda$ is small, because the expected time between arrivals is $1/\lambda$. The second way we can interpret it is to choose an interval of time $t$ and let $X_t$ be the number of jumps that have occurred up to time $t$. It is a fact that $X_t$ is a integer-valued, Poisson$(\lambda t)$ distribution. That is, 
\[\mathbb{P}(X_t = k) = e^{-\lambda t} \frac{(\lambda t)^k}{k!}, \; k = 0, 1, 2, ...\]
In particular, $\mathbb{E}(X_t) = \lambda t$ and $\Var(X_t) = \lambda t$. 
\end{example}

\subsection{Branching Processes}
\begin{definition}
A \textit{branching process} is a type of Markov chain modeling a population in which each individual produces a random number of children (possibly $0$) and dies. The state space is $\mathcal{S} = \{0, 1, 2, 3, ...\}$. Furthermore, there is a discrete-time version and a continuous time version of the chain. In the discrete case, the state is $Z_n$, the size of the population at time $n = 0, 1, 2, ...$, and in the continuous case, the state is $Z_t$ for $t \geq 0$. 
\end{definition}

\subsubsection{Discrete-time Branching Process}
In the discrete case, all of the $Z_n$ individuals in the current generation branch at the same time and immediately die. The branching is independent and distributed according to the \textit{offspring distribution} $\{p_k\}_{k=0}^\infty$. Specifically, if $Z_n = m$, then 
\[Z_{n+1} = Y_1^n + Y_2^n + ... + Y_m^n\]
where $Y_i^n$ represents the number of offspring the $i$th individual in the $n$th generation has. All of them are distributed as
\[\mathbb{P}(Y_i^n = k) = p_k, \; k = 0, 1, 2, 3, ...\]
where $p_k$ is the probability that a parent has $k$ children. Note that if $p_0 \neq 0$, then there is positive probability that $Y_i^n = 0$ for all $i$, meaning that the population can go extinct. A sample branching process up to the second generation is shown below. 
\begin{center}
\begin{tikzpicture}[scale=0.8]
    \draw[fill] (0,4) circle (0.05);
    \draw[fill] (-2,2) circle (0.05);
    \draw[fill] (0,2) circle (0.05);
    \draw[fill] (2,2) circle (0.05);
    \draw[dashed] (0,4)--(-2,2);
    \draw[dashed] (0,4)--(0,2);
    \draw[dashed] (0,4)--(2,2);
    \draw[fill] (-3,0) circle (0.05);
    \draw[fill] (-1,0) circle (0.05);
    \draw[dashed] (-2,2)--(-3,0);
    \draw[dashed] (-2,2)--(-1,0);
    \draw[dashed] (2,2)--(3,0);
    \draw[dashed] (2,2)--(1,0);
    \draw[dashed] (2,2)--(2,0);
    \draw[fill] (3,0) circle (0.05);
    \draw[fill] (2,0) circle (0.05);
    \draw[fill] (1,0) circle (0.05);
    \node at (5,4) {$Z_0 = 1$};
    \node at (5,2) {$Z_1 = 3$};
    \node at (5,0) {$Z_2 = 5$};
    \draw[->] (4.3,4)--(3.5,4);
    \draw[->] (4.3,2)--(3.5,2);
    \draw[->] (4.3,0)--(3.5,0);
    \node at (-5, 3) {$Y_1^0 = 3$};
    \node at (-5, 1.5) {$Y_1^1 = 2$};
    \node at (-5, 1) {$Y_2^1 = 0$};
    \node at (-5, 0.5) {$Y_3^1 = 3$};
\end{tikzpicture}
\end{center}

Suppose that the mean number of offspring of a single parent is finite. 
\[\mu = \mathbb{E}(Y) = \sum_{k=0}^\infty k \, \mathbb{P}(Y = k) = \sum_{k=0}^\infty k \, p_k < \infty\]
If $Y_1$ and $Y_2$ are two independent, discrete random variables, we can define their convolution and use the fact that $\mathbb{P}(Y_i = k) = p_k$ to get
\begin{align*}
    \mathbb{P}(Y_1 + Y_2 = k) & = \sum_j \mathbb{P}(Y_1 = k - j) \, \mathbb{P}(Y_2 = j) \\
    & = \sum_{j=0}^\infty p_{k-j} p_j, \;\; k = 0, 1, 2, ...
\end{align*}
This is a two-fold convolution of the sequence $\{p_k\}$ with itself, denoted
\[p_k^{*2} = \sum_{j=0}^\infty p_{k-j} \, p_j\]
Extending this, we can find the $m$-fold convolution of the sequence $\{p_j\}$ with itself, represented by the sequence $\{p_j^{*m}\}$, where $p_k^{*m}$ is the $k$th term in this sequence. This gives us
\[p_k^{*n+1} = \sum_{j=0}^\infty p_{k-j} \, p_j^{*n}\]
for all $n \in \mathbb{N}$. Using this, we can write down the transition probabilities for the Markov chain $Z_n$. 
\[\mathbb{P}(Z_{n+1} = k \, | \, Z_n = m) = \begin{cases}
0 & \text{if } m = 0 \\
p_k^{*m} & \text{if } m \geq 1, k \geq 0
\end{cases}\]
where $\mathbb{P}(Z_{n+1} = k \, | \, Z_n = m)$ represents the probability of the $n$th generation consisting of $m$ individuals producing a total of $k$ offspring for the $(n+1)$th generation. Thus, the branching process is completely determined by the distribution of $Z_0$ and the offspring distribution $\{p_k\}_{k=0}^\infty$. 

\begin{lemma}
Given this discrete-time branching process, let $\mu$ be the mean of the offspring distribution. Then, 
\[\mathbb{E}(Z_n \, | \, Z_0 = 1) = \mu^n\]
If $\mu > 1$, the mean of $Z_n$ grows exponentially, and if $\mu_1$, the mean of $Z_n$ decreases exponentially. 
\end{lemma}

\subsubsection{Continuous-time Branching Process}
A continuous time branching process $Z_t$ has very similar structure to the discrete time branching process, except that the times between branch events (for each individual) are independent exponentially distributed random variables Exp$(\lambda)$, where the parameter $\lambda> 0$ is the branching rate. It is as though each individual has an independent alarm clock which rings as a time that is Exp$(\lambda)$, independently of all other clocks. So, if there are currently $N$ individuals, then the next alarm will ring at rate $\lambda N$; that is, the time until the next ring is distributed as Exp$(\lambda N)$, since it is the minimum of $N$ independent Exp$(\lambda)$ random variables. When an individual branches (clock rings), that individual produces a random number of offspring, according to the offspring distribution $\{p_k\}$, as before. So, a continuous time branching process has the same geneological structure as the discrete time process, but the times between branch events is randomized. Consequently, whether or not the process eventually goes extinct, depends only on the offspring distribution, not on the branching rate $\lambda$. 

Let $m_1(t) = \mathbb{E}(Z_t)$ denote the expected population size at time $t$. Then, it is a fact that $m_1(t)$ satisfies the ordinary differential equation
\[\frac{d}{d t} m_1 (t) = \lambda(\mu - 1) m_1 (t)\]
where 
\[\mu = \sum_{k=1}^\infty k p_k\]
is the mean of the offspring distribution. Solving this equation reveals that 
\[m_1 (t) = e^{\lambda (\mu-1) t} m_1 (0)\]
If $\mu > 1$, the mean population size grows exponentially, and if $\mu < 1$, the mean population size decreases exponentially. 

\subsubsection{Extinction Probability, Generating Functions}
The expression for the transition probabilities of $Z_n$ (disrete case) is quite difficult to work with. Alternatively, it can be convenient to work with generating functions. 

\begin{definition}
The \textit{generating function} for the offspring distribution is the function 
\[G(s) \equiv \sum_{k=0}^\infty p_k \, s^k = \mathbb{E}(s^Y)\]
where $Y \sim \{p_k\}$ is a random variable representing the number of children produced by a given individual. Note that $G$ is a power series that simply encodes information about the offspring distribution (also a sequence) $\{p_k\}_{k=0}^\infty$. 
\end{definition}

\begin{theorem}
\begin{enumerate}
    \item The radius of convergence of $G(s)$ is at least $1$. $G(s)$ defines a continuous function on $|s| \leq 1$. 
    \item On the interval $[0,1]$, $G(s)$ is increasing and convex. If $p_0 + p_1 < 1$, then $G(s)$ is strictly convex for $s \in [0,1]$. 
    \item $G(0) = p_0$. 
    \item $G(1) = 1$. 
    \item $G^\prime(1^-) = \mu$ is the expected number of offspring of a single individual. 
\end{enumerate}
\end{theorem}
\begin{proof}
We use the fact that 
\[\sum_{k=0}^\infty p_k = 1 \text{ and } 0 \geq p_k \geq \; \forall k = 0, 1, 2, ...\]
\end{proof}

\begin{theorem}
Suppose that $Z_0 = 1$ and that $p_0 + p_1 < 1$. Then
\[\lim_{n \rightarrow \infty} \mathbb{P}(Z_n = 0) = \mathbb{P}(\text{eventual extinction}) = t\]
where $t \in [0, 1]$ is the smallest non-negative root of the equation $t = G(t)$. If $\mu \leq 1$, then $t = 1$ (clearly, since the population will exponentially decrease on average). If $\mu > 1$, there is a positive probability that the population never goes extinct. 
\end{theorem}
\begin{proof}
Let $t$ be the probability that an individual's descendent family tree goes extinct. That is, $t = \mathbb{P}(Z_n = 0$ for some $n \geq 1 \; | \; Z_0 = 1)$. To derive the equation $t = G(t)$, let us condition on the first generation, with $Y_1$ denoting the number of offspring of the single parent. 
\begin{align*}
    t & = \mathbb{P}(\text{eventual extinction} \; | \; Z_0 = 1) \\
    & = \sum_{k=0}^\infty \mathbb{P}(\text{eventual extinction} \; | \; Z_0 = 1, Y_1 = k) \, \mathbb{P}(Y_1 = k \; | \; Z_0 = 1) \\
    & = \sum_{k=0}^\infty \mathbb{P}(\text{eventual extinction} \; | \;Z_0 =1, Y_1 = k) \, p_k
\end{align*}
That is, given that there are $k$ children of the first individual, the probability that this first individual's descendent family tree will go extinct is equal to the probability that each of the $k$ children's trees go extinct. These $k$ extinction events are independent. Therefore, 
\[\mathbb{P}(\text{eventual extinction} \; | \;Z_0 = 1, Y_1 = k) = t^k\]
which implies that 
\[t = \sum_{k=0}^\infty \mathbb{P}(\text{eventual extinction} \; | \; Z_0 = 1, Y_1 = k) \, p_k = \sum_{k=0}^\infty t^k \, p_k = G(t)\]
Additionally, under the hypothesis that $p_0 + p_1 < 1$, then $G(s)$ is strictly convex on $[0,1]$. Hence if $G^\prime (1) = \mu \leq 1$, the smallest non-negative root of $t = G(t)$ must be $t=1 \implies$ extinction occurs with probability 1. On the other hand, if $G^\prime (1) = \mu > 1$, then the smallest root of $t = G(t)$ occurs in the interval $[0,1)$. 
\end{proof}

Note that this result applies to both the discrete time case and the continuous time case. In continuous-time chains, whether or not the population goes extinct does not depend on $\lambda$, the rate at which individuals give birth. The $\lambda$ affects the time at which extinction occurs (if it occurs), but it does not affect the probability that it occurs. However, the extinction probability certainly does depend on the offspring distribution. 

\begin{definition}
A random variable $X$ is a \textit{counting variable} if it takes values in $\{0, 1, 2, ...\}$. 
\end{definition}

Note that generating functions is a mapping from $X$, the set of counting variables (all assumed to be pairwise independent) to the algebra of power series over variable $s$.
\[G: X \longrightarrow F[[s]]\]

\begin{lemma}
Let $X$ and $Y$ be two independent random counting variables, with generating functions $G_X (s) = \mathbb{E}(s^X)$ and $G_Y (s) = \mathbb{E}(s^Y)$. Then, the generating function for the random variable $Z = X + Y$ is $G_Z(s) = G_X (s) G_Y (s)$. That is, the generating function mapping $G$ is a homomorphism that maps addition to multiplication. In particular, if $X$ and $Y$ are iid, then $G_Z (s) = G_X (s)^2$. 
\end{lemma}
\begin{proof}
Since $X$ and $Y$ are independent, 
\[G_Z (s) = \mathbb{E}(s^Z) = \mathbb{E}(s^{X+Y}) = \mathbb{E}(s^X s^Y) = \mathbb{E}(s^X) \mathbb{E}(s^Y) = G_X (s) G_Y (s)\]
\end{proof}

Applying this argument iteratively, we get the following lemma. 
\begin{lemma}
Let $N \geq 1$ be a fixed positive integer. Let $Y_1, Y_2, ..., Y_N$ be independent, identically distributed random counting variables with generating function $G_Y (s) = \mathbb{E}(s^Y)$. Then, the generating function for the sum $Z = Y_1 + ... + Y_n$ is 
\[G_Z (s) = G_Y (s)^N\]
\end{lemma}

Now, suppose that $N$ is not fixed, but another random variable. We wish to describe the distribution of the sum of a random number of random variables. 

\begin{lemma}
Let $Y_1, Y_2, Y_3, ...$ be a collection of independent, identically distributed random variables with generating function $G_Y (s) = \mathbb{E}(s^Y)$. Let $N$ be a random counting variable, independent of the $Y_i$. Let $N$ have generating function $G_N (s)$. Then the generating function for $Z = Y_1 + Y_2 + ... + Y_N$ is 
\[G_Z (s) = G_N \big( G_Y (s) \big)\]
\end{lemma}
\begin{proof}
Just condition on $N = k$ 
\begin{align*}
    G_Z (s) = \mathbb{E}(s^Z) & = \sum_{k=0}^\infty \mathbb{E}\big( s^Z \,|\, N=k\big) \, \mathbb{P}(N=k) \\
    & = \sum_{k=0}^\infty \mathbb{E}(s^{Y_1 + ... + Y_k} \,|\,N=k) \, \mathbb{P}(N=k) \\
    & = \sum_{k=0}^\infty G_Y (s)^k \, \mathbb{P}(N=k) \\
    & = \mathbb{E}\big( G_Y (s)^N \big) = G_N \big( G_Y (s) \big)
\end{align*}
\end{proof}

\begin{theorem}
Let $G(s)$ be the generating function for the offspring distribution $G(s) = \sum_{k=0}^\infty p_k s^k$. Suppose that $Z_0 = 1$ and let $G_n (s) = \mathbb{E}(s^{Z_n})$ be the generating function for the random variable $Z_n$. Then, 
\[G_{n+m} (s) = G_n \big(G_m (s)\big) = G_m \big( G_n (s) \big)\]
Hence, 
\[G_n (s) = G(G(G(...(G(s))...))) \;\;\;\; \text{n-fold composition}\]
\end{theorem}

\begin{example}
Suppose the offspring distribution is
\[p_k = q p^k, \;\; k \geq 0\]
for some $p \in (0, 1)$, where $q = 1-p$. Thus, the number of children from a given parent is $Y = X - 1$, where $X \sim$ Geom$(q)$. Then, $\mathbb{E}(Y) = \frac{1}{q} - 1 = \frac{p}{q}$. With some computation, this means that
\[G(s) = \frac{q}{1- p s}\]
and $t = \min \{1, \frac{q}{p}\}$. 
\end{example}


\end{document}

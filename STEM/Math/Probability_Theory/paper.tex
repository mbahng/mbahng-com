\documentclass{article}

% packages
  % basic stuff for rendering math
  \usepackage[letterpaper, top=1in, bottom=1in, left=1in, right=1in]{geometry}
  \usepackage[utf8]{inputenc}
  \usepackage[english]{babel}
  \usepackage{amsmath} 
  \usepackage{amssymb}
  % \usepackage{amsthm}

  % extra math symbols and utilities
  \usepackage{mathtools}        % for extra stuff like \coloneqq
  \usepackage{mathrsfs}         % for extra stuff like \mathsrc{}
  \usepackage{centernot}        % for the centernot arrow 
  \usepackage{bm}               % for better boldsymbol/mathbf 
  \usepackage{enumitem}         % better control over enumerate, itemize
  \usepackage{hyperref}         % for hypertext linking
  \usepackage{fancyvrb}          % for better verbatim environments
  \usepackage{newverbs}         % for texttt{}
  \usepackage{xcolor}           % for colored text 
  \usepackage{listings}         % to include code
  \usepackage{lstautogobble}    % helper package for code
  \usepackage{parcolumns}       % for side by side columns for two column code
  

  % page layout
  \usepackage{fancyhdr}         % for headers and footers 
  \usepackage{lastpage}         % to include last page number in footer 
  \usepackage{parskip}          % for no indentation and space between paragraphs    
  \usepackage[T1]{fontenc}      % to include \textbackslash
  \usepackage{footnote}
  \usepackage{etoolbox}

  % for custom environments
  \usepackage{tcolorbox}        % for better colored boxes in custom environments
  \tcbuselibrary{breakable}     % to allow tcolorboxes to break across pages

  % figures
  \usepackage{pgfplots}
  \pgfplotsset{compat=1.18}
  \usepackage{float}            % for [H] figure placement
  \usepackage{tikz}
  \usepackage{tikz-cd}
  \usepackage{circuitikz}
  \usetikzlibrary{arrows}
  \usetikzlibrary{positioning}
  \usetikzlibrary{calc}
  \usepackage{graphicx}
  \usepackage{caption} 
  \usepackage{subcaption}
  \captionsetup{font=small}

  % for tabular stuff 
  \usepackage{dcolumn}

  \usepackage[nottoc]{tocbibind}
  \pdfsuppresswarningpagegroup=1
  \hfuzz=5.002pt                % ignore overfull hbox badness warnings below this limit

% New and replaced operators
  \DeclareMathOperator{\Tr}{Tr}
  \DeclareMathOperator{\Sym}{Sym}
  \DeclareMathOperator{\Span}{span}
  \DeclareMathOperator{\std}{std}
  \DeclareMathOperator{\Cov}{Cov}
  \DeclareMathOperator{\Var}{Var}
  \DeclareMathOperator{\Corr}{Corr}
  \DeclareMathOperator{\pos}{pos}
  \DeclareMathOperator*{\argmin}{\arg\!\min}
  \DeclareMathOperator*{\argmax}{\arg\!\max}
  \newcommand{\ket}[1]{\ensuremath{\left|#1\right\rangle}}
  \newcommand{\bra}[1]{\ensuremath{\left\langle#1\right|}}
  \newcommand{\braket}[2]{\langle #1 | #2 \rangle}
  \newcommand{\qed}{\hfill$\blacksquare$}     % I like QED squares to be black

% Custom Environments
  \newtcolorbox[auto counter, number within=section]{question}[1][]
  {
    colframe = orange!25,
    colback  = orange!10,
    coltitle = orange!20!black,  
    breakable, 
    title = \textbf{Question \thetcbcounter ~(#1)}
  }

  \newtcolorbox[auto counter, number within=section]{exercise}[1][]
  {
    colframe = teal!25,
    colback  = teal!10,
    coltitle = teal!20!black,  
    breakable, 
    title = \textbf{Exercise \thetcbcounter ~(#1)}
  }
  \newtcolorbox[auto counter, number within=section]{solution}[1][]
  {
    colframe = violet!25,
    colback  = violet!10,
    coltitle = violet!20!black,  
    breakable, 
    title = \textbf{Solution \thetcbcounter}
  }
  \newtcolorbox[auto counter, number within=section]{lemma}[1][]
  {
    colframe = red!25,
    colback  = red!10,
    coltitle = red!20!black,  
    breakable, 
    title = \textbf{Lemma \thetcbcounter ~(#1)}
  }
  \newtcolorbox[auto counter, number within=section]{theorem}[1][]
  {
    colframe = red!25,
    colback  = red!10,
    coltitle = red!20!black,  
    breakable, 
    title = \textbf{Theorem \thetcbcounter ~(#1)}
  } 
  \newtcolorbox[auto counter, number within=section]{proposition}[1][]
  {
    colframe = red!25,
    colback  = red!10,
    coltitle = red!20!black,  
    breakable, 
    title = \textbf{Proposition \thetcbcounter ~(#1)}
  } 
  \newtcolorbox[auto counter, number within=section]{corollary}[1][]
  {
    colframe = red!25,
    colback  = red!10,
    coltitle = red!20!black,  
    breakable, 
    title = \textbf{Corollary \thetcbcounter ~(#1)}
  } 
  \newtcolorbox[auto counter, number within=section]{proof}[1][]
  {
    colframe = orange!25,
    colback  = orange!10,
    coltitle = orange!20!black,  
    breakable, 
    title = \textbf{Proof. }
  } 
  \newtcolorbox[auto counter, number within=section]{definition}[1][]
  {
    colframe = yellow!25,
    colback  = yellow!10,
    coltitle = yellow!20!black,  
    breakable, 
    title = \textbf{Definition \thetcbcounter ~(#1)}
  } 
  \newtcolorbox[auto counter, number within=section]{example}[1][]
  {
    colframe = blue!25,
    colback  = blue!10,
    coltitle = blue!20!black,  
    breakable, 
    title = \textbf{Example \thetcbcounter ~(#1)}
  } 
  \newtcolorbox[auto counter, number within=section]{code}[1][]
  {
    colframe = green!25,
    colback  = green!10,
    coltitle = green!20!black,  
    breakable, 
    title = \textbf{Code \thetcbcounter ~(#1)}
  } 

  \BeforeBeginEnvironment{example}{\savenotes}
  \AfterEndEnvironment{example}{\spewnotes}
  \BeforeBeginEnvironment{lemma}{\savenotes}
  \AfterEndEnvironment{lemma}{\spewnotes}
  \BeforeBeginEnvironment{theorem}{\savenotes}
  \AfterEndEnvironment{theorem}{\spewnotes}
  \BeforeBeginEnvironment{corollary}{\savenotes}
  \AfterEndEnvironment{corollary}{\spewnotes}
  \BeforeBeginEnvironment{proposition}{\savenotes}
  \AfterEndEnvironment{proposition}{\spewnotes}
  \BeforeBeginEnvironment{definition}{\savenotes}
  \AfterEndEnvironment{definition}{\spewnotes}
  \BeforeBeginEnvironment{exercise}{\savenotes}
  \AfterEndEnvironment{exercise}{\spewnotes}
  \BeforeBeginEnvironment{proof}{\savenotes}
  \AfterEndEnvironment{proof}{\spewnotes}
  \BeforeBeginEnvironment{solution}{\savenotes}
  \AfterEndEnvironment{solution}{\spewnotes}
  \BeforeBeginEnvironment{question}{\savenotes}
  \AfterEndEnvironment{question}{\spewnotes}
  \BeforeBeginEnvironment{code}{\savenotes}
  \AfterEndEnvironment{code}{\spewnotes}

  \definecolor{dkgreen}{rgb}{0,0.6,0}
  \definecolor{gray}{rgb}{0.5,0.5,0.5}
  \definecolor{mauve}{rgb}{0.58,0,0.82}
  \definecolor{lightgray}{gray}{0.93}

  % default options for listings (for code)
  \lstset{
    autogobble,
    frame=ltbr,
    language=Python,                           % the language of the code
    aboveskip=3mm,
    belowskip=3mm,
    showstringspaces=false,
    columns=fullflexible,
    keepspaces=true,
    basicstyle={\small\ttfamily},
    numbers=left,
    firstnumber=1,                        % start line number at 1
    numberstyle=\tiny\color{gray},
    keywordstyle=\color{blue},
    commentstyle=\color{dkgreen},
    stringstyle=\color{mauve},
    backgroundcolor=\color{lightgray}, 
    breaklines=true,                      % break lines
    breakatwhitespace=true,
    tabsize=3, 
    xleftmargin=2em, 
    framexleftmargin=1.5em, 
    stepnumber=1
  }

% Page style
  \pagestyle{fancy}
  \fancyhead[L]{Probability Theory}
  \fancyhead[C]{Muchang Bahng}
  \fancyhead[R]{Spring 2023} 
  \fancyfoot[C]{\thepage / \pageref{LastPage}}
  \renewcommand{\footrulewidth}{0.4pt}          % the footer line should be 0.4pt wide
  \renewcommand{\thispagestyle}[1]{}  % needed to include headers in title page

\begin{document}

\title{Probability Theory}
\author{Muchang Bahng}
\date{Spring 2023}

\maketitle
\tableofcontents
\pagebreak

  An overview of probability using measures. We will denote probability measures defined over $\sigma$-algebras with $\mathbb{P}$ and probability functions defined over some sample space $\Omega$ or $\mathbb{R}$ with $P$ or $p$. When we say countable, we mean finite or countably infinite. I have used resources from: 
  \begin{enumerate}
    \item Rick Durret's \textit{Elementary Probability} and \textit{Probability} textbooks. 
    \item Dr. Krishna's \textit{Probability Foundation for Electrical Engineers} lectures at IIT. 
    \item Various quant interview books and websites for examples. 
  \end{enumerate}

\section{Measure Spaces}

  Let's do a little refresher on measure theory. 

  \subsection{Sigma Algebras and Measures}

    \begin{definition}[$\sigma$-algebra]
      A $\boldsymbol{\sigma}$\textbf{-algebra} on a set $X$ is a collection of subsets of $X$, denoted $\mathcal{A} \subset 2^X$, satisfying
      \begin{enumerate}
        \item Contains Empty Set: $\emptyset \in \mathcal{A}$ 
        \item Stability under Complementation: $A \in \mathcal{A} \implies A^c \in \mathcal{A}$, where $A^c = X - A$ 
        \item Stability under Countable Union: If $\{A_i\}$ is a countable sequence of sets, then 
        \begin{equation}
          \bigcup_i A_i \in \mathcal{A}
        \end{equation}
      \end{enumerate}
    \end{definition}

    At first, we might wonder why we need $\sigma$-algebras in the first place. We want to identify sets that are measurable in the way that their size can be determined, but why not just use $2^X$? This is because of the Banach-Tarski paradox, which gives you contradictions if you try to define a measure over $2^X$.  

    \begin{lemma}[Additional Property of $\sigma$-Algebras]
      A commonly known property of any $\sigma$-algebra $\mathcal{A}$ is that it is stable under countable intersections, too. 
      \begin{equation}
        A_1, A_2, \ldots, \in \mathcal{A} \implies \bigcap_{k=1}^\infty A_k \in \mathcal{A}
      \end{equation}
    \end{lemma}
    \begin{proof}
      We can utilize the fact that 
      \begin{equation}
        \bigcap_{k=1}^\infty A_k = X \setminus \bigcup_{k=1}^\infty A_k^c
      \end{equation}
    \end{proof}

    A $\sigma$-algebra is similar to the topology $\tau$ of topological space. Both $\mathcal{A}$ and $\tau$ require $\emptyset$ and $X$ to be in it. The three differences are that (i) $\tau$ does not allow compelmentation, (ii) $\tau$ allows any (even uncountable) union of sets (condition is strengthened), and (iii) $\tau$ allows only finite intersection of sets (condition is weakened). Now in order to construct $\sigma$-algebras, the following theorems are useful since they allow us to construct $\sigma$-algebras from other $\sigma$-algebras. It turns out that the intersection of $\sigma$-algebras is a $\sigma$-algebra, but not for unions. 

    \begin{theorem}
      Let $\{\mathcal{A}_k\}$ be a family of $\sigma$-algebras of $X$. Then, $\cap \mathcal{A}_k$ is also a $\sigma$-algebra of $X$. 
    \end{theorem}
    \begin{proof}
      Clearly, $\emptyset, X$ is in $\cap \mathcal{A}_k$. To prove complementation, 
      \begin{equation}
        A \in \bigcap \mathcal{A}_k \implies A \in \mathcal{A}_k \; \forall k \implies A^c \in A_k \; \forall k \implies A^c \in \bigcap \mathcal{A}_k
      \end{equation}
      To prove countable union, let $\{A_j\}_{j \in J}$ be some countable family of subsets in $\cap \mathcal{A}_k$. Then, 
      \begin{equation}
        A_j \in \bigcap \mathcal{A}_k \; \forall j \in J \implies A_j \in \mathcal{A}_k \; \forall k \forall j \implies \bigcup A_j \in \mathcal{A}_k \; \forall k \implies \bigcup A_j \in \bigcap \mathcal{A}_k
      \end{equation}
    \end{proof}

    This allows us to easily prove the following proposition, which just establishes the existence of $\sigma$-algebras. 

    \begin{proposition}
      Let $F \subset 2^X$ be a collection of subsets of $X$. Then there exists a unique smallest $\sigma$-algebra $\sigma(F)$ containing $F$. $\sigma(F)$ is called the $\sigma$-algebra \textbf{generated} by $F$. 
    \end{proposition}
    \begin{proof}
      Let us denote $\mathcal{M}$ as the set of all possible $\sigma$-algebras $\mathcal{B}$ of $X$. $\mathcal{M}$ is nonempty since it contains $2^X$. Then, the intersection 
      \begin{equation}
        \bigcap_{\mathcal{B} \in \mathcal{M}} \mathcal{B}
      \end{equation}
      is the unique smallest $\sigma$-algebra. 
    \end{proof}

    Now, how do we measure a size on subsets of $X$? We use measures. 

    \begin{definition}[Measure]
      Given a measurable space $(X, \mathcal{A})$, a \textbf{measure} is a function $\mu : \mathcal{A} \longrightarrow [0, +\infty]$ satisfying 
      \begin{enumerate}
        \item Positive Definite: $\mu(A) \geq \mu(\emptyset) = 0$ 
        \item Countable Additivity: For all countable collections $\{A_k\}_{k=1}^\infty$ of pairwise disjoint subsets $A_k \in \mathcal{A}$, 
        \begin{equation}
          \mu \bigg( \bigsqcup_{k=1}^\infty A_k \bigg) = \sum_{k=1}^\infty \mu(A_k)
        \end{equation}
        Remember that we are allowed to take countable unions inside our $\sigma$-algebra, so this makes sense. Disjointness is clearly important since if it wasn't, then $\mu(A) = \mu(A \cup A) = 2 \mu(A)$, which is absurd. 
      \end{enumerate}
      The triplet $(X, \mathcal{A}, \mu)$ is called a \textbf{measure space}. 
    \end{definition}

    \begin{theorem}[Properties of Measure]
      Let $\mu$ be a measure on $(X, \mathcal{A})$. 
      \begin{enumerate}
        \item Monotonicity: If $A \subset B$, then $\mu(A) \leq \mu(B)$. 
        \item Subadditivity: If $A \subset \cup_{i=1}^\infty A_i$, then $\mu(A) \leq \sum_{i=1}^\infty \mu (A_i)$ 
        \item Continuity from Above: If $A_1 \subset A_2 \subset A_3 \subset \ldots$, then 
        \begin{equation}
          \mu\bigg( \bigcup_{k=1}^\infty A_k \bigg) = \lim_{k \rightarrow \infty} \mu(A_k)
        \end{equation}
        \item Continuity from Below: If $A_1 \supset A_2 \supset A_3 \supset \ldots$ and $\mu(A_1) < \infty$, then 
        \begin{equation}
          \mu\bigg( \bigcap_{k=1}^\infty A_k \bigg) = \lim_{k \rightarrow \infty} \mu(A_k)
        \end{equation}
      \end{enumerate}
    \end{theorem}
    \begin{proof}
    Listed. 
    \begin{enumerate}
      \item Let $B \setminus A \coloneqq B \cap A^c$. Then, since $A$ and $B \setminus A$ are disjoint, we have 
      \begin{equation}
        \mu(B) = \mu\big( A \cup (B \setminus A) \big) = \mu(A) + \mu(B \setminus A) \geq \mu(A)
      \end{equation}
      
      \item We again try to divide this union into disjoint sets. Let $A_i^\prime = A \cap A_i$, and let $B_1 = A_1^\prime$ with 
      \begin{equation}
        B_i = A_i \setminus \bigcup_{j=1}^{i-1} A^\prime_j
      \end{equation}
      Since $B_i$'s are disjoint with $B_i \subset A_i$, we can use the first property to get 
      \begin{equation}
        \mu(A) = \sum_{i=1}^\infty \mu(B_i) \leq \sum_{i=1}^\infty \mu(A_i)
      \end{equation}
      
      \item This is the first time we introduce limits. With the fact that $\mu(A_k)$ must be nondecreasing, we can use real analysis and see that it is bounded by $\infty$, meaning that it must have a limit. But why does this limit equal to the left hand side? We can see that 
      \begin{align}
        \mu\bigg( \bigcup_{k=1}^\infty A_k \bigg) & = \mu(A_1) + \sum_{k=2}^\infty \mu(B_k) \\
        & = \mu(A_1) + \lim_{k \rightarrow \infty} \sum_{k=2}^\infty \mu(B_k) \\
        & = \lim_{k \rightarrow \infty} \mu(A_1 \cup B_2 \cup \ldots B_k)  = \lim_{k \rightarrow \infty} \mu(A_k) 
      \end{align}
      where $B_k = A_k \setminus A_{k-1}$. 
      
      \item The $\mu(A_1) < \infty$ is a necessary condition, since if we take $A_k = [k, \infty)$ on the real number line, then we have $\cap_{k=1}^\infty A_k = \emptyset$, but the limit of the measure is $\infty$. Well we can define $B_k = A_k \setminus A_{k+1}$ and write $\cap_{k=1}^\infty A_k = A_1 \setminus \cup_{k=1}^\infty B_k$, which means that 
      \begin{align}
        \mu\bigg( \bigcap_{k=1}^\infty A_k \bigg) & = \mu\bigg( A_1 \setminus \bigcup_{k=1}^\infty B_k \bigg) \\
        & = \mu(A_1) - \mu\bigg( \bigcup_{k=1}^\infty B_k\bigg) \\
        & = \mu(A_1) - \sum_{k=1}^\infty \mu(B_k) \\
        & = \mu(A_1) - \lim_{K \rightarrow \infty} \sum_{k=1}^K \mu(B_k) \\
        & = \lim_{K \rightarrow \infty} \bigg( \mu(A_1) - \sum_{k=1}^K \mu(B_k) \bigg) \\
        & = \lim_{K \rightarrow \infty} \mu \bigg( A_1 \setminus \bigcup_{k=1}^K B_k \bigg) = \lim_{K \rightarrow \infty} \mu(A_K)
      \end{align}
      Now the first line uses the fact that if $A \subset B$, then $\mu(B \setminus A) + \mu(A) = \mu(B)$, and with the further assumption that $\mu(A) < \infty$, we can subtract on both sides like we do with regular arithmetic. 
    \end{enumerate}
    \end{proof}

    \begin{theorem}[Inclusion Exclusion Principle]
      One familiar property commonly seen in probability and combinatorics is the inclusion exclusion principle. If $A, B \in \mathcal{A}$, 
      \begin{equation}
        \mu(A \cup B) = \mu(A) + \mu(B) - \mu(A \cap B)
      \end{equation}
      and by induction, if $A_1, \ldots, A_n \in \mathcal{F}$, then 
      \begin{equation}
        \mu\bigg( \bigcup_{i=1}^n A_i \bigg) = \sum_{i=1}^n \mu(A_i) - \sum_{i < j} \mu(A_i \cap A_j) + \sum_{i < j < k} \mu(A_i \cap A_j \cap A_k) + \ldots + (-1)^{n-1} \mu\bigg( \bigcap_{i=1}^n \mu(A_i) \bigg)
      \end{equation}
    \end{theorem}

    Finally, here is a definition which will be useful shortly when talking about how $\sigma$-algebras model knowledge. 

    \begin{definition}[Sub-$\sigma$-Algebras]
      Given a $\sigma$-algebra $\mathcal{F}$, a \textbf{sub-$\boldsymbol{\sigma}$-algebra} of $\mathcal{F}$ is a $\sigma$-algebra $\mathcal{G}$ s.t. $\mathcal{G} \subset \mathcal{F}$. 
    \end{definition}

    This will allows us to compare $\sigma$-algebras by taking two $\sigma$-algebras $\mathcal{G}, \mathcal{H} \subset \mathcal{F}$, which $\mu$ is guaranteed to be defined on since it is defined over $\mathcal{F}$. 

    \subsubsection[Construction of Measure on Rn]{Construction of Measure on $\mathbb{R}^n$}

    Let $\mathbb{R}^n$ be the continuum and $\mathcal{R}^n$ be the \textbf{Borel $\boldsymbol{\sigma}$-algebra}, defined as the $\sigma$-algebra generated by the open sets of $\mathbb{R}^n$. 

    \begin{example}[Stieltjes Measure Function]
      Measures on $(\mathbb{R}, \mathcal{R})$ are defined by giving a \textbf{Stieltjes measure function} with the following properties: 
      \begin{enumerate}
        \item $F$ is nondecreasing 
        \item $F$ is right continuous: 
        \begin{equation}
          \lim_{y \downarrow x} F(y) = F(x)
        \end{equation}
      \end{enumerate}
    \end{example}

    \begin{theorem}
      Associated with each Stieltjes measure function $F$ there is a unique measure $\mu$ on $(\mathbb{R}, \mathcal{R})$ with 
      \begin{equation}
        \mu((a, b]) = F(b) - F(a)
      \end{equation}
      When $F(x) = x$, then the resulting measure is called the \textbf{Lebesgue measure}. 
    \end{theorem}

    This is quite a hard proof, but we outline the construction of this measure on $\mathbb{R}$. First, we would like to define a "nice" set of half-open half-closed intervals, which we show is a semialgebra $\mathcal{S}$. We can easily define a measure $\mu$ on this semialgebra. We can extend this semialgebra to an algebra $\overline{\mathcal{S}}$, along with a proper extension $\overline{\mu}$ that is a unique measure on $\overline{\mathcal{S}}$. 

    \begin{definition}[Semialgebra, Algebra]
      A collection $\mathcal{S}$ of sets is said to be a \textbf{semialgebra} if 
      \begin{enumerate}
        \item it is closed under intersection 
        \item If $S \in \mathcal{S}$, then $S^c$ is a finite disjoint union of sets in $\mathcal{S}$
      \end{enumerate}
      A collection $\mathcal{A}$ of subsets is said to be an \textbf{algebra} if 
      \begin{enumerate}
        \item it is closed under union 
        \item it is closed under complementation
        \item the first two imply that it is closed under intersection
      \end{enumerate}
      We can see that a set that is a $\sigma$-algebra $\implies$ it is an algebra. 
    \end{definition}

    Here is an example of a semialgebra, which we will utilize in building a measure on $\mathbb{R}^n$.  

    \begin{example}
      Let $\mathcal{S}_d$ be the empty set plus all sets of the form 
      \begin{equation}
        (a_1, b_1] \times \ldots \times (a_d, b_d] \subset \mathbb{R}^d
      \end{equation}
      where $-\infty \leq a_i < b_i \leq +\infty$. $\mathcal{S}_d$ is a semialgebra since 
      \begin{equation}
        \bigg( \prod_i (a_i^1 , b_i^1] \bigg) \cap \bigg( \prod_i (a_i^2, b_i^2] \bigg) = \prod_i (\max\{a_i^1, a_i^2\}, \min\{b_i^1, b_i^2\}]
      \end{equation}
      and ...
    \end{example}

    Now, we show that we can extend this semialgebra to an algebra. 

    \begin{lemma}
      If $\mathcal{S}$ is a semialgebra, then $\overline{\mathcal{S}} = \{\text{finite disjoint unions of sets in } \mathcal{S}\}$ is an algebra, called the algebra generated by $\mathcal{S}$. 
    \end{lemma}
    \begin{proof}

    \end{proof}

    \begin{example}
      Given $\mathbb{R}$ and its semialgebra $\mathcal{S}_1$, then $\overline{\mathcal{S}}_1$ consists of the empty set and all sets of the form 
      \begin{equation}
        \bigcup_{i=1}^n (a_i, b_i] \text{ where } -\infty \leq a_i < b_i \leq +\infty
      \end{equation}
    \end{example}

    Now as for extending our measure function to $\overline{\mathcal{S}}$, we can simply use the properties. Note that since since an algebra is constructed from finite disjoint unions of a semialgebra, given that the finite collection $\{A_i\}_{i=1}^n$ all reside in $\mathcal{S}$ and are disjoint, then their disjoint union must be in $\overline{\mathcal{S}}$ and must be measurable, defined as 
    \begin{equation}
      \overline{\mu} \bigg( \bigsqcup_{i=1}^n A_i \bigg) = \sum_{i=1}^n \mu(A_i)
    \end{equation}

    \begin{definition}[$\sigma$-finite measure]
      Given a measure on an algebra $\mathcal{A}$, $\mu$ is said to be \textbf{$\boldsymbol{\sigma}$-finite} if there is a sequence of sets $A_1, A_2, \ldots \in \mathcal{A}$ s.t. $\mu(A_i) < \infty$ and $\cup_i A_i = \Omega$ . 
    \end{definition}

    \begin{theorem}
      Let $\mathcal{S}$ be a semialgebra and let $\mu$ defined on $\mathcal{S}$ have $\mu(\emptyset) = 0$. Suppose 
      \begin{enumerate}
        \item if $S \in \mathcal{S}$ is a finite disjoint union of sets $\{S_i\}_{i=1}^n$, then 
        \begin{equation}
          \mu(S) = \sum_{i=1}^n \mu(S_i)
        \end{equation}
        \item f $S$ is a countably infinite disjoint union of sets $\{S_j\}_{j=1}^\infty$, then 
        \begin{equation}
          \mu(S) \leq \sum_{j=1}^\infty \mu(S_j)
        \end{equation}
      \end{enumerate}
      Then, $\mu$ has a unique extension $\bar{\mu}$ that is a measure on $\overline{\mathcal{S}}$, the algebra generated by $\mathcal{S}$. If $\bar{\mu}$ is $\sigma$-finite, then there is a unique extension $\nu$ that is a measure on $\sigma(\mathcal{S})$ (the smallest $\sigma$-algebra containing $\mathcal{S}$). 
    \end{theorem}

  \subsection{Probability Spaces}

    \begin{definition}[Probability Space]
      A \textbf{probability space} is a measure space $(\Omega, \mathcal{F}, \mathbb{P})$ with $\mathbb{P}(\Omega) = 1$. 
      \begin{enumerate}
        \item $\Omega$ is called the \textbf{sample space} and an element $\omega \in \Omega$ is called an outcome. 
        \item $\mathcal{F}$ is called the \textbf{event space} and an element $A \in \mathcal{F}$ is called an event. 
        \item The measure of an event $\mathbb{P}(A)$ is called the \textbf{probability} of that event. 
      \end{enumerate}
      We can think of the sample space $\Omega$ as the set of all conceivable futures and an event $F \in \mathcal{F}$ as some subset of conceivable futures. The probability $\mathbb{P}(F)$ represents our degree of certainty that our future will be contained in such an event. If some measure space $X$ has a finite total measure, we can construct a probability space from it by normalizing the measure. 
    \end{definition}

    \subsubsection{Sigma-Algebras as Models of Knowledge}

      Let us focus on the $\sigma$-algebra $\mathcal{F}$. We can see that the $\sigma$-algebra \textit{models our knowledge of the experiment}. That is, given some outcome space $\Omega$, let us have two $\sigma$-algebras $\mathcal{F}$ and $\mathcal{G}$ such that $\mathcal{F} \subset \mathcal{G}$, i.e. $\mathcal{F}$ is a sub-$\sigma$-algebra of $\mathcal{G}$. What does this mean? Remember that the elements of the event space are the events that can be measured. If $\mathcal{G}$ is \textit{finer} than $\mathcal{F}$, then every set $F$ that is $\mathcal{F}$-measurable is also $\mathcal{G}$-measurable, and so someone who has knowledge of $\mu$ over $\mathcal{G}$ knows more than another who has knowledge of $\mu$ over $\mathcal{F}$. 

      For example, let us have a dice roll, with $\Omega = \{1, 2, 3, 4, 5, 6\}$.
      \begin{enumerate}
        \item Abby's knowledge is modeled by $\mathcal{F} = \{\emptyset, \{1, 2, 3\}, \{4, 5, 6\}, \Omega\}$, with 
        \begin{equation}
          \mathbb{P}(F) = \begin{cases} 
            0 & \text{ if } F = \emptyset \\
            1/2 & \text{ if } F = \{1, 2, 3\}, \{4, 5, 6\} \\ 
            1 & \text{ if } F = \Omega \end{cases}
        \end{equation}
        \item Bob's knowledge is modeled by $\mathcal{G} = 2^\Omega$ with the following values 
        \begin{equation}
          \mathbb{P}(\{1\}) = \mathbb{P}(\{2\}) = \mathbb{P}(\{3\}) = \mathbb{P}(\{4\}) = \mathbb{P}(\{5\}) = \mathbb{P}(\{6\}) = \frac{1}{6}
        \end{equation}
      \end{enumerate}
      We can see that $\mathcal{F} \subset \mathcal{G}$ and that Bob has more information than Abby since from the values of $\mathbb{P}$ over his $\sigma$-algebra, he can deduce that $\mathbb{P}(\{1, 2, 3\}) = \mathbb{P}(\{1\}) + \mathbb{P}(\{2\}) = \mathbb{P}(\{3\}) = 1/2$ (and likewise for $4, 5, 6$). All Abby knows is that the probability that the roll is $1, 2, 3$ is $1/2$, but in her view, the individual probabilities may not be uniformly $1/6$ at all (it could be $\mathbb{P}(\{1\}) = \mathbb{P}(\{2\}) = 0$ and $\mathbb{P}(\{3\}) = 1/2$, for example). More specifically, Bob has \textit{complete} information of the experiment since $\mathcal{G} = 2^\Omega$, so he knows the probability of every possible event. But no matter how little information one has about the experiment, \textit{everybody} will always know that the probability that \textit{any} outcome will happen is $1$ (hence $\mathbb{P}(\Omega) = 1$) and the probability that no outcome will happen is $0$ ($\mathbb{P}(\emptyset) = 0$), which is consistent with the definition of $\sigma$-algebras requiring to have $\emptyset$ and $\Omega$. Note that we can have two $\sigma$-algebras s.t. both model incomplete information and aren't strictly finer than one another, i.e. $\mathcal{F} \not\subset \mathcal{G}$ and $\mathcal{G} \not\subset \mathcal{F}$. 

      Note that given the same random experiment, we don't need to always have the same sample space or the same random variable. For example, let's have a coin toss. One could be interested in whether it lands heads or tails, which means $\Omega = \{0, 1\}$, but another could be interested in the number of times the coin flips midair, in which $\Omega = \mathbb{N}_0$. We could even be interested in the set of all trajectories of the coin, which would result in a huge space of all trajectories of the flip, or the velocity at which it lands on the table, which would lead to $\Omega = \mathbb{R}^+$. 

      Note that as you get more and more information, your $\sigma$-algebra can "grow" and get closer to something that models complete information. This means that given some $\sigma$-algebra $\mathcal{F}$ that models complete information, we can take a sequence of nondecreasing sub-$\sigma$-algebras of $\mathcal{F}$ 
      \begin{equation}
        \mathcal{F}_1 \subset \mathcal{F}_2 \subset \ldots \subset \mathcal{F}_i \subset \ldots
      \end{equation}
      such that $\mathcal{F}_i \subset \mathcal{F}$, which models our increasing knowledge of the experiment. 

      \begin{definition}[Filtration]
        Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space and $I$ be an index set with total order $\geq$ (usually, $\mathbb{N}, \mathbb{R}$). For every $i \in I$, let $\mathcal{F}_i$ be a sub-$\sigma$-algebra of $\mathcal{F}$ satisfying 
        \begin{equation}
          \mathcal{F}_i \subset \mathcal{F}_j \text{ if } i \geq j
        \end{equation}
        Note that we do not write it as a sequence like before since $I$ may be uncountable. Then, a \textbf{filtration} $\mathbb{F} = \{\mathcal{F}_i\}_{i \in I}$ is a family of $\sigma$-algebras that are ordered nondecreasingly. If $\mathcal{F}$ is a filtration, then $(\Omega, \mathcal{F}, \mathbb{F}, \mathbb{P})$ is called a \textbf{filtered probability space}. 
      \end{definition}

      \begin{example}[Filtration of 3 Coin Tosses]
        Let us describe a concrete example of a 3-coin toss filtration. The probability space is 
        \begin{equation}
          \Omega = \{000, 001, 010, 011, 100, 101, 110, 111\}
        \end{equation}
        which has $8$ outcomes so a complete $\sigma$-algebra would consist of $2^8 = 256$ outcomes. 
        \begin{enumerate}
          \item Before the experiment, we have no information at all, so 
          \begin{equation}
            \mathcal{F}_0 = \{\emptyset, \Omega\}
          \end{equation}
          which has $2^{2^0} = 2$ elements. 
          
          \item After the first coin toss, we would have information on what the first flip landed on (whether it was of form $( 0, \ast, \ast)$ or $(1, \ast, \ast)$), so we have a $\sigma$-algebra generated by these two events 
          \begin{align*}
            \mathcal{F}_1 & = \sigma(\{( 0, \ast, \ast)\}, \{(1, \ast, \ast)\}) \\
            & = \sigma(\{000, 001, 010, 011\}, \{100, 101, 110, 111\}) \\
            & = \{\emptyset, \{000, 001, 010, 011\}, \{100, 101, 110, 111\}, \Omega\}
          \end{align*}
          which has $2^{2^1} = 4$ elements. 
          
          \item After the second coin toss, we would have information on what the first two flips landed on (whether it was of form $( 0, 0, \ast)$, $( 0, 1, \ast)$, $(1, 0, \ast)$, $(1, 1, \ast)$), so we have a $\sigma$-algebra generated by these 4 events 
          \begin{align}
            \mathcal{F}_2 & = \sigma(\{( 0, 0, \ast)\}, \{( 0, 1, \ast)\}, \{(1, 0, \ast)\}, \{(1, 1, \ast)\}) \\
            & = \sigma(\{000, 001\}, \{010, 011\}, \{100, 101\}, \{110, 111\}) \\
            & = \{\emptyset, \{000, 001\}, \{010, 011\}, \{100, 101\}, \{110, 111\}, \\
            & \;\;\;\;\;\; \{000, 001, 010, 011\}, \{000, 001, 100, 101\}, \{000, 001, 110, 111\}, \\
            & \;\;\;\;\;\; \{010, 011, 100, 101\}, \{010, 011, 110, 111\}, \{100, 101, 110, 111\}, \\ 
            & \;\;\;\;\;\; \{000, 001, 010, 011, 100, 101\}, \{000, 001, 010, 011, 110, 111\}, \\
            & \;\;\;\;\;\; \{000, 001, 110, 101, 110, 111\}, \{010, 011, 110, 101, 110, 111\},  \Omega\}
          \end{align}
          which has $2^{2^2} = 16$ elements. 
          
          \item After the third coin toss, we would have information on what the first three flips landed on (all 8 possibilities in $\Omega$), so we have a $\sigma$-algebra generated by these 8 events 
          \begin{align}
            \mathcal{F}_3 & = \sigma(\{000\}, \{001\}, \{010\}, \{011\}, \{100\}, \{101\}, \{110\}, \{111\})
          \end{align}
          This is too big to write down explicitly, but it has $2^{2^3} = 256$ elements. 
        \end{enumerate}
      \end{example}

    \subsubsection{Types of Probability Spaces}

      \begin{definition}[Discrete Probability Space]
        If $\Omega$ is a countable set, then we can take its $\sigma$-algebra $\mathcal{F}$ to be the power set of $\Omega$ and construct the measurable space $(\Omega, 2^\Omega, \mathbb{P})$. From the axioms, for any event $A \in \mathcal{F}$, we have
        \begin{equation}
          \mathbb{P} (A) = \sum_{\omega \in A} \mathbb{P}(\{\omega\}) \text{ and } \sum_{\omega \in \Omega} \mathbb{P}(\{\omega\}) = 1
        \end{equation}
        The greatest $\sigma$-algebra $F = 2^{\Omega}$ describes the complete information. The cases $\mathbb{P}(\{\omega\}) = 0$ is permitted by the definition, but rarely used since such $\omega$ can safely be excluded from the sample space. Therefore, we can define the probability measure $\mathbb{P}$ by simply defining it for all singleton sets $\{\omega\}$. 
      \end{definition}

      This may be confusing, since for discrete spaces, it looks like we're assigning probabilities to each $\omega \in \Omega$, but we are actually assigning them to singleton \textit{sets}. We should be writing $\mathbb{P}(\{\omega\})$, but sometimes we abuse notation and write $\mathbb{P}(\omega)$. 

      \begin{example}
        Consider the flip of a fair coin with outcomes either hands or tails. Then, $\Omega = \{H, T\}$. The $\sigma$-algebra $F = 2^{\Omega}$ contains $2^2 = 4$ events: 
        \begin{align*}
          \{\} &= \text{Neither heads nor tails} \\
          \{H\} &= \text{Heads} \\
          \{T\} &= \text{Tails} \\
          \{H, T\} &= \text{Either heads or tails}
        \end{align*}
        That is, $\mathcal{F} = \{\{\}, \{H\}, \{T\}, \{H, T\}\}$. Our probability measure $\mathbb{P}$ is defined
        \begin{equation}
          \mathbb{P}(f) = \begin{cases}
          0 & f = \{\} \\
          0.5 & f = \{H\} \\
          0.5 & f = \{T\} \\
          1 & f = \{H, T\}
          \end{cases}
        \end{equation}
      \end{example}

      Being able to consider the event space as $2^X$ is very nice, since countability of $X$ allows us to avoid the Banach-Tarski paradox. It doesn't matter whether $\mathcal{F} = 2^X$ itself is uncountable or not. 

      \begin{example}[3 Coin Tosses]
        A fair coin is tossed 3 times, creating 8 possible outcomes. 
        \begin{equation}
          \Omega = \{HHH, HHT, HTH, HTT, THH, THT, TTH, TTT\}
        \end{equation}
        The complete information is described by the $\sigma$-algebra $\mathcal{F} = 2^{\Omega} = 2^8 = 256$ events, where each of the events is a subset of $\Omega$. 

        Alice knows the outcome of the second toss only. Thus, her incomplete information is described by the partition 
        \begin{equation}
          \Omega = A_1 \sqcup A_2 = \{HHH, HHT, THH, THT\} \sqcup \{HTH, HTT, TTH, TTT\}
        \end{equation}
        and the corresponding $\sigma$-algebra is 
        \begin{equation}
        \mathcal{F}_{Alice} = \{\emptyset, A_1, A_2, \Sigma\}
        \end{equation}
        Bryan knows only the total number of tails, so his partition contains 4 parts: 
        \begin{align*}
          \Omega & = B_0 \sqcup B_1 \sqcup B_2 \sqcup B_3 \\
            & = \{HHH\} \sqcup \{HHT, HTH, TTH\} \sqcup \{TTH, THT, HTT\} \sqcup \{TTT\}
        \end{align*}
        When we calculate Bryan's event space, we have
        \begin{align*}
          \mathcal{F}_{Bryan} & = \big\{\emptyset, \{HHH\}, \{HHT\}, \{HTH\}, \{THH\}, \{HHT, HTH\}, \{HHT, THH\}, \\
          & \;\;\;\;\{TTH, THT\}, \{TTH\}, \{THT\}, \{HTT\}, \{TTH, THT\},\{TTH, HTT\}, \\
          & \;\;\;\;\{THT, HTT\}, \{TTT\}, \Omega \big\} 
        \end{align*}
        Note that the event space of Bryan (and Alice) is not merely just $2^{\Omega}$ since we have some predetermined knowledge of the outcome space $\Omega$. Therefore, we can partition it into 4 cases and construct the event space by putting only the events that are subsets of each partition. For example, it wouldn't make sense to have an event 
        \begin{equation}
          \{HHH, TTT\}
        \end{equation}
        since the events $\{HHH\}$ and $\{TTT\}$ are in completely different outcome spaces (given the number of tails). That is, if we knew that 3 tails were thrown, the event $\{HHH, TTT\}$ wouldn't make any sense. However, the event $\Omega$ or $\emptyset$ is viable since they describe the case of whether the coin was tossed at all or not. 
        Furthermore, $\mathcal{F}_{Alice}$ and $\mathcal{F}_{Bryan}$ are incomparable. That is, $\mathcal{F}_{Alice} \not\subseteq \mathcal{F}_{Bryan}$ and $\mathcal{F}_{Bryan} \not\subseteq \mathcal{F}_{Alice}$, even though both are subalgebras of $2^{\Omega}$. 
      \end{example}

      \begin{example}[Geometric Measure on $\mathbb{N}$]
        Let $\Omega = \mathbb{N}$ and $\mathcal{F} = 2^\mathbb{N}$. We can completely define the probability measure by assigning them to singletons $k \in \mathbb{N}$. One such assignment is 
        \begin{equation}
          \mathbb{P}(\{k\}) = \frac{1}{2^k}
        \end{equation}
        or more generally, 
        \begin{equation}
          \mathbb{P}(\{k\}) = p (1 - p)^{k-1}
        \end{equation}
      \end{example}

      \begin{example}[Poisson Measure on $\mathbb{N}_0$]
        Let $\Omega = \mathbb{N} \cup \{0\}$. Then, $\mathcal{F} = 2^\Omega$ and we can define $\mathbb{P}$ on the singleton sets as 
        \begin{equation}
          \mathbb{P}(\{k\}) = \frac{e^{-\lambda} \lambda^k}{k!}
        \end{equation}
        for any $\lambda > 0$. We can then compute the probability of, say all primes, by taking 
        \begin{equation}
          \mathbb{P}(\text{primes}) = \sum_{k \text{ prime}} \mathbb{P}(\{k\})
        \end{equation}
        which we know to be monotonically increasing and bounded above, so it must converge. Whether this has a closed form solution is another matter. Again, in reality we are assigning probability measures on all $\mathcal{F}$-measurable sets, but just doing it through assignment of measure through singleton sets. 
      \end{example}
      
      \begin{example}[Voters]
        If $100$ voters are to be drawn randomly from among all voters in California and asked whom they will vote for governor, then the set of all sequences of $100$ Californian voters would be the sample space $\Omega$. We assume that sampling without replacement is used: only sequences of $100$ different voters are allowed. For simplicity an ordered sample is considered, that is a sequence $\{Alice, Bryan\}$ is different from $\{Bryan, Alice\}$. We also take for granted that each potential voter knows exactly his/her future choice, that is he/she doesn’t choose randomly.

        Alice knows only whether or not Arnold Schwarzenegger has received at least $60$ votes. Her incomplete information is described by the $\sigma$-algebra $\mathcal{F}_{Alice}$ that contains:
        \begin{enumerate}
          \item the set of all sequences in $\Omega$ where at least $60$ people vote for Schwarzenegger
          \item the set of all sequences where fewer than $60$ vote for Schwarzenegger
          \item the whole sample space $\Omega$
          \item the empty set $\emptyset$
        \end{enumerate}
        Bryan knows the exact number of voters who are going to vote for Schwarzenegger. His incomplete information is described by the corresponding partition $\Omega = B_0 \sqcup B_1 \ldots B_{100}$ and the $\sigma$-algebra $\mathcal{F}_{Bryan}$ consists of $2^{101}$ events. 

        In this case Alice’s $\sigma$-algebra is a subset of Bryan’s: $\mathcal{F}_{Alice} \subset \mathcal{F}_{Bryan}$. Bryan’s $\sigma$-algebra is in turn a subset of the much larger "complete information" $\sigma$-algebra $2^{\Omega}$ consisting of $2^{n(n-1)\ldots (n-99)}$ events, where $n$ is the number of all potential voters in California. 
      \end{example}

      Now if we move to uncountable outcome spaces, then things are not as nice, which is why we need to machinery of measure theory to study them. Let us try to model a probability measure on $\Omega = [0, 1]$. It is uncountable, and it turns out that $2^\Omega$ has cardinality strictly greater than even the continuum. If we try to model a uniform probability measure $\mathbb{P}$, then for some subset $A \in 2^\Omega$, it should be the case that $\mathbb{P}(A) = \mathbb{P}(A \oplus k)$, where $A \oplus k$ is just some translated version of $A$ still contained within $[0, 1]$. This applies to singleton sets, and it turns out that if we try to assign a nonzero probability measure to any singleton $\{k\}$, then the probability measure of $\Omega$ blows up to infinity, which we can't have. So the only thing we can do is have every singleton have zero probability. Remember that a measure by definition has the \textit{countable additivity} property, which says that 
      \begin{equation}
        \mu \bigg( \bigsqcup_{k=1}^\infty A_k \bigg) = \sum_{k=1}^\infty \mu(A_k)
      \end{equation}
      for all \textit{countable} collections $\{A_k\}$. Summation is not defined for uncountable collections, and so having a probability $0$ on every singleton does not imply that the probability of any uncountable set has is $0$. That is, having $\mathbb{P}(\{k\}) = 0$ for all $k \in [0, 1]$ does not tell you what $\mathbb{P}([0, 1])$ is. So now rather than assigning probabilities to singletons, like we did with discrete sets, the approach is to assign probabilities directly to our event space $\mathcal{F}$. We can do this by directly assigning the Lebesgue measure to the Borel algebra of $[0, 1]$, which has the properties 
      \begin{enumerate}
        \item $\mathbb{P}((a, b)) = \mathbb{P}([a, b)) = \mathbb{P}((a, b]) = \mathbb{P}([a, b]) = b - a$
        \item Translation invariance as stated above. 
      \end{enumerate}
      Over uncountable $\Omega$, we cannot afford to work with $2^\Omega$, since there is an impossibility theorem that says that there is no measure defined on $2^{[0, 1]}$ with the two properties above. Therefore, we must work with a smaller $\sigma$-algebra. Since the subsets of interest are usually intervals (or more generally, open sets), people usually take the Borel $\sigma$-algebra of open intervals on $[0, 1]$. The Lebesgue measure on $\mathbb{R}$ is not a probability measure since it $\lambda(\mathbb{R}) = \infty$, but we can construct a uniform probability measure on any bounded set of $\mathbb{R}$. Usually, these continuous probability spaces are $\mathbb{R}^n$, and we define some measure $\mu$ directly on its $\sigma$-algebra. 

      \begin{definition}[Atom]
        Let $(\Omega, \mathcal{F}, \mathbb{P})$ be uncountable. If for some $\omega \in \Omega$, $\mathbb{P}(\{\omega\}) \neq 0$, then $\omega$ is called an \textbf{atom}. 
      \end{definition}

      Now, given a general (discrete or continuous, or a combination of both) distribution, the set of all the atoms are an at most countable (maybe empty) set whose probability is the sum of probabilities of all atoms (by countable additivity). That is, given $\omega_1, \omega_2, \ldots$ atoms, 
      \begin{equation}
        \mathbb{P} \bigg( \bigsqcup_{i=1}^\infty \{\omega_i\} \bigg) = \sum_{i=1}^\infty \mathbb{P}(\{\omega_i\})
      \end{equation}
      \begin{enumerate}
        \item If this sum is equal to $1$ then all other points can be safely excluded from the sample space $\Omega$, returning us to the discrete case. 
        \item If this sum is $0$ then we just have some continuous sample space. This means $\mathbb{P}(\{\omega\}) = 0$ for all $\omega \in \Omega$, and so $\Omega$ must be uncountable (since if it was countable, then we should be able to sum the $\mathbb{P}(\{\omega\})$'s to get $1$, but it's $0$). Remember that summation is only defined for at most countable elements. 
        \item If the sum of probabilities of all atoms is strictly between $0$ and $1$, then the probability space decomposes into a discrete, atomic part and a non-atomic, continuous part. 
      \end{enumerate}

    \subsubsection{Conditioning on Events}

      \begin{definition}[Conditional Probability w.r.t. Events]
        Given a measure space $(\Omega, \mathcal{F}, \mathbb{P})$, let $B$ be an event such that $\mathbb{P}(B) > 0$. The \textbf{conditional probability} of $A$ given $B$ is defined 
        \begin{equation}
          \mathbb{P}(A \mid B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)}
        \end{equation}
      \end{definition}

      Note that we can't condition on events that have probability $0$, which is why we need the $\mathbb{P}(B) > 0$ condition. If this is the case, it doesn't even make sense to talk about a conditional probability $\mathbb{P}(A \mid B)$. For example, if we take the probability space $[0, 1]$ with its Borel algebra and the Lebesgue measure, then we cannot condition something on the rationals, e.g. $\mathbb{P}(\{\omega < 0.5\} \mid \omega \in \mathbb{Q})$ does not make sense. In fact, doing so can lead to contradictions, one being the \textbf{Borel-Kolmogorov paradox}. 

      An extremely useful theorem is that the conditional probability taken as a measure gives us a new viable measure on the same probability space $\Omega$. 

      \begin{theorem}
        Given probability space $(\Omega, \mathcal{F}, \mathbb{P})$, let $B \in \mathcal{F}$ with $\mathbb{P}(B) > 0$. Then, $\mathbb{P}( \cdot \mid B): \mathcal{F} \longrightarrow [0, 1]$ is a probability measure on $(\Omega, \mathcal{F})$. 
      \end{theorem}
      \begin{proof}
        We prove the properties of a probability measure. 
        \begin{enumerate}
          \item The empty set has measure $0$. 
          \begin{equation}
            \mathbb{P}(\emptyset \mid B) = \frac{\mathbb{P}( \emptyset \cap B)}{\mathbb{P}(B)} = \frac{\mathbb{P}(\emptyset)}{\mathbb{P}(B)} = \frac{0}{\mathbb{P}(B)} = 0
          \end{equation}
          \item The entire space has measure $1$. 
          \begin{equation}
            \mathbb{P}(\Omega \mid B) =  \frac{\mathbb{P}( \Omega \cap B)}{\mathbb{P}(B)} = \frac{\mathbb{P}(B)}{\mathbb{P}(B)} = 1
          \end{equation}
          \item Countable additivity of disjoint events. Let $A_i \in \mathcal{F}$ for $i = 1, 2, \ldots$ which are disjoint. Then, their union is in $\mathcal{F}$ by definition of $\sigma$-algebra. Now, 
          \begin{align*}
            \mathbb{P}\bigg( \bigcup_{i=1}^\infty A_i \bigg| B \bigg) & = \frac{1}{\mathbb{P}(B)} \mathbb{P} \bigg[ \Big( \bigcup_{i=1}^\infty A_i \Big) \cap B \bigg] \\
            & = \frac{1}{\mathbb{P}(B)} \mathbb{P} \bigg[ \bigcup_{i=1}^\infty (A_i \cap B) \bigg] \\
            & = \frac{1}{\mathbb{P}(B)} \sum_{i=1}^\infty \mathbb{P} (A_i \cap B) \\
            & = \sum_{i=1}^\infty \frac{\mathbb{P} (A_i \cap B)}{\mathbb{P}(B)} = \sum_{i=1}^\infty \mathbb{P}(A_i \mid B) 
          \end{align*}
        \end{enumerate}
      \end{proof}

      \begin{lemma}[Law of Total Probability]
        Suppose $A_1, A_2, ..., A_n$ is a partition of $\Omega$. Then, 
        \begin{equation}
          \{B \cap A_k\}_{k=1}^n
        \end{equation}
        is a partition of $B$, and 
        \begin{equation}
          \mathbb{P}(B) = \sum_{k=1}^n \mathbb{P} (B|A_k)\, \mathbb{P}(A_k)
        \end{equation}
        This is also called the \textit{Partition rule}. 
      \end{lemma}

      \begin{theorem}[Bayes Rule]
        Let $A, B \in \mathcal{F}$. Then, 
        \begin{equation}
          \mathbb{P}(B \mid A) = \frac{\mathbb{P}(A \mid B) \, \mathbb{P}(B)}{\mathbb{P}(A)}
        \end{equation}
      \end{theorem}
      \begin{proof}
        We know that 
        \begin{equation}
          \mathbb{P}(A \mid B) = \frac{\mathbb{P} (A \cap B)}{\mathbb{P}(B)} \text{ and } \mathbb{P}(B \mid A) = \frac{\mathbb{P}(B \cap A)}{\mathbb{P}(B)}
        \end{equation}
        and so we can write 
        \begin{equation}
          \mathbb{P} (A \mid B) \, \mathbb{P}(B) = \mathbb{P}(A \cap B) = \mathbb{P}(B \mid A) \, \mathbb{P}(A)
        \end{equation}
      \end{proof}

  \subsection{Distributions, Random Variables, Measurable Functions}

    Random variables are motivated by the following. When you have a random experiment, the experimenter may not be interested in the specific elementary outcomes. So if you have sample space $\Omega$, you may not be concerned about what $\omega \in \Omega$ shows up, but more interested in some numerical function of the elementary outcome. For example, if you toss a coin 10 times, you're not interested in what sequence in $\{0, 1\}^{10}$ shows up, but you may want to just know how many heads came up. In other words, your interest defines a numerical function $X: \Omega \rightarrow \mathbb{R}$. This is useful, since in many cases the sample space $\Omega$ can be extremely complicated (e.g. the sample space of all weather conditions) and the elementary outcomes also complicated, so you may want to know some simpler aspect (e.g. the temperature). 

    The name "random variable" is very misleading. It's not random nor a variable. It is a deterministic function $X: (\Omega, \mathcal{F}, \mathbb{P}) \longrightarrow \mathbb{R}$ that assigns numbers to outcomes. The only source of randomness itself is which $\omega \in \Omega$ is chosen. But we can't just choose any function on $\Omega$; they must satisfy the nice property of measurability. Now, to talk about random variables, recall that the definition of a measurable function $f: (X, \mathcal{A}) \longrightarrow \mathbb{R}$ is one where the preimage of every Borel set $B \in \mathcal{R}$ is in $\mathcal{A}$. With a potential measure $\mu$, this allows us to define the Lebesgue integral of $f$. Note that this is also equivalent to the more easily provable fact that the preimage of every half-interval $(-\infty, t)$ is in $\mathcal{A}$. That is, $f^{-1}((-\infty, t]) \in \mathcal{A}$ for all $t \in \mathbb{R}$. 

    \begin{definition}[Random Variable]
      A \textbf{random variable} $X$ on probability space $(\Omega, \mathcal{F}, \mathbb{P})$ is an $\mathcal{F}$-measurable function $X: (\Omega, \mathcal{F}, \mathbb{P}) \longrightarrow \mathbb{R}$. That is, for every subset $B \in \mathcal{R}$, its preimage 
      \begin{equation}
        X^{-1} (B) = \{\omega \in \Omega \mid X(\omega) \in B\} \in \mathcal{F}
      \end{equation}
    \end{definition}

    The reason we want $X$ to be $\mathcal{F}$-measurable is because now we can define probabilities on Borel sets $B$ of $\mathbb{R}$ by computing the probabilities of the preimage of $B$, which must be $\mathcal{F}$-measurable. In a way, a random variable "pushes forward" the probability measure $\mathbb{P}$, originally defined on $\mathcal{F}$, to $\mathcal{R}$.  

    \begin{definition}[Probability Law of Random Variable $X$]
      Let $X$ be a random variable on probability space $(\Omega, \mathcal{F}, \mathbb{P})$. The \textbf{probability law of $X$} is a function $\mathbb{P}_X : \mathcal{R} \longrightarrow [0, 1]$ defined, for each Borel set $B$ of $\mathbb{R}$, as 
      \begin{equation}
        \mathbb{P}_X (B) \coloneqq \mathbb{P} \big( X^{-1}(B) \big) = \mathbb{P} \big( \{\omega \in \Omega \mid X(\omega) \in B\} \big)
      \end{equation}
      Note that $\mathbb{P}$ refers to the probability measure on $\mathcal{F}$, and $\mathbb{P}_X$ refers to the probability law on $\mathcal{R}$. In shorthand, we can write $\mathbb{P}_X = \mathbb{P} \circ X^{-1}$. By abuse of notation, it is generally written
      \begin{equation}
        \mathbb{P}(X \in B)
      \end{equation}
      It is important to get used to this notation. Whenever we write $\mathbb{P}(X \ldots)$, we are always working in the probability law of $X$. Furthermore, whatever condition we put within the parentheses describes a measurable set. For example, 
      \begin{enumerate}
        \item $\mathbb{P}(X = x)$ describes the probability law of $X$ evaluated on the set $\{x\}$. 
        \item $\mathbb{P}(X \leq x)$ describes the probability law of $X$ evaluated on the set $(-\infty, x]$. 
        \item $\mathbb{P}(Y \leq y)$ describes the probability law of $Y$ evaluated on the set $(-\infty, y]$.
        \item $\mathbb{P}(a \leq Y < b)$ describes the probability law of $Y$ evaluated on the set $[a, b)$. 
        \item $\mathbb{P}(Z \in \mathbb{Q})$ describes the probability law of $Z$ evaluated on the set $\mathbb{Q}$. 
      \end{enumerate}
    \end{definition}

    \begin{theorem}[$\sigma$-Algebra Generated by Random Variable $X$]
      Given probability space $(\Omega, \mathcal{F}, \mathbb{P})$ and a random variable $X$, $\mathbb{P}_X$ is a probability measure on $(\mathbb{R}, \mathcal{R})$. Now, it turns out that the collection of all preimages of Borel sets under $X$ forms a $\sigma$-algebra on $\Omega$. We call it 
      \begin{equation}
        \sigma(X) \coloneqq \big\{ A \subset \Omega \mid A = X^{-1}(B) \text{ for some } B \in \mathcal{R} \big\}
      \end{equation}
      which is a $\sigma$-algebra of $\Omega$. Since $X$ is a measurable function, every $X^{-1}(B)$ is $\mathcal{F}$-measurable, and so $\sigma(X)$ is a sub-$\sigma$-algebra of $\mathcal{F}$. It is never the case that $\sigma(X) \supset \mathcal{F}$, since that means that $X$ itself is not $\mathcal{F}$-measurable. 
    \end{theorem}

    \begin{theorem}
      Given $(\Omega, \mathcal{F}, \mathbb{P})$ and a random variable $X: \Omega \rightarrow \mathbb{R}$, let us define a probability law $\mathbb{P}_X = \mathbb{P} \circ X^{-1}$. Then, 
      \begin{equation}
        (\mathbb{R}, \mathcal{R}, \mathbb{P}_X)
      \end{equation}
      is a probability space. 
    \end{theorem}

    This theorem is extremely useful, since in practical applications, one does not consider an abstract $\Omega$ and works immediately in $\mathbb{R}$. Once we have determined our numerical values of interest (heads or tails, number of heads, sum of dice rolls) with our random variable $X$, we can just throw away $(\Omega, \mathcal{F}, \mathbb{P})$ and work directly in probability space $(\mathbb{R}, \mathcal{R}, \mathbb{P}_X)$. Therefore, we don't actually control $\Omega$ by explicitly defining it as we said before. 

    We could just leave $\Omega$ to be some arbitrary large set, and construct an appropriate random variable $X$ that will generate an appropriate $\sigma$-algebra $\sigma(X)$ that captures the information of the experiment. This allows us to "simplify" the $\sigma$-algebra $\mathcal{F}$ to the scope of the random variable. That is, let $\Omega$ be the sample space of all trajectories of a coin flip before it comes to rest. If we are just looking at whether it is heads or tails, we can define $X$ to have image $\{0, 1\}$. Then, $\sigma(X)$ will be a sub-$\sigma$-algebra of $\mathcal{F}$ that looks at only the four subsets $\emptyset, \Omega$, the set of all trajectories landing heads, and the set of all trajectories landing tails. This simplifies $\mathcal{F}$ to a scope that we are interested in. 

    Let us review once more on the hierarchy of random variables. We usually classify random variables $X$ by the smallest $\sigma$-algebra that they generate, which is $\sigma(X)$. That is, not only is $X$ $\sigma(X)$-measurable, but for all $\sigma$-algebras $\mathcal{G}$ s.t. $\sigma(X) \subset \mathcal{G} \subset \mathcal{F}$, $X$ is also $\mathcal{G}$-measurable. Remember, since this is the case, the only relevant measure on these random variables is how coarse/small $\sigma(X)$ is. 
    \begin{enumerate}
      \item The finest random variable has $\sigma(X) = \mathcal{F}$. 
      
      \item The coarsest random variable is a constant random variable, which has $\sigma(X)$ to be the trivial $\sigma$-algebra $\mathcal{H} = \{\emptyset, \Omega\}$. Note that a constant random variable is still $\mathcal{F}$-measurable. 
      
      \item Every other random variable $X$ has $\mathcal{H} \subset \sigma(X) \subset \mathcal{F}$. 
    \end{enumerate}

    \subsubsection{Cumulative Distribution Function}

      Now, remember that the Borel algebra $\mathcal{R}$ is generated by the semi-infinite intervals of form $(-\infty, t]$ (for all $t \in \mathbb{R}$), which are considered "nice" Borel sets. So, $\mathbb{P}_X( (-\infty, t])$ is well defined for all $t \in \mathbb{R}$. In fact, this has a name, and when we talk about the "distribution" of some random variable, we refer to the CDF. 

      \begin{definition}[Cumulative Distribution Function]
        Given $(\Omega, \mathcal{F}, \mathbb{P})$ and a random variable $X: \Omega \rightarrow \mathbb{R}$. Then, the \textbf{cumulative distribution function} of $X$ is defined 
        \begin{equation}
          F_X (x) =\mathbb{P}\big( \{\omega \in \Omega \mid X(\omega) \leq x\} \big)
        \end{equation}
        We can also define this with the probability law $\mathbb{P}_X$ as 
        \begin{equation}
          F_X (x) = \mathbb{P}_X \big( (-\infty, x] \big)
        \end{equation}
        By abuse of notation, we will write the CDF as $P(X \leq x)$. It satisfies the properties: 
        \begin{enumerate}
          \item Limits: 
          \begin{equation}
            \lim_{x \rightarrow -\infty} F_X (x) = 0 \text{ and } \lim_{x \rightarrow \infty} F_X (x) = 1
          \end{equation}
          \item Monotonicity: 
          \begin{equation}
          x \leq y \implies F_X (x) \leq F_X (y)
          \end{equation}
          \item Right-continuity: For all $x \in \mathbb{R}$
          \begin{equation}
            \lim_{\epsilon \rightarrow 0^+} F_X (x + \epsilon) = F_X (x)
          \end{equation}
          So, if there are jumps, the hole can exist as the function approaches a value from the left. 
        \end{enumerate}
        What is remarkable is that any function satisfying these three properties satisfies these 3 properties gives you a viable CDF (and as shown below, completely determines a unique random variable). 
      \end{definition}

      So if you give me the probability law for all Borel sets of $\mathbb{R}$, then I can easily define the CDF since $(-\infty, x]$ are also Borel sets. It turns out that if we know \textit{just} the CDF, then since the semi-infinite intervals form a generating class of $\mathcal{R}$, it turns out that we can completely define $\mathbb{P}_X$. The proof of the theorem below is a bit more involved, using $\pi$-systems, but it is good to know. 

      \begin{theorem}
        The CDF $F_X (\cdot)$ uniquely specifies the probability law $\mathbb{P}_X$ for any random variable $X$. 
      \end{theorem}

      To summarize, given a probability space $(\Omega, \mathcal{F}, \mathbb{P})$, a random variable just pushes a measure onto the measure space $(\mathbb{R}, \mathcal{R})$. If we only care about the values of the random variable, then we can forget about $\Omega$ and only look at $(\mathbb{R}, \mathcal{R}, \mathbb{P}_X)$. The CDF on $(\mathbb{R}, \mathcal{R}, \mathbb{P}_X)$ will be well defined since semi-finite intervals are also Borel. If I am just given a CDF $F_X (\cdot)$, then this is enough for me to specify a unique probability measure $\mathbb{P}_X$ on $(\mathbb{R}, \mathcal{R})$. So although $\mathbb{P}_X$ contains the complete description of the random variable $X$, in practice we will use $F_X$ since it also captures all the information of $X$ and it's easier to work with. 

    \subsubsection{Types of Random Variables}

      You classify random variables based on the nature of the measure $\mathbb{P}_X$ induced on the real line. Note that we can have a continuous probability space $\Omega$ with a discrete random variable $X$ (e.g. coin flips). There are only three fundamental types of measures: discrete, continuous and singular random variables. In fact, a result in measure theory called \textit{Lebesgue's Decomposition Theorem} says that every measure on $\mathbb{R}$ are either one of these 3 or mixtures thereof. We are used to the first two; the third one is very bizzare and has little to no practical applications. 

      Note that if we are working in a discrete probability space $\Omega$, then we can simply take the $\sigma$-algebra to be $2^\Omega$, and so we can take any function on $\Omega$ as a random variable since its preimage will always be in $2^\Omega$. 

      \begin{definition}[Discrete Random Variable]
        Given $(\Omega, \mathcal{F}, \mathbb{P})$, let us have a random variable $X$ that induces a probability law on $(\mathbb{R}, \mathcal{R})$. $X$ is said to be \textbf{discrete} if there exists a countable set $E \subset \mathbb{R}$ s.t. $\mathbb{P}_X (E) = 1$ (i.e. $E$'s preimage has probability measure $1$). Since $E$ is at most countable, we can enumerate it $E = \{e_1, e_2, \ldots\}$, and by countable additivity of disjoint sets, we have 
        \begin{equation}
          1 = \mathbb{P}_X (E) = P\bigg( \bigcup_{i=1}^\infty \{e_i\} \bigg) = \sum_{i=1}^\infty \mathbb{P}_X (\{e_i\}) = \sum_{i=1}^\infty P(X = e_i)
        \end{equation}
        and for any $B \in \mathcal{R}$, 
        \begin{equation}
          \mathbb{P}_X (B) = \sum_{x \in E \cap B} P(X = x)
        \end{equation}
        Therefore, the entire probability measure is determined by the probabilities of the singleton sets $P(X = e_i)$. Therefore, the function 
        \begin{equation}
          p_X (x) \coloneqq P(X = x)
        \end{equation}
        is called the \textbf{probability mass function} of $X$, and we can compute using the Lebesgue integral, which reduces to the summation: 
        \begin{equation}
          \mathbb{P}_X (B) = \int_B p_X (x) \, d \mathbb{P}_X = \sum_{x \in E \cap B} p_X (x)
        \end{equation}
      \end{definition}

      Sometimes, the definition of discrete $X$ involves having a countable image in $\mathbb{R}$, but our definition allows us to have some $B \in \mathcal{R}$ where its preimage is not necessarily the sample space $\Omega$, but a smaller subset of measure $1$. What's nice about the discrete random variable is that the probability mass function $p_X$ completely describes its probability law. The CDF of a discrete probability function will look like an increasing series of steps. If we have $E = \{e_1, e_2, e_3, e_4, e_5\}$, its CDF would look like: 
      \begin{center}
        \includegraphics[scale=0.25]{img/Discrete_CDF.jpg}
      \end{center}
      If $E$ was countable, then it would have countably infinite discontinuities. Now we'll give some examples of discrete random variables, and in here we'll completely ignore the sample space $\Omega$, since once we have a random variable $X$, we can just work in $(\mathbb{R}, \mathcal{R}, \mathbb{P}_X)$. Remember that we will write $P(X = x)$ as shorthand for $\mathbb{P}_X (\{x\})$. 

      \begin{definition}[Indicator/Bernoulli Random Variable]
        Given $(\Omega, \mathcal{F}, \mathbb{P})$, let $A \in \mathcal{F}$ be an event. A useful random variable is the \textbf{indicator random variable} $1_A: \Omega \longrightarrow \mathbb{R}$ defined  
        \begin{equation}
          1_A (\omega) = \begin{cases} 1 & \text{ if } \omega \in A \\ 0 & \text{ if } \omega \not\in A \end{cases}
        \end{equation}
        This is a random variable since the preimages of $\emptyset, \{0\}. \{1\}, \{0, 1\}$ are $\emptyset, A^c, A, \Omega$, which are all $\mathcal{F}$-measurable. Since the probability measure of $A$ is $\mathbb{P}(A) = p$, then $\mathbb{P}(A^c) = 1 - \mathbb{P}(A) = 1 - p$, and so we get the PMF 
        \begin{equation}
          p_{1_A} (x) = \begin{cases} 1 - p & \text{ if } x = 0 \\ p & \text{ if } x = 1 \end{cases}
        \end{equation}
        The CDF of this function will look like a step function 
        \begin{equation}
          F_{1_A} (x) = \begin{cases} 0 & \text{ if } x < 0  \\ P(A^c) & \text{ if } 0 \leq x < 1 \\ 1 & \text{ if } 1 \leq x \end{cases}
        \end{equation}
      \end{definition}

      \begin{example}[Uniform Random Variable]
        Given a finite set $E = \{e_i\}_{i=1}^n \subset \mathbb{R}$, we define the PMF as 
        \begin{equation}
          p_X (e_i) = \mathbb{P}(X = e_i) = \frac{1}{n} \; \forall i = 1, 2, \ldots n
        \end{equation}
        which induces the probability measure $\mathbb{P}_X (B) = \sum_{x \in E \cap B} p_X (x)$. 
      \end{example}

      The Bernoulli RV leads to the geometric and binomial random variables. 

      \begin{example}[Geometric Random Variable]
        Given $E = \mathbb{N}$, we can define the PMF associated with random variable $X \sim \mathrm{Geometric}(p)$ as 
        \begin{equation}
          p_X (k) =\mathbb{P}(X = k) = (1 - p)^{k-1} p \text{ for } k \in \mathbb{N}, \; p \in [0, 1]
        \end{equation}
        which induces the probability measure $\mathbb{P}_X (B) = \sum_{x \in E \cap B} p_X (x)$. We can interpret this as the number of times you have to (independently) toss a $p$-coin (probability of heads is $p$) until you get a heads. 
      \end{example}

      \begin{example}[Binomial Random Variable]
        We let $E = \mathbb{N}_0$ and define the PMF associated with random variable $X \sim \mathrm{Binomial}(n, p)$ as 
        \begin{equation}
          p_X (k) = \mathbb{P}(X = k) = \binom{n}{k} p^k (1 - p)^{n - k} \text{ for } k \in E, p \in [0, 1]
        \end{equation}
        We can interpret this as the number of heads occurring in a sequence of $n$ independent tosses of a $p$-coin. 
      \end{example}

      \begin{example}[Poisson Random Variable]
        We let $E = \mathbb{N}_0$ and define the PMF of $X \sim \mathrm{Poisson}(\lambda)$ as 
        \begin{equation}
          p_X (k) = \frac{e^{-\lambda} \lambda^k}{k!} \text{ for } k \in E, \; \lambda > 0
        \end{equation}
      \end{example}

      \begin{definition}[Negative Binomial Distribution]
        The negative binomial distribution, denoted NB$(r, p)$ is defined as
        \begin{equation}
          \mathbb{P}(X = x) \equiv \binom{k+r-1}{k} \, (1-p)^r \, p^k
        \end{equation}
        It can be interpreted as the distribution that models the number of successes in a sequence of iid Bernoulli-$p$ trials before a specified number $r$ failures occurs. 
      \end{definition}

      A slight generalization of a discrete random variable is a simple random variable. Recall that the indicator random variable is a function $1_A: \Omega \rightarrow \mathbb{R}$ defined 
      \begin{equation}
        1_A (\omega) \coloneqq \begin{cases} 1 & \text{ if } \omega \in A \\
        0 & \text{ if else } \end{cases}
      \end{equation}
      As simple random variable generalizes this into multiple sets that form a partition of $\Omega$. It is analogous to a simple function, introduced in measure theory. 

      \begin{definition}[Simple Random Variable]
        Let $\{A_i\}_i$ form a partition of probability space $\Omega$. A \textbf{simple random variable} $X$ is a random variable of the form 
        \begin{equation}
          X (\omega) = \sum_{i} a_i 1_{A_i} (\omega)
        \end{equation}
        that assigns value $a_i$ if the input $\omega \in A_i$. 
      \end{definition}

      Now, let's move on to continuous random variables. 

      \begin{definition}[Absolutely Continuous Measures]
        Let $\mu, \nu$ be measures defined on $(\Omega, \mathcal{F})$. We say that $\nu$ is \textbf{absolutely continuous} w.r.t. $\mu$ if for every $N \in \mathcal{F}$ s.t. $\mu(N) = 0$, we have $\nu(N) = 0$. 
      \end{definition}

      \begin{definition}[Continuous Random Variable]
        A random variable $X$ is \textbf{continuous} if its induced measure $\mathbb{P}_X: (\mathbb{R}, \mathcal{R}) \rightarrow [0, 1]$ is absolutely continuous w.r.t. the Lebesgue measure $\lambda: (\mathbb{R}, \mathcal{R}) \rightarrow \mathbb{R}$, i.e. if for every Borel set $N$ of Lebesgue measure $0$, we have $\mathbb{P}_X (N) = 0$ also. 
      \end{definition}

      A common misconception is that a random variable $X$ is continuous if the induced measure on every singleton set in $\mathcal{B}(R)$ is $0$, i.e. $\mathbb{P}_X (\{x\}) = 0$ for all $x \in \mathbb{R}$. The definition above implies this since the Lebesgue measure of every singleton set is $0$. 

      We introduce a theorem that is useful to know, but we won't prove it. 

      \begin{theorem}[Radon-Nikodym Theorem (Special Case)]
        Let $X$ be a continuous random variable. Then, there exists a nonnegative measurable function $f_X : \mathbb{R} \longrightarrow [0, \infty)$ s.t. for any $B \in \mathcal{R}$, we have 
        \begin{equation}
          \mathbb{P}_X (B) = \int_B f_X \, d\lambda
        \end{equation}
        where the above is the Lebesgue integral. Note that we must define using the Lebesgue integral because Riemann integral is not compatible with any Borel set. $f_X$ is called the \textbf{probability density function}, aka \textbf{PDF}. Furthermore, we can get $f_X$ from $\mathbb{P}_X$ by taking the \textbf{Radon-Nikodym derivative} (which we will not define now)
        \begin{equation}
          f_X = \frac{d \mathbb{P}_X}{d \lambda}
        \end{equation}
        which basically says that if we have a set of very small Lebesgue measure $d \lambda$ tending to $0$, then its probability measure $\mathbb{P}_X$ will also be very small, and the infinitesimal ratio of these two measures on an arbitrarily small set is $f_X$. Also, note that the integral does not change if the value of $f$ changes on sets of Lebesgue measure $0$, and so there is no unique PDF describing $\mathbb{P}_X$. It is unique up to sets of Lebesgue measure $0$, so when we refer to such a PDF $f_X$, we are really talking about an equivalence class of functions. 
      \end{theorem}

      This theorem guarantees the existence of some $f_X$ that completely describes the probability law $P_X$! Take a special case of when $B = (-\infty, x])$, and we can define the CDF as 
      \begin{equation}
        F_X (x) = P_X ((-\infty, x]) = \int_{(-\infty, x]} f_X \, d\lambda
      \end{equation}
      If the set of integration is an interval (and the function is continuous a.e.), then the Lebesgue integral and Riemann integral coincides, and we get the familiar formula 
      \begin{equation}
        F_X (x) = \int_{-\infty}^x f_X (t)\,dt
      \end{equation}
      and we can differentiate it to get back the PDF $f_X$ (or more accurately, some function that agrees with $f_X$ a.e.). We can show that the CDF of a continuous random variable $X$
      \begin{enumerate}
        \item is absolutely continuous, and 
        \item is differentiable almost everywhere, which means that its PDF will be defined almost everywhere (and we can fill in the undefined points however we want). 
      \end{enumerate}
      Note that the PDF $f_X$ itself has no interpretation as a probability (indeed, we can change its value at a countable number of points to anything we want). It is only when we integrate it over some Borel set that gives us a probability. 

      \begin{example}[Uniform Random Variable]
        Let us define the uniform probability measure $P_X$ on $(\mathbb{R}, \mathcal{R})$ with the CDF 
        \begin{equation}
          F_X = \begin{cases} 0 & \text{ if } x < 0 \\
          x & \text{ if } 0 \leq x \leq 1 \\
          1 & \text{ if } 1 < x \end{cases}
        \end{equation}
        It is differentiable almost everywhere except for at the two points $x = 0$ and $x = 1$. Therefore, the PDF $f_X$ is defined for all real numbers except $x = 0$ and $x = 1$. But it doesn't matter: we can assign any value $f_X$ we want on $0$ and $1$ since it won't affect the integral of it. In this example, we just set 
        \begin{equation}
          f_X = \begin{cases} 1 & \text{ if } 0 \leq x \leq 1 \\
          0 & \text{ if else} \end{cases} 
        \end{equation}
      \end{example}

      \begin{example}[Exponential Random Variable]
        The exponential random variable has the following CDF: 
        \begin{equation}
          F_X (x) = \begin{cases} 1 - e^{-\lambda x} & \text{ if } x \geq 0 \\ 0 & \text{ if } x < 0 \end{cases} \text{ for } \lambda > 0
        \end{equation}
        which is differentiable everywhere except at $x = 0$. Differentiating it (and assigning a convenient value at $x = 0$ $f(0) = \lambda$) gives the PDF 
        \begin{equation}
          f_X (x) = \begin{cases} \lambda e^{-\lambda x} & \text{ if } x \geq 0 \\ 0 & \text{ if else} \end{cases}
        \end{equation}
      \end{example}

      \begin{example}[Gaussian Random Variable]
        The PDF is easier to specify for the Gaussian, so we define the Gaussian RV as having PDF 
        \begin{equation}
          f_X (x) = \frac{1}{\sigma \sqrt{2 \pi}} \exp \bigg( -\frac{(x - \mu)^2}{2 \sigma^2} \bigg) \text{ for } \mu \in \mathbb{R}, \sigma > 0
        \end{equation}
        Note that this PDF decreases very quickly as we get further from $\mu$. The CDF cannot be written in closed form, and we call the CDF of the standard Gaussian the \textbf{error function}: 
        \begin{equation}
          \mathrm{Erf}(x) = F_X (x) = \int_{-\infty}^x \frac{1}{\sqrt{2 \pi}} e^{- t^2 / 2} \, dt
        \end{equation}
      \end{example}

      \begin{example}[Cauchy Random Variable (Standardized)]
        The Cauchy random variable gives the PDF 
        \begin{equation}
          f_X (x) = \frac{1}{\pi} \frac{1}{1 + x^2} \text{ for } x \in \mathbb{R}
        \end{equation}
        Integrating this gives the inverse tangent, which after scaling it down by $\pi$ satisfies the conditions of the CDF. Note that the Cauchy distribution falls off much more slowly around the mean (at a rate of $\frac{1}{1 + x^2}$, like a power law) than the Gaussian (which is even \textit{faster} than an exponential, it is at the rate of $e^{-x^2}$). If such a PDF falls off at a slow rate, like a power law, then this is called a \textit{heavy-tailed random variable}. 
      \end{example}

      \begin{example}[Gamma Random Variable]
        The PDF associated with random variable $X \sim \mathrm{Gamma}(n, \lambda)$ is defined 
        \begin{equation}
          f_X(x) = \frac{\lambda^n x^{n-1}}{\Gamma(n)} e^{-\lambda x} \text{ for } x \geq 0
        \end{equation}
        where $\Gamma$ is the gamma function, which is an extension of the factorial function to the domain of complex numbers. 
        \begin{equation}
          \Gamma(x) \coloneqq \int_{0}^\infty z^{x-1} e^{-z}\, dz, \;\;\;\;\; \text{Re}(x) > 0
        \end{equation}
      \end{example}

      \begin{example}[Beta Random Variable]
        The PDF associated with random variable $X \sim \mathrm{Beta}(\alpha, \beta)$, for positive reals $\alpha, \beta$, is defined 
        \begin{equation}
          f_X (x) \equiv \frac{x^{\alpha-1} \,(1-x)^{\beta-1}}{B(\alpha, \beta)}, \text{ where } B(\alpha, \beta) \equiv \frac{\Gamma(\alpha) \Gamma(\beta)}{\Gamma(\alpha + \beta)}
        \end{equation}
        and $\Gamma$ is the Gamma function. 
      \end{example}

      \begin{example}[Uniform RV defined on Cantor Set]
        The cantor set $C \subset [0, 1]$ is defined by removing $(1/3, 2/3)$ from $[0, 1]$ and then removing the middle third from each interval that remains. We define the distribution on this set by defining its CDF: We set 
        \begin{enumerate}
          \item $F(x) = 0$ for $x \leq 0$ and $F(x) = 1$ for $x \geq 1$. 
          \item $F(x) = 1/2$ for $x \in [1/3, 2/3]$, 
          \item $F(x) = 1/4$ for $x \in [1/9, 2/9]$ and $F(x) = 3/4$ for $x \in [7/9, 8/9]$, ... 
        \end{enumerate}
        and extend $F$ to all of $[0 ,1]$ using monotonicity. 
      \end{example} 

      \begin{example}[Dense Discontinuities]
        Let $q_1, q_2, \ldots$ be an enumeration of the rationals. Let $\alpha_i > 0$ have $\sum_{i=1}^\infty \alpha_i = 1$, and let 
        \begin{equation}
          F(x) = \sum_{i=1}^\infty \alpha_i 1_{[q_i, \infty)} (x)
        \end{equation}
        where $1_{[q_i, \infty)} (x) = 1$ if $x \in [q_i, \infty)$ and $0$ if otherwise. 
      \end{example}

      To summarize, once we have a random variable $X: \Omega \rightarrow \mathbb{R}$, we can throw away the sample space and work in $(\mathbb{R}, \mathcal{R}, \mathbb{P}_X)$ with the induced measure $\mathbb{P}_X$, which is known as the \textbf{probability distribution} of $X$.  
      \begin{enumerate}
        \item If $X$ is discrete, then let there be some at most countable set $E = \{e_i\}$ where $P(E) = 1$. it turns out that $\mathbb{P}_X$ can be completely defined by a probability mass function $p_X : \mathbb{R} \rightarrow \mathbb{R}$ defined 
        \begin{equation}
          p_X (x) = \mathbb{P}_X (\{x\}).
        \end{equation}
        Given that we have this PMF , we can define $\mathbb{P}_X$ as such: Given any Borel $B \in \mathcal{R}$, 
        \begin{equation}
          \mathbb{P}_X (B) = \sum_{x \in E \cap B} p_X (x)
        \end{equation}
        \item If $X$ is continuous, then the Radon-Nikodym Theorem asserts the existence of a nonnegative probability density function $f_X$ that completely describes the probability law $\mathbb{P}_X$. Given that we have this PDF, we can then define $\mathbb{P}_X$ as such: Given any Borel $B \in \mathcal{R}$, 
        \begin{equation}
          \mathbb{P}_X (B) = \int_B f_X \, d\lambda
        \end{equation}
      \end{enumerate}

    \subsubsection{Space of Measurable Functions}

      Now it turns out that the space of $\mathcal{F}$-measurable functions $X: \Omega \rightarrow \mathbb{R}$ forms a function space, which means that the set of all random variables on $\Omega$ forms a vector space. We formally show it here. 

      \begin{lemma}
        The set of all $\mathcal{F}$-measurable functions $X: (\Omega, \mathcal{F}) \rightarrow \mathbb{R}$ forms a vector space, denoted $L_\mathcal{F} (\Omega; \mathbb{R})$, or $L_\mathcal{F} (\Omega)$ for short. 
      \end{lemma}
      \begin{proof}

      \end{proof}

      Naturally, we can put the $L^p$-norm on this space, defined 
      \begin{equation}
        ||X||_p \coloneqq \bigg( \int_\Omega |X|^p \, d\mathbb{P} \bigg)^{1/p}
      \end{equation}
      Moreover, if $p = 2$, then we can put an inner product defined 
      \begin{equation}
        \langle X, Y \rangle = \bigg( \int_\Omega X Y \,d \mathbb{P} \bigg)^{1/2}
      \end{equation}

      \begin{definition}
        The Banach space of $\mathcal{F}$-measurable functions is denoted $L_\mathcal{F}^p (\Omega)$, and the Hilbert space is denoted $L_\mathcal{F}^2 (\Omega)$. 
      \end{definition}

      This means that if we have some probability space $(\Omega, \mathcal{F}, \mathbb{P})$ and  sub-$\sigma$-algebra $\mathcal{G} \subset \mathcal{F}$, then any $\mathcal{G}$-measurable function is also $\mathcal{F}$-measurable, since if the preimage of every $B \in \mathcal{R}$ is in $\mathcal{G}$, then it $B \in \mathcal{F}$. This immediately results in the following. 

      \begin{theorem}
        If $\mathcal{G}$ is a sub-$\sigma$-algebra of $\mathcal{F}$, then $L_\mathcal{G} (\Omega)$ is a subspace of $L_\mathcal{F} (\Omega)$. 
      \end{theorem}

      This means that as we get coarser and coarser random variables, the space in which these random variables live in get smaller and smaller, until we get to the constant random variables, which form a $1$-dimensional line in $L_\mathcal{F} (\Omega)$. The origin is simply the constant $0$ random variable. 

  \subsection{Independence}

    \begin{definition}[Independence of $2$ Events]
      Given probability space $(\Omega, \mathcal{F}, \mathbb{\mathbb{P}})$, events $A, B \in \mathcal{F}$ are said to be \textbf{independent under $\mathbf{\mathbb{P}}$} if 
      \begin{equation}
        \mathbb{P}(A \cap B) = \mathbb{P}(A) \, \mathbb{P}(B)
      \end{equation}
      This leads to the immediate property that if $\mathbb{P}(B) > 0$, with $A, B$ independent, then 
      \begin{equation}
        \mathbb{P}(A \mid B) = \mathbb{P}(A)
      \end{equation}
    \end{definition}

    Note that $A$ and $B$ may be independent under one measure, but not under another measure. The property that $\mathbb{P}(A \mid B) = \mathbb{P}(A)$ is \textit{not} the definition of independence, since it has the more restricting property that $\mathbb{P}(B) > 0$, so only refer to the definition that $\mathbb{P}(A \cap B) = \mathbb{P}(A) \, \mathbb{P}(B)$. This is the true definition of independent events that we should rely on, not the one that says that $A$ and $B$ are independent if "one does not affect the other." This old definition is misleading and false. For example, take the probability space $[0, 1]$, with Borel $\sigma$-algebra, and Lebesgue measure $\mathbb{P} = \lambda$, and let $A = \mathbb{Q}$ and $B = \mathbb{R} \setminus \mathbb{Q}$. Then, contradictory to our old definition, $A$ and $B$ are independent since $\mathbb{P}(A \cap B) = \mathbb{P}(A) \, \mathbb{P}(B) = 0$! By the definition, an event $A$ is independent of itself if $\mathbb{P}(A) = 0$ or $1$ (e.g. $A$ is rationals, irrationals, cantor set, $\emptyset$, $\Omega$, etc.). 

    \begin{definition}[Independence of $n$ Events]
      Given probability space $(\Omega, \mathcal{F}, \mathbb{P})$, 
      \begin{enumerate}
        \item Let us have a finite collection of events $A_1, A_2, \ldots, A_n \in \mathcal{F}$. They are \textbf{independent} if for all nonempty $I_0 \subset \{1, 2, \ldots n\}$, 
        \begin{equation}
          \mathbb{P} \bigg( \bigcap_{i \in I_0} A_i \bigg) = \prod_{i \in I_0} \mathbb{P}(A_i)
        \end{equation}
        Note that it is not enough to just prove that 
        \begin{equation}
          \mathbb{P}(A_1 \cap \ldots \cap A_n) = \prod_{i=1}^n \mathbb{P}(A_i)
        \end{equation}
        We must verify this for all $2^n$ possible choices (to be precise, we don't need to prove for $I_0 = \emptyset$ and $I_0 = \{A_i\}$), so for $2^n - n - 1$ choices. 
        
        \item Let $\{A_i\}_{i \in I}$ be a collection of events indexed by a possibly uncountable $I$. They are independent if for all nonempty and finite $I_0 \subset I$, we have 
        \begin{equation}
          \mathbb{P} \bigg( \bigcap_{i \in I_0} A_i \bigg) = \prod_{i \in I_0} \mathbb{P}(A_i)
        \end{equation}
      \end{enumerate}
    \end{definition}

    Now when we are trying to compare two $\sigma$-algebras, the measure defined for one may not even be defined on the other. To ensure that a measure is defined on both, it makes sense to take its $\sigma$-algebra and construct two sub-$\sigma$-algebras, which $\mu$ is guaranteed to be defined on. 

    \begin{definition}[Independence of $\sigma$-Algebras]
      Let us have probability space $(\Omega, \mathcal{F}, \mathbb{P})$. 
      \begin{enumerate}
        \item Let $\mathcal{F}_1, \mathcal{F}_2$ be two sub-$\sigma$-algebras of $\mathcal{F}$. $\mathcal{F}_1$ and $\mathcal{F}_2$ are independent if for any $A_1 \in \mathcal{F}_1, A_2 \in \mathcal{F}_2$, $A_1$ and $A_2$ are independent. 
        \item Let $\{ \mathcal{F}_i\}_{i \in I}$ be an arbitrary collection of sub-$\sigma$-algebras of $\mathcal{F}$, indexed by possibly uncountable $I$. Then, they are independent if for any choices of $A_i \in \mathcal{F}_i$ for $i \in I$, $\{A_i\}_{i \in I}$ are independent events. 
      \end{enumerate}
    \end{definition}

    \begin{definition}[Independent Random Variables]
      Two random variables $X, Y$ are \textbf{independent} if $\sigma(X)$ and $\sigma(Y)$ are independent $\sigma$-algebras. That is, for any Borel sets $B_1, B_2 \in \mathcal{R}$, the events $X^{-1}(B_1)$ and $Y^{-1}(B_2)$ are independent: 
      \begin{equation}
        \mathbb{P}\big[ X^{-1}(B_1) \cap Y^{-1}(B_2) \big] = \mathbb{P}(X^{-1}(B_1)) \, \mathbb{P}(Y^{-1}(B_2))
      \end{equation}
      or by abusing notation, 
      \begin{equation}
        \mathbb{P}(X \in B_1, Y \in B_2) = \mathbb{P}(X \in B_1) \, \mathbb{P}(Y \in B_2)
      \end{equation}
    \end{definition}

    If $X, Y$ are independent, then we can say something about the CDFs 
    \begin{equation}
      F_{X, Y} (x, y) = F_X (x) \, F_Y (y)
    \end{equation}
    In fact, we can say something stronger. 

    \begin{theorem}
      $X$ and $Y$ are independent RVs if and only if 
      \begin{equation}
        F_{X, Y} (x, y) = F_X (x) \, F_Y (y)
      \end{equation}
    \end{theorem}

    Moving onto multiple variables, we can define that $X_1, X_2, \ldots, X_n$ are independent RVs if $\sigma(X_1), \ldots, \sigma(X_n)$ are independent $\sigma$-algebras. 

  \subsection{Functions of Random Variables}

    In many applications, it happens that we are interested not in the value of the random variable $X$, but a function of it. That is, given a probability space $(\Omega, \mathcal{F}, \mathbb{P})$, let us have a random variable $X: \Omega \rightarrow \mathbb{R}$. We can then define another function $f: \mathbb{R} \rightarrow \mathbb{R}$ and consider the potential random variable $f \circ X : \Omega \rightarrow \mathbb{R}$. We say potential because we don't know yet whether $f \circ X$ is measurable (i.e. the preimage of every Borel set in $\mathbb{R}$ is in $\mathcal{F}$). This condition suffices if $f$ itself is a measurable function, i.e. for every Borel set $B \in \mathcal{R}$, its preimage $f^{-1} (B)$ is Borel in $\mathbb{R}$, and by measurablility of $X$, its preimage under $X$ is $\mathcal{F}$-measurable, making $f \circ X$ a viable random variable. With this new random variable $f \circ X$, we would now like to answer the question: What is the probability law $\mathbb{P}_{f \circ X}$ of $\mathbb{R}$? 

    This also works for joint random variables, which we will learn later. Given a joint random variable $(X_1, X_2, \ldots X_n): \Omega \rightarrow \mathbb{R}^n$, we can define a measurable function $f: \mathbb{R}^n \longrightarrow \mathbb{R}$ and define the scalar random variable $f \circ (X_1, \ldots X_n)$ on $\Omega$. But again, we want to find what the CDF of this composition. 

    \subsubsection{Maximum/Minimum of Random Variables}

      Let $X_1, X_2, \ldots, X_n$ be random variables of $(\Omega, \mathcal{F}, \mathbb{P})$ with joint CDF $F_{X_1 \ldots X_n} (x_1, \ldots, x_n)$. Let $Y_n = \min (X_1, \ldots, X_n)$ and $Z_n = \max(X_1, \ldots, X_n)$. Note that $Y_n$ and $Z_n$ are also functions of $\Omega$ to $\mathbb{R}$. To prove that they are random variables, we just have to prove that $\min$ and $\max$ are measurable functions from $\mathbb{R}^n$ to $\mathbb{R}$, which we can do by proving that the preimage of all semi-infinite interval $(-\infty, x]$ are Borel in $\mathbb{R}^n$. 
      \begin{enumerate}
        \item The preimage of $(-\infty, x]$ under $\max$ is just the set of all $n$-vectors whose max is less than $x$, which is just the semi-infinite cuboid $(-\infty, x]^n \subset \mathbb{R}^n$, which is Borel in $\mathbb{R}^n$. 
        \item The preimage of $(-\infty, x]$ under $\min$ is the set of all $n$-vectors whose min is less than $x$, i.e. at least one element must be less than $x$. But this is just the complement of all vectors that have elements all greater than $x$, which is $\mathbb{R}^n \setminus (x, +\infty)^n \subset \mathbb{R}^n$, which is Borel in $\mathbb{R}^n$. 
      \end{enumerate}
      Now we must determine the CDF of $Y_n$ and $Z_n$. 
      \begin{enumerate}
        \item We have 
        \begin{align*}
          F_{Z_n} (z) & = \mathbb{P}(\{ \omega \mid Z_n (\omega) \leq z \}) \\
          & = \mathbb{P}(\{ \omega \mid X_1 (\omega) \leq z, \ldots, X_n (\omega) \leq z\}) \\
          & = F_{X_1 \ldots X_n} (z, \ldots, z)
        \end{align*}
        where the last equality is describes simply the joint CDF of the joint distribution $(X_1, \ldots, X_n)$. If we assume independence of $X_i$'s, it simplifies out to 
        \begin{equation}
          \prod_{i} F_{X_i} (z)
        \end{equation}
        and if iid, then we have $[F_{X} (z) ]^n$, where $X$ is the common distribution. 
        \item For $Y_n$, we work with complements again and have 
        \begin{align*}
          F_{Y_n} (y) & = \mathbb{P}(\{ \omega \mid Y_n (\omega) \leq y \}) \\ 
          & = 1 - \mathbb{P}(\{ \omega \mid Y_n (\omega) > y \}) \\
          & = 1 - \mathbb{P}(\{ \omega \mid X_1 (\omega) > y, \ldots X_n (\omega) > y \}) \\
        \end{align*}
        where $\mathbb{P}(\{ \omega \mid X_1 (\omega) > y, \ldots X_n > y \})$ can be calculated from the joint distribution. If we assume independence of $X_i$, it simplifies out to 
        \begin{equation}
          1 - \prod_{i} \mathbb{P}(\{\omega \mid X_i(\omega) > y \}) = 1 - \prod_{i} \big( 1 - F_{X_i} (y) \big)
        \end{equation}
        and if iid, then we have $1 - [1 - F_{X} (y)]^n$. 
      \end{enumerate}

      \begin{example}[Uniforms]
        Let $X_1, X_2$ be iid distributed as $\mathrm{Uniform}[0, 1]$, and let $Z = \max(X_1, X_2)$ with $Y = \min(X_1, X_2)$, i.e. $Z$ is the greater of the two and $Y$ is the lesser. We would expect the PDF of $Z$ to have more mass towards $1$ and the PDF of $Y$ to have more mass towards $0$. Our common CDF is 
        \begin{equation}
          F_{X} (x) = \begin{cases} 0 & \text{ if } x < 0 \\
          x & \text{ if } 0 \leq x \leq 1 \\
          1 & \text{ if } 1 < x \end{cases}
        \end{equation}
        Let's calculate the CDF of $Z$. 
        \begin{align*}
          F_{Z} (z) & = \mathbb{P}(\{\omega \mid Z(\omega) \leq z\}) \\
          & = \mathbb{P}(\{ \omega \mid X_1 (\omega) \leq z, X_2 (\omega) \leq z\}) \\
          & = F_{X_1, X_2} (z, z) \\
          & = [F_{X} (z)]^2 = \begin{cases} 0 & \text{ if } x < 0 \\
          x^2 & \text{ if } x \in [0, 1] \\
          1 & \text{ if } 1 < x \end{cases}
        \end{align*}
        This CDF is differentiable everywhere except the two points $0$ and $1$, so we can get the PDF to be $f_Z (z) = 2 z$ for $z \in (0, 1)$ and $0$ otherwise. For the values of $f_Z$ at $0$ and $1$, we can fill it in with anything we want (since the measure of these sets are $0$), so we will just defined $f_Z (0) = 0$ and $f_Z(1) = 2$, getting 
        \begin{equation}
          f_Z (z) = \begin{cases} 2 z & \text{ if } z \in [0, 1] \\
          0 & \text{ if else} \end{cases}
        \end{equation}
        Let's calculate the CDF of $Y$. 
        \begin{align*}
          F_{Y} (y) & = \mathbb{P}(\{ \omega \mid Y(\omega) \leq y\}) \\
          & = 1 - \mathbb{P}(\{ \omega \mid Y(\omega) > y\}) \\
          & = 1 - \mathbb{P}(\{\omega \mid X_1 (\omega) > y, X_2 (\omega) > y \}) \\
          & = 1 - \mathbb{P}(\{\omega \mid X_1 (\omega) > y\}) \, \mathbb{P}(\{ X_2 (\omega) > y \}) \\ 
          & = 1 - [1 - F_X (y)]^2 = \begin{cases} 0 & \text{ if } y < 0 \\
          1 - (1 - y)^2 & \text{ if } y \in [0, 1] \\
          1 & \text{ if } y > 1 \end{cases} 
        \end{align*}
        and differentiating it (with setting any values of the PDF at the nondifferentiable points $0$ and $1$) gives 
        \begin{equation}
          f_Y (y) = \begin{cases} 2 - 2y & \text{ if } y \in [0, 1] \\
          0 & \text{ if else} \end{cases}
        \end{equation}
      \end{example}

      \begin{example}[Exponentials]
        Let $X_1, X_2, \ldots, X_n$ be independent exponential random variables with parameters $\lambda_1, \ldots, \lambda_n$, respectively (not identical!). Then, for each $X_i$, its CDF is 
        \begin{equation}
          F_{X_i} (x) = 1 - e^{-\lambda_i x} \text{ for } x \geq 0
        \end{equation}
        and let $Y = \min(X_1, \ldots, X_n)$. Then, we have 
        \begin{align*}
          F_Y (y) & = 1 - \prod_{i=1}^n [ 1 - F_{X_i} (y)] \\
          & = 1 - \prod_{i=1}^n e^{-\lambda_i x} \\
          & = 1 - e^{- ( \sum_{i=1}^n \lambda_i ) x}
        \end{align*}
        which is the CDF of an exponential distribution. So, 
        \begin{equation}
          Y \sim \mathrm{Exponential}(\lambda_1 + \ldots + \lambda_n)
        \end{equation}
        This is nice, since the minimum of a bunch of exponentials is an exponential. However, this is not the case for the maximum. 
      \end{example}

      This has nice practical applications. For example, recall the memoryless property of the exponential, which nicely models radioactive decay. If we have $n$ elements each decaying at some $\mathrm{Exponential}(\lambda_i)$ rate, then we can model the time at which the first alpha particle will emit amongst all $n$ elements will also be an exponential. These processes where the inter-emission times are exponentials are called Poisson process, which we will discuss later. 

      \begin{definition}[Order Statistic]
        Let $X_1, X_2, ..., X_n$ be a finite collection of independent, identically distributed random variables. Suppose that they are continuously distributed with density $f$ and CDF $F$. Define the random variable $X_{(k)}$ to be the $k$th ranked value, called the \textbf{$k$th order statistic}. This means that 
        \begin{equation}
          X_{(1)} = \min\{X_1, X_2, ..., X_n\}, \;\; X_{(n)} = \max\{X_1, X_2, ..., X_n\}
        \end{equation}
        and in general, for any $k \in \{1, 2, ..., n\}$, 
        \begin{equation}
          X_{(k)} = X_j \text{ if } \sum_{l=1}^n \mathbb{I}_{X_l < X_j} = k - 1
        \end{equation}
        which means that exactly $k-1$ of the values of $X_l$ are less than $X_j$. Since $F$ is continuous, 
        \begin{equation}
          X_{(1)} < X_{(2)} < ... < X_{(n)}
        \end{equation}
        holds with probability $1$. This leads us to define the random variable $X_{(k)}$ representing the $k$th order statistic.
        \begin{equation}
          f_{(k)} (y) = \begin{cases} 
          n \, \binom{n-1}{k-1} y^{k-1} (1-y)^{n-k} & y \in (0, 1) \\
          0 & y \not\in (0,1)
          \end{cases}
        \end{equation}
        That is, $X_{(k)}$ has the Beta$(k, n-k_1)$ distribution. 
      \end{definition}

    \subsubsection{Convolutions and Sums of Random Variables}

      Now given two random variables $X, Y: \Omega \rightarrow \mathbb{R}$ that each push their own probability laws $\mathbb{P}_X, \mathbb{P}_Y$ onto $\mathbb{R}$, their sum $Z = X + Y$ is also a random variable that pushes its own probability law $\mathbb{P}_Z$. We must actually prove that $Z$ is a random variable, which we can do by proving that the preimage of every $(-\infty, x]$ is $\mathcal{F}$-measurable. Equivalently (by complementation), we must prove that the preimage of every $(x, +\infty)$ (that is, all sets of form $\{ \omega \mid Z(\omega) > z\}$) is $\mathcal{F}$-measurable. Now we can write $z$ as the sum of two numbers $z = q + (z - q)$, where $q \in \mathbb{R}$, and say that 
      \begin{equation}
        \{ \omega \mid Z(\omega) > z\} = \bigcup_{q \in \mathbb{R}} \{ \omega \mid X (\omega) > q , \; Y(\omega) > z - q\}
      \end{equation}
      But using the fact that $\mathbb{Q}$ is dense in $\mathbb{R}$, we can turn this from an uncountable union to a countable union and say 
      \begin{align}
        \{ \omega \mid Z(\omega) > z\} & = \bigcup_{q \in \mathbb{Q}} \{ \omega \mid X (\omega) > q , \; Y(\omega) > z - q\} \\
        & = \bigcup_{q \in \mathbb{Q}} \big( \{\omega \mid X(\omega) > q\} \cap \{ \omega \mid Y(\omega) > z - q\} \big) 
      \end{align}
      and since I have a countable union of (an intersection of) these $\mathcal{F}$-measurable sets, $\{ \omega \mid Z(\omega) > z\}$ is $\mathcal{F}$-measurable, and we are done. This equation above even gives us a hint of how to compute the CDF of $Z$. 

      \begin{theorem}
        Given random variables $X_1, X_2, \ldots, X_n$ of probability space $(\Omega, \mathcal{F}, \mathbb{P})$, 
        \begin{enumerate}
          \item $X_1 + \ldots + X_n$ is a random variable.
          \item $X_1 \cdot \ldots \cdot X_n$ is a random variable. 
        \end{enumerate}
      \end{theorem}

      For simplicity, we will only consider jointly discrete or jointly continuous random variables. The probability law $\mathbb{P}_Z$ can be confusing to define, since given some Borel set $B \in \mathcal{R}$, we must now look at the preimage under the \textit{sum} $X + Y$. A simpler way to approach this is to consider the joint distribution $X, Y$ and look at its distribution, which we call the \textbf{convolution} of $X$ and $Y$. This is especially simple to consider for discrete random variables. 

      \begin{definition}[Sums of Discrete Random Variables]
        Take two discrete random variables $X, Y$ with their joint PMF $p_{X, Y} (x, y)$ and their sum $Z = X + Y$. We can see that the PMF of $Z$ is 
        \begin{equation}
          p_Z (z) = \sum_{(x, y) \,:\, x + y = z} p_{X, Y} (x, y) = \sum_{x \in \mathcal{X}} p_{X, Y} (x, z - x)
        \end{equation}
        which by abuse of notation, we denote
        \begin{equation}
          \mathbb{P}(Z = z) = \sum_{x \in \mathcal{X}} \mathbb{P}(X = x, Y = z - x)
        \end{equation}
        The CDF is very simple, since we just have to sum over all $(x, y)$ such that their sum is less than $z$: 
        \begin{equation}
          F_Z (z) = \sum_{(x, y) \,:\, x + y \leq z} p_{X, Y} (x, y)
        \end{equation}
        which by abuse of notation, we write 
        \begin{equation}
          \mathbb{P}(Z \leq z) = \sum_{(x, y) \,:\, x + y \leq z} \mathbb{P}(X = x, Y = y)
        \end{equation}
        If $X$ and $Y$ are independent, then their joint distribution is the product of their singular distributions, and so we have 
        \begin{equation}
          p_Z (z) = \sum_x p_X (x) \, p_Y (z - x) \coloneqq p_X \ast p_Y
        \end{equation}
        where $p_Z = p_X \ast p_Y$ is called the convolution of $p_X$ and $p_Y$. By abuse of notation, 
        \begin{equation}
          \mathbb{P}(Z = z) = \sum_{x \in \mathcal{X}} \mathbb{P}(X = x) \, \mathbb{P}(Y = z - x)
        \end{equation}
      \end{definition}

      \begin{example}[Sums of Poisson RVs]
        Let $X_1$ and $X_2$ be independent Poisson random variables with parameters $\lambda_1, \lambda_2 > 0$, and let $Z = X_1 + X_2$. The PMF of each $X_i$ is 
        \begin{equation}
          p_{X_i} (k) = \frac{e^{-\lambda_i} \lambda_i^k}{k!} \text{ for } k \in \mathbb{Z}
        \end{equation}
        and taking the convolution gives the PMF of $Z$: 
        \begin{align*}
          p_Z (z) & = (p_{X_1} \ast p_{X_2}) (z) \\
          & = \sum_{k=-\infty}^{+\infty} \frac{e^{-\lambda_1} \lambda_1^k}{k!} \cdot \frac{e^{-\lambda_2} \lambda_2^{z - k}}{(z - k)!} \\
          & = \sum_{k=0}^{z} \frac{e^{-\lambda_1} \lambda_1^k}{k!} \cdot \frac{e^{-\lambda_2} \lambda_2^{z - k}}{(z - k)!} \\ 
          & = \frac{e^{-(\lambda_1 + \lambda_2)}}{z!} \sum_{k=0}^z \binom{z}{k} \lambda_1^k \lambda_2^{z - k} \\
          & = \frac{e^{-(\lambda_1 + \lambda_2)} (\lambda_1 + \lambda_2)^z}{z!} 
        \end{align*}
        for $z \in \mathbb{N}_0$, which is the PMF of another Poisson. So, $Z \sim \mathrm{Poisson}(\lambda_1 + \lambda_2)$. 
      \end{example}

      This has a nice visualization, since the joint distribution of $X$ and $Y$ over $\mathbb{R}^2$ is being "summed up/integrated" over the diagonals of $\mathbb{R}^2$, i.e. the lines where $x + y = z$ for some $z$, sort of like marginalizing over these diagonals. This creates a new "diagonally marginal distribution" $Z$. 

      \begin{definition}[Sums of Continuous Random Variables]
        Take two continuous random variables $X, Y$ with their joint PDF $f_{X, Y} (x, y)$ and their sum $Z = X + Y$. To calculate the CDF, we must basically integrate the joint PDF over the borel set $\{(x, y) \in \mathbb{R}^2 \mid x + y \leq z\}$. 
        \begin{align*}
          \mathbb{P}(Z \leq z) = F_Z (z) & = \int_{(x, y) \,:\, x + y \leq z} f_{X, Y} (x, y) \,dy\,dx \\
          & = \int_{-\infty}^{+\infty} \int_{-\infty}^{z - x} f(x, y) \,dy \,dx
        \end{align*}
        We can see that the PDF of $Z$ is 
        \begin{equation}
          f_{Z} (z) = \int_{\mathbb{R}} f_{X, Y} (x, z - x) \, dx
        \end{equation}
        If $X$ and $Y$ are independent, then 
        \begin{equation}
          f_{Z} (z) = \int_{\mathbb{R}} f_{X} (x) \, f_Y (z - x) \,dx \coloneqq f_X \ast f_Y
        \end{equation}
        where $f_Z = f_X \ast f_Y$ is the convolution of $f_X$ and $f_Y$. 
      \end{definition}

      \begin{definition}[Convolution]
        Given two functions $f, g: \mathbb{R} \longrightarrow \mathbb{R}$, the \textbf{convolution} of $f$ and $g$ is a new function $f \ast g$ defined  
        \begin{equation}
          (f \ast g) (t) \coloneqq \int_\mathbb{R} f(t)\, g(t - \tau) \, d \tau
        \end{equation}
      \end{definition}

      Usually, when we take convolutions, it is not pretty and even for nice distributions like two Gaussians, convolving them is quite complicated. What we can do is transform them (using Laplace, Fourier, etc.) to make calculations easier and more elegant, which we will talk about later.  

      \begin{example}
        Let $X_1$ and $X_2$ be independent exponential with parameters $\lambda_1, \lambda_2$, with individual PDFs $f_{X_i} (x) = \lambda_i e^{-\lambda_i x}$ for $x \geq 0$. Let $Z = X_1 + X_2$. Then, 
        \begin{align*}
          f_Z (z) = (f_{X_1} \ast f_{X_2})(z) & = \int_{-\infty}^\infty \lambda_1 e^{-\lambda_1 x} \, \lambda_2 e^{-\lambda_2 (z -x)} \, dx \\
          & = \int_{0}^z \lambda_1 e^{-\lambda_1 x} \, \lambda_2 e^{-\lambda_2 (z -x)} \, dx \\ 
          & = \lambda_1 \lambda_2 e^{-\lambda_2 z} \int_0^z e^{(\lambda_2 - \lambda_1) x}\,dx \\
          & = \begin{cases} \frac{\lambda_1 \lambda_2}{\lambda_2 - \lambda_1} \big( e^{-\lambda_1 z} - e^{-\lambda-2 z} \big) & \text{ if } \lambda_1 \neq \lambda_2 \\
          \lambda^2 z e^{-\lambda z} & \text{ if } \lambda_1 = \lambda_2 = \lambda \end{cases} 
        \end{align*}
        The distribution for when $\mu_1 = \mu_2$ is called the Erlang distribution, which has many applications, but the other case is an ugly form and not studied very well. 
      \end{example}

      \begin{theorem}[Sums of Discrete Variables]
        Assume that $X$ and $Y$ are independent. 
        \begin{enumerate}
          \item $X \sim$ Binomial$(n, p)$, $Y \sim$ Binomial$(m, p)$ $\implies X + Y \sim$ Binomial$(n + m, p)$. 
          \item $X \sim$ Poisson$(\lambda)$, $Y \sim$ Poisson$(\gamma)$ $\implies X + Y \sim$ Poisson$(\lambda + \gamma)$. 
          \item If $X_1, ..., X_n$ are Geometric$(p)$, then $X_1 + ... + X_n$ is NB$(n, p)$. 
        \end{enumerate}
      \end{theorem}

      \begin{theorem}[Sums of Densities]
        Assume that $X$ and $Y$ are independent. 
        \begin{enumerate}
          \item $X \sim$ Normal$(\mu_1, \sigma_1^2)$, $Y \sim$ Normal$(\mu_2, \sigma_2^2)$ $\implies X + Y \sim$ Normal $(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2)$. 
          \item If $X_1, X_2, ..., X_n$ are Exponential$(\lambda)$, then $X_1 + ... + X_n \sim$ Gamma$(n, \lambda)$.
          \item $X \sim$ Gamma$(n, \lambda)$, $Y \sim$ Gamma$(m, \lambda)$ $\implies X + Y \sim$ Gamma$(n + m, \lambda)$. 
          \item $X \sim$ Gamma $(n, \lambda)$, $Y \sim$ Exponential $(\lambda)$ $\implies X + Y \sim$ Gamma$(n+1, \lambda)$. 
        \end{enumerate}
      \end{theorem}

    \subsubsection{Sum of Random Number of Random Variables}

      Now we consider a random variable where the number of terms we are summing is a random variable. Let $\{X_i\}_i$ be a countable sequence of independent random variables with CDF $F_{X_i}$. Let $N$ be a positive integer-valued random variable with PMF $p_N(n) = \mathbb{P}(N = n)$. Assume that $N$ is independent of $X_i$'s. Now, consider the function 
      \begin{equation}
        S_N \coloneqq \sum_{i=1}^N X_i
      \end{equation}
      To interpret this, consider the sample space $\Omega$. We have all $X_i$'s and $N$ defined on the same $\Omega$. Once $\omega \in \Omega$ realizes, the $\{X_i\}$'s will realize as a sequence of numbers, and $N$ will realize as a positive integer. We simply sum them up according to the rule $S_N$, and by this definition, $S_N$ is a real-valued function on $\Omega$. We first have to prove that $S_N$ is a random variable (since we only know that a \textit{fixed} sum of random variables is a random variable), and then we must find the CDF of $S_N$ $\mathbb{P}(S_N \leq x)$. 

      First, note that the realization of $N$ partitions the sample space as 
      \begin{equation}
        \Omega = \bigsqcup_{n = 1}^\infty \{\omega \mid N(\omega) = n\}
      \end{equation}
      Once I have this partition, I can invoke the partition rule and write 
      \begin{align*}
        \mathbb{P}(S_N \leq x) & = \sum_{k=1}^\infty \mathbb{P}(S_N \leq x \mid N = k) \, \mathbb{P}(N = k) \\
        & = \sum_{k=1}^\infty \mathbb{P}(S_k \leq x \mid N = k) \, \mathbb{P}(N = k) & (\text{conditioned on } N = k) \\
        & = \sum_{k=1}^\infty \mathbb{P}(S_k \leq x) \, \mathbb{P}(N = k) & (N \text{ is indep. of } X_i \text{s})
      \end{align*}
      where $\mathbb{P}(N = k)$ is known since we know the PMF of $N$, and the CDFs $\mathbb{P}(S_k \leq x)$ can be computed by computing the deterministic sums and computing their CDF. 

      \begin{example}
        Let $X_i$'s be iid $\mathrm{Exponential}(\lambda)$, and $N \sim \mathrm{Geometric}(p)$. We know that the deterministic sum of iid exponentials gives an Erlang. So, $S_N = \sum_{i=1}^N X_i$, and its CDF is 
        \begin{equation}
          \mathbb{P}(S_N \leq x) = \sum_{k=1}^\infty \mathbb{P}(S_k \leq x) \, \mathbb{P}(N = k)
        \end{equation}
        where $\mathbb{P}(N = k) = (1 - p)^{k - 1} p$. The PDF of the Erlang is 
        \begin{equation}
          p_{S_k} (x) = \frac{\lambda^n x^{n-1}}{(n - 1)!} e^{-\lambda x}
        \end{equation}
        and doing the brute force calculations gives a clean $S_N \sim \mathrm{Exponential}(\lambda p)$. 
      \end{example}

    \subsubsection{General Transformations of Random Variables}

      Now we will look at more general transformations that are not just minimum, maximum, deterministic sums, or random sums. Let us have a probability space $(\Omega, \mathcal{F}, \mathbb{P})$, a random variable $X: \Omega \rightarrow \mathbb{R}$, and a measurable function $f: \mathbb{R} \rightarrow \mathbb{R}$. Now given that we know the CDF (and therefore distribution) of $X$, we want to find the CDF of random variable $Y = f(X) = f \circ X$ (which we have established as a random variable already due to measurability of $f$): $F_Y (y) = \mathbb{P}(Y \leq y)$, which is just $\mathbb{P}_Y ((-\infty, y])$ (where $\mathbb{P}_Y$ is the probability law on $Y$). But rather than trying to take the preimage of the entire composite random variable $Y$ and calculating $\mathbb{P}\big( Y^{-1}((-\infty, y]) \big)$ under the probability on $\mathcal{F}$, let's just take the preimage one step at a time. Note that $f^{-1} \big( (-\infty, y] \big) = \{x \in \mathbb{R} \mid f(x) \leq y\}$. We can then write the CDF of $Y$ in terms of the probability law of $X$: 
      \begin{align*}
        F_Y (y) & = \mathbb{P}_X \big( f^{-1} ((-\infty, y]) \big) \\
        & = \mathbb{P}_X \big( \{x \in \mathbb{R} \mid f(x) \leq y\} \big) \\
        & = \mathbb{P} \big( X^{-1} \circ f^{-1} ((-\infty, y]) \big) 
      \end{align*}
      Depending on how complicated $f$ is, this may be easy or not, but conceptually, this is no problem. But theoretically, this is as far as we can go. Let's move onto some examples. 

      \begin{example}[Chi-Squared Distribution]
        Let $X \sim \mathcal{N}(0, 1)$ and $Y = f(X) = X^2$. Note that $X$ takes values in $(-\infty, +\infty)$ and $Y$ in $[0, +\infty)$. Then, we can write 
        \begin{align*}
          F_Y (y) & = \mathbb{P}(Y \leq y) \\ 
          & = \mathbb{P}_Y ( (-\infty, y]) \\
          & = \mathbb{P}_Y (  [0, y]) & (\text{range of } Y) \\
          & = \mathbb{P}_X ( f^{-1} ([0, y]) ) & (\text{work in prob. law of } X) \\
          & = \mathbb{P}_X ( [-\sqrt{y}, \sqrt{y}] ) \\
          & = \int_{-\sqrt{y}}^{\sqrt{y}} f_X (x) \,dx 
        \end{align*}
        Rewriting this in our abuse of notation notation, we have 
        \begin{align*}
          F_Y (y) & = \mathbb{P}(Y \leq y) \\
          & = \mathbb{P}(X^2 \leq y) \\
          & = \mathbb{P}( -\sqrt{y} \leq X \leq \sqrt{y}) \\
          & = 2 \mathbb{P}(0 \leq X \leq \sqrt{y}) & (\text{Symmetry of Gaussian})\\
          & = \frac{2}{\sqrt{2} \pi} \int_0^{\sqrt{y}} e^{-x^2 / 2} \,dx 
        \end{align*}
        and this is clearly differentiable, since it is written like an integral. Doing so gives the PDF
        \begin{equation}
          f_Y (y) = \frac{1}{\sqrt{2 \pi y}} e^{-y/2} \text{ for } y \geq 0
        \end{equation}
        This describes the PDF of a \textbf{Chi-Squared} distribution. 
      \end{example}

      \begin{example}[Log-Normal Distribution]
        Let $X \sim \mathcal{N}(0, 1)$ and $Y = f(X) = e^X$. Note that the range of $f$ is $(0, +\infty)$. So, 
        \begin{align*}
          F_Y (y) & = \mathbb{P}(Y \leq y) \\
          & = \mathbb{P}_Y ((-\infty, y]) \\
          & = \mathbb{P}_Y ( (0, y]) \\ 
          & = \mathbb{P}_X ( f^{-1} ((0, y]) ) \\
          & = \mathbb{P}_X ( (-\infty, \ln{y}] ) \\
          & = \int_{-\infty}^{\ln{y}} f_X (x)\,dx 
        \end{align*}
        Rewriting this in our abuse of notation notation, we have 
        \begin{align*}
          F_Y (y) & = \mathbb{P}(e^X \leq y) \\
          & = \mathbb{P}(X \leq \ln(y)) \\ 
          & = \int_{-\infty}^{\ln(y)} \frac{1}{\sqrt{2} \pi} e^{-x^2/ 2} \,dx 
        \end{align*}
        We can differentiate this to get 
        \begin{equation}
          f_Y (y) = \frac{1}{y \sqrt{2 \pi}} e^{-\frac{(ln{y})^2}{2}} \text{ for } y \geq 0
        \end{equation}
        This describes the PDF of a \textbf{log-normal} distribution. 
      \end{example}

      We now show a more specific formula under more specific assumptions about the transformation. Suppose $X$ is a \textit{continuous} random variable with density $f_X$ and $g: \mathbb{R} \rightarrow \mathbb{R}$ a monotonic differentiable function. Then, the CDF of the random variable $Y = g(X)$ can be written in the probability law of $X$, which can then by written as an integral by invoking the Radon-Nikodym theorem: 
      \begin{align*}
        \mathbb{P}(Y \leq y) & = \mathbb{P}_X (f^{-1} ((-\infty, y]) \\
        & = \int_{x \,:\, g(x) \leq y} f_X (x) \,dx
      \end{align*}
      Note that we can now talk about the actual inverse $g^{-1}$ since differentaible and monotonic implies invertibility. 
      \begin{enumerate}
        \item Assuming $g$ is monotonically increasing, we can use the change of variables $x = g^{-1} (t)$ and $g(x) = t \implies g^\prime (x) \,dx = dt$ to get the above integral as 
        \begin{equation}
          \int_{-\infty}^{g^{-1} (y)} f_X (x) \,dx = \int_{-\infty}^t \frac{f_X \big( g^{-1} (t) \big)}{g^\prime \big( g^{-1} (t)\big)} \,dt 
        \end{equation}
        but since this is simply the CDF of $Y$, the PDF must equal 
        \begin{equation}
          f_Y (y) = \frac{f_X (g^{-1} (y) )}{g^\prime (g^{-1} (t))}
        \end{equation}
        \item If $g$ is monotonically decreasing, we get 
        \begin{equation}
          f_Y (y) = \frac{f_X (g^{-1} (y) )}{- g^\prime (g^{-1} (t))}
        \end{equation}
      \end{enumerate}
      In general, we can consider both cases by putting an absolute value 
      \begin{equation}
        f_Y (y) = \frac{f_X (g^{-1} (y) )}{|g^\prime (g^{-1} (t))|}
      \end{equation}
      and $g^\prime (g^\prime (y))$ is the Jacobian, the same one that we use when we perform a change of variables in integration. 

      \begin{example}[Log-Normal Revisited]
        Given $X \sim \mathcal{N}(0, 1)$ and $Y = e^X$ (which is monotonically increasing), we can simply plug in the formula to get the PDF: 
        \begin{equation}
          f_Y (y) = \frac{f_X (g^{-1} (y) )}{|g^\prime (g^{-1} (t))|} = \frac{f_X (\ln{y}) }{ | e^{\ln{y}} |} = \frac{1}{\sqrt{2 \pi} y} e^{-(\ln{y})^2 / 2}
        \end{equation}
        for $y > 0$. This domain is important since $\ln{y}$ is only defined for $y > 0$. 
      \end{example}

      \begin{example}
        Given $X \sim \mathcal{N}(0, 1)$ and $Y = f(X) = X^2$, we cannot use the formula since $f$ is not monotonic on the range of $X$, which is $(-\infty, +\infty)$.  
      \end{example}

      \begin{example}
        Given $X \sim \mathrm{Exponential}(\lambda)$ and $Y = f(X) = X^2$, it may seem like the formula is not applicable here, but $f$ \textit{is} monotonic on the range of $X$, which is $[0, + \infty)$. 
      \end{example}

      However, there is much less chance of error by deriving using first principles, so I would recommend using it always rather than these formulas. 

      Let's do the $n$-dimensional version of this. Given random variables $X_1, X_2, \ldots, X_n$ iid random variables with joint density $f_{X_1 \ldots X_n} (x_1, \ldots, x_n)$, we define the transformation $g: \mathbb{R}^n \rightarrow \mathbb{R}^n$ as 
      \begin{equation}
        \begin{bmatrix} Y_1 \\ \vdots \\ Y_N \end{bmatrix} = \begin{bmatrix} g_1 (X_1) \\ \vdots \\ g_n (X_N) \end{bmatrix}
      \end{equation}
      Then, the PDF of $Y$ will be 
      \begin{align*}
        f_{Y_1 \ldots Y_n} (y_1, \ldots, y_n) & = f_{X_1 \ldots X_n} \big( \mathbf{g}^{-1} (\mathbf{y}) \big) \cdot | \mathbf{J}(\mathbf{y})| \\
        & = f_{X_1 \ldots X_n} \big( g_1^{-1}(y_1), \ldots, g_n^{-1} (y_n) \big) \cdot | \mathbf{J}(\mathbf{y})| \\
      \end{align*}
      where 
      \begin{equation}
        \mathbf{J}(y) = \mathrm{det}\begin{pmatrix} 
        \frac{\partial x_1}{\partial y_1} & \ldots & \frac{\partial x_n}{\partial y_1} \\
        \vdots & \ddots & \vdots \\ 
        \frac{\partial x_1}{\partial y_n} & \ldots & \frac{\partial x_n}{\partial y_n} \end{pmatrix}
      \end{equation}

\section{Integration}

  \subsection{Construction and Properties}

    \subsubsection{Simple Functions}

      Remember that Riemann integration is characterized by the approximation of step functions, which are the "building blocks" of Riemann integrable functions. To define the Lebesgue integral, we will consider a generalization of step functions called \textit{simple functions}. A function will be Lebesgue integrable if it can be approximated by these simple functions in some appropriate way. 

      \begin{definition}[Simple Functions]
        For $A \subset X$ (any subset, not just in some $\sigma$-algebra), the \textbf{characteristic}, or \textbf{indicator} \textbf{function} of $A$ is the function $1_A : X \longrightarrow \mathbb{R}$ defined 
        \begin{equation}
          1_A (x) = \begin{cases} 1 & \text{ if } x \in A \\ 0 & \text{ if else} \end{cases}
        \end{equation}
        A function $\phi: \mathbb{R} \longrightarrow \mathbb{R}$ is called a \textbf{simple function} if it is a finite linear combination of characteristic functions. 
        \begin{equation}
          \phi = \sum_{i=1}^n a_i 1_{A_i}
        \end{equation}
      \end{definition}

      \begin{lemma}[Measurability on Simple Functions]
        Now, let $(X, \mathcal{A})$ be a measurable space. Then, 
        \begin{equation}
          \phi = \sum_{i=1}^n a_i 1_{A_i} : (X, \mathcal{A}) \longrightarrow \mathbb{R}
        \end{equation}
        is measurable if all $A_i$ are measurable, i.e. $A_i \in \mathcal{A}$ for all $i$. 
      \end{lemma}
      \begin{proof}
        Let $T$ be an open set in $\mathbb{R}$. Then, for characteristic function $1_A$, 
        \begin{equation}
          1_A^{-1} (T) = \begin{cases} 
          \emptyset & \text{ if } 0, 1 \not\in T \\
          A & \text{ if } 1 \in T, 0 \not\in T \\
          X \setminus A & \text{ if } 0 \in T, 1 \not\in T \\
          X & \text{ if } 0, 1 \in T
          \end{cases}
        \end{equation}
        and so $1_A$ must be measurable if $A \in \mathcal{A}$ (which also by definition implies that $A^c = X \setminus A \in \mathcal{A}$). If $1_{A_i}$ is measurable, then the linear combination of measurable functions is also measurable. 
      \end{proof}

      Also observe that the coefficients need not be unique, since we can write 
      \begin{equation}
        1 \cdot 1_{[0, 1]} + 1 \cdot 1_{[0.5, 1]} = 1 \cdot 1_{[0, 0.5]} + 2 \cdot 1_{[0.5, 1]}
      \end{equation}
      If the $E_i$'s are disjoint, then this decomposition is unique and is called the \textbf{standard representation} of $\phi$. 

      \begin{example}[Step Function as Simple Function]
        For $a, b \in \mathbb{R}$, with $a < b$, let $f: [a, b] \longrightarrow \mathbb{R}$ be a step function. That is, there exists a partition $a = x_0 < x_1 < \ldots < x_n = b$ and constants $c_1, c_2, \ldots, c_n \in \mathbb{R}$ s.t. $f(x) = c_i$ for all $x \in (x_{i-1}, x_i)$ and each $i = 1, \ldots, n$. Then, $f$ is equal to the following simple function, taken over all open intervals and the points $x_j$ at the boundary of each interval. 
        \begin{equation}
          f = \sum_{i=1}^n c_i 1_{(x_{i-1}, x_i)} + \sum_{j=0}^n f(x_j) 1_{\{x_j\}}
        \end{equation}
        If we ignore the behavior of $f$ on the partition points $x_j$'s, then $f$ agrees almost everywhere with the simple function 
        \begin{equation}
          \sum_{i=1}^n c_i 1_{(x_{i-1}, x_i)}
        \end{equation}
      \end{example}

      If the $A_i$'s above are just intervals in $\mathbb{R}$, then $\phi$ reduces to a step function. But the entire problem with intervals is that they are too coarse. We can't work with them, so we generalize them to all measurable sets in $(X, \mathcal{A})$. The Riemann integral is built on an approximation scheme of a function, which we usually want to be continuous to satisfy this approximation, and so, if we want to build an approximation scheme for Lebesgue integrals, we want a similar scheme, i.e. if we take a sequence of simple measurable functions, I can get arbitrarily close to any measurable function $f$. This is exactly what we show below. 

      \begin{theorem}
        If $f: (X, \mathcal{A}) \longrightarrow [0, \infty]$ is measurable, there are simple measurable functions $f_k : (X, \mathcal{A}) \longrightarrow [0, \infty)$ s.t. 
        \begin{equation}
          f_k \leq f_{k+1} \text{ and } f = \lim_{k \rightarrow \infty} f_k
        \end{equation}
        where the inequalities and limits are pointwise. 
      \end{theorem}
      \begin{proof}
        We give a general picture of this proof for a function $f: \mathbb{R} \longrightarrow [0, \infty]$. We can first divide the codomain of the graph below into segments of $t = 1, 2, \ldots$, and take the preimage of all these units under $f$ to get $f_1$. More specifically, $A_1^t = f^{-1} ([t, \infty])$ for all $t$. By measurability of $f$, $A_1^t$ is measurable, and we can assign $f_1 = 1_{A^1_1} + 1_{A_1^2} \leq f$. 
        \begin{center}
          \includegraphics[scale=0.23]{img/Lebesgue_1.jpg}
        \end{center}
        Doing this again with finer subintervals of the codomain gives us, with $f_2 = 1_{A_2^1} + 1_{A_2^2} + 1_{A_2^3} + 1_{A_2^4} \leq f$. 
        \begin{center}
          \includegraphics[scale=0.23]{img/Lebesgue_2.jpg}
        \end{center}
        and in general, we have $f_k = \sum_{j=1}^\infty \frac{1}{2^{k-1}} 1_{A^j_k}$. But we said a simple function is a \textit{finite} sum, and if $\infty$ is in the range of $f$, then this becomes a problem. We can quickly fix this by just truncating the summation at a certain point in the codomain ($f_1$ only considers intervals up to $1$, $f_2$ up to $2$ and so on), ultimately giving us 
        \begin{equation}
          f_k = \sum_{j=1}^{k 2^{k-1}} \frac{1}{2^{k-1}} 1_{A^j_k}
        \end{equation}
      \end{proof}

    \subsubsection{Lebesgue Integral}

      Finally, we can learn how to integrate. We require the positiveness condition on $f$ below because our previous theorem on approximating arbitrary functions with simple measurable functions $f_k$ requires that it be positive, too. 

      \begin{definition}[Lebesgue Integral of Positive Simple Functions]
        If $f = \sum_{k=1}^n c_k 1_{A_k}$ is a positive simple Lebesgue measurable function on measure space $(X, \mathcal{A}, \mu)$, then the \textbf{Lebesgue integral} of $f$ is 
        \begin{equation}
          \int f \, d\mu = \sum_{k=1}^n c_k \mu(A_k)
        \end{equation}
      \end{definition}

      This Lebesgue integral agrees with the Riemann integral for step functions. Let $c_1, \ldots, c_n \in [0, \infty)$ and $a = x_0 < x_1 < \ldots < x_n = b$ be a partition. Let $f: [a, b] \longrightarrow [0, \infty]$ be a step function taking the value $c_i$ on the interval $(x_{i-1}, x_i)$ for $i = 1, \ldots, n$. Then the Riemann integral of $f$ is simply 
      \begin{equation}
        \int f(x) \,dx = \sum_{i=1}^n c_k |x_i - x_{i-1}|
      \end{equation}
      The Lebesgue integral is 
      \begin{align*}
        \int f \, d \mu & = \sum_{i=1}^n c_i \mu((x_{i-1}, x_i)) + \sum_{j=0}^n f(x_j) \mu(\{x_j\}) \\
        & = \sum_{i=1}^n c_k |x_i - x_{i-1}|
      \end{align*}
      which agrees with the Riemann integral. In the Riemann integral, we write $dx$ to indicate the variable that is being integrated over, but in the Lebesgue integral, we write $d \mu$, the measure which we are integrating over. Therefore, there are many possible values that can come out of a Lebesgue integral of a certain function, while a Riemann integral outputs only one value if exists. 

      \begin{example}
        Consider the simple function (consisting of one characteristic function) $1_{\mathbb{Q} \cap [0, 1]}$. $\mathbb{Q} \cap [0, 1]$ is a Lebesgue measurable set of $\mathbb{R}$, and we have $1_{\mathbb{Q} \cap [0, 1]} \geq 0$, so its Lebesgue integral is given by the above definition: 
        \begin{equation}
          \int_{\mathbb{R}} 1_{\mathbb{Q} \cap [0, 1]} \, d\lambda = 1 \cdot \lambda(\mathbb{Q} \cap [0, 1]) = 0
        \end{equation}
      \end{example}

      \begin{definition}[Lebesgue Integral on Positive Measurable Functions]
        If $f: (X, \mathcal{A}, \mu) \longrightarrow [0, \infty]$ is measurable, then 
        \begin{equation}
          \int_X f \, d\mu = \sup \Big\{ \int g\, d\mu \,\Big|\, g \text{ simple }, g \leq f\Big\}
        \end{equation}
      \end{definition}

      Unlike Riemann integration, which looks at both the supremum and infimum of integrals of simple functions, Lebesgue integration only looks at the supremum, given that $f$ is nonnegative, so for all these $f$, the Lebesgue integral always exists. Defining Lebesgue integration for all real-valued functions, requires a simple extension. 

      \begin{definition}[Lebesgue Integral]
      Given a function $f: (X, \mathcal{A}, \mu) \longrightarrow \mathbb{R}$, we can split $f$ into a positive and negative part: 
      \begin{equation}
        f = f^+ - f^-
      \end{equation}
      where $f^+ = \max(f, 0)$ and $f^- = \max(-f, 0)$. Then, the Lebesgue integral of $f$ is 
      \begin{equation}
        \int f \, d \mu = \int f^+ \, d\mu - \int f^- \, d\mu
      \end{equation}
      given that at least one of these integrals is finite. If one is infinite and the other is finite, then we can call it infinite. If we have \textit{both} infinite integrals, then the integral doesn't exist. It has the properties: 
      \begin{enumerate}
        \item Monotonicity: 
        \begin{equation}
          g \leq f \implies \int g \, d\mu \leq \int f\, d\mu
        \end{equation}
        \item Scalar Multiplication: 
        \begin{equation}
          \int c f \, d\mu = c \int f \, d\mu
        \end{equation}
        \item Addition:
        \begin{equation}
          \int f + g \, d\mu = \int f \,d\mu + \int g \,d\mu
        \end{equation}
      \end{enumerate}
      \end{definition}

      Since $|f| = f^+ + f^-$, $f$ is also Lebesgue integrable if 
      \begin{equation}
        \int |f| \, d\mu < \infty
      \end{equation}
      since by triangle inequality, we have 
      \begin{equation}
        \bigg| \int f \, d\mu \bigg| \leq \int |f| \, d \mu
      \end{equation}

      \begin{definition}
        The set of all functions $f: (X, \mathcal{A}, \mu) \longrightarrow \mathbb{R}$ that are Lebesgue integrable is denoted $\mathcal{L}^1(X, \mathcal{A}, \mu; \mathbb{R})$, or for short $\mathcal{L}^1(X, \mathcal{A}, \mu)$. 
      \end{definition}

      \begin{theorem}
        $f: \mathbb{R} \longrightarrow \mathbb{R}$ is Riemann integrable iff it is continuous $\lambda$ almost everywhere. If so, then $f$ is Lebesgue measurable and 
        \begin{equation}
          \int_{[a, b]} f \,d\lambda = \int_a^b f \, dx
        \end{equation}
        for all $a < b$. 
      \end{theorem}

    \subsubsection{Integral Inequalities}

      We introduce 3 important inequalities on the integral. 

      \begin{theorem}[Jensen's Inequality]
        Suppose $\phi$ is convex, that is, 
        \begin{equation}
          \lambda \phi(x) + (1 - \lambda) \phi(y) \geq \phi (\lambda x + (1 - \lambda) y)
        \end{equation}
        for all $\lambda \in (0, 1)$ and $x, y \in \mathbb{R}$. If $\mu$ is a probability measure, and $f$ and $\varphi(f)$ are integrable, then 
        \begin{equation}
          \varphi\bigg( \int f \,d\mu \bigg) \leq \int \varphi(f) \,d\mu
        \end{equation}
      \end{theorem}

      \begin{theorem}[Holder's Inequality]
        If $p, q$ are Holder conjugates, then 
        \begin{equation}
          \int |f g|\, d\mu \leq ||f||_p ||g||_q
        \end{equation}
      \end{theorem}

      \begin{corollary}[Cauchy-Schwarz Inequality]
        Given that $p = q = 2$ above, then we have 
        \begin{equation}
          \int |f g|\, d\mu \leq ||f||_2 ||g||_2
        \end{equation}
        which is similar to the familiar equation $\langle u, v \rangle \leq ||u|| ||v||$. 
      \end{corollary}

    \subsubsection{Convergence Theorems}

      Now, we want to give conditions that guarantee 
      \begin{equation}
        \lim_{n \rightarrow \infty} \int f_n \,d \mu = \int \big( \lim_{n \rightarrow \infty} f_n \big) \, d\mu
      \end{equation}

      \begin{definition}[Convergence in Measure]
        A sequence of functions $f_n \rightarrow f$ \textbf{in measure} if for any $\epsilon > 0$, 
        \begin{equation}
          \mu\big( \{x \,:\, |f_n (x) - f(x)| > \epsilon \}\big) \rightarrow 0 \text{ as } n \rightarrow \infty
        \end{equation}
      \end{definition}

      \begin{theorem}[Bounded Convergence Theorem]
        Let $E$ be a set with $\mu(E) < \infty$. Suppose $f_n = 0$ on $E^c$, $|f_n (x)| \leq M$, and $f_n \rightarrow f$ in measure. Then, 
        \begin{equation}
          \int f \,d\mu = \lim_{n \rightarrow \infty} \int f_n d\mu
        \end{equation}
      \end{theorem}

      \begin{lemma}[Fatou's Lemma]
        If $f_n \geq 0$, then
        \begin{equation}
          \lim_{n \rightarrow \infty} \inf \int f_n \,d\mu \geq \int \Big( \lim_{n \rightarrow \infty} \inf f_n \Big) \,d\mu
        \end{equation}
      \end{lemma}

      \begin{theorem}[Monotone Convergence Theorem]
        Given a nondecreasing sequence of measurable nonnegative functions $\{f_n\}$, its limit $f_n \uparrow f$ always exists (since $f_n$ is nondecreasing), is measurable, and 
        \begin{equation}
          \int f_n \, d\mu \uparrow \int f \, d\mu
        \end{equation}
        This allows us to integrate the limit of nice functions $f_n$ by integrating these $f_n$ first and then finding what the values converge to. 
      \end{theorem}

      \begin{theorem}[Dominated Convergence Theorem]
        If $f_n \rightarrow f$ a.e., $|f_n| \geq g$ for all $n$, and $g$ is integrable, then 
        \begin{equation}
          \int f_n \,d\mu \rightarrow \int f\, d\mu
        \end{equation}
      \end{theorem}

    \subsubsection{Product Measures, Fubini's Theorem}

      Let $(X, \mathcal{A}, \mu_1)$ and $(Y, \mathcal{B}, \mu_2)$ be two measure spaces. Let 
      \begin{align*}
        \Omega & = X \times Y = \{(x, y) \mid x \in X, y \in Y\} \\
        \mathcal{S} & = \{A \times B \mid A \in \mathcal{A}, B \in \mathcal{B}\}
      \end{align*}
      The sets in $\mathcal{S}$ are called \textbf{rectangles}. It is easy to see that $\mathcal{S}$ is a semi-algebra: 
      \begin{align*}
        (A \times B) \cap (C \times D) & = (A \cap C) \times (B \cap D) \\
        (A \times B)^c & = (A^c \times B) \cup (A \times B^c) \cup (A^c \times B^c) 
      \end{align*}

      \begin{theorem}
        There is a unique measure $\mu = \mu_1 \times \mu_2$ (or denoted $\mu_1 \otimes \mu_2$) on $\mathcal{F}$ with 
        \begin{equation}
          \mu(A \times B) = \mu_1 (A) \, \mu_2 (B)
        \end{equation}
      \end{theorem}

      \begin{theorem}[Fubini's Theorem]
        Let $(X, \mathcal{A}, \mu_1)$ and $(Y, \mathcal{B}, \mu_2)$ be two measure spaces and $(X \times Y, \mathcal{F}, \mu = \mu_1 \times \mu_2)$ be their product space. Then, if $f \geq 0$ or $\int_{X \times Y} |f| \,d\mu < \infty$, then 
        \begin{equation}
          \int_X \int_Y f(x, y) \, \mu_2 \mu_1 = \int_{X \times Y} f \,d\mu = \int_Y \int_X f(x, y) \, \mu_1 \mu_2
        \end{equation}
      \end{theorem}

  \subsection{Random Vectors}

    Now when we consider several random variables, they will all be defined on the same probability space. Given two random variables $X$ and $Y$ on $(\Omega, \mathcal{F}, \mathbb{P})$, they will each induce a probability law $\mathbb{P}_X$ and $\mathbb{P}_Y$ which completely characterizes them. Note that it is the same underlying randomness that is feeding these random variables, and so if I know some information about the value of $X$, then we know something about outcome $\omega$, which can be used to find something about the value of $Y$. To capture this, we can imagine the map $(X, Y) : \Omega \longrightarrow \mathbb{R}^2$ defined $(X, Y)(\omega) \coloneqq (X(\omega), Y(\omega))$. And just like how $X$ induces a measure $P_X$ onto $\mathbb{R}$, we can imagine $(X, Y)$ inducing a measure onto $\mathcal{B}(\mathbb{R}^2)$, which can be generated by all semi-infinite rectangles $(-\infty, x] \times (-\infty, y]$. Ideally, we would want to put a measure $\mathbb{P}_{X, Y}$ on $\mathbb{R}^2$ s.t. 
    \begin{equation}
      \mathbb{P}_{X, Y}(B) \coloneqq \mathbb{P}((X, Y)^{-1}(B))
    \end{equation}
    where $(x, y)^{-1}(b) = \{ \omega \in \omega \mid (x(\omega), y(\omega)) \in b\}$ denotes the preimage of $(x, y)$. but is $(x, y)^{-1}(b)$ $\mathcal{f}$-measurable? it turns out that it is. 

    \begin{theorem}
      Let $f: (X, \mathcal{A}, \mu) \longrightarrow \mathbb{R}^n$ have component functions $f_1, f_2, \ldots, f_n$. Then, $f$ is measurable (i.e. $f^{-1} (B) \in \mathcal{A}$ for all $B \in \mathcal{B}(\mathbb{R}^n)$) if and only if all of its component functions are measurable (i.e. $f_i^{-1} (B) \in \mathcal{A}$ for all $B \in \mathcal{B}(\mathbb{R}^n)$). 
    \end{theorem}

    From the theorem above, I have a probability law $\mathbb{P}_{X, Y}$ on all Borel sets of $\mathbb{R}^2$, making $(\mathbb{R}^2, \mathcal{B}(\mathbb{R}^2), \mathbb{P}_{X, Y})$ a probability space. Now, since $X$ and $Y$ are both random variables dependent on the same $\omega \in \Omega$, we could expect certain "combinations" of $X$ and $Y$ to be more probable than other combinations. 

    \begin{definition}[Joint Probability Law]
      Given two random variables $X, Y$ on $(\Omega, \mathcal{F}, \mathbb{P})$, the \textbf{joint random variable} $(X, Y): \Omega \longrightarrow \mathbb{R}^2$ is a measurable function defined 
      \begin{equation}
        (X, Y) (\omega) \coloneqq (X(\omega), Y(\omega))
      \end{equation}
      which induces a \textbf{joint probability law} $\mathbb{P}_{X, Y}: \mathcal{B}(\mathbb{R}^2) \longrightarrow [0, 1]$ defined 
      \begin{equation}
        \mathbb{P}_{X, Y}(B) \coloneqq \mathbb{P}((X, Y)^{-1}(B)) \; \forall B \in \mathcal{R}
      \end{equation}
      of $X, Y$. This law captures everything there is about the interdependence of $X$ and $Y$. 
    \end{definition}

    Given joint probability law $\mathbb{P}_{X, Y}$, we can get the probability laws of $X$ and $Y$ separately. For example, we can take a specific Borel set of $\mathbb{R}$ representing the outcomes of $X$ and look at every single combination of it with every $Y$. But knowing $\mathbb{P}_X$ and $\mathbb{P}_Y$ is not enough to know the joint $\mathbb{P}_{X, Y}$. 

    \begin{definition}[Marginal Probability Law]
      Given a joint probability law $\mathbb{P}_{X, Y}$ of $X, Y$, we can get the \textbf{marginal probability law} of $X$ by feeding in Borel sets of form $B \times \mathbb{R} \in \mathcal{B}(\mathbb{R}^2)$. 
      \begin{equation}
        \mathbb{P}_X (B) = \mathbb{P}_{X, Y} (B \times \mathbb{R})
      \end{equation}
      and the marginal probability law of $Y$ as 
      \begin{equation}
        \mathbb{P}_Y (B) = \mathbb{P}_{X, Y} (\mathbb{R} \times B)
      \end{equation}
    \end{definition}

    \begin{definition}[Joint Cumulative Distribution Function]
      Since sets of the form $(-\infty, x] \times (-\infty, y]$ are Borel in $\mathbb{R}^2$, the \textbf{joint cumulative distribution function} 
      \begin{align*}
        F_{X, Y} & \coloneqq \mathbb{P}_{X, Y} \big( (-\infty, x] \times (-\infty, y] \big) \\
        & = \mathbb{P} \big( \{\omega \mid X(\omega) \leq x\} \cap \{ \omega \mid Y(\omega) \leq y\} \big)
      \end{align*}
      is well-defined. By abuse of notation, we will write $F_{X, Y} (x, y) = \mathbb{P}(X \leq x, Y \leq y)$. The marginal CDFs are defined 
      \begin{align*}
        F_X (x) & \coloneqq \mathbb{P}_{X, Y} ((-\infty, x) \times \mathbb{R}) \\
        F_Y (y) & \coloneqq \mathbb{P}_{X, Y} (\mathbb{R} \times (-\infty, y))
      \end{align*}
    \end{definition}

    \begin{lemma}[Properties of Joint CDF]
      Some common properties of the joint CDF are as follows: 
      \begin{enumerate}
        \item Limits. 
        \begin{equation}
          \lim_{(x, y) \rightarrow (+\infty, +\infty)} F_{X, Y} (x, y) = 1 \text{ and } \lim_{(x, y) \rightarrow (-\infty, -\infty)} F_{X, Y} (x, y) = 0
        \end{equation}
        \item Monotonicity. 
        \begin{equation}
          x_1 \leq x_2, \; y_1 \leq y_2 \implies F_{X, Y} (x_1, y_1) \leq F_{X, Y}(x_2, y_2)
        \end{equation}
        \item Continuity from above. 
        \begin{equation}
          \lim_{\epsilon \rightarrow 0^+} F_{X, Y} (x + \epsilon, y + \epsilon) = F_{X, Y} (x, y) \text{ for all } x, y \in \mathbb{R}
        \end{equation}
        \item Maringal CDFs. 
        \begin{equation}
          \lim_{y \rightarrow \infty} F_{X, Y} (x, y) = F_X (x), \;\;\;\; \lim_{x \rightarrow \infty} F_{X, Y} (x, y) = F_Y (y)
        \end{equation}
      \end{enumerate}
    \end{lemma}

    \subsubsection{Joint Discrete Random Variables}

      \begin{definition}[Joint PMF]
        Given discrete random variables $X$ and $Y$, let their countable images be denoted $E_X, E_Y \subset \mathbb{R}$. Then, $E_X \times E_Y$ is also countable, and so the joint random variable $(X, Y)$ is also discrete. This means that we can write for some Borel $B$ of $\mathbb{R}^2$, 
        \begin{equation}
          \mathbb{P}_{X, Y} (B) = \sum_{(x, y) \in (E_X \times E_Y) \cap B} \mathbb{P}_{X, Y} (\{(x, y)\})
        \end{equation}
        and we can define the PMF as $p_{X, Y} (x, y) \coloneqq \mathbb{P}_{X, Y} (\{(x, y)\})$. By abuse of notation, we write $p_{X, Y} (x, y) = \mathbb{P} (X = x, Y = y)$ and write 
        \begin{equation}
          \mathbb{P}_{X, Y} (B) = \sum_{(x, y) \in (E_X \times E_Y) \cap B} \mathbb{P} (X = x, Y = y)
        \end{equation}
        If you give me a joint PMF $p_{X, Y}$, by the definition above this determines the entire probability law of $\mathbb{P}_{X, Y}$. 
      \end{definition} 

      \begin{definition}[Conditional PMF]
        Let $X, Y$ be discrete random variables on $(\Omega, \mathcal{F}, \mathbb{P})$. The \textbf{conditional PMF} of $X$ given $Y = y$ is defined 
        \begin{equation}
          p_{X \mid Y} (x \mid y) \coloneqq \frac{p_{X, Y} (x, y)}{p_Y (y)} = \frac{\mathbb{P}_{X, Y} (\{x, y\})}{\mathbb{P}_Y (\{y\})}
        \end{equation}
        and again by abuse of notation, we can simply write 
        \begin{equation}
          \mathbb{P}(X = x \mid Y = y) \coloneqq \frac{\mathbb{P}(X = x, Y = y)}{\mathbb{P}(Y = y)}
        \end{equation}
      \end{definition}

      \begin{theorem}[TFAE]
        Let $X$ and $Y$ be discrete random variables. Then, the following are equivalent: 
        \begin{enumerate}
          \item $X$ and $Y$ are independent. 
          \item For all $x, y \in \mathbb{R}$, the events $\{X = x\}$ (aka $X^{-1} (\{x\})$) and $\{Y = y\}$ (aka $Y^{-1} (\{y\})$) are independent. That is, 
          \begin{equation}
            \mathbb{P} \big[ X^{-1}(\{x\}) \cap Y^{-1}(\{y\}) \big] = \mathbb{P}(X^{-1}(\{x\})) \, \mathbb{P}(Y^{-1}(\{y\}))
          \end{equation}
          \item For all $x, y \in \mathbb{R}$, $p_{X, Y} (x, y) = p_X (x) \cdot p_Y (y)$. 
          \item For all $x, y \in \mathbb{R}$ such that $p_Y (y) > 0$, we have $p_{X \mid Y}(x \mid y) = p_X (x)$. 
        \end{enumerate}
      \end{theorem}

    \subsubsection{Joint Continuous Random Variables}

      \begin{definition}
        $X$ and $Y$ are jointly continuous if the joint law $\mathbb{P}_{X, Y}$ is absolutely continuous w.r.t. the Lebesgue measure on $\mathbb{R}^2$ (i.e. a Borel set of Lebesgue measure $0$ must have $P_{X, Y} = 0$ also). 
      \end{definition}

      However, $X$ and $Y$ continuous does not always imply that $(X, Y)$ are jointly continuous! If we have $X \sim \mathcal{N}(0, 1)$ and $Y = 2 X \sim \mathcal{N}(0, 4)$. Jointly continuous allows us to define a PDF on it. 

      \begin{theorem}[Radon-Nikodym Theorem]
        If $X$ and $Y$ are jointly continuous RVs, then there exists a measurable $f_{X, Y} : \mathbb{R}^2 \longrightarrow [0, \infty)$ s.t. for any $B \in \mathcal{B}(\mathbb{R}^2)$, we have 
        \begin{equation}
          \mathbb{P}_{X, Y} (B) = \int_B f_{X, Y} \, d\lambda
        \end{equation}
      \end{theorem}

      The Radon-Nikodym Theorem guarantees the existence of such $f_{X, Y}$. Taking $B = (-\infty, x] \times (-\infty, y]$, we can define the joint CDF as 
      \begin{equation}
        F_{X, Y} (x, y) = \mathbb{P}(X \leq x, Y \leq y) \coloneqq \mathbb{P}_{X, Y} \big( (-\infty, x] \times (-\infty, y] \bigg) = \int_{-\infty}^x \int_{-\infty}^y f_{X, Y} (s, t) \, dt \,ds
      \end{equation}

  \subsection{Expectation}

    \begin{definition}[Expectation]
      Given a probability space $(\Omega, \mathcal{F}, \mathbb{P})$ and a random variable $X: \Omega \longrightarrow \mathbb{R}$, the \textbf{expectation} of $X$ is defined 
      \begin{equation}
        \mathbb{E}[X] \coloneqq \int_\Omega X \, d\mathbb{P}
      \end{equation}
      Generally, if we are integrating over the entire probability space, then it is conventional to not write $\Omega$ in the integral at all: $\mathbb{E}[X] = \int X \,d\mathbb{P}$. 
    \end{definition}

    \begin{definition}[Expectation of Discrete RV]
      If $X$ is a discrete random variable \textit{that takes positive values}, then let $E = \{e_1, e_2, \ldots\}$ denote the set where $\mathbb{P}_X(E) = 1$, and let $E_i = X^{-1} (\{e_i\}) \subset \Omega$. Then, we can see that since $X$ is constantly $e_i$ on $E_i$, 
      \begin{equation}
        \int_{E_i} X \, d\mathbb{P} = e_i \cdot \mathbb{P}(E_i) = e_i \cdot \mathbb{P}_X (\{e_i\}) = e_i \cdot \mathbb{P}(X = e_i)
      \end{equation}
      which implies 
      \begin{equation}
        \mathbb{E}[X] = \int_\Omega X \, d\mathbb{P} = \sum_{i=1}^\infty \int_{E_i} X \, d\mathbb{P} = \sum_{i=1}^\infty e_i \cdot \mathbb{P}(X = e_i)
      \end{equation}
      If $X$ is discrete RV possibly taking negative values, then let $X = X^+ - X^-$, where $X^+ = \max(X, 0)$ and $X^- = - \min(X, 0)$. Then, we can compute 
      \begin{equation}
        \mathbb{E}[X] = \mathbb{E}[X^+] - \mathbb{E}[X^-]
      \end{equation}
      which is well-defined as long as we don't have "$\infty - \infty$."
    \end{definition}

    Note that the reason why expectations of the form $\infty - \infty$ are indeterminate is because of the Riemann rearrangement theorem. 

    \begin{theorem}[Riemann's Rearragenement Theorem]
      Given a series $\sum a_n$ that is conditionally convergent (i.e. converges but not absolutely convergent), the terms can be arranged so that the new series converges to an arbitrary real number, or diverges. 
    \end{theorem}

    \begin{lemma}[Properties of Expectation]
      Let $X$ and $Y$ be random variables with finite expectations. 
      \begin{enumerate}
        \item Monotonicity: If $X \leq Y$ (i.e. $X(\omega) \leq Y(\omega)$ for all $\omega \in \Omega$), then 
        \begin{equation}
          \mathbb{E}[X] \geq \mathbb{E}[Y]
        \end{equation}
        
        \item Non-Negativity: This is implied from the above if we set the lower bound to the constant random variable $0$. If $X \geq 0$, then 
        \begin{equation}
          \mathbb{E}[X] \geq 0
        \end{equation}
        
        \item Linearity: For all $a, b, c \in \mathbb{R}$, 
        \begin{equation}
          \mathbb{E}[a X + b Y + c] = a \mathbb{E}[X] + b \mathbb{E}[Y] + c
        \end{equation}
      \end{enumerate}
    \end{lemma}

    We now show a widely-used, but nontrivial, theorem. 

    \begin{theorem}[Expectation of Independent Events]
      Given independent RVs $X$ and $Y$, 
      \begin{equation}
        \mathbb{E}[X Y] = \mathbb{E}[X] \, \mathbb{E}[Y]
      \end{equation}
    \end{theorem}
    \begin{proof}
      We show only for simple random variables which will give us a start in proving for all random variables in full generality. Let $X$ and $Y$ be simple random variables, i.e. 
      \begin{equation}
        X = \sum_i a_i 1_{A_i} \text{ and } Y = \sum_j b_j 1_{B_j}
      \end{equation}
      Since $\{A_i\}_i$ and $\{B_j\}_j$ are both partitions, $\{A_i \cap B_j\}_{i, j}$ is also a partition, and 
      \begin{equation}
        X Y = \sum_{i, j} a_i b_j \, 1_{A_i \cap B_j}
      \end{equation}
      Its expectation can be expanded out by linearity, and since $\mathbb{E}[ 1_{A} ] = \mathbb{P}(A)$, we have
      \begin{align*}
        \mathbb{E}[X Y] & = \sum_{i, j} a_i b_j \, \mathbb{P}(A_i \cap B_j) \\
        & = \sum_{i, j} a_i b_j \, \mathbb{P}(A_i)\, \mathbb{P}(B_j) = \mathbb{E}[X] \, \mathbb{E}[Y]
      \end{align*}
      Now that we have proved for simple random variables, we can just approximate $X$ from below using simple functions. 
    \end{proof}

    \begin{theorem}[Tail Sum Formula]
      If a discrete random variable $X$ takes values in the non-negative integers $\{0, 1, 2, 3, ...\}$, then 
      \begin{equation}
        \mathbb{E}[X] = \sum_{k=1}^\infty \mathbb{P}(X \geq k)
      \end{equation}
      In any case (continuous or discrete), if $X$ is a non-negative random variable, then 
      \begin{equation}
        \mathbb{E}[X] = \int_0^\infty \mathbb{P}(X > x) \, dx = \int_0^\infty 1 - F(x) \, dx
      \end{equation}
      where $F$ is the CDF of $X$. 
    \end{theorem}
    \begin{proof}
      Suppose that $X$ takes values in $\{0, 1, 2, 3, ...\}$. Then, 
      \begin{align*}
        \mathbb{E}[X] & = \sum_{k \geq 1} k \, \mathbb{P}(X=k) \\
        & = \sum_{k\geq 1} \sum_{j=1}^k \mathbb{P}(X = k) \\
        & = \sum_{k \geq 1} \sum_{j=1}^k 1_{j \leq k} \, \mathbb{P}(X=k) \\
        & = \sum_{j=1}^\infty \sum_{k \geq 1} 1_{j \leq k} \, \mathbb{P}(X =k) \\
        & = \sum_{j=1}^\infty \sum_{k \geq j} \mathbb{P}(X=k) \\
        & = \sum_{j=1}^\infty \mathbb{P}(X \geq j)
      \end{align*}
    \end{proof}

    \begin{corollary}
      For any $m > 0$ and $\alpha > 0$,  
      \begin{equation}
        \mathbb{P} \big(|X| > \alpha \big) \leq \frac{1}{\alpha^m} \mathbb{E} \big( |X|^m \big)
      \end{equation}
    \end{corollary}

    \begin{example}[Geometric RV]
      Recall that given $X \sim \mathrm{Geometric}(p)$, we have $\mathbb{P}(X = i) = (1 - p)^{i-1} p$ for $i \geq 1$. So, 
      \begin{equation}
        \mathbb{E}[X] = \sum_{x=1}^\infty x \, \mathbb{P}(X = x) = \sum_{x=1}^\infty x \, (1 - p)^{i-1} p = \frac{p}{(1 - (1 - p))^2} = \frac{1}{p}
      \end{equation}
    \end{example}

    \begin{example}[Infinite Expectation]
      Let us have discrete random variable s.t. $\mathbb{P}(X = k) = \frac{6}{\pi^2} \frac{1}{k^2}$ for $k \geq 1$. So, 
      \begin{equation}
        \mathbb{E}[X] = \sum_{k=1}^\infty k \, \mathbb{P}(X = k) = \frac{6}{\pi^2} \sum_{k=1}^\infty \frac{1}{k} = +\infty
      \end{equation}
    \end{example}

    \begin{example}[Undefined Expectation]
      Let $\mathbb{P}(X = k) = \frac{3}{\pi^2} \frac{1}{k^2}$ for $k \in \mathbb{Z}\setminus \{0\}$. The expectation of this can be computed by getting the expectation of all the positive terms and the negative terms. 
      \begin{equation}
        \mathbb{E}[X] = \mathbb{E}[X^+] - \mathbb{E}[X^{-}] = \sum_{k=1}^\infty k \cdot \frac{3}{\pi^2} \frac{1}{k^2} + \sum_{k=1}^\infty (-k) \cdot \frac{3}{\pi^2} \frac{1}{k^2} = \infty - \infty
      \end{equation}
      Note that by the Riemann rearrangement theorem, we can't just say that the expectation is $0$ since the terms "cancel out." We could only do this if the series is absolutely convergent also, which works if $X$ takes positive values only. 
    \end{example}

    Note that when we compute expectation, what we do it multiply the PMF/PDF by $x$ and sum/integrate over it. The Cauchy distribution is a power function of form $\frac{1}{x^2}$, so if we multiply it by $x$, we have the new $\frac{1}{x}$ which is divergent. 

    \subsubsection{Law of the Unconscious Statistician}

      Given probability space $(\Omega, \mathcal{F}, \mathbb{P})$ and a random vector $X: \Omega \rightarrow \mathbb{R}^n$, this induces a probability law $\mathbb{P}_X$ acting as a measure on $\mathbb{R}^n$. Assume that this probability law $\mathbb{P}_X$ is known. Now introduce a function $g: \mathbb{R}^n \rightarrow \mathbb{R}$. We can create a new random variable $Y = g \circ X : \Omega \rightarrow \mathbb{R}$ with its own probability law $\mathbb{P}_Y$ on $\mathbb{R}$. Since we already know the probability distribution of $X$, so we can easily get the expected value of $X$ as (in the discrete case) 
      \begin{equation}
        \mathbb{E}[X] = \sum_{x \in \mathcal{X}} x \cdot \mathbb{P}(X = x)
      \end{equation}
      where $\mathcal{X}$ is the support of $X$. But what if we wanted to get the expected value of $Y$? 
      \begin{equation}
        \mathbb{E}[Y] = \sum_{y \in \mathcal{Y}} y \cdot \mathbb{P}(Y = y) = ?
      \end{equation}
      The problem is that we don't know the probability distribution of $Y$. But since we know that all the values of $X$ are transformed by $g$, we are taught to compute it in terms of the probability distribution of $X$. 
      \begin{equation}
        \mathbb{E}[Y] = \sum_{x \in \mathcal{X}} g(x) \cdot \mathbb{P}(X = x)
      \end{equation}
      This "identity" that is often used must actually be treated as a rigorous theorem. This is like a change of basis formula that allows us to shift to a convenient space to compute integrals. 

      \begin{theorem}[LOTUS]
        Given probability space $(\Omega, \mathcal{F}, \mathbb{P})$, a random variable $X: \Omega \rightarrow \mathbb{R}^n$, and a function $g: \mathbb{R}^n \rightarrow \mathbb{R}$, the expectation of $g(X)$ is 
        \begin{equation}
          \mathbb{E}[g(X)] = \int_\Omega g(X) \,d\mathbb{P} = \int_{\mathbb{R}^n} g \, d\mathbb{P}_X = \int_\mathbb{R} \,d \mathbb{P}_{g(X)}
        \end{equation}
        It is usually the case that we don't know the distribution of $g(X)$ since $g$ is too complicated (hard to compute the right integral) and we don't want to integrate over an abstract space $\Omega$ where we can't do calculus on (hard to compute the left integral). But we do know the distribution of $X$, so we can indeed compute the middle integral. 
      \end{theorem}

      Note that if $g: \mathbb{R} \rightarrow \mathbb{R}$ is the identity function $\mathrm{id}$, then we have 
      \begin{equation}
        \mathbb{E}[X] = \int_\Omega X \,d\mathbb{P} = \int_{\mathbb{R}} \mathrm{id} \, d\mathbb{P}_X
      \end{equation}
      \begin{enumerate}
        \item For the discrete case, the above integral simplifies to 
        \begin{equation}
          \mathbb{E}[g(X)] = \int_{\mathbb{R}^n} g \,d \mathbb{P}_X = \sum_{x \in \mathcal{X} \subset \mathbb{R}^n} g(x) p_X (x)
        \end{equation}
        
        \item For the continuous case, we have 
        \begin{equation}
          \mathbb{E}[g(X)] = \int_{\mathbb{R}^n} g \,d \mathbb{P}_X = \int_{\mathbb{R}^n} g (x) \, f_X (x) \,dx
        \end{equation}
      \end{enumerate}

    \subsubsection{Expectation w.r.t. Different Measures}

      Sometimes, we just write the expectation of a measurable function $f: (S, \mathcal{S}) \rightarrow \mathbb{R}$ as $\mathbb{E}[f]$. If we need to specify with respect to what measure we are integrating over, we write 
      \begin{equation}
        \mathbb{E}_\mu [f] \coloneqq \int_S f \, d\mu
      \end{equation}
      Usually, if $f$ represents some transformation of a random variable $X: \Omega \rightarrow S$, then we assume that we are integrating w.r.t. the probability measure $\mathbb{P}$ defined on $\Omega$ or the probability law $\mathbb{P}_X$ induced by $X$. 
      \begin{equation}
        \mathbb{E}[f] = \mathbb{E}[f(X)] = \int_\Omega f(X) \,d\mathbb{P} = \int_S f \,d\mathbb{P}_X
      \end{equation}

      \begin{example}[Expectation of Exponential RV]
      The PDF of exponential random variable $X$ is defined $f_X = k e^{-k x}$ for $x \geq 0$. So, 
      \begin{equation}
        \mathbb{E}[X] = \int_\mathbb{R} x f_X \, d\lambda = \int_0^\infty x k e^{-k x} \, dx = \frac{1}{k}
      \end{equation}
      Similarly, if we want the expectation of $X^2$, then we can get 
      \begin{equation}
        \mathbb{E}[X^2] = \int_\mathbb{R} x^2 f_X \,d\lambda = \int_0^\infty x^2 k e^{-k x} \,dx = \frac{2}{k^2}
      \end{equation}
      \end{example} 

      \begin{example}[Expectation of Gaussian RV]
        The expectation of a Gaussian random variable $X$ is
        \begin{equation}
          \mathbb{E}[X] = \int_{-\infty}^\infty x \cdot \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{(x - \mu)^2}{2 \sigma^2}} \, dx = \mu
        \end{equation}
      \end{example}

      \begin{example}[Expectation of One-Sided Cauchy]
        If we have $f_X (x) = \frac{2}{\pi} \frac{1}{1 + x^2}$ for $x \geq 0$, then 
        \begin{equation}
          \mathbb{E}[X] = \int_0^\infty \frac{2}{\pi} \frac{x}{1 + x^2} \,dx
        \end{equation}
        and making the substitution $t = \frac{1 + x^2}, \; dt = 2x$, we have 
        \begin{equation}
          \int_1^\infty \frac{1}{\pi} \frac{1}{t} \,dt = \frac{\ln(t)}{\pi} \bigg|_1^\infty = +\infty
        \end{equation}
      \end{example}

      \begin{example}[Expectation of Two-Sided Cauchy]
        The two-sided Cauchy is just another copy of the one sided into the negatives, so $f_X (x) = \frac{1}{\pi} \frac{1}{1 + x^2}$ for $x \in \mathbb{R}$. The expectation of $X$ should be split up into for positive and negative images, but computing it gives 
        \begin{equation}
          \mathbb{E}[X] = \mathbb{E}[X^+] - \mathbb{E}[X^-] = \int_0^\infty \frac{1}{\pi} \frac{x}{1 + x^2}\,dx - \int_{-\infty}^0 \frac{1}{\pi} \frac{x}{1 + x^2}\,dx = \infty - \infty
        \end{equation}
        and so it is undefined. 
      \end{example}

      With LOTUS, we can make sense of an extremely important inequality. 

      \begin{theorem}[Jensen's Inequality]
        If $f$ is a convex function, then $\mathbb{E}[f(X)] \geq f (\mathbb{E}[X])$. 
      \end{theorem}
      \begin{proof}
        We will assume that $f$ is differentiable for simplicity and let $\mathbb{E}[X] = \mu$. Define the linear function centered at $\mu$ to be $l(x) \coloneqq f(\mu) + f^\prime (\mu) (x - \mu)$. Then, we know that $f(x) \geq l(x)$ for all $x$, so 
        \begin{align*}
          \mathbb{E}[f(X)] & \geq \mathbb{E}[ l(X)] \\ 
          & = \mathbb{E}[f(\mu) + f^\prime (\mu) \, (X - \mu)] \\
          & = \mathbb{E}[f(\mu)] + f^\prime (\mu) ( \mathbb{E}[X] - \mu) \\
          & = \mathbb{E}[f(\mu)] \\
          & = f(\mathbb{E}[X])
        \end{align*}
      \end{proof}

      A nice way to visualize which side is greater (which I tend to always forget) is to think about a Bernoulli($p$) distribution. $f(\mathbb{E}[X])$ is visualized to be lower than the region in which the $\mathbb{E}[f(X)]$ must lie. 

      \begin{figure}[H]
        \centering 
        \includegraphics[scale=0.4]{img/jensen_visual.png}
        \caption{The function $f$ essentially transforms the Bernoulli defined on $0, 1$ to the Bernoulli defined on $f(0), f(1)$. Therefore, $\mathbb{E}[f(X)] \in [f(0), f(1)]$, which lies completely over $f(\mathbb{E}[X])$.} 
        \label{fig:jensen_visual}
      \end{figure}

  \subsection{Variance, Covariance, Correlation}

    \begin{definition}[Variance]
      Let $X$ be a random variable and suppose $\mathbb{E}[X] < \infty$. The \textbf{variance} of $X$ is defined 
      \begin{equation}
        \mathrm{Var}[X] = \sigma^2_X \coloneqq \mathbb{E} [ (X - \mathbb{E}[X])^2 ]
      \end{equation}
      and $\sigma_X = \sqrt{\mathrm{Var}[X]}$ is called the \textbf{standard deviation}. This is a measure of how much the probability distribution deviates from its mean. We can use linearity of expectation to write 
      \begin{align*}
        \mathrm{Var}[X] & = \mathbb{E} \big[ X^2 + \mathbb{E}[X]^2 - 2 X \mathbb{E}[X] \big] \\
        & = \mathbb{E}[X^2] + \mathbb{E}[X]^2 - 2 \mathbb{E}[X] \mathbb{E}[X] \\
        & = \mathbb{E}[X^2] - \mathbb{E}[X]^2
      \end{align*}
      which is often easier to compute, since it only requires us to compute the expectation of $X$ and $X^2$. Since variance is always nonnegative, we also know that $\mathbb{E}[X^2] \geq \mathbb{E}[X]^2$. The variance is always defined, whether it's finite or $+\infty$. 
    \end{definition}

    Likewise for expectation, the variance of a function $f$ w.r.t. $\mu$ is 
    \begin{equation}
      \Var_\mu (f) = \mathbb{E}_\mu [f^2] - \mathbb{E}_\mu [f]^2
    \end{equation}

    \begin{proposition}
      The variance of a random variable $X$ is $0$ if and only if it constant almost everywhere on $\Omega$. 
    \end{proposition}
    \begin{proof}
      The if part is easy, so let's prove the only if part. Let $\mathbb{E} [ (X - \mathbb{E}[X])^2 ] = 0$. Then, we can think of the function $x \mapsto (x - \mathbb{E}[X])^2$ and write the variance as 
      \begin{equation}
        \mathrm{Var}[X] = \int_\Omega (X - \mathbb{E}[X])^2 \, d\mathbb{P} = 0
      \end{equation}
      But by nonnegativity of the function, we know that $(X - \mathbb{E}[X])^2 = 0$ w/ probability $1$, which implies that $X = \mathbb{E}[X]$ with prob. $1$. 
    \end{proof}

    \begin{lemma}[Properties of Variance]
      Let $X$ and $Y$ be random variables with well-defined variances. 
      \begin{enumerate}
        \item Translation Invariance: Given that $X + a$ is a new random variable defined $(X + a)(\omega) = X(\omega) + a$, 
        \begin{equation}
          \mathrm{Var}[X] = \mathrm{Var}[X + a]
        \end{equation}
        \item Quadratic Scaling: Given that $aX$ is a new random variable defined $(aX)(\omega) = a\,X(\omega)$, 
        \begin{equation}
          \mathrm{Var}[aX] = a^2 \mathrm{Var}[X]
        \end{equation}
      \end{enumerate}
    \end{lemma}

    From the properties of expectation and variance, we can now \textbf{standardize} a random variable $X$. If $X$ is a random variable with mean $\mu = \mathbb{E}[X]$ and variance $\sigma^2 = \Var(X)$, then the random variable 
    \begin{equation}
      Y = \frac{X - \mu}{\sigma}
    \end{equation}
    has mean $\mathbb{E}(Y) = 0$ and variance $\Var(Y) = 1$. 

    \begin{example}[Bernoulli]
      Given $X \sim \mathrm{Bernoulli}(p)$, we have
      \begin{align*}
        \mathbb{E}[X] & = 0 \cdot \mathbb{P}(X = 0) + 1 \cdot \mathbb{P}(X = 1) = p \\
        \mathbb{E}[X^2] & =  0^2 \cdot \mathbb{P}(X = 0) + 1^2 \cdot \mathbb{P}(X = 1) = p
      \end{align*}
      and so $\mathrm{Var}[X] = p - p^2 = p(1 - p)$. 
    \end{example}

    \begin{example}[Poisson]
      Given $X \sim \mathrm{Poisson}(X)$, then 
      \begin{align*}
        \mathbb{E}[X] & = \sum_{k = 0}^\infty k \cdot \frac{e^{-\lambda} \lambda^k}{k!} = \sum_{k = 1}^\infty \cdot \frac{e^{-\lambda} \lambda^k}{(k-1)!} = \lambda \sum_{k = 1}^\infty \cdot \frac{e^{-\lambda} \lambda^{k-1}}{(k-1)!} = \lambda\\
        \mathbb{E}[X] & = \sum_{k=0}^\infty k^2 \cdot \frac{e^{-\lambda} \lambda^k}{k!} = \ldots = \lambda^2 + \lambda
      \end{align*}
      So $\mathrm{Var}[X] = \lambda^2 + \lambda - \lambda^2 = \lambda$. 
    \end{example}

    \begin{example}[Uniform]
      Let $X \sim \mathrm{Uniform}[a, b]$. Then, 
      \begin{align*}
        \mathbb{E}[X] & = \int_\mathbb{R} x f_X \, d\lambda = \int_a^b x \cdot \frac{1}{b - a}\,dx = \frac{a + b}{2} \\
        \mathbb{E}[X^2] & = \int_\mathbb{R} x^2 f_X \,d\lambda = \int_a^b \frac{x^2}{b - a} \,dx = \frac{a^2 + ab + b^2}{3} 
      \end{align*}
      So $\mathrm{Var}[X] = \ldots = \frac{1}{12} (b - a)^2$. This is consistent with the fact that if we spread out our measure over a wider interval, then the variance will be bigger. 
    \end{example}

    \begin{example}[Exponential]
      Let $X \sim \mathrm{Exp}(\lambda)$. Then, $\mathbb{E}[X] = \frac{1}{\lambda}$ and $\mathbb{E}[X^2] = \frac{2}{\lambda^2}$, so 
      \begin{equation}
        \mathrm{Var}[X] = \frac{1}{\lambda^2}
      \end{equation}
      This is consistent with the fact that if $\lambda$ is greater, then the PDF is much more concentrated at $0$, making the variance small. 
    \end{example}

    Just like how we explained that computing finiteness or infiniteness of expectation is similar to multiplying the PMF/PDF by $x$ and determining if the series/integral converges or diverges, we can do the same for variance by multiplying the PMF/PDF by $x^2$. For a probability distribution of form $\frac{1}{x^2}$, it diverges if we multiply by $x$ and also diverges if we multiply by $x^2$. But also, we could construct a distribution where the expectation may be finite, but the variance may be infinite. For example, if we have a distribution of form $\frac{1}{x^3}$, multiplying it by $x$ leads to form $\frac{1}{x^2}$, which is finite (so finite expectation), but multiplying by $x^2$ leads to a harmonic, i.e. infinite variance. 

    \begin{definition}[Moment]
      The \textbf{$\mathbf{n}$th (raw) moment} of a random variable $X$ is $\mathbb{E}[X^n]$. Unlike the raw moment, which is calculated around the origin, the \textbf{$\mathbf{n}$th central moment} of $X$ is its moment centered around its mean $\mathbb{E}[(X - \mathbb{E}[X])^n]$. 
      \begin{enumerate}
        \item the first moment is the mean $\mathbb{E}[X]$
        \item the second central moment is the variance $\mathbb{E}[(X - \mathbb{E}[X])^2]$ 
        \item the third central moment, divided by $\sigma^3$, is the skew $\frac{1}{\sigma^3} \mathbb{E}[(X - \mathbb{E}[X])^3]$ 
      \end{enumerate}
    \end{definition}

    The variance is a measure for one random variable $X$, which measures how much it deviates from its mean. Now, the covariance is defined for two random variables and captures how they jointly vary. 

    \begin{definition}[Covariance]
      The \textbf{covariance} of random variables $X$ and $Y$ is defined as 
      \begin{align*}
        \mathrm{Cov}[X, Y] & = \mathbb{E} \big[ (X - \mathbb{E}[X]) (Y - \mathbb{E}[X]) \big] \\
        & = \mathbb{E}[X Y] - \mathbb{E}[X] \, \mathbb{E}[Y]
      \end{align*}
      where the intermediate expectations are well-defined. $X$ and $Y$ are said to be \textbf{uncorrelated} if 
      \begin{equation}
        \mathrm{Cov}[X, Y] = 0
      \end{equation}
    \end{definition}

    The covariance is also easy to interpret. Given two random variables $X$ and $Y$, if whenever $X$ is greater than its expected value $\mathbb{E}[X]$, $Y$ also tends to be greater than $\mathbb{E}[Y]$, then the covariance will be some positive number. If they tend to be on opposite sides of their expected values, then the covariance will be negative. And the degree with which these RVs lie on which side of the expected value determines the magnitude of the covariance. 

    \begin{theorem}
      If $X$ and $Y$ are independent random variables, then they are uncorrelated, meaning that independence is a stronger condition. 
    \end{theorem}

    We show an example of why the converse is not true. Consider $X \sim \mathrm{Uniform}[-1, 1]$. We can show that $x$ and $Y = X^2$ are dependent but uncorrelated. It is clearly dependent, but its covariance is 
    \begin{align*}
      \mathrm{Cov}(X, Y) & = \mathbb{E}[X Y] - \mathbb{E}[X] \, \mathbb{E}[Y] \\
      & = \mathbb{E}[X^3] - \mathbb{E}[X] \, \mathbb{E}[X^2] \\
      & = \int_{-1}^1 x^3 \cdot 1 \,dx - 0 \cdot \mathbb{E}[X^2] = 0
    \end{align*}

    \begin{theorem}[Variance of Sums of Random Variables]
      If $X$ and $Y$ are two random variables, then 
      \begin{equation}
        \mathrm{Var}(X + Y) = \mathrm{Var}[X] + \mathrm{Var}(Y) + 2 \mathrm{Cov}(X, Y)
      \end{equation}
      and by induction, we can show that 
      \begin{equation}
        \mathrm{Var}\bigg( \sum_i X_i\bigg) = \sum_{i} \mathrm{Var}(X_i) + \sum_{i, j} \mathrm{Cov}(X_i, X_j)
      \end{equation}
    \end{theorem}
    \begin{proof}
      Simple computation. The LHS expands to 
      \begin{align*}
        \mathbb{E}[(X + Y)^2] - \mathbb{E}[X + Y]^2 & = \mathbb{E}[X^2 + 2XY + Y^2] - (\mathbb{E}[X] + \mathbb{E}[Y])^2 \\
        & = \mathbb{E}[X^2] + 2 \mathbb{E}[XY] + \mathbb{E}[Y^2] - \mathbb{E}[X]^2 - 2 \mathbb{E}[X] \mathbb{E}[Y] - \mathbb{E}[Y]^2 \\
        & = \big( \mathbb{E}[X^2] - \mathbb{E}[X]^2 \big) + \big( \mathbb{E}[Y^2] - \mathbb{E}[Y]^2 \big) + 2 \big( \mathbb{E}[XY] - \mathbb{E}[X] \mathbb{E}[Y] \big) \\
        & = \mathrm{Var}[X] + \mathrm{Var}(Y) + 2 \mathrm{Cov}(X, Y) 
      \end{align*}
    \end{proof}

    Therefore, if we have $n$ random variables $X_1, \ldots, X_n$, then we can compute their pairwise covariance $\mathrm{Cov}(X_i, X_j)$ and compute their \textbf{covariance matrix} $\boldsymbol{\Sigma}$, which is an $n \times n$ symmetric matrix with entries 
    \begin{equation}
      \boldsymbol{\Sigma}_{ij} = \mathrm{Cov}(X_i, X_j) \text{ for } i, j = 1, \ldots, n
    \end{equation}

    \begin{theorem}[Simple Bound on Covariance]
      If $X$ and $Y$ are two random variables with finite variance, then the magnitude of their covariance is bounded by the following inequality. 
      \begin{equation}
        |\Cov(X,Y)| \leq \sqrt{\Var(X) \, \Var(Y)} = \std(X) \, \std(Y)
      \end{equation}
    \end{theorem}

    Finally we define the correlation. 

    \begin{definition}[Correlation Coefficient]
      The \textbf{correlation coefficient} of random variables $X$ and $Y$ is defined 
      \begin{equation}
        \rho_{X, Y} = \mathrm{Corr}(X, Y) \coloneqq \frac{\mathrm{Cov}(X, Y)}{\sigma_X \sigma_Y} = \frac{\mathrm{Cov}(X, Y)}{\sqrt{\mathrm{Var}[X] \, \mathrm{Var}(Y)}}
      \end{equation}
    \end{definition}

    By definition, this implies that $-1 \leq \Corr(X, Y) \leq 1$. When $\Corr(X, Y) > 0$ (which also means that $\Cov(X, Y) > 0$), it is said that $X$ and $Y$ are \textit{positively correlated}, and when $\Corr(X, Y) < 0$ (which also means that $\Cov(X, Y) < 0$), it is said that they are \textit{negatively correlated}. 

    \begin{theorem}
      $\Corr(X,Y) = \pm 1$ indicates a linear relationship between $X$ and $Y$. 
      \begin{enumerate}
        \item Let $\Corr(X, Y) = 1$. Then, there exists a $m>0$ and $b \in \mathbb{R}$ such that $Y = m X + b$. 
        \item Let $\Corr(X, Y) = -1$. Then, there exists a $m<0$ and $b \in \mathbb{R}$ such tat $Y = m X + b$. 
      \end{enumerate}
      This implies that $\Corr(X, Y) = \pm 1$ indicates that the joint distribution of $(X, Y)$ is concentrated on a line in $\mathbb{R}^2$. 
    \end{theorem}

    \subsubsection{Hilbert Space of Random Variables}

      In some sense the correlation is a scaled version of the covariance. It is scale-invariant, and it is always a number that lies between $-1$ and $1$, making it a nice way to represent the correlation between two variables without having to worry about scale. We can prove this. 

      \begin{theorem}[Cauchy-Schwartz]
        For any two random variables $X, Y$, we have $|\mathrm{Cov}(X, Y)| \leq \sigma_X \sigma_Y$, or in other words, 
        \begin{equation}
          -1 \leq \rho_{X, Y} \leq 1
        \end{equation}
        Furthermore, whenever $\rho_{X, Y} = 1$ or $-1$, there exists a deterministic relationship between $X$ and $Y$. 
        \begin{enumerate}
          \item If $\rho_{X, Y} = 1$, there exists a $a > 0$ s.t. 
          \begin{equation}
            Y - \mathbb{E}[Y] = a (X - \mathbb{E}[X])
          \end{equation}
          \item If $\rho_{X, Y} = -1$ there exists a $a < 0$ s.t. 
          \begin{equation}
            Y - \mathbb{E}[Y] = a (X - \mathbb{E}[X])
          \end{equation}
        \end{enumerate}
        This implies that $\Corr(X, Y) = \pm 1$ indicates that the joint distribution of $(X, Y)$ is concentrated on a line in $\mathbb{R}^2$. 
      \end{theorem}

      The fact that this is called the Cauchy-Schwartz inequality hints at the existence of inner products, norms, and vector spaces. That is, we can treat the random variables $X, Y$ as vectors in the functional space of real-valued maps over $\Omega$. In some sense, $\mathrm{Cov}(X, Y)$ sort-of plays the role of an inner product. 
      \begin{enumerate}
        \item It satisfies symmetricity: 
        \begin{equation}
          \mathrm{Cov}(X, Y) = \mathbb{E}[X Y] - \mathbb{E}[X] \, \mathbb{E}[Y] =  \mathbb{E}[Y X] - \mathbb{E}[Y] \, \mathbb{E}[X] = \mathrm{Cov}(Y, X)
        \end{equation}
        
        \item It satisfies binlinearity. It suffices to show only for first argument, since we have symmetricity. 
        \begin{align*}
          \mathrm{Cov}(aX + bY, Z) & = \mathbb{E}[(a X + b Y) Z] - \mathbb{E}[a X + b Y] \, \mathbb{E}[Z] \\
          & = a \mathbb{E}[X Z] + b \mathbb{E}[Y Z] - a \mathbb{E}[X] \mathbb{E}[Z] - b \mathbb{E}[Y] \, \mathbb{E}[Z] \\
          & = a \big( \mathbb{E}[X Z] - \mathbb{E}[X] \mathbb{E}[Z] \big) + a \big( \mathbb{E}[Y Z] - \mathbb{E}[Y] \, \mathbb{E}[Z] \big) \\
          & = a \, \mathrm{Cov}(X, Z) + b \, \mathrm{Cov}(Y, Z)
        \end{align*}

        \item We want the inner product of $X$ with itself to always be greater than $0$, with equality holding iff $X = 0$. Indeed, we have 
        \begin{equation}
          \mathrm{Cov}(X, X) = \mathrm{Var}[X] \geq 0
        \end{equation}
        but it is not necessarily true that $\mathrm{Var}[X] = 0 \implies X = 0$. We can say that $X$ is equal to a constant almost everywhere at best. We can solve this problem by looking at the functional subspace of $0$-mean random variables (which is a vector space due to linearity of expectation). So now all random variables $X$ that are $0$ almost everywhere have inner product $0$, so we must add an equivalence class on this subspace that says two $X, Y$ are equivalent if they agree almost everywhere. 
      \end{enumerate}

      The standard deviation $\sigma_X$ and $\sigma_Y$ act as norms on this quotient subspace of $0$-mean random variables. So the correlation coefficient $\rho_{X, Y}$ can be interpreted as the cosine of the angle between $X$ and $Y$. This now makes our desired space a Hilbert space, and our uncorrelated random variables are like orthogonal vectors. 

      \begin{definition}
        Let $L^2_\mathcal{F} (\Omega)$ be the function space consisting of equivalence classes of $0$-mean random variables $X: (\Omega, \mathcal{F}) \rightarrow \mathbb{R}$ that are almost surely equal. Then, 
        \begin{enumerate}
          \item we can define the inner product on this space as 
          \begin{equation}
            \langle X, Y \rangle \coloneqq \mathbb{E}[X Y] - \mathbb{E}[X] \mathbb{E}[Y] = \mathbb{E}[X Y] = \int_\Omega X Y \,d\mathbb{P}
          \end{equation}

          \item which induces the $L^2$-norm on this space defined 
          \begin{equation}
            ||X||_2 \coloneqq \mathrm{Var}(X) = \mathbb{E}[X^2] - \mathbb{E}[X]^2 = \mathbb{E}[X^2] = \int_\Omega X^2 \,d\mathbb{P}
          \end{equation}
        \end{enumerate}
        We set $L^2_\mathcal{F} (\Omega)$ to be a Banach space with bounded norm $\mathbb{E}[X^2] < \infty$. 
      \end{definition}

\section{Convergence}

  \subsection{Borel-Cantelli Lemmas}

    There are many Borel-Cantelli lemmas, and we will introduce the two most famous ones. To understand what these lemmas say, given a sequence $A_1, A_2, \ldots$ of events in $\sigma$-algebra $\mathcal{F}$, we must first understand what the daunting term  
    \begin{equation}
      \bigcap_{n=1}^\infty \bigcup_{i = n}^\infty A_i
    \end{equation}
    means. Now let's try to explain what the intersection of the unions mean. First, remember that $\sigma$-algebras are stable under both countable unions and countable intersections, this is also in $\mathcal{F}$. We can interpret 
    \begin{equation}
      \bigcap_{n=1}^\infty \bigcup_{i=n}^\infty A_i = \{ A_n \text{ i.o.}\}
    \end{equation}
    as the \textit{event that infinitely many $A_n$'s occur}, where i.o. means "infinitely often." To parse this, let's start from the innermost term and call it 
    \begin{equation}
      B_n = \bigcup_{i=n}^\infty A_i \implies \{A_n \text{ i.o.}\} = \bigcap_{n=1}^\infty B_n
    \end{equation}
    $B_n$ is the event that at least one of the $A_n, A_{n+1}, A_{n+2}, \ldots$ occurs, often referred to as the \textit{$n$th tail event}. Now the intersection of all $B_n$'s is the event that \textit{all} $B_n$'s occur. In other words, this is the event that for no matter how big of an $N \in \mathbb{N}$ I choose, there is always at least an event $A_n$ with $n > N$ that occurs. This is shortly summarized as the event that infinitely many $A_n$'s occur. 

    \begin{lemma}[1st Borel-Cantelli Lemma]
      Given probability space $(\Omega, \mathcal{F}, \mathbb{P})$, if $A_1, A_2, \ldots$ is a sequence of events such that 
      \begin{equation}
        \sum_{n=1}^\infty \mathbb{P}(A_n) < \infty
      \end{equation}
      the almost surely (with probability $1$) only finitely many $A_n$'s will occur. 
      \begin{equation}
        \mathbb{P} \bigg( \bigcap_{n=1}^\infty \bigcup_{i = n}^\infty A_i \bigg) = 0
      \end{equation}
    \end{lemma}
    \begin{proof}
      Setting $B_n$ as above, we have 
      \begin{align*}
        \mathbb{P}\bigg( \bigcap_{n=1}^\infty B_n \bigg) & = \lim_{n \rightarrow \infty} \mathbb{P}(B_n) & (\text{continuity of probability}) \\
        & = \lim_{n \rightarrow \infty} \mathbb{P} \bigg( \bigcup_{i=1}^\infty A_i \bigg) & (\text{substitute } B_i) \\
        & \leq \lim_{n \rightarrow \infty} \sum_{i = n}^\infty \mathbb{P}(A_i) = 0 & (\text{tail sum of convergent series is } 0)
      \end{align*}
    \end{proof}

    The second Borel-Cantelli lemma is like a partial contrapositive to the first lemma, where it starts with the assumption that the sum of the $\mathbb{P}(A_n)$'s are infinite (along with the addition case that they are independent). 

    \begin{lemma}[2nd Borel-Cantelli Lemma]
      If $A_1, A_2, \ldots$ are independent events such that 
      \begin{equation}
        \sum_{n=1}^\infty \mathbb{P}(A_n) = \infty,
      \end{equation}
      then almost surely (with probability $1$) infinitely many $A_n$'s will occur. That is, 
      \begin{equation}
        \mathbb{P} \bigg( \bigcap_{n=1}^\infty \bigcup_{i = n}^\infty A_i \bigg) = 1
      \end{equation}
    \end{lemma}

    The intuition behind this lemma is challenging: We can let $\mathbb{P}(A_n) = P_n$ and interpret the sum as a series of $P_n$'s. Since the series $P_1 + P_2 + \ldots$ is finite, this implies that 
    \begin{equation}
      \lim_{n \rightarrow \infty} P_n = 0
    \end{equation}
    (but not the converse) and going to zero rather fast such that the series is finite. So, you are working with a sequence of events $A_n$ that are becoming more and more unlikely rather fast. The lemma says that beyond a certain point $n_0$, none of the events $A_n$ will occur almost surely. For the second lemma, we can go as far as we like in the sequence of $A_n$'s, up to any $A_{n_0}$, but beyond that there is always an infinite number of $A_n$'s that occur beyond $A_{n_0}$. 

  \subsection{Transforms}

    \subsubsection{Probability Generating Function (PGF)}

      The PGF is only defined for discrete random variable, and is analogous to the Z-transform in singal processing. 

      \begin{definition}[Probability Generating Function]
        Let $X$ be a discrete random variable taking values in $\mathbb{N}_0$. Then, the \textbf{probability generating function} of $X$ is defined 
        \begin{equation}
          G_X (z) \coloneqq \mathbb{E}[z^X] = \sum_{i=0}^\infty z^i \, \mathbb{P}(X = i)
        \end{equation}
        Now there is the problem of convergence, but we will not pay attention to this technicality for now and just consider the PGF as a tool. 
      \end{definition}

      \begin{example}[PGF of Poisson]
        The random variable $X \sim \mathrm{Poisson}(\lambda)$ has pmf $\mathbb{P}(X = i) = \frac{e^{-\lambda} \lambda^i}{i!}$ for $i \in \{0, 1, \ldots\}$. Then, 
        \begin{equation}
          G_X (z) = \mathbb{E}[z^X] = \sum_{i=0}^\infty z^i \, \frac{e^{-\lambda} \lambda^i}{i!} = \sum_{i=0}^\infty \frac{e^{-\lambda} (\lambda z)^i}{i!} = e^{\lambda(z - 1)}
        \end{equation}
      \end{example}

      \begin{example}[PGF of Geometric]
        For $X \sim \mathrm{Geometric}(p)$, its PGF is 
        \begin{equation}
          G_X (z) = \sum_{i=1}^\infty z^i \, (1 - p)^i p = \frac{p z}{1 - z(1 - p)}
        \end{equation}
      \end{example}

      \begin{lemma}[Properties of PGF]
        Given random variable $X$ and its PGF $G_X$, we have the following: 
        \begin{enumerate}
          \item Evaluate at $z = 1$: 
          \begin{equation}
            G_X (1) = \mathbb{E}[1^X] = \mathbb{E}[1] = 1
          \end{equation}
          \item Derivative at $z = 1$: 
          \begin{equation}
            \frac{d G_X (z)}{d z} \bigg|_{z = 1} = \mathbb{E}[X]
          \end{equation}
          \item $k$th derivative at $z = 1$: 
          \begin{equation}
            \frac{d^k G_X (z)}{d z^k} \bigg|_{z = 1} = \mathbb{E}[X (X-1) (X-2) \ldots (X-k +1)]
          \end{equation}
          \item Transformation: Given the sum $Z = X + Y$ (where $X, Y$ are independent), rather than computing its convolution, the PGF of $Z$ is simply the product of the PGFs of $X$ and $Y$: 
          \begin{equation}
            G_Z (z) = G_X (z) \, G_Y (z)
          \end{equation}
          For example, since a $\mathrm{Poisson}(\lambda)$ random variable has PGF of form $e^{\lambda (z - 1)}$, if we have two Poissons $X$ and $Y$ with parameters $\lambda, \mu$, then we can easily multiply their PGFs to get the PGF of $Z = X + Y$, which is $e^{(\lambda + \mu)(z - 1)}$, which is the PGF of a $\mathrm{Poisson}(\lambda + \mu)$ random variable. 
        \end{enumerate}
      \end{lemma}

    \subsubsection{Moment Generating Function (MGF)}

      \begin{definition}[Moment Generating Function (MGF)]
        The \textbf{moment generating function} associated with a random variable $X$ is a function $M_X: \mathbb{R} \longrightarrow [0, \infty]$ defined 
        \begin{equation}
          M_X (s) \coloneqq \mathbb{E}[e^{s X}]
        \end{equation}
        It is like an exponential moment. The region of convergence of $M_X$ is the set $D_X = \{s \mid M_X (s) < \infty\}$. 
        and we always have $M_X (0) = 1$, so $0 \in D_X$ always. 
      \end{definition}

      \begin{lemma}[Properties of MGF]
        Let $X$ be a random variable with MGF $M_X (s)$. 
        \begin{enumerate}
          \item $M_X (0) = 1$, so $0$ is always in the region of convergence. 
          \item If $Y = a X + b$, then 
          \begin{equation}
            M_Y (s) = e^{b s} M_X (a s)
          \end{equation}
          \item If $X$ and $Y$ are independent and $Z = X + Y$, then 
          \begin{equation}
            M_Z (s) = M_X (s) \, M_Y (s)
          \end{equation}
        \end{enumerate}
      \end{lemma}
      \begin{proof}
        Listed. 
        \begin{enumerate}
          \item $M_X (0) = \mathbb{E}[e^{0 X}] = \mathbb{E}[1] = 1$. 
          \item We have 
          \begin{align*}
            M_Y (s) & = \mathbb{E} [e^{s(a X + b)}] \\
            & = \mathbb{E}[ e^{a s X } e^{b s}] \\
            & = \mathbb{E}[e^{(as) X}] \, \mathbb{E}[e^{b s}] \\
            & = e^{b s} M_X (a s)
          \end{align*}
          where the penultimate step was due to independence of constant RV with any other RVs. 
          \item We can see 
          \begin{equation}
            M_Z (s) = \mathbb{E}[ e^{s (X + Y)}] = \mathbb{E}[e^{s X} \, e^{s Y}] = \mathbb{E}[e^{s X}] \, \mathbb{E}[e^{s Y}] = M_X (s) \, M_Y (s)
          \end{equation}
          since $X, Y$ independent means that any function of $X$ and $Y$ are independent. 
        \end{enumerate}
      \end{proof}

      \begin{theorem}[Inversion Theorem]
        Suppose $M_X (s)$ is finite for all $s \in [-\epsilon, \epsilon]$ for some $\epsilon > 0$. Then, $M_X$ uniquely determines the CDF of $X$. This implies that if $X$ and $Y$ are random variables such that $M_X (s) = M_Y (s)$ for all $s \in [-\epsilon, \epsilon]$ for some $\epsilon > 0$, then $X$ and $Y$ have the same CDF. 
      \end{theorem}

      This theorem is useful for comparing random variables with the MGFs, but a limitation is that it is not always clear that the MGF is defined beyond $0$. Now, we explain why this is called a moment generating function. 

      \begin{theorem}[Moment Generating Property]
        Suppose $M_X (s) < \infty$ for $s \in [-\epsilon, \epsilon]$ with $\epsilon > 0$. Then, the derivatives at $s = 0$ generate the moments of $X$: 
        \begin{equation}
          \frac{d^m M_X (s)}{d s^m} \bigg|_{s = 0} = \mathbb{E}[X^m]
        \end{equation}
      \end{theorem}
      \begin{proof}
        A hand-wavy proof is that we can take the derivative and put it "in" the expectation. 
        \begin{equation}
          \frac{d}{ds} \mathbb{E}[e^{s X}] = \mathbb{E} \big[ \frac{d}{ds} e^{s X} \big] = \mathbb{E}[X e^{s X}]
        \end{equation}
        which evaluates to $\mathbb{E}[X]$ when $s = 0$. Differentiating $m$ times just gets $\mathbb{E}[X^m e^{s X}]$. However, this should be questioned, since the expectation is an integral and we are putting the derivative inside the integral. 
      \end{proof}

      \begin{example}[Exponential RV]
        The PDF of $X \sim \mathrm{Exponential}(\mu)$ is $f_X (x) = \mu e^{-\mu x}$ for $x \geq 0$. The MGF is 
        \begin{equation}
          M_X (s) \coloneqq \int_0^\infty \mu e^{-\mu x} e^{s x} \, dx = \begin{cases} 
          \frac{\mu}{\mu - s} & \text{ for } s < \mu \\
          \infty & \text{ for } s \geq \mu 
        \end{cases}
        \end{equation}
      \end{example}

      \begin{example}[Gaussian RV]
        The PDF of a standard Gaussian $X$ is $f_X (x) = \frac{1}{\sqrt{2\pi}} e^{-x^2 / 2}$ for $x \in \mathbb{R}$, and the MGF is 
        \begin{equation}
          M_X (s) = \int_{-\infty}^\infty \frac{1}{\sqrt{2 \pi}} e^{- x^2 / 2} e^{s x} \,dx = e^{s^2 / 2} \int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}} e^{-\frac{(x - s)^2}{2}}\,dx = e^{s^2 / 2}
        \end{equation}
        which is valid for all $s \in \mathbb{R}$. 
      \end{example}

      \begin{example}[Cauchy RV]
        If we have $f_X (x) = \frac{1}{\pi} \frac{1}{1 + x^2}$ for $x \in \mathbb{R}$, the MGF is 
        \begin{equation}
          M_X (s) = \int_{-\infty}^\infty \frac{e^{s x}}{\pi (1 + x^2)} \,dx = \begin{cases} 1 & \text{ if } s = 0 \\
          \infty & \text{ if } s > 0 \\ 
          \infty & \text{ if } s < 0 \end{cases}
        \end{equation}
        So the region of convergence is just $\{0\}$. It is infinity everywhere else since the exponential function grows exponentially as $x \rightarrow \pm \infty$. 
      \end{example}

      \begin{example}
        Given $X_1 \sim \mathrm{Exponential}(\lambda_1)$ and $X_2 \sim \mathrm{Exponential}(\lambda_2)$ are independent, the MGF of $Z = X_1 + X_2$ is 
        \begin{equation}
          M_Z (s) = M_X (s) \, M_Y (s) = \frac{\lambda_1 \lambda_2}{(\lambda_1 - s)(\lambda_2 - s)} \text{ for } s < \min\{\lambda_1, \lambda_2\}
        \end{equation}
        and we can perform our inverse transform on it. 
      \end{example}

    \subsubsection{Characteristic Function}

      We can see that the MGF has its limitations: for some random variables (like the Cauchy), its MGF was not defined at all beyond $\{0\}$. On the contrary, the characteristic function is always defined everywhere and is finite everywhere (in fact, is bounded by $1$, shown below). Also, it is a bit easier to invert (similar to how the Fourier transform is a bit easier to invert than the Laplace). 

      \begin{definition}[Characteristic Function]
        Given a random variable $X: \Omega \longrightarrow \mathbb{R}$, the \textbf{characteristic function} is defined to be 
        \begin{align*}
          \varphi_X (t) & = \mathbb{E}[ e^{i t X} ] \\
          & = \mathbb{E}[\cos{(t X)}] + i \mathbb{E}[ \sin{(t X)}]
        \end{align*}
      \end{definition}

      If $X$ admits a PDF, then the characteristic function is its Fourier transform with a small sign reversal. 
      \begin{equation}
        \varphi_X (t) = \int_\mathbb{R} e^{i t x} f_X (x)\,dx
      \end{equation}

      \begin{theorem}[Properties of CF]
        Let $X$ be a random variable with CF $\varphi_X (t)$. 
        \begin{enumerate}
          \item $\varphi_X (0) = 1$  and $|\varphi_X (t)| \leq 1$ for all $t \in \mathbb{R}$. 
          \item If $Y = a X + b$, then 
          \begin{equation}
            \varphi_Y (t) = e^{i b t} \varphi_X (a t)
          \end{equation}
          \item If $X$ and $Y$ are independent random variables and $Z = X + Y$, then 
          \begin{equation}
            \varphi_Z (t) = \varphi_X (t) \, \varphi_Y (t)
          \end{equation}
          \item $\varphi_X (t)$ is uniformly continuous on $\mathbb{R}$, i.e. for all $t \in \mathbb{R}$, there exists a $\phi(h) \downarrow 0$ as $h \downarrow 0$ such that 
          \begin{equation}
            |\varphi_X (t + h) - \varphi_X (t)| \leq \phi(h)
          \end{equation}
          \item $\varphi_X$ is a nonnegative-definite kernel, i.e. for any $n$ reals $t_1, \ldots, t_n$ and $n$ complex numbers $z_1, \ldots z_n$, we have 
          \begin{equation}
            \sum_{i, j} z_i \varphi_X (t_i - t_j) \, \bar{z}_j \geq 0
          \end{equation}
        \end{enumerate}
      \end{theorem}
      \begin{proof}
        Listed. 
        \begin{enumerate}
          \item We just set $\varphi_X (0) = \mathbb{E}[e^{i 0 X}] = \mathbb{E}[1] = 1$, and for continuous random variables, we can bound 
          \begin{align*}
            \big| \varphi_X (t) \big| & = \bigg| \int_{-\infty}^\infty e^{i t x} f_X (x) \,dx  \bigg| \\
            & \leq \int_{-\infty}^\infty \big| e^{i t x} f_X (x) \big| \,dx \\
            & \leq \int_{-\infty}^\infty \big| e^{i t x} \big| \cdot \big|f_X (x) \big| \,dx \\
            & = \int_{-\infty}^\infty f_X (x) \,dx = 1 
          \end{align*}
          
          \item We have 
          \begin{equation}
            \varphi_Y (t) = \mathbb{E}[ e^{i t (a X + b)}] = \mathbb{E}[ e^{i a t X} \, e^{i b t}] = \mathbb{E}[ e^{i (at) X}] \, \mathbb{E}[e^{i b t}] = e^{i b t} \varphi_X (a t)
          \end{equation}
          
          \item We have 
          \begin{equation}
            \varphi_Z (t) = \mathbb{E}[ e^{i t (X + Y)}] = \mathbb{E}[ e^{i t X} \, e^{i t Y}] = \mathbb{E}[e^{i t X}] \, \mathbb{E}[e^{i t Y}] = \varphi_X (t) \, \varphi_Y (t)
          \end{equation}
          
          \item We have 
          \begin{align*}
            | \varphi_X (t + h) - \varphi_X (t)| & = | \mathbb{E} [ e^{i tX} (e^{i h X} - 1)] | \\ 
            & \leq \mathbb{E}[ | e^{i tX} (e^{i h X} - 1)|] \\
            & \leq \mathbb{E}[ |e^{i h X} - 1|] \ldots
          \end{align*}
        \end{enumerate}
      \end{proof}

      Now this next theorem states the uniqueness of each characteristic function. It is a highly nontrivial result. 

      \begin{theorem}[Inversion Theorem]
        If two random variables have the same characteristic function, then their CDFs are the same. Further, if $X$ is a continuous random variable, then the PDF can be recovered from the characteristic function as follows: 
        \begin{equation}
          f_X (x) = \lim_{T \rightarrow \infty} \frac{1}{2 \pi} \int_{-T}^T e^{- i t x} \varphi_X (t) \,dt
        \end{equation}
        for every $x$ where $f_X (x)$ is continuous. 
      \end{theorem}

      Just like how we can recover moments from the MGF, we can always recover the moments from the characteristic function, with the added advantage that the CF will always exist. 

      \begin{theorem}[Moment Generating Property]
        Let $X$ be a random variable and $\varphi_X (t)$ its CF. 
        \begin{enumerate}
          \item If $\varphi_X^{(k)} (t)$ ( the $k$th derivative) exists at $t = 0$, then 
          \begin{align*}
            \mathbb{E}[|X^k|] < \infty & \text{ for } k \text{ even} \\
            \mathbb{E}[|X^{k-1}|] < \infty & \text{ for } k \text{ odd}
          \end{align*}
          
          \item If $\mathbb{E}[|X^k|] < \infty$, then 
          \begin{equation}
            \varphi_X^{(k)} (0) = i^k \mathbb{E}[X^k]
          \end{equation}
          
          \item Further, given that the moments are finite, we can expand the CF by moments of $X$ as 
          \begin{equation}
            \varphi_X (t) = \sum_{j=0}^k \frac{\mathbb{E}[X^j]}{j!} (i t)^j + o(t^k)
          \end{equation}
        \end{enumerate}
      \end{theorem}

      \begin{example}[Bernoulli]
        Given $X \sim \mathrm{Bernoulli}(p)$, we have 
        \begin{align*}
          \mathbb{E}[ e^{i t X}] & = \sum_{x \in \{0, 1\}} e^{i t x} \cdot \mathbb{P}(X = x) \\
          & = e^{i t 0} (1 - p) + e^{i t} p \\
          & = 1 - p + p e^{i t}
        \end{align*}
      \end{example}

      \begin{example}[Exponential]
        Given $X \in \mathrm{Exponential}(\lambda)$, we have 
        \begin{align*}
          \mathbb{E}[e^{i t X}] & = \int_0^\infty e^{i t x} \lambda e^{-\lambda x} \,dx \\
          & = \int_0^\infty \lambda \, e^{-(\lambda - i t) x} \,dx \\
          & = \frac{\lambda}{\lambda - it} \text{ for all } t \in \mathbb{R}
        \end{align*}
        where the complex integral requires some complex analysis. 
      \end{example}

  \subsection{Convergence of Random Variables}

    Unlike convergence of numbers, which is well-defined with respect to some metric or topology, there are many types of convergence of random variables. We must always specify which convergence when talking about them. Remember that a random variable $X$ is just a function from $\Omega$ to $\mathbb{R}$, so we can talk about pointwise convergence. That is, given a sequence of random variables $\{X_n\}$ and some $\omega \in \Omega$, the sequence 
    \begin{equation}
      X_1(\omega), X_2 (\omega), X_3(\omega), \ldots
    \end{equation}
    is simply a sequence of real numbers. If this sequence converges to the real number $X(\omega)$, then $X_n$ converges to $X$ at $\omega$. If this occurs for all $\omega \in \Omega$, then we have sure convergence, and if this happens for an event (a $\mathcal{F}$-measurable subset of $\Omega$) with probability $1$, then we have almost sure convergence. 

    \begin{definition}[Sure Convergence of RVs]
      The sequence of random variables $\{ X_n\}_{n \in \mathbb{N}}$ is said to \textbf{converge pointwise} or \textbf{converge surely} to $X$ if 
      \begin{equation}
        X_n (\omega) \rightarrow X (\omega)
      \end{equation}
      for every $\omega \in \Omega$. That is, we can choose \textit{any} $\omega \in \Omega$, and the realized sequence $X_1 (\omega), X_2 (\omega), \ldots$ will always converge to $X(\omega)$. We can visualize the function $X$ with a surface defined over $\Omega$ and can imagine the $X_n$'s as surfaces that converges to that of $X$. 
      \begin{center}
        \includegraphics[scale=0.3]{img/sure_convergence.jpg}
      \end{center}
    \end{definition}

    But this definition is too strong of a form of convergence, since in probability we don't care about values over sets of measure $0$. That is, if we have two probability distributions that differ from each other on a set of measure $0$, then they can be considered essentially the same probability distribution. 

    \begin{definition}[Almost Sure Convergence of RVs]
      The sequence of random variables $\{ X_n\}_{n \in \mathbb{N}}$ is said to \textbf{converge almost surely} or \textbf{converge with probability 1} to $X$ if $X_n (\omega) \rightarrow X (\omega)$ on a subset of probability $1$. That is, 
      \begin{equation}
        \mathbb{P} \big( \{ \omega \in \Omega \mid \lim_{n \rightarrow \infty} X_n (\omega) = X(\omega) \} \big) = 1
      \end{equation}
      Considering small technicalities, it can be shown that this set of $\omega$'s can be considered an event in $\mathcal{F}$. This can be visualized similarly as sure convergence, but now the surfaces don't have to converge on sets of measure $0$. 
      \begin{center}
        \includegraphics[scale=0.3]{img/almost_sure_convergence.jpg}
      \end{center}
      Crudely put, we just have to look at each $\omega \in \Omega$, see if $X_n (\omega)$ converges to $X(\omega)$ as $n \rightarrow \infty$, and determine if the set of all $\omega$'s that satisfy this have probability $1$. In other words, let us have some experiment with outcome space $\Omega$. With probability $1$, some $\omega \in \Omega$ will be realized, which will realize the sequence of realized random variables
      \begin{equation}
        X_1 (\omega), X_2 (\omega), X_3 (\omega), \ldots
      \end{equation}
      that will converge to $X(\omega)$. Visually, we can imagine selecting a random point in $\Omega$, which will not hit the curve or point (with probability $1$), and in these cases, the sequence of points will converge to $X(\omega)$. 
      \begin{center}
        \includegraphics[scale=0.3]{img/almost_sure_convergence_2.jpg}
      \end{center}
    \end{definition}

    \begin{definition}[Convergence in Probability]
      The sequence of random variables $\{ X_i\}_{i \in \mathbb{N}}$ is said to \textbf{converge to $X$ in probability} if for all $\epsilon > 0$, 
      \begin{equation}
        \lim_{n \rightarrow \infty} \mathbb{P} \big( |X_n - X| > \epsilon \big) = 0
      \end{equation}
      To understand what this means, fix an $\epsilon > 0$. Then, $X_1$ may be very far from $X$, meaning that the event $|X_1 - X| > \epsilon$, i.e. the set of all $\omega \in \Omega$ satisfying $|X_1(\omega) - X (\omega)| > \epsilon$ may be a larger portion of $\Omega$. Now, as we increase $n$, this event will become smaller (in the way that it's probability decreases) until it reaches $0$. 
      \begin{center}
        \includegraphics[scale=0.3]{img/convergence_in_probability.jpg}
      \end{center}
    \end{definition}

    \begin{example}
      Given $X_n \sim \mathrm{Exponential}(n)$ with $f_{X_n} (x) = n e^{-nx}$, we show that the sequence converges in probability to the $0$ random variable. Given $\epsilon > 0$, we have 
      \begin{align*}
        \lim_{n \rightarrow \infty} \mathbb{P}(|X_n - 0| > \epsilon) & = \lim_{n \rightarrow \infty} \mathbb{P}(X_n > \epsilon \cup X_n < -\epsilon) \\
        & = \lim_{n \rightarrow \infty} \mathbb{P}(X_n > \epsilon) \\
        & = \lim_{n \rightarrow \infty} \int_\epsilon^\infty n e^{-nx} \,dx \\
        & = \lim_{n \rightarrow \infty} e^{-n \epsilon} = 0
      \end{align*}
      We can imagine this since given any small $\epsilon > 0$, we can see that increasing $n$ results in the distribution of $X_n$ to decrease at a faster rate, and thus a bigger portion of the distribution would lie within $\epsilon$ of the $0$ random variable. 
      \begin{center}
        \includegraphics[scale=0.3]{img/convergence_in_prob_exponential.jpg}
      \end{center}
    \end{example}

    \begin{definition}[Convergence in rth Mean]
      We say $X_n$ \textbf{converges to $X$ in the $r$th mean} if 
      \begin{equation}
        \lim_{n \rightarrow \infty} \mathbb{E} \big[ |X_n - X|^r \big] = 0
      \end{equation}
      For $r = 2$, $X_n$ is said to converge to $X$ in the \textbf{mean-squared sense}. 
    \end{definition}

    \begin{definition}[Convergence in Distribution]
      We say $X_n$ \textbf{converges to $X$ in distribution} if the CDF of $X_n$ converges pointwise to the CDF of $X$, i.e. 
      \begin{equation}
        \lim_{n \rightarrow \infty} F_{X_n} (x) = F_X (x)
      \end{equation}
      for all $x$ where $F_{X}$ is continuous. 
    \end{definition}

    So for practical purposes there are 5 notions of convergence that we will work with: 
    \begin{enumerate}
        \item Sure convergence: $X_n \xrightarrow{p.w.} X$ 
        \item Almost sure convergence: $X_n \xrightarrow{a.s.} X$ 
        \item Convergence in probability: $X_n \xrightarrow{i.p.} X$ 
        \item Convergence in $r$th mean: $X_n \xrightarrow{rth} X$ (Mean square: $X_n \xrightarrow{m.s.} X$) 
        \item Convergence in distribution: $X_n \xrightarrow{D} X$
    \end{enumerate}

    \begin{theorem}[Hierarchy of Convergence]
      The following implications hold: 
      \begin{enumerate}
        \item Pointwise c. $\implies$ almost sure c. $\implies$ c. in probability $\implies$ c. in distribution. 
        \item $r$th mean c. $\implies$ c. in probability $\implies$ c. in distribution. 
      \end{enumerate}
    \end{theorem}

    Trying to understand these relationships can be very hard, so we will take some time to do that, with some examples. First, convergence in distribution is clearly the weakest, since convergence in distribution does not imply that the random variables need be close to each other. Take a look at the random variables $X \sim \mathrm{Bernoulli}(1/2)$ and $Y = 1 - X$. $X$ and $Y$ are both $\mathrm{Bernoulli}(1/2)$ with the same distribution, but they are \textit{not} the same random variable since $X - Y = 1$ always. Therefore, we can think of two random variables that have the same distribution but are not "close" to each other as functions over $\Omega$ that divide it into identical, but differently cut, distributions. 
    \begin{center}
      \includegraphics[scale=0.23]{img/prob_in_distribution.jpg}
    \end{center}

    \begin{example}[C. in Distribution $\centernot\implies$ C. in Probability]
      Let $X_1, X_2, \ldots$ be such that $X_i = X$ for all $i$ where $X \sim \mathrm{Bernoulli}(1/2)$. This does not mean that the $X_i$'s are iid Bernoulli; they are all copies of the same $X$, i.e. forms a constant sequence. Let $Y = 1 - X$. Clearly, $X_n \xrightarrow{D} Y$ since the CDF of every $X_i$ is the same as that of $Y$, but $|X_n - Y| = 1$ for all $n$, so there is no convergence.  
    \end{example}

    \begin{example}[C. in Distribution $\centernot\implies$ C. in Probability]
      Let $X_1, X_2, \ldots \sim \mathcal{N}(0, 1)$ and $Y = -X$. Then, by symmetry of the standard Gaussian, both $X$ and $Y$ have the same CDF, but they are not the same random variable: their signs are opposite. 
    \end{example}

    \subsubsection{Convergence in Probability vs Almost Surely} 

      Convergence almost surely and convergence with probability are very different. Almost sure convergence has the limit inside the probability, which indicates that we are talking about convergence of a sequence of random variables. On the other hand, convergence in probability has the limit on the outside, which talks about convergence of a sequence of probabilities. But a key point is that almost sure convergence implies convergence in probability. It happens so because there could exist a subset of small probability in $\Omega$ where the $X_n$'s and $X$ need not be close, but the probabilities of them deviating over whole $\Omega$ is small. 

      \begin{example}[C. in Probability $\centernot\implies$ C. Almost Surely]
        Consider the interval $\Omega = [0, 1]$ and the subsets $A_1 = [0, 0.1], A_2 = [0.1, 0.2], \ldots$, such that at $A_{10} = [0.9, 1.0]$, the size with halve and will go to the left boundary, $A_{11} = [0, 0.05], \ldots$. Then, the sequence of indicator random variables 
        \begin{equation}
          X_n \coloneqq 1_{A_n}
        \end{equation}
        looks like it's converging to the $0$ random variable. Indeed, $X_n \xrightarrow{i.p.} 0$ since the probability that $X_n$ deviates from $0$ by more than some small $\epsilon$ is simply the measure of $A_n$ itself, which decreases to $0$. That is, given some small $\epsilon > 0$, we have 
        \begin{equation}
          \lim_{n \rightarrow \infty} \mathbb{P} (|X_n - 0| > \epsilon) = \lim_{n \rightarrow \infty} \mathbb{P}(1_{A_n} > \epsilon ) = \lim_{n \rightarrow \infty} \mathbb{P}(A_n) = 0
        \end{equation}
        Now let's show that this doesn't converge almost surely. For \textit{any} outcome $\omega \in \Omega$, the sequence of random variables $X_1(\omega), X_2(\omega), \ldots$ will hit these intervals $A_n$ infinitely many times and will not converge to $0$, since there will always be a $1$ down the sequence. They will occur with decreasing frequency but they will always occur. Therefore, with probability $1$, whatever realized sequence will not converge to the $0$ random variable. 
      \end{example}

      Here is another standard counterexample. 

      \begin{example}[C. in Probability $\centernot\implies$ C. Almost Surely]
        Let us take the sequence $X_1, X_2, \ldots$ of independent random variables where $X_n \sim \mathrm{Bernoulli}(1/n)$. That is, 
        \begin{equation}
          \mathbb{P}(X_n = 1) = \frac{1}{n} \text{ and } \mathbb{P}(X_n = 0) = 1 - \frac{1}{n}
        \end{equation}
        So, as $n$ gets large we expect $X_n$ to realize values of $0$ more and more. Showing that $X_n \xrightarrow{i.p.} 0$ is easy, since we can compute for any $\epsilon > 0$
        \begin{align*}
          \lim_{n \rightarrow \infty} \mathbb{P}(|X_n - 0| > \epsilon) & = \lim_{n \rightarrow \infty} \mathbb{P}(|X_n| > \epsilon) \\
          & = \lim_{n \rightarrow \infty} \mathbb{P}(X_n = 1) \\
          & = \lim_{n \rightarrow \infty} \frac{1}{n} = 0
        \end{align*}
        We want to show that this does not converge almost surely to $0$, i.e. there is some set of nonzero measure such that for some $\omega$ in that set, the sequence $X_1 (\omega), X_2(\omega), \ldots$ does not converge to $0$. This can be hard to see at first, but the fact that we have independence and the terms are $\frac{1}{n}$ hints at the Borel-Cantelli lemma. Let $A_n$ be the event that $\{X_n = 1\}$ (i.e. the preimage of the singleton set under $X_n$: $X_n^{-1} ( \{1\})$). Then, the $A_n$'s are independent, and 
        \begin{equation}
          \sum_{n=1}^\infty \mathbb{P}(A_n) = +\infty
        \end{equation}
        By the Borel-Cantelli lemma 2, this implies that almost surely infinitely many $A_n$'s will occur. That is, we can choose as large of an $n$ as we like, go down the sequence until we look at $X_n, X_{n+1}, \ldots$, and we are guaranteed with probability $1$ that at least one of the $X_i$'s after $n$ will realize a $1$. This means that in every realization of $X_1, X_2, \ldots$, we will get a sequence of $0$s and $1$s, but since BCL states that no matter how far down the road you will always get at least another $1$, this sequence does not converge to $0$.  
      \end{example}

      The commonality between these two examples is that sequence of random variables satisfies convergence in probability as follows: As $n$ increases, $X_n$ is more and more likely to be near $X$ (in the way that $|X_n - X| < \epsilon$ for some $\epsilon > 0$), ultimately satisfying this closeness property with probability $1$ as $n \rightarrow \infty$. For example, we could have 
      \begin{align*}
        \mathbb{P}(|X_1 - X| > \epsilon) & = 1 \\
        \mathbb{P}(|X_2 - X| > \epsilon) & = 1/2 \\
        \mathbb{P}(|X_3 - X| > \epsilon) & = 1/3 \\
        \ldots & = \ldots 
      \end{align*}
      This definitely satisfies convergence in probability, but this leaves open the possibility that $\mathbb{P}(|X_n - X| > \epsilon)$ an infinite number of times, although at infrequent intervals. Therefore, when looking at the sequence 
      \begin{equation}
        X_1, X_2, X_3, \ldots
      \end{equation}
      each random variable \textit{individually} may have less chance of being more than $\epsilon$ away from $X$, but since there is an infinite number of them in the sequence, the sequence \textit{in totality} may contain an infinite number of cases where $|X_n - X| > \epsilon$. Convergence almost surely tells us that we are \textit{guaranteed} (with probability $1$) that this sequence will converge to $X$. That is, we can specify an $N \in \mathbb{N}$ such that $|X_n - X| < \epsilon$ for all $n > N$. 

      Let us define some $\epsilon > 0$ and consider a sequence of random variables $\{X_n\}_{n \in \mathbb{N}}$. Given some outcome $\omega \in \Omega$, we will consider it a \textit{success} if $|X_n(\omega) - X(\omega)| < \epsilon$ and \textit{failure} if not. Then, convergence in probability tells us that the probability of failure goes to $0$ as $n$ goes to infinity and therefore we get better and better estimates of $X$. Convergence almost surely is a bit stronger and says that the total number of failures is \textit{finite}. That is, after a certain point $N$, the random variable $X_n$ will \textit{always} estimate $X$ within an error of $\epsilon$ (i.e. such that $|X_n - X| < \epsilon$). But since you don't know when you've exhausted all failures, there is not much of a difference from a practical point of view. 

    \subsubsection{Complete Convergence}

      When proving almost sure convergence, we'd ideally just look at all the $\omega \in \Omega$ where $X_n (\omega) \rightarrow X(\omega)$, and if this set has probability measure $1$, then we are done. But this is not very practical, so we use the following theorem, which gives a sufficient condition for $X_n \xrightarrow{a.s.} X$. 

      \begin{theorem}
        If for all $\epsilon > 0$, 
        \begin{equation}
          \sum_{n=1}^\infty \mathbb{P}(|X_n - X| > \epsilon ) < \infty
        \end{equation}
        then $X_n \xrightarrow{a.s.} X$. This condition is a bit stronger, since not only are we saying that $\mathbb{P}(|X_n - X| > \epsilon)$ tends to $0$ as $n \rightarrow \infty$, but that it goes down fast enough to keep the series convergent. 
      \end{theorem}
      \begin{proof}
        Let the event that $|X_n - X| > \epsilon$ be denoted $A_n (\epsilon)$ (i.e. the preimage of $(\epsilon, \infty)$ under the map $|X_n - X|$, which is a $\mathcal{F}$-measurable set). Since the sum of their probabilities is finite, by the Borel-Cantelli lemma 1, finitely many $A_n(\epsilon)$'s will occur with probability $1$. This means that for any $\epsilon > 0$, $|X_n - X| \leq \epsilon$ for all large enough $n$, meaning that it converges to $0$. 
      \end{proof}

  \subsection{Laws of Large Numbers}

    \begin{theorem}[Weak Law of Large Numbers]
      Let $X_1, X_2, ..., X_n$ be a sequence of iid random variables, with finite mean $\mathbb{E}[X]$. Then, the average of the random variables $S_n / n$ converges in probability to $\mathbb{E}[X]$. 
      \begin{equation}
        \frac{S_n}{n} = \frac{1}{n} \sum_{i=1}^n X_i \xrightarrow{i.p} \mathbb{E}[X]
      \end{equation}
      That is, for any $\epsilon > 0$, 
      \begin{equation}
        \lim_{n \rightarrow \infty} \mathbb{P} \bigg( \bigg| \Big( \frac{1}{n} \sum_{k=1}^n X_k \Big) - \mathbb{E}[X] \bigg| > \epsilon \bigg) = 0
      \end{equation}
    \end{theorem}
    \begin{proof}
      We first do the proof assuming additionally that $X$ has finite variance, so $\mathrm{Var}[X] < \infty$. We will show that the random variable $S_n/n$ converges in mean square to $\mathbb{E}[X]$, which will imply convergence in probability. Note that $\mathbb{E}[S_n / n] = \mathbb{E}[X]$, and 
      \begin{align*}
        \lim_{n \rightarrow \infty} \mathbb{E} \bigg[ \bigg| \frac{S_n}{n} - \mathbb{E}[X] \bigg|^2 \bigg] & = \lim_{n \rightarrow \infty} \mathbb{E} \bigg[ \bigg| \frac{S_n}{n} - \mathbb{E}\Big[\frac{S_n}{n}\Big] \bigg|^2 \bigg] \\
        & = \lim_{n \rightarrow \infty} \mathrm{Var}\Big( \frac{S_n}{n} \Big) \\
        & = \lim_{n \rightarrow \infty} \frac{\mathrm{Var}(S_n)}{n^2} \\
        & = \lim_{n \rightarrow \infty} \frac{\mathrm{Var}[X]}{n} = 0
      \end{align*}
    \end{proof}

    \begin{theorem}[Strong Law of Large Numbers]
      Let $X_1, X_2, ..., X_n$ be a sequence of iid random variables, with finite mean $\mathbb{E}(X_k)$ and with finite variance. Then, the average of the random variables $S_n / n$ converges almost surely to $\mathbb{E}[X]$. 
      \begin{equation}
        \frac{S_n}{n} \xrightarrow{a.s.} \mathbb{E}[X]
      \end{equation}
      That is, 
      \begin{equation}
        \mathbb{P} \bigg( \Big\{ \omega \in \Omega \mid \lim_{n \rightarrow \infty} \Big( \frac{1}{n} \sum_{i=1}^n X_i (\omega) \Big) = \mathbb{E}[X] \Big\} \bigg) = 1
      \end{equation}
    \end{theorem}

    Now let's compare these two laws. They both deal with averages of random variables, i.e. we keep sampling from $X$ and compute the averages $\overline{X}_n$. The weak law states that for a specified large $n$, the average $\overline{X}_n$ is likely to be near $\mathbb{E}[X]$. But it leaves open the possibility that $|\overline{X}_n - \mathbb{E}[X]| > \epsilon$ happens an infinite number of times (although less frequently). So no matter how big of an $n$ we choose, there could always be an $\overline{X}_n$ in the future that fails to satisfy $|\overline{X}_n - \mathbb{E}[X]| > \epsilon$. However, the strong law shows that this almost surely will not occur. That is, with probability $1$, we have for any $\epsilon > 0$ the inequality $|\overline{X}_n - \mathbb{E}[X]| < \epsilon$ for all large enough $n$ greater than a certain $N$. Note that the weak law does not guarantee the existence of such an $N$. 

    This result is very useful because it justifies experiments that estimate some value by taking averages. 

    \begin{example}[Estimating Speed of Light]
      Say that we are conducting an experiment to justify the speed of light, which will have true value $\mu$. The laws of large numbers say that in theory, after obtaining enough data, we can get arbitrarily close to the true speed of light. Choose $\epsilon > 0$ arbitrarily small. We can obtain $n$ estimates $X_1, \ldots, X_n$ of the speed of light and compute the average 
      \begin{equation}
        \overline{X}_n = \frac{1}{n} \sum_{i=1}^n X_i
      \end{equation}
      As we obtain more data, we can compute $\overline{X}_n$ for each $n = 1, 2, \ldots$. The weak law says that $\mathbb{P}(|\overline{X}_n - \mu| > \epsilon) \rightarrow 0$ as $n \rightarrow \infty$, i.e. the probability of our estimate being off by more than $\epsilon$ goes to $0$ (though it may happen with nonzero probability if we consider the infinite sequence). The strong law says that the number of times $|\overline{X}_n - \mu|$ is greater than $\epsilon$ is finite (with probability $1$), and after a certain point our estimates will perfectly lie within the error $\epsilon$. This gives us considerable confidence in the value $\overline{X}_n$ because it guarantees the existence of some $N \in \mathbb{N}$ s.t. $|\overline{X}_n - \mu| < \epsilon$ for all $n > N$, i.e. the average \textit{never} fails for $n > N$. 
    \end{example}

  \subsection{Concentration Inequalities}

    Concentration inequalities give you probability bounds on random variables taking atypical values. For example, given a random variable with certain mean and variance, the probability of that random variable taking values outside a certain range around the mean is very small. It's called concentration because the probability concentrates around a certain range. 

    The basic question here is that we would like to model a random variable $X$ over a probability space $\Omega$ and have some data $X_1, X_2, \ldots, X_n$ iid according to $X$. Let us have a fixed function $f: \mathbb{R}^n \longrightarrow \mathbb{R}$ that transforms the joint random variable $(X_1, \ldots, X_n)$ to create a new scalar RV 
    \begin{equation}
      f(X_1, \ldots, X_n) = f \circ (X_1, \ldots, X_n) : \Omega \longrightarrow \mathbb{R}
    \end{equation}
    $f(X_1, \ldots, X_n)$ is a random variable so it has a mean, denote it $\mathbb{E}[f]$. Then concentration generally refers to the probability that the value of $f$ is at least some distance further from its mean. 
    \begin{equation}
      \mathbb{P} \big( |f(x) - \mathbb{E}[f] | \geq t \big) \leq \epsilon
    \end{equation}
    for some small positive $\epsilon$. Usually, we would like this $\epsilon$ to be an exponentially decaying function of $t$ so that the bound goes down fast. This is what's so great about the Gaussian, which is why we'll introduce it here. 

    \begin{theorem}[Gaussian Tail Inequality]
      Given $X \sim \mathcal{N}(0, 1)$, the inequality says that the probability of $X$ taking values past a certain $t$ decays exponentially. 
      \begin{equation}
        \mathbb{P} \big( |X| > t \big) \leq \frac{2 e^{-t^2/2}}{t}
      \end{equation}
      If we have $X_1, \ldots, X_n \sim \mathcal{N}(0, 1)$, then 
      \begin{equation}
        \mathbb{P} \big( |\overline{X}| > t \big) \leq \frac{2}{\sqrt{n} t} e^{-n t^2/2}
      \end{equation}
      We can assume that the coefficient is less than $1$ if $n$ is large. The above tells us that this bound exponentially decays with $t$ but also with the number of samples $n$. 
    \end{theorem}
    \begin{proof}
      We can simply check 
      \begin{equation}
        \phi(s) = \frac{1}{\sqrt{2\pi}} e^{-s^2/2} \implies \phi^\prime (s) = s \, \phi(s)
      \end{equation}
      and use this to evaluate
      \begin{align*}
        \mathbb{P}(X > t ) & = \int_t^\infty \phi(s) \,ds \\
        & = \int_t^\infty \frac{s}{s} \phi(s) \,ds \\
        & < \frac{1}{t} \int_t^\infty s \phi(s)\,ds \\
        & = \frac{1}{t} \int_t^\infty \phi^\prime (s)\,ds \\
        & = \frac{\phi(t)}{t}
      \end{align*}
    \end{proof}

    Due to the exponential nature of the probability bound, we are extremely confident in getting the majority of our samples from a small interval. If we had taken some distribution like a Cauchy, with PDF of form 
    \begin{equation}
      f(x) \propto \frac{1}{1 + x^2}
    \end{equation}
    Then we see that even though the shape looks like a Gaussian at first glance, the fat tails go down at the rate of $1/x^2$. It turns out that due to this, when we sample numerically, we occasionally get extreme values. 

    \begin{theorem}[Markov's Inequality]
      If $X$ is a non-negative random variable of finite expectation and $\alpha > 0$, then 
      \begin{equation}
        \mathbb{P}(X > \alpha) \leq \frac{\mathbb{E}[X]}{\alpha}
      \end{equation}
      That is, the probability that $X$ takes a value greater than $\alpha$ is at most the expectation of $X$ divided by $\alpha$. This is meaningful only when $\mathbb{E}[X] < \alpha$, since otherwise the RHS will be greater than $1$.  
    \end{theorem}
    \begin{proof}
      Given any $\alpha > 0$, we can set 
      \begin{equation}
        X = X \cdot 1_{X \leq \alpha} + X \cdot 1_{X > \alpha}
      \end{equation}
      and by linearity, 
      \begin{align*}
        \mathbb{E}[X] & = \mathbb{E}[X \cdot 1_{X \leq \alpha} + X \cdot 1_{X > \alpha}] \\
        & \geq \mathbb{E}[ X \cdot 1_{X > \alpha}] \\
        & \geq \alpha \mathbb{E}[1_{X > \alpha}] \\
        & = \alpha \, \mathbb{P}(X > x) 
      \end{align*}
    \end{proof}

    In other words, the probability that $X > \alpha$ goes down at least as fast as $1/\alpha$. For example, setting $\alpha = 2 \mathbb{E}[X]$, the probability that $X$ takes value that is at least twice its expectation is at most $1/2$. Furthermore, as $X$ gets very large, the probability that it will take a value beyond a large $\alpha$ goes down faster than $1/\alpha$. But this is a very conservative inequality, and usually the probability goes down much faster. 

    Markov's inequality is very conservative but very general, too. If we make further assumptions about the random variable $X$, we can often make stronger bounds. Chebyshev's inequality assumes a (possibly negative) random variable with finite variance and states that the probability will go down as $1/x^2$. 

    \begin{theorem}[Chebyshev Inequality]
      Given (possibly negative) random variable $X$, if $\mathbb{E}[X] = \mu < +\infty$ and $\Var(X) = \sigma^2 < +\infty$, then for all $\alpha > 0$, 
      \begin{equation}
        \mathbb{P} \big( |X - \mu| > k \sigma \big) \leq \frac{1}{k^2} \iff \mathbb{P}(|X - \mu| > \alpha) \leq \frac{\mathrm{Var}[X]}{\alpha^2}
      \end{equation}
      That is, the probability that $X$ takes a value further than $k$ standard deviations away from $\mu$ goes down by $1/k^2$. Therefore, if $\sigma$ is small, then this bound will be small since there is more concentration in the mean. 
    \end{theorem}
    \begin{proof}
      We apply Markov's inequality to the non-negative random variable $|X - \mu|$. 
      \begin{equation}
        \mathbb{P}(|X - \mu| > \alpha) = \mathbb{P}(|X - \mu|^2 > \alpha^2) \leq \frac{\mathbb{E}(|X - \mu|^2)}{\alpha^2} = \frac{\mathrm{Var}[X]}{\alpha^2}
      \end{equation}
      since the numerator on the RHS is the definition of variance. 
    \end{proof}

    Chebyshev inequality is just Markov's inequality applied to $X^2$ (assuming $0$ mean), and often yields a better bound. But even Chebyshev's inequality turns out to be quite loose, and even this $1/k^2$ is not a very nice bound. We could apply Markov's inequality to higher powers of $X$, e.g. given a random variable $X$, we can apply Markov's inequality to the $k$th power of nonnegative random variable $|X - \mathbb{E}[X]|$: 
    \begin{equation}
      \mathbb{P} (|X - \mathbb{E}[X] | > \alpha) = \mathbb{P}\big( |X - \mathbb{E}[X] |^k > \alpha^k \big) \leq \frac{\mathbb{E}( |X - \mathbb{E}[X] |^k )}{\alpha^k}
    \end{equation}
    The natural culmination of all this is to apply Markov's inequality to $e^X$ (or, for a little flexibility, $e^{t X}$, where $t$ is a constant to be optimized). This gives us an exponential bound on $\mathbb{P}(X > \alpha)$. 

    \begin{example}[Gaussian]
      For the normal distribution, recall the 67-95-99.7 rule. It is well known that the probability of a random variable taking values within $2$ standard deviations from the mean is 95\%, so the probability that it takes outside is 5\%, or $1/20$, which is less than the $1/2^2 = 1/4$ bound given by Chebyshev. 
    \end{example}

    \subsubsection{Chernoff Bound and MGFs}

      \begin{theorem}[Chernoff Bound]
        Given a (possibly negative) random variable $X$, assume that its moment generating function $M_X (s) = \mathbb{E}[e^{s X}]$ is finite for every $s \in [-\epsilon, \epsilon]$. Then, since $x \mapsto e^{s x}$ is monotonically increasing, we have the identity 
        \begin{equation}
          \mathbb{P}(X > \alpha) = \mathbb{P}(e^{s X} > e^{s \alpha}) \text{ for } s > 0
        \end{equation}
        But since the new random variable $e^{s X}$ is nonnegative, we can now go back to Markov inequality and write 
        \begin{equation}
          \mathbb{P}(X > \alpha) = \mathbb{P}(e^{s X} > e^{s \alpha}) \geq \frac{\mathbb{E}[e^{s X}]}{e^{s \alpha}} = M_X (s) \, e^{-s \alpha}
        \end{equation}
        for $s > 0$ (for identity above to hold) \textit{and} $s \in D_X$ (and it is in domain of convergence). Now, we have an exponentially decaying bound in terms of $\alpha$. We have the freedom to choose $s$, since our bound is in terms of $\alpha$, so we must choose $s$ that minimizes $M_X (s) \, e^{-s \alpha}$. Ultimately, our best bound is 
        \begin{equation}
          \mathbb{P}(X > \alpha) \leq \inf_{s > 0} M_X (s) \, e^{-s \alpha}
        \end{equation}
        After we optimize over $s$ what remains on the RHS is a function of $\alpha$. 
      \end{theorem}

      Now, we can calculate the MGF of $X$ directly if we knew the distribution of $X$, but we can also get bounds on it given some coarse statistics of $X$. 

      \begin{lemma}
      Let $X$ be a $0$-mean random variable s.t. $a \leq X \leq b$ with probability $1$. Then for all $t > 0$, 
      \begin{equation}
        \mathbb{E}[ e^{t X}] \leq e^{t^2 (b - a)^2 / 8}
      \end{equation}
      \end{lemma}
      \begin{proof}
        We can write $x = \lambda a + (1 - \lambda b)$, $0 \leq \lambda \leq 1$, and convexity of the exponential tells us that 
        \begin{equation}
          e^{tx} \leq \lambda e^{ta} + (1 - \lambda) e^{tb}
        \end{equation}
        Plugging in $\lambda = (b - x) / (b - a)$ then gives 
        \begin{equation}
          e^{tx} \leq \frac{b - x}{b - a} e^{tx} + \frac{x - a}{b - a} e^{tb}
        \end{equation}
        Take expectations of both sides, and using linearity of expectation and the fact that $\mathbb{E}[X] = 0$. 
        \begin{equation}
          \mathbb{E}[e^{tX}] \leq \frac{b - \mathbb{E} X}{b - a} e^{ta} + \frac{\mathbb{E} X - a}{b - a} e^{tb} = \frac{b e^{ta} - a e^{tb}}{b - a} \leq e^{t^2 (b - a)^2 / 8}
        \end{equation}
      \end{proof}

    \subsubsection{Hoeffding's Inequality}

      Hoeffding's inequality is one of the most important inequalities in concentration of measure. The proof of this inequality involves many useful tricks. 

      \begin{theorem}[Hoeffding's Inequality]
        Let $X_1, X_2, \ldots, X_n$ be independent (not necessarily identical) random variables s.t. $a_i \leq X_i \leq b_i$ almost surely. Consider the random variable $\overline{X} = \frac{1}{n} (X_1 + \ldots + X_n)$. Then, for all $t > 0$, we have the two inequalities
        \begin{align*}
          \mathbb{P}\big( \overline{X} - \mathbb{E}[\overline{X}] \geq t \big) & \leq \exp \bigg( -\frac{2 n^2 t^2}{\sum_{i=1}^n (b_i - a_i)^2} \bigg) \\
          \mathbb{P}\big( \overline{X} - \mathbb{E}[\overline{X}] \leq -t \big) & \leq \exp \bigg( -\frac{2 n^2 t^2}{\sum_{i=1}^n (b_i - a_i)^2} \bigg)
        \end{align*}
        which can be combined to produce 
        \begin{equation}
          \mathbb{P}\big( \big| \overline{X} - \mathbb{E}[\overline{X}] \big| \geq t \big) \leq 2 \exp \bigg( -\frac{2 n^2 t^2}{\sum_{i=1}^n (b_i - a_i)^2} \bigg)
        \end{equation}
        We can create an equivalent bound on the sum $S_n = X_1 + \ldots + X_n$: 
        \begin{align*}
          \mathbb{P}\big(| S_n - \mathbb{E}[S_n] | \geq t\big) & = \mathbb{P}\big( n |\overline{X} - \mathbb{E}[\overline{X}] | \geq t \big) \\
            & = \mathbb{P} \big( |\overline{X} - \mathbb{E}[X] | \geq \frac{t}{n} \big) \\
            & \leq 2 \exp \bigg( -\frac{2 t^2}{\sum_{i=1}^n (b_i - a_i)^2} \bigg) 
        \end{align*}
      \end{theorem}
      \begin{proof}
        We will prove just with the case where $X_1, \ldots X_n$ are all bounded by $[a, b]$, which gives 
        \[\mathbb{P} \big( |\overline{X} - \mathbb{E}[\overline{X}] | \geq t \big) \leq 2 \exp \bigg( - \frac{2 n t^2}{(b - a)}\bigg)\] 
        Now, we can write 
        \begin{align*}
          \mathbb{P}(\overline{X}_n > \epsilon ) & = \mathbb{P} \Big( \sum_{i=1}^n X_i \geq n \epsilon \Big) \\
          & = \mathbb{P} \big( e^{t \sum X_i} \geq e^{t n \epsilon} \big) & (\text{Variational Technique}) \\
          & \leq e^{- t n \epsilon} \, \mathbb{E}[e^{t \sum X_i}] & (\text{Markov's Inequality}) \\
          & = e^{-t n \epsilon} \, \big( \mathbb{E}[ e^{t X_i}] \big)^n & (\text{Independence}) \\
          & \leq e^{-t n \epsilon} e^{n \frac{t (b - a)^2}{2}} & (\text{prev. lemma}) 
        \end{align*}
        The step where we introduce an extra parameter $t$ is called a variational technique, used for optimization, and we can adjust $t$ to make it as small as possible. Taking the derivative of the final expression w.r.t. $t$ and solving for $0$ gives us $t = \frac{4 \epsilon}{(b - a)^2}$, and substituting into the expression gives the bound as 
        \begin{equation}
          \mathbb{P}(\overline{X}_n > \epsilon ) \leq \exp \bigg(- \frac{2 n \epsilon^2}{(b - a)^2}
        \end{equation}
      \end{proof}

      By further rearranging, we can write it as 
      \begin{equation}
        \mathbb{P} \bigg( | \overline{X} - \mathbb{E}[\overline{X}] | \geq t \sqrt{\frac{\sum_{i=1}^n (b_i - a_i)^2}{n^2}} \bigg) \leq 2 \exp(-2t^2)
      \end{equation}
      which now looks like our Chebyshev inequality, but without a notion of standard deviation. But note the fact if $a_i \leq X_i \leq b_i$, then $\mathrm{Var}(X_i) \leq (b_i - a_i)^2$ (since $\mathrm{Var}(X_i) = \mathbb{E}[(X_i - \mathbb{E}[X_i])^2] \leq \mathbb{E}[(b_i - a_i)^2]$). So, we have 
      \begin{equation}
        \mathrm{Var}(\overline{X}) \leq \frac{\sum_{i=1}^n (b_i - a_i)^2}{n^2} \implies \mathbb{P}\bigg( |\overline{X} - \mathbb{E}[\overline{X}] | \geq t \sqrt{\frac{\sum_{i=1}^n (b_i - a_i)^2}{n^2}} \geq \mathrm{Var}(\overline{X}) \bigg) \leq 2\exp(-2t^2)
      \end{equation}
      which allows us to interpret Hoeffding's inequality in a more familiar way. It says that the probability that the sample average is more than $t$ standard deviations from its expectation is at most $2 e^{-2t^2}$. 

      \begin{corollary}
        If $X_1, X_2, \ldots, X_n$ are independent with $\mathbb{P}(a_i \leq X_i \leq b_i) = 1$ and common mean $\mu$, then 
        \begin{equation}
          \mathbb{P}\bigg[ \big| \overline{X}_n - \mu \big| \leq \sqrt{ \frac{\sum_{i=1}^n (b_i - a_i)^2}{2n^2} \log \Big(\frac{2}{\delta}\Big)} \bigg] \geq 1 - \delta
        \end{equation}
      \end{corollary}

      \begin{example}[Bernoulli]
        Applying Hoeffding's inequality to a sequence of $n$ $p$-coin tosses $X_1, \ldots, X_n \sim \mathrm{Bernoulli}(p)$ gives 
        \begin{equation}
          \mathbb{P}\big( | \overline{X}_n - p | > \epsilon \big) \leq 2 \exp^{-2 n \epsilon^2}
        \end{equation}
      \end{example}

      \begin{example}[Mean]
        Suppose we have $X_1, X_2, \ldots X_n \sim \mathrm{Bernoulli}(p)$, all iid. Then, by Hoeffding's inequality, the average $\overline{X} = \frac{1}{n} (X_1 + \ldots + X_n)$ is tightly concentrated around $p$. 
        \begin{equation}
          \mathbb{P} \big( | \overline{X} - p | \geq t \big) \leq 2 e^{-2 n t^2}
        \end{equation}
        Note that $b_i - a_i = 1 - 0 = 1$ for all $i$. There is an exponential decay in the probability of the sample mean deviating from its expectation. 
      \end{example}

      \begin{example}[Hypercube]
        Pick $X \in [-1, +1]^d$ uniformly at random, i.e. choose iid $X_1,\ldots, X_d \sim \mathrm{Uniform}[-1, +1]$. The expectation is 
        \begin{equation}
          \mathbb{E} ||X||^2 = \sum_{i=1}^d \mathbb{E} X_i^2 = \sum_{i=1}^d \int_{-1}^1 x^2 f_X (x) \,dx = \sum_{i=1}^d \int_{-1}^1 \frac{1}{2} x^2 \,dx = \frac{d}{3}
        \end{equation}
        Then, it can be shown that $||X|| = $ is tightly concentrated around $\sqrt{d/3}$. We show this again with Hoeffding's inequality by showing the concentration of $||X||^2$ around $d/3$. 
        \begin{equation}
          \mathbb{P} \bigg( \bigg| ||X||^2 - \frac{d}{3} \bigg| \geq t \bigg) \leq 2 \exp \Big( - \frac{ d t^2}{2} \Big)
        \end{equation}
        This tells us that if we choose the uniform random vector $X \in [-1, +1]^d$, the vast majority of our samples will have $||X|| \approx \sqrt{d/3}$. 
      \end{example}


      Hoeffding's inequality does not use any information about the random variables expect for the fact that they are bounded. If the variance of $X_i$ is small, then we can get a sharper inequality from Bernstein's inequality. 

      \begin{theorem}[Bernstein's Inequality]
        If $\mathbb{P}(|X_i| \leq c) = 1$ and $\mathbb{E}[X_i] = 0$, set $\overline{X} = \frac{1}{n} \sum_{i=1}^n X_i$. Then, for any $t > 0$, 
        \begin{equation}
          \mathbb{P} \big( \big| \overline{X} \big| > \epsilon \big) \leq 2 \exp \bigg( - \frac{n \epsilon^2}{2 \sigma^2 + 2 c \epsilon /3} \bigg)
        \end{equation}
        where $\sigma^2 = \frac{1}{n} \sum_{i=1}^n \mathrm{Var}(X_i)$. 
      \end{theorem}

    \subsubsection{Concentration of Lipshitz Functions}

      Observing the Hoeffding bound, one might wonder whether such concentration applies only to averages or sums of random variables. After all, what's so special about averages? It turns out that the relevant feature of the average that yields tight concentration is that it is smooth in the way that if we change the value of one random variable the function does not change dramatically. 

      \begin{theorem}[Bounded Difference Inequality]
        Let has have independent random variables $X_1, X_2, \ldots, X_n$ and a function $f: \mathbb{R}^n \longrightarrow \mathbb{R}$ that satisfies the \textbf{bounded difference property} that 
        \begin{equation}
          \big| f(x_1, \ldots, x_k, \ldots, x_n) - f(x_1, \ldots, x_k^\prime, \ldots, x_n) \big| \leq c_k
        \end{equation}
        for every $x, x^\prime \in \mathbb{R}^n$. That is, the function changes by at most $c_k$ if its $k$th coordinate is changed. Then, for all $t \geq 0$, we have the concentration inequality: 
        \begin{align*}
          \mathbb{P} \big( f(X_1, \ldots, X_n) - \mathbb{E}[ f(X_1, \ldots, X_n)] \geq t \big) & \leq \exp \bigg(- \frac{2t^2}{\sum_{k=1}^n c_k^2} \bigg) \\
          \mathbb{P} \big( f(X_1, \ldots, X_n) - \mathbb{E}[ f(X_1, \ldots, X_n)] \leq -t \big) & \leq \exp \bigg(- \frac{2t^2}{\sum_{k=1}^n c_k^2} \bigg)
        \end{align*}
        Combining the two gives 
        \[\mathbb{P} \big( \big| f(X_1, \ldots, X_n) - \mathbb{E}[ f(X_1, \ldots, X_n)] \geq t \big| \big) \leq \exp \bigg(- \frac{2t^2}{\sum_{k=1}^n c_k^2} \bigg)\]
      \end{theorem}

      In fact, any smooth function of bounded independent random variables is tightly concentrated around its expectation, and the notion of smoothness is Lipshitz continuity. 

      \begin{definition}
        A function $f: \mathbb{R}^n \longrightarrow \mathbb{R}$ is $L$-Lipschitz w.r.t. the $l_p$-metric if for all $\mathbf{x}, \mathbf{y} \in \mathbb{R}^n$, 
        \begin{equation}
          |f(\mathbf{x}) - f(\mathbf{y})| \leq L ||\mathbf{x} - \mathbf{y}||_p
        \end{equation}
      \end{definition}

      \begin{example}
        For $x = (x_1, x_2, \ldots, x_n)$, we define the average $a(x) = \frac{1}{n} (x_1 + \ldots + x_n)$. Then, $a$ is $(1/n)$-Lipschitz w.r.t. the $l_1$ metric, since for any $\mathbf{x}, \mathbf{y}$, 
        \begin{align*}
          |a(\mathbf{x}) - a(\mathbf{y})| & = \bigg| \frac{1}{n} \big[ (x_1 - y_1) + \ldots + (x_n - y_n) \big] \bigg| \\
          & = \frac{1}{n} \big( |x_1 - y_1| + \ldots + |x_n - y_n| \big) \\
          & = \frac{1}{n} ||\mathbf{x} - \mathbf{y} ||_1
        \end{align*}
      \end{example}

      It turns out that Hoeffding's bound holds for all Lipschitz functions w.r.t. the $l_1$ metric. 

      \begin{theorem}
        Suppose $X_1, X_2, \ldots, X_n$ are independent and bounded with $a_i \leq x_i \leq b_i$. Then, for any $f: \mathbb{R}^n \longrightarrow \mathbb{R}$ that is $L$-Lipschitz w.r.t. the $l_1$-metric, we have 
        \begin{align*}
          \mathbb{P} [ f \geq \mathbb{E}(f) + t] & = \mathbb{P} [ f - \mathbb{E}(f) \geq t] \leq \exp\bigg(- \frac{2 t^2}{L^2 \sum_{i=1}^n (b_i - a_i)^2} \bigg) \\
          \mathbb{P} [ f \leq \mathbb{E}(f) - t] & = \mathbb{P} [ f - \mathbb{E}(f) \leq -t] \leq \exp\bigg(- \frac{2 t^2}{L^2 \sum_{i=1}^n (b_i - a_i)^2} \bigg)
        \end{align*}
        and combining these inequalities gives 
        \begin{equation}
          \mathbb{P} [ |f - \mathbb{E}(f)| \geq t] \leq \exp\bigg(- \frac{2 t^2}{L^2 \sum_{i=1}^n (b_i - a_i)^2} \bigg)
        \end{equation}
      \end{theorem}

  \subsection{Central Limit Theorem}

    By the law of large numbers, the sample averages converge almost surely (and therefore converge in probability) to the expected value $\mu$ as $n \rightarrow \infty$. The CLT describes the size and the distributional form of the stochastic fluctuations around $\mu$ during this convergence. That is, it states that as $n$ gets larger, the distribution of the difference $\overline{X}_n - \mu$ approximates a $\mathcal{N}(0, \sigma^2 / n)$ distribution, where $\sigma^2$ is the variance of $X$. 


    Roughly speaking, the (weak) law of large numbers says that 
    \begin{equation}
      \frac{S_n - n \mathbb{E}[X]}{n} \xrightarrow{i.p.} 0
    \end{equation}
    That is, if we consider the sequence of functions $\{S_n - n \mathbb{E}[x]\}_{n \in \mathbb{N}}$, this sequence is sublinear (i.e. is $o(n)$). CLT does two things: 
    \begin{enumerate}
      \item It specifically quantifies this fluctuation $\{S_n - n \mathbb{E}[X]$ by saying that it is approximately of order $\sqrt{n}$. 
      \item Furthermore, this fluctuation, when divided by $\sqrt{n}$ converges in distribution to a Gaussian. 
      \begin{equation}
        \frac{S_n - n \mathbb{E}[X]}{\sqrt{n}} \xrightarrow{D} \mathcal{N}(0, \sigma_X^2)
      \end{equation}
    \end{enumerate}

    \begin{theorem}[Central Limit Theorem]
      Let $X_1, X_2, X_3, ...$ be a sequence of iid random variables, with mean $\mu = \mathbb{E}[X]$ and with variance $\Var(X) = \sigma^2 < \infty$. Then, the sequence of random variables $\{\overline{X}_n\}_{n \in \mathbb{N}}$ converges in distribution to a Gaussian $\mathcal{N}(\mu, \sigma^2 / n)$. That is, 
      \begin{equation}
        \frac{\overline{X}_n - \mu}{\sigma \sqrt{n}} \xrightarrow{D} \mathcal{N}(0, 1)
      \end{equation}
    \end{theorem}
    \begin{proof}
      Let $Z_i = \frac{X_i - \mu}{\sigma}$ and let $U_n = \frac{1}{\sqrt{n}} \sum_{i=1}^n Z_i$ (we can normalize the $X_i$'s since they have finite mean and variance). Note that since we have finite second moments 
      \begin{align*}
        & \mathbb{E}[Z_i] = 0 < \infty \\
        & \mathrm{Var}[Z_i] = \mathbb{E}[ (Z_i - \mathbb{E}[Z_i])^2] = \mathbb{E}[Z_i^2] = 1 < \infty 
      \end{align*}
      we can Taylor expand the characteristic function $\varphi_{Z_i} (t)$ up to at least the second order (from moment generating property theorem). So, we have 
      \begin{align*}
        \varphi_{Z_i} (t) & = 1 + \frac{\mathbb{E}[Z_i]}{1!} (i t)^1 + \frac{\mathbb{E}[Z_i^2]}{2!} (i t)^2 + o (t^2) \\
        & = 1 + 0 + \frac{1}{2} (i t)^2 + o (t^2) \\
        & = 1 - \frac{1}{2} t^2 + o(t^2) 
      \end{align*}
      Now calculate the CF of $U_n$. Since the $Z_i$'s are iid, we can get 
      \begin{equation}
        \varphi_{U_n} (t) = \Big( \varphi_{Z_i} \big( \frac{t}{\sqrt{n}} \big) \Big)^n = \Big( 1 - \frac{t^2}{2n} + o \big( \frac{t^2}{n} \big) \Big)^n
      \end{equation}
      The $o(t^2 / n)$ term vanishes as $n \rightarrow \infty$, and using the limit $e^x = \lim_{n \rightarrow \infty} \big( 1 + \frac{x}{n} \big)^n$, we have 
      \begin{equation}
        \lim_{n \rightarrow \infty} \varphi_{U_n} (t) = \lim_{n \rightarrow \infty} \Big( 1 - \frac{t^2}{2n} + o \big( \frac{t^2}{n} \big) \Big)^n = e^{-t^2 / 2}
      \end{equation}
      which is precisely the CF of a standard Gaussian random variable. Since CFs are unique, our result is proven. Essentially, we have proved convergence in distribution of random variables by showing convergence of their characteristic functions. 
    \end{proof}

    A big misconception is that this normalized sum has PDF that converges to a bell curve. It is the CDF (by definition of convergence in distribution) that converges to that of a Gaussian. That way, we can state this for discrete, continuous, mixtures: doesn't matter. They don't even need to have a density, since if we just took a bunch of Bernoulli's, the PMF of their sum would never be defined for an irrational number like $\pi$. But it would be defined for the CDF, and even though the CDF of a discrete random variable will have jumps, these jumps would get smaller and smaller until it converges pointwise. Even if the $X_i$'s had densities, the CLT does not say that their mean converges to the PDF of a normal. Just because the CDF converges, it doens't mean the PDF will look similiar. 

    It also turns out (?) that we can use CLT to prove the weak law of large numbers, since (roughly speaking) as $n$ increases, the distribution of $\overline{X}_n$ concentrates more and more around $\mu$, and therefore the probability of $|\overline{X}_n - \mu| < \epsilon$ tends to $1$. 

\section{Conditional Expectation}

    Conditional expectation is extremely important, especially in the context of stochastic processes, which is talked about in more detail in another set of notes. 

    First, note that when we talk about the probability of event $A$ happening, or equivalently, the probability of $\omega \in A$, we can write this as the expected value of the indicator function $A$. 
    \begin{equation}
      \mathbb{P}(A) = \mathbb{E}[1_A]
    \end{equation}
    This will come in handy later in connecting conditional probability and expectation. 

    Now conditional expectation is quite tricky to understand at first. We will start by defining it given a $\sigma$-algebra and then given a random variable. 

    \begin{definition}[Conditional Expectation]
      Given a probability space $(\Omega, \mathcal{F}, \mathbb{P})$, a sub-$\sigma$-algebra $\mathcal{G} \subset \mathcal{F}$, and an $\mathcal{F}$-measurable random variable $X$ (with $\mathbb{E}[X] < \infty$), the \textbf{conditional expectation of $X$ given $\mathcal{G}$} is defined to be the $\mathcal{G}$-measurable random variable $Y = \mathbb{E}[X \mid \mathcal{G}]$ satisfying 
      \begin{equation}
        \int_A X \,d\mathbb{P} = \int_A Y \,d \mathbb{P}
      \end{equation}
      or equivalently, 
      \begin{equation}
        \mathbb{E}[X \cdot 1_A] = \mathbb{E}[Y \cdot 1_A]
      \end{equation}
      for all $A \in \mathcal{G}$. Any $Y$ satisfying these two conditions is said to be a \textbf{version} of $\mathbb{E}[X \mid \mathcal{F}]$. The critical detail to note here is that the conditional expectation $Y$, has the same expected value as $X$ does, not over just the whole $\mathcal{G}$, but \textit{in every subset $G$ of $\mathcal{G}$}. 
    \end{definition}

    We state without proof that $\mathbb{E}[X \mid \mathcal{G}]$ exists and is almost surely unique. For now, we can interpret this as the best approximation of the $\mathcal{F}$-measurable $X$ with the $\mathcal{G}$-measurable $Y$. Here is a useful analogy. Say that we have some "fine" function $X$ defined on the interval $[0, 1]$ with a fine Borel $\sigma$-algebra $\mathcal{F}$. 
    \begin{enumerate}
      \item If we are given some sub-$\sigma$-algebra $\mathcal{G}$ composed of $\emptyset, [0, 0.5], (0.5, 1], [0, 1]$, then $Y$ would be the step function defined constantly on these intervals. 
      \item If we are given a finer sub-$\sigma$-algebra $\mathcal{H}$ generated by $[0, 0.25), [0.25, 0.5), [0.5, 0.75), [0.75, 1]$, then this would give a $\mathcal{H}$-measurable function that is a better approximation of $X$. 
    \end{enumerate}
    \begin{center}
      \includegraphics[scale=0.33]{img/function_approximation_conditional.jpg}
    \end{center}

    Therefore, we can see that if $\mathbb{E}[X \mid \mathcal{G}]$ is $\mathcal{F}$-measurable, then 
    \begin{equation}
      X = \mathbb{E}[X \mid \mathcal{G}]
    \end{equation}
    since its value coincides with $X$ for every event in $\mathcal{F}$. One way to think about it is that $\mathbb{E}[X \mid \mathcal{G}]$ is the conditional expectation of $X$ (which is "detailed" up to resolution $\sigma(\mathcal{G})$) taken with a camera of resolution $\mathcal{G}$. The finer (bigger) the $\sigma$-algebra is, the higher the resolution. 

  \subsection{Properties of Conditional Expectation}

    \begin{theorem}[Tower Rule]
      The expectation of $X$ and its approximation always coincides. 
      \begin{equation}
        \mathbb{E}[X] = \mathbb{E}[\mathbb{E}[X \mid \mathcal{G}]]
      \end{equation}
    \end{theorem}

    \begin{lemma}
      Let $\mathbb{E}[|X|] , \mathbb{E}[|Y|] < \infty$. Then, 
      \begin{enumerate}
        \item Conditional expectation is linear
        \begin{equation}
          \mathbb{E}[a X + b Y \mid \mathcal{G}] = a \mathbb{E}[X \mid \mathcal{G}] + b \mathbb{E}[Y \mid \mathcal{G}]
        \end{equation}
        
        \item If $X \leq Y$, then 
        \begin{equation}
          \mathbb{E}[X \mid \mathcal{G}] \leq \mathbb{E}[Y \mid \mathcal{G}]
        \end{equation}
      \end{enumerate}
    \end{lemma}

    \begin{theorem}[Jensen's Inequality]
      If $\varphi$ is convex and $\mathbb{E}[|X|], \mathbb{E}[|\varphi(X)|] < \infty$, then 
      \begin{equation}
        \varphi(\mathbb{E}[X \mid \mathcal{G}]) \leq \mathbb{E}[\varphi(X) \mid \mathcal{G}]
      \end{equation}
    \end{theorem}

    \begin{theorem}
      Conditional expectation is a contraction in $L^p$, $p \geq 1$. 
    \end{theorem}

    \begin{theorem}
      If $X$ is $\mathcal{F}$-measurable and $\mathbb{E}[|Y|], \mathbb{E}[|XY|] < \infty$, then 
      \begin{equation}
        \mathbb{E}[XY \mid \mathcal{F}] = X \mathbb{E}[Y \mid \mathcal{F}]
      \end{equation}
    \end{theorem}

    \begin{theorem}
      Suppose $\mathbb{E}[X^2] < \infty$. Then, $\mathbb{E}[X \mid \mathcal{G}]$ is the $\mathcal{G}$-measurable function $Y$ that minimizes the mean squared error 
      \begin{equation}
        \mathbb{E}[ (X - Y)^2]
      \end{equation}
    \end{theorem}

    This gives a nice geometric interpretation of $\mathbb{E}[X \mid \mathcal{G}]$. Given that $X$ lives in the Hilbert space $L^2_\mathcal{F} (\Omega)$, $\mathbb{E}[X \mid \mathcal{G}]$ is the projection of $X$ onto the subspace $L^2_\mathcal{G} (\Omega)$. 
    \begin{center}
      \includegraphics[scale=0.3]{img/Hilbert_Space_projection.jpg}
    \end{center}
    Therefore, we can change the way think about $\mathbb{E}[X]$. It is not just a value, but rather, we can think of it as our best prediction of $X$ given no information. Specifically, 
    \begin{equation}
      \mathbb{E}[X] = \mathbb{E}[X \mid \{\emptyset, \Omega\}]
    \end{equation}
    That is, letting $\mathcal{G}$ be the trivial $\sigma$-algebra, we must find the best approximation of $X$ that is $\mathcal{G}$-measurable. But any random variable that is $\mathcal{G}$-measurable must be constant, since if we take the preimage of any singleton set $\{x\} \in \mathcal{R}$, then it must be either $\emptyset$ ($X$ does not map to it) or $\Omega$ ($X$ maps all of $\Omega$ to it).

  \subsection{Perfect Information vs No Information}

    Now let us state some properties on how certain $\sigma$-algebras can change the conditional expectation of certain random variables. 

    \begin{theorem}[Perfect Information]
      If $X$ is $\mathcal{G}$-measurable, then 
      \begin{equation}
        \mathbb{E}[X \mid \mathcal{G}] = X
      \end{equation}
      That is, the values of $X$ are defined on $\sigma(X) \subset \mathcal{F}$ and so has a detail level of $\sigma(X)$. But if we condition it on an even finer $\mathcal{G} \supset \sigma(X)$, then we are taking a picture of $X$ with something that has overly high resolution, and so our best approximation of $X$ is $X$ itself. Indeed, if $X$ lives in $L_\mathcal{G} (\Omega)$, then its projection onto $L_\mathcal{G} (\Omega)$ is $X$ itself. 
    \end{theorem}

    \begin{theorem}[Irrelevant Information]
      If $X$ is independent of $\mathcal{G}$, i.e. $\sigma(X)$ and $\mathcal{G}$ are independent $\sigma$-algebras, then 
      \begin{equation}
        \mathbb{E}[X \mid \mathcal{G}] = \mathbb{E}[X]
      \end{equation}
      That is, our best approximation of $X$ given information $\mathcal{G}$ is $\mathbb{E}[X]$ itself, i.e. if you don't know anything about $X$, then the best guess is the mean $\mathbb{E}[X]$. To see why, note that independence means that for all $A \in \mathcal{G}$ and $B \in \mathcal{R}$, 
      \begin{equation}
        \mathbb{P}(X^{-1} (B) \cap A) = \mathbb{P}(X^{-1}(B)) \cdot \mathbb{P}(A)
      \end{equation}
    \end{theorem}

    \begin{theorem}[Trivial Information]
      If $\mathcal{G} = \{\emptyset, \Omega\}$, then 
      \begin{equation}
        \mathbb{E}[X \mid \mathcal{G}] = \mathbb{E}[X]
      \end{equation}
      This makes sense since we're trying to measure $\sigma(X)$-measurable $X$ with the trivial $\mathcal{G}$, and the only function that is measurable w.r.t. the trivial $\sigma$-algebra is a constant function (since the preimage of every Borel set in $\mathcal{R}$ must be either $\Omega$ or $\emptyset$). This is the same as projecting $X$ to the line of constant functions in $L_\mathcal{F}(\Omega)$. 
    \end{theorem}

    \begin{theorem}
      If $\mathcal{F}_1 \subset \mathcal{F}_2$, then 
      \begin{enumerate}
        \item $\mathbb{E}[ \mathbb{E}[X \mid \mathcal{F}_1] \mid \mathcal{F}_2] = \mathbb{E}[X \mid \mathcal{F}_1]$. 
        \item $\mathbb{E}[ \mathbb{E}[X \mid \mathcal{F}_2] \mid \mathcal{F}_1] = \mathbb{E}[X \mid \mathcal{F}_1]$. 
      \end{enumerate}
      In other words, the smaller $\sigma$-algebra always wins. 
    \end{theorem}

    We can see this visually since in both cases, we are projecting $X$ onto $L^2_{\mathcal{F}_1} (\Omega)$ and onto $L^2_{\mathcal{F}_2} (\Omega)$, but either way, we end up in $L^2_{\mathcal{F}_1} (\Omega)$. Additionally, this is also consistent with our camera analogy, where $\mathbb{E}[X \mid \mathcal{G}]$ is like taking a picture of random variable $X$ with a camera of resolution $\mathcal{G}$. Conditional expectation is essentially an averaging/blurring operator. So, $\mathbb{E}[\mathbb{E}[X \mid \mathcal{G}] \mid \mathcal{H}]$ is like taking a picture of $X$ with resolution $\mathcal{H}$ and then with $\mathcal{G}$. The lower resolution would always win. 

  \subsection{Computation of Conditional Expectation}

    \begin{definition}
      Given probability space $(\Omega, \mathcal{F}, \mathbb{P})$, the conditional expectation of $Y$ given $X$ is the random variable 
      \begin{equation}
        \mathbb{E}[X \mid Y] \coloneqq \mathbb{E}[X \mid \sigma(Y)]
      \end{equation}
      Note that since both $X$ and $Y$ are random variables, they are both $\mathcal{F}$-measurable. However, this doesn't mean that they may be $\mathcal{G}$-measurable for some sub-$\sigma$-algebra $\mathcal{G}$. So, as long as $\sigma(Y) \subset \sigma(X)$ (neither of which may be $\mathcal{F}$), we have some nontrivial approximation. 
    \end{definition}

    Now let's introduce a new way to think about expectation and conditional expectation in general. 
    \begin{enumerate}
      \item The first step is to think of $\mathbb{E}[X]$ not as a value $\mu$ but as the best estimate for the value of a random variable $X$ in the absence of any information. To minimize the squared error 
      \begin{equation}
        \mathbb{E}[ (X - e)^2] = \mathbb{E}[ X^2 - 2 e X + e^2] = \mathbb{E}[X^2] - 2 e \mathbb{E}[X] + e^2
      \end{equation}
      we differentiate with respect to $e$ to obtain $2e - 2 \mathbb{E}[X] = 0 \implies e = \mathbb{E}[X]$. For example, if I throw a fair die and you have to estimate its value $X$, according to the analysis above, your best bet is to guess $\mathbb{E}[X] = 3.5$ since $\Omega = \{1, 2, 3, 4, 5, 6\}$. On specific rolls of the die, this will be an over-estimate of an underestimate, but on the long run it minimizes the mean square error. 
      
      \item If we \textit{do} have additional information, then we use conditional expectation. Suppose that I tell you that $X$ is an even number. Then, I would guess that the possible values of $X$ are $\{2, 4, 6\}$, and so the our conditional expectation is $4$. Similarly, if I told you that $X$ is odd, then the conditional expectation is $3$. This additional information can be put into a random variable $Y: \Omega \rightarrow \mathbb{R}$ defined 
      \begin{equation}
        Y(\omega) = \begin{cases} 0 & \text{ if } \omega = 2, 4, 6 \\ 1 & \text{ if } \omega = 1, 3, 5 \end{cases}
      \end{equation}
      Then, we can say that $\mathbb{E}[X \mid Y = 0] = 4$ and $\mathbb{E}[X \mid Y = 1] = 3$. We can interpret this as the conditional expectation given the $\sigma$-algebra generated by these two sets $\{2, 4, 6\}$ and $\{1, 3, 5\}$. 
      
      \item Now, imagine that I roll the die and I tell you the parity of $X$. You should see that a single numerical response cannot cover both cases. You would respond $3$ if I tell you $X$ is odd ($Y = 1$) and $4$ if I tell you $X$ is even ($Y = 0$). A single numerical response is not enough because the particular piece of information I give you is \textit{itself random}. In fact, your response is necessarily a function of this particular piece of information, represented in our notation as 
      \begin{equation}
        g(Y) = \mathbb{E}[X \mid Y] = \begin{cases} 3 & \text{ if } Y = 1 \\ 4 & \text{ if } Y = 0 \end{cases}
      \end{equation}
      This is a function of $Y$, and it is consistent with our understanding of $\mathbb{E}[X \mid Y]$ as our "best estimate" of $X$ with random variable $Y$. 
      \begin{center}
        \includegraphics[scale=0.3]{img/conditional_expect_dice.jpg}
      \end{center}
    \end{enumerate}

    From the visual above, we can see that we take the joint distribution $X \times Y$, and for each value $Y = y$, we can estimate $X$ as $\mathbb{E}[X \mid Y = y]$. But now there is the additional uncertainty of what value $Y$ will take, which turns this value estimate $\mathbb{E}[X \mid Y = y]$ in a distribution $\mathbb{E}[X \mid Y]$. So, for the discrete case, 
    \begin{equation}
      \mathbb{P}\big( \mathbb{E}[X \mid Y] = \mathbb{E}[X \mid Y = y]) = \mathbb{P}(Y = y)
    \end{equation}

    Now we can talk about how to compute conditional expectation. In essence, the conditional expectation $\mathbb{E}[X \mid Y]$ is simply a function of a random variable $Y$ that is this best approximation. Given a joint random variable $(X, Y): \Omega \rightarrow \mathbb{R}^2$, we can fix a value of $Y = y$. Therefore, we are given the information that event $Y^{-1}(\{y\})$ happened, and so we can construct our conditional distribution $X \mid Y = y$, which defines a new probability measure. Taking the expectation of that gives us a number. 


    \begin{definition}[Discrete Conditional Expectation Given $Y = y$]
      Let $X, Y$ be discrete random variables, with joint random variable $(X, Y): \Omega \rightarrow \mathbb{R}^2$ and its joint PMF $p_{X, Y} (x, y)$. Recall that the conditional PMF is 
      \begin{equation}
        p_{X\mid Y}(x \mid y) \coloneqq \frac{p_{X, Y} (x, y)}{p_Y (y)}
      \end{equation}
      The \textbf{conditional expectation} of $X$ given $Y = y$ is 
      \begin{equation}
        \mathbb{E}[X \mid Y = y] = \sum_{x \in \mathcal{X}} x \, p_{X \mid Y} (x \mid y)
      \end{equation}
    \end{definition}

    \begin{definition}[Continuous Conditional Expectation Given $Y = y$]
      Let $X, Y$ be jointly continuous with joint PDF $f_{X, Y} (x, y)$. Recall that the conditional PDF is 
      \begin{equation}
        f_{X \mid Y} (x \mid y) \coloneqq \frac{f_{X, Y} (x, y)}{f_Y (y)}
      \end{equation}
      The \textbf{conditional expectation} of $X$ given $Y = y$ is 
      \begin{equation}
        \mathbb{E}[X \mid Y = y] = \int_{x \in \mathbb{R}} x \, f_{X \mid Y} (x \mid y) \, dx
      \end{equation}
      Again, we can set $\psi(y) \coloneqq \mathbb{E}[X \mid Y = y]$, which is a function of $y$ and therefore a random variable. 
    \end{definition}

    As a visual, we can take a "slice" of the joint distribution of some value of $Y$, look at the distribution of $X$ on this slice, and compute its expectation. That is, for every value of $Y = y$, there exists some (conditional) distribution of $X$ with PMF of $p_{X \mid Y} (x \mid y)$ or PDF of $f_{X \mid Y} (x \mid y)$. 
    \begin{center}
      \includegraphics[scale=0.3]{img/conditional_exp.jpg}
    \end{center}
    So given a value of $Y = y$, we generally know something about $X$ (e.g. if I know humidity, I know something about the temperature) and want to find the best estimate of $X$. This is precisely the conditional expectation $\mathbb{E}[X \mid Y = y]$, and we can interpret this as a regression function $\psi(y) \coloneqq \mathbb{E}[X \mid Y = y]$, which predicts the expected value of $X$ given $Y = y$. 

    But since we don't know what exactly $Y$ is, this process is random itself, and it is only after this $Y$ is realized that we can provide the expected value of $X$. Thus, by replacing the little $y$'s with the big $Y$'s, we can construct a random variable that will estimate $X$ for us given $Y$, denoted $\mathbb{E}[X \mid Y] = \psi(Y)$. This turns out to be a $\sigma(Y)$-measurable random variable itself. 

    \begin{example}
      Let $f_{X, Y} (x, y) = \frac{1}{x}$ for $0 < y \leq x \leq 1$. Find $\mathbb{E}[Y \mid X]$. We first calculate the marginal density of $X$, which will allow us to calculate the conditional density of $Y$: 
      \begin{equation}
        f_X (x) = \int_0^x \frac{1}{x} \,dy = \frac{y}{x} \bigg|_0^x = 1 \implies f_{Y \mid X} (y \mid x) = \frac{f_{X, Y} (x, y)}{f_X (x)} = \frac{1/x}{1} = \frac{1}{x} \text{ for } 0 < y \leq x \leq 1
      \end{equation}
      Since the conditional density of $Y$ is not dependent on $x$, $Y$ is uniform from $0$ to $x$. Now calculate the expectation: 
      \begin{equation}
        \mathbb{E}[Y \mid X = x] = \int_0^x y \cdot \frac{1}{x} \,dy = \frac{x}{2}
      \end{equation}
      and so the conditional expectation is 
      \begin{equation}
        \mathbb{E}[Y \mid X] = \frac{1}{2} X
      \end{equation}
    \end{example}

    A fundamental result in statistical learning theory is that if we have two random variables $X$ and $Y$, the best predictor of $Y$ as a function of $X$ is the conditional expectation $\mathbb{E}[Y \mid X]$. 

    \begin{theorem}
      Let us have two random variables $X$ and $Y$, with $g(X) = \mathbb{E}[Y \mid X]$. Then, the function $g$ minimizes the cost function $\mathbb{E}[ (Y - h(X))^2]$. That is, 
      \begin{equation}
        \inf_{h \text{ meas.}} \mathbb{E}[( Y - h(X))^2] = \mathbb{E} [(Y - g(X))^2]
      \end{equation}
    \end{theorem}

    We restate the tower rule again. 

    \begin{theorem}[Tower Rule]
      We know that $\mathbb{E}[Y \mid X]$ is the random variable $\psi(X)$ that is a transformed version of $X$. Then, we have
      \begin{equation}
        \mathbb{E}[ \mathbb{E}[Y \mid X]] = \mathbb{E}[Y]
      \end{equation}
      This is confusing notation due to the iterated expectations, but note that the term on the inside is a transformed random variable of $X$, while the expectation on the outside computes the expectation of this transformed random variable. So, letting $\psi(X) = \mathbb{E}[Y \mid X]$, we can equivalently write 
      \begin{equation}
        \mathbb{E}[ \psi(X)] = \mathbb{E}[Y]
      \end{equation}
      Intuitively, this makes sense, since $\mathbb{E}[Y \mid X]$ is the random variable that tries to "model" $Y$ given (random) information from $X$, so its expectation must be the expectation of $Y$ itself. 
    \end{theorem}
    \begin{proof}
      We can just expand this out. We will do it for the discrete case. 
      \begin{align*}
        \mathbb{E}[ \mathbb{E}[Y \mid X]] & = \sum_x p_X (x) \, \underbrace{\mathbb{E}[Y \mid X = x]}_{\psi(x)} \\
        & = \sum_x \bigg( p_X (x) \cdot \sum_y y \cdot p_{Y \mid X} (y \mid x) \bigg) \\
        & = \sum_x \bigg( p_X (x) \cdot \sum_y y \cdot \frac{p_{X, Y} (x, y)}{p_X (x)} \\
        & = \sum_{x, y} y \cdot p_{X, Y} (x, y) \\
        & = \sum_y \bigg( y \cdot \sum_x p_{X, Y} (x, y) \bigg) \\
        & = \sum_y y \cdot p_Y (y) \\
        & = \mathbb{E}[Y]
      \end{align*}
    \end{proof}

    \begin{example}
      Consider the random sum of random variables $S_N = \sum_{i=1}^N X_i$, where $X_i$ are iid and $N$ is independent of $X_i$'s. Then, we can use the tower rule to write $\mathbb{E}[S_N] = \mathbb{E}[\mathbb{E}[S_N \mid N]]$. $\mathbb{E}[S_N \mid N]$ is a transformed random varibale of $N$, and to compute its closed form we should just compute $\mathbb{E}[S_N \mid N = n]$ and replace $n$ with $N$. 
      \begin{equation}
        \mathbb{E}[S_N \mid N = n] = \mathbb{E}[S_n] = \mathbb{E} \bigg( \sum_{i=1}^n X_i \bigg) = n \mathbb{E}[X]
      \end{equation}
      remember that $\mathbb{E}[X]$ is just a number, so replacing $n$ with $N$ gives $\mathbb{E}[S_N \mid N] = N \mathbb{E}[X]$, i.e. the random variable $N$ multiplied by $\mathbb{E}[X]$. Therefore, 
      \begin{equation}
        \mathbb{E}[\mathbb{E}[S_N \mid N]] = \mathbb{E}[N \mathbb{E}[X]] = \mathbb{E}[N] \, \mathbb{E}[X]
      \end{equation}
      This makes sense intuitively, since we want to approximate this value by taking the expected value of $X$ and multiplying it by the expected number of summands. 
    \end{example}

    Now, we can simply use the property to talk about what $\mathbb{P}(X \mid Y)$ means. Formally, using the fact that the probability that $X$ will realize in $A$, i.e. $\mathbb{P}(X \in A) = \mathbb{E}_X[1_A]$, we can define the conditional probability as 
    \begin{equation}
      \mathbb{P}(X \in A \mid Y) = \mathbb{E}[ 1_X \mid Y]
    \end{equation}
    We can interpret this in multiple ways, in increasing level of rigor. Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space, with random variables $X, Y$ with their probability laws $\mathbb{P}_X, \mathbb{P}_Y$. 
    \begin{enumerate}
      \item When $Y$ realizes, we can use this definition to have a better educated guess of where $X$ will land. But since $Y$ is random, so is our guess. 
      
      \item Let us have the joint distribution $(X, Y)$. Given that $Y = y$, we can take the conditional distribution $X \mid Y = y$ and compute the event that this random variable lands in $A$ by replacing the little $y$'s with the big $Y$'s. 
      
      \item Let $A$ be an event in $\mathcal{R}$. Then, the probability that $X$ will land in $A$ is 
      \begin{equation}
        \mathbb{P}(X \in A) = \mathbb{P}_X(A) = \mathbb{P}(X^{-1} (A))
      \end{equation}
      where the first is abuse of notation, the second is the probability law of $\mathbb{R}$, and the third is the probability law of $\Omega$. Then, $1_A$ generates a $\sigma$-algebra $\sigma(1_A)$ on $\Omega$, consisting of the sets $\{\emptyset, X^{-1}(A), X^{-1}(A)^c, \Omega\}$. 
    \end{enumerate}

  \subsection{Conditional Expectation given Multiple Random Variables}

    Now the $\sigma$-algebra generated by multiple random variables should intuitively be bigger than the $\sigma$-algebra generated by one random variable. We can't simply take the union of the individual $\sigma$-algebras. 

    \begin{definition}[$\sigma$-algebra generated by Multiple Random Variables]
      Given random variables $\{X_i\}_{i \in I}$ indexed by some set (possibly uncountable), the $\sigma$-algebra generated by this collection is defined 
      \begin{equation}
        \sigma(X_1, \ldots, X_n) \coloneqq \sigma \bigg( \bigcup_{i \in I} \sigma(X_i) \bigg)
      \end{equation}
    \end{definition}

    Let us look at $\mathbb{E}[X \mid Y, Z]$ and compare it to $\mathbb{E}[X \mid Y]$. From this definition, we know that the information about $X$ contained in $\sigma(Y, Z)$ is at least as great as the corresponding information in $\sigma(Y)$. Therefore, we can simply define conditional expectation as such: 

    \begin{definition}[Conditional Expectation given Multiple Random Variables]
      Given random variables $\{X_i\}_{i \in I}$, the conditional expectation is defined 
      \begin{equation}
        \mathbb{E}[Y \mid \{X_i\}_{i \in I} ] = \mathbb{E}[ Y \mid \sigma(\{X_i \}_{i \in I}]
      \end{equation}
      which is the random variable where 
      \begin{equation}
        \int_A Y \, d\mathbb{P} = \int_A \mathbb{E}[Y \mid \{X_i\}_{i \in I} ] \, d\mathbb{P}
      \end{equation}
      for all $A \in \sigma(\{X_i \}_{i \in I})$. 
    \end{definition}

  \subsection{Conditional Variance}

    Similar to conditional expectation, we can define the conditional variance $\mathrm{Var}(Y \mid X = x)$ as a function $h(x)$ that outputs the variance of $Y$ given $X = x$. We have
    \begin{align}
      \mathrm{Var}(Y \mid X = x) & = \mathbb{E}\big[ ( Y - \mathbb{E}[Y \mid X = x] )^2 \mid X = x \big]
    \end{align}

    \begin{definition}[Conditional Variance]
      The \textbf{conditional variance} of $Y$ given $X$ is defined as 
      \begin{equation}
        \Var(Y \mid X) = \mathbb{E} \big[ \big( Y - \mathbb{E}[Y \mid X] \big)^2 \mid X \big]
      \end{equation}
      which tells us how much variance is left if we use $\mathbb{E}[Y \mid X]$ to predict $Y$. 
    \end{definition}

\section{Order Statistics}

    Let $X_1, X_2, ..., X_n$ be a finite collection of independent, identically distributed random variables. Suppose that they are continuously distributed with density $f$ and CDF $F$. 

    \begin{definition}[Order Statistic]
      Define the random variable $X_{(k)}$ to be the $k$th ranked value, called the \textit{$k$th order statistic}. This means that 
      \begin{equation}
        X_{(1)} = \min\{X_1, X_2, ..., X_n\}, \;\; X_{(n)} = \max\{X_1, X_2, ..., X_n\}
      \end{equation}
      and in general, for any $k \in \{1, 2, ..., n\}$, 
      \begin{equation}
        X_{(k)} = X_j \text{ if } \sum_{l=1}^n \mathbb{I}_{X_l < X_j} = k - 1
      \end{equation}
      which means that exactly $k-1$ of the values of $X_l$ are less than $X_j$. Since $F$ is continuous, 
      \begin{equation}
        X_{(1)} < X_{(2)} < ... < X_{(n)}
      \end{equation}
      holds with probability $1$. This leads us to define the random variable $X_{(k)}$ representing the $k$th order statistic.
      \begin{equation}
        f_{(k)} (y) = \begin{cases} 
          n \, \binom{n-1}{k-1} y^{k-1} (1-y)^{n-k} & y \in (0, 1) \\
        0 & y \not\in (0,1)
        \end{cases}
      \end{equation}
      That is, $X_{(k)}$ has the Beta$(k, n-k_1)$ distribution. 
    \end{definition}

  \subsection{Poisson Arrival Process}

    A \textbf{Poisson Arrival Process} with rate $\lambda > 0$ on the interval $[0, \infty)$ is a model for the occurence of some events which may have at any time. We can interpret the process as a collection of random points in $[0, \infty)$ which are the times at which the arrivals occur. 
    \begin{enumerate}
      \item \textit{Interpretation 1}. Set $T_0 = 0$. The arrival times are random variables $0 < T_1 < T_2 < T_3 < ...$ such that the inter-arrival waiting times
      \begin{equation}
        W_k = T_k - T_{k-1}, \;\;\; k \geq 0
      \end{equation}
      have the property that $\{W_k\}_{k=1}^\infty$ are independent Exp$(\lambda)$ random variables. 

      \item \textit{Interpretation 2}. For any interval $I \subset [0, \infty)$, let 
      \begin{equation}
        N_I \equiv \text{ number of arrivals that occur in interval } I
      \end{equation}
      Then, $N_I \sim$ Poisson$(\lambda |I|)$, and for any collection of disjoint intervals $I_1, I_2, ..., I_n$, the random variables 
      \begin{equation}
        \{N_{I_k}\}_{k=1}^n
      \end{equation}
    are independent. 
    \end{enumerate}

    \begin{theorem}
      These two interpretations of the arrival process are equivalent. 
    \end{theorem}
    \begin{proof}
      In the 2nd interpretation, the statement $N_I \sim$ Poisson$(\lambda |I|)$ means that 
      \begin{equation}
        \mathbb{P}(N_I = m) = e^{-\lambda |I|} \frac{(\lambda |I|)^m}{m!}, \;\;\; m = 0, 1, 2, 3, ...
      \end{equation}
      where $|I|$ is the length of interval $I$. From the first perspective, notice that 
      \begin{equation}
        T_k = W_1 + W_2 + ... + W_k
      \end{equation}
      so that the $k$th arrival time $T_k$ is a sum of $k$ independent Exp$(\lambda)$ random variables. Thus, 
      \begin{equation}
        T_k \sim \text{Gamma}(k, \lambda)
      \end{equation}
      and therefore has density
      \begin{equation}
      \lambda e^{-\lambda t} \frac{(\lambda t)^{k-1}}{(k-1)!}, \;\;\; t>0
      \end{equation}
      Note that the arrival times $T_i$ are not independent of each other, but the wait times $W_i$ are indeed independent. 
    \end{proof}

    We can slightly modify this to create a Poisson arrival process over some finite time horizon $[0, L]$. Again, you can do this two ways: 
    \begin{enumerate}
      \item Starting with independent Exp$(\lambda)$ random variables $W_1, W_2, ...$, we define
      \begin{equation}
        T_k = \sum_{i=1}^k W_i
      \end{equation}
      Once you have $T_k > L$, stop. 
      \item We let $N \sim$ Poisson$(\lambda L)$, since we are only working in finite interval $L$. Given $N = n$, let $U_1, U_2, ..., U_n \sim$ Uniform$([0, L])$. These define the arrival times, and let us order them to get
      \begin{equation}
        T_k = U_{(k)}, \;\; k = 1, 2, ..., N
      \end{equation}
      where $U_{(k)}$ is the $k$th ordered point, with $T_1 = \min(U_1, ..., U_N)$. 
    \end{enumerate}

    \begin{lemma}[Memoryless Property]
      The Exp$(\lambda)$ distribution has the property that for all $t, s \geq 0$, 
      \begin{equation}
        \mathbb{P}(W > t + s \; | \; W > t) = \mathbb{P}(W > s)
      \end{equation}
      which is called the \textit{memoryless property}. We can interpret this in the following way. Let $W$ be the time you have to wait for the first arrival. Given that you already waited $t$ units of time, the probability that you have the wait $s$ additional units of time is just the probability that you wait at least $s$ from the beginning. That is, knowing that $t$ units of time have elapsed does not affect the distribution of the remaining waiting time. 
    \end{lemma}

    \begin{theorem}
      Let $W$ be a continuously distributed random variable. Then $W \sim$ Exp$(\lambda)$ for some $\lambda > 0$ if and only if $W$ satisfies the memoryless property. 
    \end{theorem}

\section{Markov Chains}

  I have an entire set of notes dedicated to stochastic processes, but we talk about it on a basic level here. 

  \subsection{Discrete Time Chains}

    \begin{definition}[Markov Chain]
      A \textbf{Markov chain} is a sequence of random variables $\{X_n\}_{n=0}^\infty$, which take values in some set $\mathcal{S}$, called the \textbf{state space} satisfying the \textbf{Markov property}. Since we are working with discrete time chains, we will assume that $\mathbb{S}$ is a countable (and in most cases, finite). Thus, the $X_n$ will all be discrete random variables. We can also think of $X_n$ as a discrete "time" index; that is, $X_n$ is the state of the system at time $n$. Therefore, the sequence of random variables models a system evolving in a random way. 
    \end{definition}

    \begin{definition}[Markov Property]
      A sequence of random variables $\{X_i\}$ satisfies the \textbf{Markov property} if 
      \begin{equation}
        \mathbb{P}(X_{n+1} = y \; | \; X_n = x_n, X_{n-1} = x_{n-1}, ..., X_0 = x_0\} = \mathbb{P}(X_{n+1} = y \; | \; X_n = x_n\}
      \end{equation}
      holds for any choice oc states $y, x_n, x_{n-1}, ..., x_0 \in \mathcal{S}$ and for any $n \geq 1$. 
    \end{definition}

    Colloquially, given that one is at state $X_n = x_n$, knowing all the previous states does not help in predicting $X_{n+1}$. Knowing only the current state is relevant in predicting the next one. We can model this entire system using a matrix. 

    \begin{definition}[Transition Matrix]
      Assuming that the chain is \textit{time-homogeneous}, the \textit{transition probability matrix} $P$ has elements $P_{x y}$ defined
      \begin{equation}
        P_{x y} = P(x, y) = \mathbb{P}(X_1 = y \,|\, X_0 = x) = \mathbb{P}(X_{n+1} = y \,|\, X_n = x)
      \end{equation}
      which is the probability of moving from state $x$ to state $y$ in one step. The time homogeneous condition refers to the last equality; that is, the one-step transition probabilities don't change with the time index $n$. Note that if $\mathcal{S}$ is finite, then $P$ is a $|S| \times |S|$ matrix, and if $\mathcal{S}$ is countably infinite, then $P$ is an infinite-dimensional matrix. The axioms of probability imply that $A^T$ is an entry-wise nonnegative stochastic matrix.
    \end{definition}

    \begin{example}[Random Walks]
      A \textit{random walk} on the integers $\mathcal{S} = \mathbb{Z}$ where a point has equal probability of moving right or left can be modeled with the probability function. 
      \begin{equation}
        P(x, y) = \mathbb{P}(X_{n+1} = y \, | \, X_n = x) = \begin{cases}
        \frac{1}{2} & y = x + 1 \\
        \frac{1}{2} & y = x - 1\\
        0 & otherwise
        \end{cases}
      \end{equation}
      This can be generalized to multiple dimensional random walks on graphs with probability function 
      \begin{equation}
        P(x, y) = \frac{1}{\text{deg}(x)}
      \end{equation}
      where deg$(x)$ is the number of adjacent nodes to node $x$. In this way, the point hops randomly from node to node, and if the graph is connected, then the walker can visit any vertex in the graph. 
    \end{example}

    \begin{example}[Discrete Moran Model]
      Consider a population of size $N$. Each individual is one of two types (say, red or blue). At each time step, the system evolves in the following way: First, one of the individuals is chosen uniformly at random to be eliminated from the population; and another individual is chosen uniformly at random to produce one offspring identical to itself. These two choices are made independently. So, if a red individual is chosen to reproduce, and a blue one is chosen for elimination, then the total number of red particles increases by one and the number of blue particles decreases by one. If a red is chosen for reproduction and a red is chosen for elimination, then there is no net change in the number of reds and blues. Let $X_n$ be the number of red individuals at time $n$. The transition matrix for this chain is
      \begin{equation}
        P_{i j} = \begin{cases}
        \frac{i}{N} \bigg(\frac{N-i}{N} \bigg) & j=i-1, i \neq 0 \\
        \bigg(\frac{N-i}{N} \bigg) \frac{i}{N} & j=i+1, i \neq N \\
        1 - 2 \bigg(\frac{N-i}{N} \bigg) \frac{i}{N} & j = i \\
        0 & \text{otherwise}
        \end{cases}
      \end{equation}
      Note that the states $X_n = 0$ and $X_n = N$ are absorbing states, which represents a phenomenon called \textit{fixation}. 
    \end{example}

    \begin{definition}[Absorbing State]
      A certain state $F$ in the state space $\mathcal{S}$ of a Markov chain is called an \textbf{absorbing state} if
      \begin{equation}
        \mathbb{P}(X_{n+1} = F \; | \; X_n = F) = 1 \iff \mathbb{P}(X_{n+1} \neq F \; | \; X_n = F) = 0
      \end{equation}
    \end{definition}

    \begin{theorem}
      Let there exist a time homogeneous Markov chain with transition probability matrix $P$. Given a probability distribution $\nu_n$ (a row vector) representing the a state of a system at time $t=n$, the probability distribution of which state the system will be at when $t=n+1$ can be calculated by 
      \begin{equation}
        \nu_{n+1} = \nu_n P
      \end{equation}
      The probability distribution of the state of the system at $t=n+k$ can be calculated by summing up all of the possible probabilities that lead to each state at $t=n+k$. It is calculated equivalently as matrix multiplication: 
      \begin{equation}
        \nu_{n+k} = \nu_n P^k
      \end{equation}
    \end{theorem}

    \begin{definition}[Initial Distribution]
      The distribution $\nu$ of a Markov chain at time $t=0$ is called the \textit{initial distribution} for the chain. That is, $\nu$ is the initial distribution if 
      \begin{equation}
        \mathbb{P}(X_0 = x) = \nu(x)
      \end{equation}
    \end{definition}

    \begin{definition}[Stationary Distribution]
      An \textit{invariant distribution}, or \textit{stationary distribution}, is a probability distribution $\pi$ such that 
      \begin{equation}
        \pi P = \pi
      \end{equation}
      This means that 
      \begin{equation}
        \pi P^k = \pi
      \end{equation}
      for all $k \in \mathbb{N}$. We can equivalently call $\pi$ the left eigenvector of matrix $P$ with eigenvalue $1$. If $\pi$ is an invariant distribution for the chain, and $X_0 \sim \pi$, then the distribution of $X_n$ does not change with $n$; it is invariant. Note that this does not mean that $X_n$ is constant; rather, it means that the distribution of $X_n$ is not changing. 
    \end{definition}

    \begin{example}
      Let us have a two node system with nodes labeled $L$ and $R$. That is, $\mathcal{S} = \{L, R\}$. Consider a chain on this state space with transition probability matrix. 
      \begin{equation}
        P = \begin{pmatrix}
        1-a & a \\ b & 1-b 
        \end{pmatrix}
      \end{equation}
      which can be visualized in the following diagram below.
      \begin{center}
      \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,
                          thick,main node/.style={circle,draw}]
          \node[main node] (R) {R};
          \node[main node] (L) [left of=R] {L};
          \path[every node/.style={font=\sffamily\small}]
          (L) edge [loop left] node {1-a} (L)
              edge [bend left] node {a} (R)
          (R) edge [loop right] node {1-b} (R)
              edge [bend left] node {b} (L);
      \end{tikzpicture}
      \end{center}

      Then, the stationary distribution is 
      \begin{equation}
        \pi = \Big( \frac{b}{a+b}, \frac{a}{a+b} \Big)
      \end{equation}
      Notice that if $a = b = 0$, then this definition is ill-defined, and any probability distribution is invariant since $P = I_2$, the identity matrix. 
    \end{example}

    \begin{definition}[Recurrent]
      A state $x \in \mathcal{S}$ is \textit{recurrent} if
      \begin{equation}
        \mathbb{P}(X_n = n \text{ for some } n \geq 1 \, | \, X_0 = x\} = 1
      \end{equation}
      That is, if the initial state is $x$, the chain has probability $1$ of returning to $x$ at some later time. If a state is not recurrent, then the state is said to be \textit{transient}. That is, if $x$ is transient, there is some positive probability that the chain will never return to $x$. 
    \end{definition}

    \begin{definition}[Communication]
      Two states $x, y \in \mathcal{S}$ are said to \textit{communicate}, denoted $x \leftrightarrow y$, if there are positive integers $n$ and $m$ such that 
      \begin{equation}
        P^{(n)} (x, y) > 0 \text{ and } P^{(m)} (y, x) > 0
      \end{equation}
      That is, there is some positive probability that the chain can go from $x$ to $y$ and from $y$ to $x$ in some number of steps. 
    \end{definition}

    \begin{definition}[Irreducible Chains]
      If all pairs $x, y \in \mathcal{S}$ communicate, then the chain is said to be \textit{irreducible}. If there exists a pair of states that do not communicate, then the chain is said to be \textit{reducible}. 
    \end{definition}

    Note that the notion of communication is an equivalence relation between states. That is, it satisfies the properties. 
    \begin{enumerate}
      \item $x \leftrightarrow x$.
      \item $x \leftrightarrow y \implies y \leftrightarrow x$.
      \item $x \leftrightarrow y, y \leftrightarrow z \implies x \leftrightarrow z$.
    \end{enumerate}
    This relation partitions the state space $\mathcal{S}$ uniquely into transient states and irreducible sub-chains
    \begin{equation}
      \mathcal{S} = T \cup C_1 \cup C_2 \cup ...
    \end{equation}
    More specifically, $T$ is the set of all transient states, and the sets $C_k$ are \textit{closed communication classes}, meaning that
    \begin{enumerate}
      \item For all $x, y \in C_k$, $x \leftrightarrow y$. 
      \item $P(x, z) = 0$ whenever $x \in C_k$ but $z \not\in C_k$. 
    \end{enumerate}
    Note that for all $x, y \not\in T$, $x$ and $y$ communicate if and only if $x$ and $y$ are in the same class $C_k$. Moreover, once the chain reaches one of the sets $C_k$, it cannot leave $C_k$. 

    \begin{definition}[Period]
      For any state $x \in \mathcal{S}$, the \textit{period} of $x$ is defined to be
      \begin{equation}
        d(x) \equiv \gcd \{n \geq 1 \; | \; P^{(n)} (x, x) > 0\}
      \end{equation}
    \end{definition}

    \begin{theorem}
      It follows that if two states $x$ and $y$ communicate, then they must have the same period: $d(x) = d(y)$. It naturally follows that if the chain is irreducible, then all states must have the same period, and we can define the period of the chain to be $d(x)$ for any $x$ we choose.
    \end{theorem}

    \begin{definition}
      If an irreducible chain has period $1$, the chain is said to be \textit{aperiodic}. Otherwise, the chain is \textit{periodic} with period $d > 1$. 
    \end{definition}

    \begin{theorem}
      Suppose $|\mathcal{S}| < \infty$. If the chain is irreducible, then there always exists a unique stationary distribution $\pi$. If the chain is also aperiodic, then for any initial distribution $\nu$, 
      \begin{equation}
        \lim_{k \rightarrow \infty} \nu P^k = \pi
      \end{equation}
      Hence
      \begin{equation}
        \lim_{k \rightarrow \infty} P^{(k)}(x, y) = \pi(y)
      \end{equation}
      for all $x, y \in \mathcal{S}$. Furthermore, for any function $F: \mathcal{S} \longrightarrow \mathbb{R}$, the limit
      \begin{equation}
        \lim_{N \rightarrow \infty} \frac{1}{N} \sum_{n=1}^N F(X_n) = \sum_{x \in \mathcal{S}} F(x)\, \pi(x) = \mathbb{E} \big( F(x) \big)
      \end{equation}
      holds with probability $1$. In particular, the limit does not depend on the initial distribution. 
    \end{theorem}
    \begin{proof}
      The Frobenius Extension to Perron's theorem (Linear Algebra, Theorem 7.31) combined with its applications to stochastic matrices (Linear Algebra, Theorem 7.30) proves this statement. 
    \end{proof}

    \begin{definition}[First Visit]
      For each $x \in \mathcal{S}$, define the \textit{first visit} to $x$ by 
      \begin{equation}
        T_x \equiv \min\{ n \geq 1 \; | \; X_n = x\}
      \end{equation}
      This $T_x$ is an integer-valued random variable. We say $T_x = + \infty$ if $X_n$ never reaches $x$. Then, we define the \textit{mean return time} to $x$ by 
      \begin{equation}
        \mu_x \equiv \mathbb{E}\big( T_x \, | \, X_0 = x)
      \end{equation}
      If $x$ is transient, then $\mu_x = + \infty$, since there is positive probability that $T_x = + \infty$. 
    \end{definition}

    \begin{definition}
      It is possible that $x$ is recurrent while $\mu_x = +\infty$. If this is the case, then $x$ is said to be \textit{null-recurrent}. If $x$ is recurrent and $\mu_x < \infty$, then $x$ is said to be \textit{positive recurrent}. 
    \end{definition}

    \begin{theorem}
      An irreducible chain has a stationary probability distribution $\pi$ if and only if all states are positive recurrent. If a chain is irreducible and all states are positive recurrent, then 
      \begin{equation}
        \pi(x) = \frac{1}{\mu_x}
      \end{equation}
      for all $x \in \mathcal{S}$. $\pi$ is also unique. 
    \end{theorem}

    \subsubsection{Exit Probabilities}

      Suppose a chain is finite and irreducible. Let $a, b \in \mathcal{S}$ be given states, and let us define $h(x)$ to be the probability of hitting $b$ before $a$, given that we start from $x$. 
      \begin{equation}
        h(x) \equiv \mathbb{P} (X_n \text{ reaches } b \text{ before } a \, | \, X_0 = x)
      \end{equation}
      Clearly, $h(b) = 1$ and $h(a) = 0$. By conditioning on the first jump out of $x$, we also have 
      \begin{align*}
        h(x) & = \mathbb{P}(X_n \text{ reaches } b \text{ before } a \, | \, X_0 = x) \\
        & = \sum_{y} \mathbb{P}(X_n \text{ reaches } b \text{ before } a \, | \, X_1 = y, X_0 = x) \, \mathbb{P}(X_1 = y \,|\,X_0 = x) \\
        & = \sum_y \mathbb{P}(X_n \text{ reaches } b \text{ before } a \, | \,X_1 = y, X_0 = x) \, P(x, y) \\
        & = \sum_y \mathbb{P}(X_n \text{ reaches } b \text{ before } a \, | \,X_1 = y) \, P(x, y) \\
        & = \sum_y h(y) \, P(x, y) 
      \end{align*}
      The sum is over all $y \in \mathcal{S}$ for which $P(x, y) \neq 0$. This gives us a linear system of equations to solve for $h$
      \begin{align*}
        & h(x) = \sum_y P(x, y) h(y) \,\, \forall x \in \mathcal{S} \setminus \{a, b\}, \\
        & h(b) = 1, \\
        & h(a) = 0
      \end{align*}

    \subsubsection{Exit Prize}

      Let $B \subset \mathcal{S}$ be some subset of the state space, and let $g: B \longrightarrow \mathbb{R}$ be some function. Consider the function 
      \begin{equation}
        h(x) = \mathbb{E}\big( g(X_\tau) \, |\, X_0 = x \big)
      \end{equation}

      where $\tau = \min\{ n\geq 0 \,|\, X_n \in B\}$ is the first time that the chain reaches some state in the set $B$ (this time is random). We can interpret $g(y)$ as a "prize" that is awarded if the chain first reaches $B$ at state $y$, which means that $h(x)$ is the expected prize, given that $X_0 = x$. If $x \in B$, then $\tau = 0 \implies h(x) = g(x)$. But if $x \not\in B$, then by the same argument as shown in exit probabilities, it is true that $h$ satisfies the linear system of equations

      \begin{align*}
        & h(x) = \sum_g P(x, y)\,h(y), \;\; \forall x \in \mathcal{S} \setminus B, \\
        & h(x) = g(x), \;\; x \in B 
      \end{align*}

      Note that Exit probability system is a special case of the Exit prize system. In the former, we have defined $B = \{a, b\}$ and $g$ defined by $g(a) = 0, g(b) = 1$. 

    \subsubsection{Occupation Times, Absorbing States}

      Suppose that a chain on a finite $\mathcal{S}$ is irreducible. Let $B \subset \mathcal{S}$ be some subset of states and let $A = \mathcal{S} \setminus B$ be the other states. Then for $x \in A$, we wish to know how many steps the chain will take before reaching a state in the set $B$. We define 
      \begin{equation}
        \tau_B = \min\{n \geq 0 \,|\, X_n \in B\}
      \end{equation}
      which represents the first time that $X$ is in $B$, an integer valued random variable. We wish to compute
      \begin{equation}
        h(x) = \mathbb{E}(\tau_B \,|\, X_0 = x)
      \end{equation}
      Clearly, $h(y) = 0$ for all $y \in B$. For $x \in A$, it takes at least one step to reach $B \implies h(x) \geq 1$ for $x \in A$. We condition on the first step from $x$. This leads to the system 
      \begin{align*}
        h(x) = 1 + \sum_{y \in \mathcal{S}} P(x, y) \, \mathbb{E}(\tau_B \,|\, X_1 = y), & \forall x \in A = \mathcal{S} \setminus B
      \end{align*}
      Since the chain is time-homogeneous, this means that
      \begin{align*}
        h(x) = 1 + \sum_{y \in \mathcal{S}} P(x, y) \, h(y), & \forall x \in A 
      \end{align*}
      Since $h(y) = 0$ for all $y \in B$, we now have
      \begin{align*}
        h(x) = 1 + \sum_{y \in A} P(x, y) \, h(y), & \forall x \in A 
      \end{align*}
      To solve this system, let us define $M$ as the $|A| \times |A|$ submatrix of $P$ obtained by keeping only the entries $P(x, y)$ with $x, y \in A$. So, the system can be written as
      \begin{align*}
        h(x) = 1 + \sum_{y \in A} M(x, y) \, h(y), & \forall x \in A
      \end{align*}
      We can solve this system of equations through the equivalent matrix equation
      \begin{equation}
        (I - M) h = 1
      \end{equation}
      where $1 = (1, 1, ..., 1)^T$ is the column vector consisting of all $1$'s. The solution vector is therefore
      \begin{equation}
        h = (I - M)^{-1} 1
      \end{equation}
      So, for a particular $x \in A$, 
      \begin{equation}
        h(x) = \sum_{y \in A} (I - M)^{-1} (x, y)
      \end{equation}

      Alternatively, we can slightly modify the chain to chain $\Tilde{X}_n$ by replacing the transition probability matrix $P$ with another one defined as 
      \begin{equation}
        \Tilde{P}(x, y) = \begin{cases}
        P(x, y) & x \in A, y \in \mathcal{S} \\
        1 & x = y \in B \\
        0 & \text{else}
        \end{cases}
      \end{equation}

      This modification means that all transitions from state in $A$ to any other state are preserved and the only transitions from a state $x \in B$ are self loops. In particular, all transitions from states $x \in B$ to states $y \in A$ are removed. Therefore, under this modified transition matrix, the states in $B$ are absorbing states. The tail sum formula implies that
      \begin{equation}
        \mathbb{E}(\tau_B \,|\, X_0 = x) = \sum_{k=0}^\infty \mathbb{P}(\tau_B > k \,|\, X_0 = x)
      \end{equation}
      Notice that since the chain $X_n$ and $\Tilde{X}_n$ have the same transition rules before hitting a state $B$, we have 
      \begin{equation}
        P^{(k)} (x, y) = \Tilde{P}^{(k)} = M^{(k)}(x, y)
      \end{equation}
      where $M$ is the $|A| \times |A|$ submatrix defined previously. Therefore, putting this all together, we have
      \begin{align*}
        \mathbb{E}(\tau_B \,|\, X_0 = x) & = \sum_{k=0}^\infty \mathbb{P}(\tau_B > k \,|\, X_0 = x) \\
        & = \sum_{k=0}^\infty \mathbb{P}(\Tilde{X}_k \in A \,|\, X_0 = x) \\
        & = \sum_{k=0}^\infty \sum_{y \in A} \Tilde{P}^{(k)} (x, y) \\
        & = \sum_{k=0}^\infty \sum_{y \in A} M^{(k)} (x, y) \\
        & = \sum_{y \in A} \bigg( \sum_{k=0}^\infty M^{(k)} \bigg) (x, y) 
      \end{align*}
      Using a theorem from linear algebra, we can show that if all the eigenvalues of a $d \times d$ matrix $M$ have modulus strictly less than $1$, then $I-M$ is invertible and
      \begin{equation}
        \sum_{k=0}^\infty M^{(k)} = (I - M)^{-1}
      \end{equation}
      where $I$ is the $d \times d$ identity matrix. If $M$ is the $|A| \times |A|$ submatrix described above, one can show that $M$ has his property and that $I - M$ is invertible. Hence, 
      \begin{equation}
        \mathbb{E}(\tau_B \,|\, X_0 = x) = \sum_{y \in A} \bigg( \sum_{k=0}^\infty M^{(k)} \bigg) (x, y) = \sum_{y \in A} (I-M)^{-1} (x, y)
      \end{equation}
      which refers to the $(x, y)$ entry of the matrix $(I - M)^{-1}$. This is indeed consistent with our previous derivation of the formula for $h(x)$, the expected number of steps before the state reaches $B$. 

  \subsection{Markov Chain Monte Carlo Algorithms}

    In statistics, Markov chain Monte Carlo (MCMC) methods comprise of a class of algorithms for sampling from a probability distribution by constructing a Markov chain that has the desired distribution as its equilibrium distribution. That way, by recording samples from the chain, one may get better approximations of the actual distribution. 

    Let there exist a state space $\mathcal{S}$ with some probability distribution $\pi(x)$ for every $x \in \mathcal{S}$. Clearly, 
    \begin{equation}
      \sum_{x \in \mathcal{S}} \pi(x) = 1
    \end{equation}
    but the problem is that we do not know that $\pi$ is. We do know, however, another function $f$ that is directly proportional to $\pi$. 
    \begin{equation}
      \pi(x) = \frac{f(x)}{c}, \text{ where } c = \sum_{x \in \mathcal{S}} f(x)
    \end{equation}
    is the normalizing constant. It is often the case that $c$ is unknown and the state space $\mathcal{S}$ is so large that computing $c$ directly is expensive. Therefore, we construct Markov chains that can provide approximations to $\pi$. 

    \subsubsection{Metropolis-Hastings Algorithm}

      This algorithm is useful because it does not require knowledge of the normalizing constant $c$. The algorithm only requires evaluations of 
      \begin{equation}
        \frac{\pi(x)}{\pi(y)} = \frac{f(x)}{f(y)}
      \end{equation}

      We first have the state space $\mathcal{S}$ consisting of all the possible states. We now construct (any) probability transition matrix $q$ for a Markov chain on $\mathcal{S}$. Note that $q$ is a $|\mathcal{S}| \times |\mathcal{S}|$ matrix and $q^T$ is a stochastic matrix. This matrix is constructed by the user and is completely well-defined and known. We start off with any initial state $x_0 \in \mathcal{S}$ and iterate the following 2-steps to construct a Markov chain. 

      \begin{enumerate}
        \item Given a state $X_n = x$, we generate a new state $X_{n+1}$ by first proposing a new state $y \in \mathcal{S}$ with probability $q(x, y)$ (determined from the matrix $q$). 
        \item With this chosen state $y$, we decide whether to accept to reject the proposal. With probability 
        \begin{equation}
          \min \bigg( 1, \frac{\pi(y) \,  q(y, x)}{\pi(x) \, q(x, y)} \bigg)
        \end{equation}
        we accept the proposal and set $X_{n+1} = y$. Otherwise, the proposal is rejected and the new state is the same $X_{n+1} = x$. 
      \end{enumerate}

      Note that there are two levels of randomness here: which state the new state $y$ will be and whether to accept this state to be the next one or not. If step two did not exist (i.e. the probability of accepting the proposal is always $1$), then this would just be a regular Markov chain represented by the matrix $q$. But the addition of step 2 means that while $q$ is used in constructing the discrete chain $X_n$, it is \textit{not} the transition probability matrix of $X_n$. 

      There is also a lot of flexibility on choosing $q$, although the performance of the algorithm (speed of convergence of the distribution of $X_n$ to the stationary distribution) will depend on the choice.

      \begin{proposition}
        For the chain defined by the Metropolis-Hastings algorithm, the distribution $\pi$ is stationary. 
      \end{proposition}
      \begin{proof}
        Let us write in shorthand 
        \begin{equation}
          \alpha(x, y) = \frac{\pi(y)\, q(y, x)}{\pi(x)\, q(x, y)}
        \end{equation}
        First, observe that if $x \neq y$, the transition probability for the chain defined by the algorithm is just
        \begin{equation}
          P(x, y) = q(x, y)\, \min\{1, \alpha(x, y)\}
        \end{equation}
        Next, we claim that for all $x, y \in \mathcal{S}$, 
        \begin{equation}
          \pi(x) P(x, y) = \pi(y) \, P(y, x)
        \end{equation}
        This condition is called \textit{detailed balance}. Assuming that $\alpha(x, y) \leq 1$, it is true that
        \begin{equation}
          \pi(x) P(x, y) = \pi(x) q(x, y) \frac{\pi(y) q(y, x)}{\pi(x) q(x, y)} = \pi(y) q(y, x)
        \end{equation}
        In this case, we also have $\alpha(y, x) = 1 / \alpha(x, y) \leq 1$. So, 
        \begin{equation}
          \pi(y) P(y, x) = \pi(y) q(y, x)
        \end{equation}
        and we have proved what we had claimed. Now, summing over $x$,
        \begin{equation}
          \sum_x \pi(x) P(x, y) = \sum_x \pi(y) P(y, x) = \pi(y) \sum_x P(y, x) = \pi(y)
        \end{equation}
        since $P^T$ is stochastic. 
      \end{proof}

    \subsubsection{Gibbs Sampling}

      Let $\mathcal{A} = \{a_1, ..., a_k\}$ be some finite set. Suppose that the state space 
      \begin{equation}
        \mathcal{S} = \mathcal{A} \times ... \times \mathcal{A} = \mathcal{A}^M
      \end{equation}
      for some $M \in \mathbb{N}$. The following algorithm generates a Markov chain on $\mathcal{S}$ with stationary distribution
      \begin{equation}
        \pi(x) = \frac{f(x_1, x_2, ..., x_M)}{c}, \;\; x = (x_1, x_2, ..., x_M) \in \mathcal{S} 
      \end{equation}
      where $c >0$ is a normalizing constant. Note that $|\mathcal{S}| = k^M$, so computing $c$ may be expensive when $M$ is large. The current state of the chain is denoted 
      \begin{equation}
        X_n = (X_n^1, X_n^2, ..., X_n^M)
      \end{equation}
      We think of $X_n$ as having $M$ components, each component taking values in $\mathcal{A}$. We start off with any initial state $X_0 = (X_0^1, X_0^2, ..., X_0^M)$ and construct a Markov chain by iterating the following two steps. 
      \begin{enumerate}
        \item Given $X_n = (X_n^1, X_n^2, ..., X_n^M)$, we generate the next state $X_{n+1}$ by picking a component index $i \in \{1, ..., M\}$ uniformly at random. 
        \item With this chosen, well-defined $i$, we choose a random $Y^i \in \mathcal{A}$ according to the distribution
        \begin{equation}
          \mathbb{P}(Y^i = a) = \frac{f\big(X_n^1 ,..., X_n^{i-1}, a, X_n^{i+1}, ..., X_n^M\big)}{\sum_{j=1}^k f\big(X_n^1 ,..., X_n^{i-1}, a_j, X_n^{i+1}, ..., X_n^M\big)}, \;\; a \in \{a_1, ..., a_k\}
        \end{equation}
        \item Then, set $X_{n+1} = \big(X_n^1, ..., X_n^{i-1}, Y^i, X_n^{i+1}, ..., X_n^M\big)$. 
      \end{enumerate}
      Note that at each step, only one component of $X_n$ is updated. Observe that the distribution above is also equal to 
      \begin{equation}
        \mathbb{P}(Y^i = a) = \frac{\pi\big(X_n^1 ,..., X_n^{i-1}, a, X_n^{i+1}, ..., X_n^M\big)}{\sum_{j=1}^k \pi \big(X_n^1 ,..., X_n^{i-1}, a_j, X_n^{i+1}, ..., X_n^M\big)}
      \end{equation}
      which is the marginal distribution of the $i$th component, given the values of the other components. 

      \begin{proposition}
        For the chain defined by this algorithm, the distribution $\pi$ is stationary. 
      \end{proposition}
      \begin{proof}
        We verify that the detailed balance condition holds. It is also helpful to note that $P(x, y) \neq 0$ if and only if $x$ and $y$ differ in one coordinate. 
      \end{proof}

  \subsection{Continuous Time Markov Chains}

    As the name suggests, in a continuous time Markov chain $X_t$, the time parameter is continuous ($t \geq 0$). As before, the system jumps randomly between states in $\mathcal{S}$, but now the jumps may occur at any time and they occur randomly. This implies that there are \textit{two} sources of randomness:
    \begin{enumerate}
      \item \textit{where} the system jumps and 
      \item \textit{when} the system jumps
    \end{enumerate}

    \begin{definition}[Continuous Time Markov Chain]
      The Markov property in the continuous time case says that for any $s, t \geq 0$ and $y \in \mathcal{S}$, 
      \begin{equation}
        \mathbb{P}(X_{t + s} = y \, | \, X_t) = \mathbb{P}(X_{t+s} = y \, | \, X_r \; \forall 0 \leq r \leq t)
      \end{equation}
      Colloquially, the conditional distribution of $X_{t+s}$ given the history up to time $t$ is the same as the conditional distribution of $X_{t+s}$ given only $X_t$. Thus, if we know the current state at $t$, knowing information about the past doesn't help us better predict the future state $X_{t+s}$. 

      In order for the Markov property to hold, the times between jumps must be exponentially distributed random variables because it is the only density that has the memoryless property. This fact has already been stated in a theorem when covering Poisson arrival processes. This is what makes Exp$(\lambda)$ so important for continuous time Markov chains. 
    \end{definition}

    \begin{lemma}
      Let $T_1, T_2, ..., T_n$ be independent exponential random variables with rates $\lambda_1, \lambda_2, ..., \lambda_n$, respectively. Then the random variable $T \equiv \min\{T_1, T_2, ..., T_n\}$ is
      \begin{equation}
        T \sim \text{Exp}\Big(\sum_{i=1}^n T_i\Big)
      \end{equation}
      Moreover, 
      \begin{equation}
        \mathbb{P}(T_k = \min\{T_1, ..., T_n\}) = \frac{\lambda_k}{\lambda_1 + ... + \lambda_n}
      \end{equation}
    \end{lemma}

    We can interpret the lemma above by imagining that we have $n$ alarm clocks all set simultaneously, which will ring independently at random times. Suppose that clock $k$ will ring after $T_k$ units of time have expired, where $T_k$ is a random variable distributed as Exp$(\lambda_k)$. Then, $T = \min\{T_1, ..., T_n\}$ is the time at which the first ring occurs. 

    \begin{example}
      The simplest and the most important continuous time Markov chains is the Poisson arrival process. The process really has a single parameter $\lambda >0$ (the rate of process) by definition and is integer valued. At each jump time, the process increases by $1$, and the time between jumps are independent, distributed as Exp$(\lambda)$. 

      Notice that when $\lambda$ is large, the arrivals occur more frequently than when $\lambda$ is small, because the expected time between arrivals is $1/\lambda$. The second way we can interpret it is to choose an interval of time $t$ and let $X_t$ be the number of jumps that have occurred up to time $t$. It is a fact that $X_t$ is a integer-valued, Poisson$(\lambda t)$ distribution. That is, 
      \begin{equation}
        \mathbb{P}(X_t = k) = e^{-\lambda t} \frac{(\lambda t)^k}{k!}, \; k = 0, 1, 2, ...
      \end{equation}
      In particular, $\mathbb{E}(X_t) = \lambda t$ and $\Var(X_t) = \lambda t$. 
    \end{example}

  \subsection{Branching Processes}

    \begin{definition}[Branching Process]
      A \textit{branching process} is a type of Markov chain modeling a population in which each individual produces a random number of children (possibly $0$) and dies. The state space is $\mathcal{S} = \{0, 1, 2, 3, ...\}$. Furthermore, there is a discrete-time version and a continuous time version of the chain. In the discrete case, the state is $Z_n$, the size of the population at time $n = 0, 1, 2, ...$, and in the continuous case, the state is $Z_t$ for $t \geq 0$. 
    \end{definition}

    \subsubsection{Discrete-time Branching Process}

      In the discrete case, all of the $Z_n$ individuals in the current generation branch at the same time and immediately die. The branching is independent and distributed according to the \textit{offspring distribution} $\{p_k\}_{k=0}^\infty$. Specifically, if $Z_n = m$, then 
      \begin{equation}
        Z_{n+1} = Y_1^n + Y_2^n + ... + Y_m^n
      \end{equation}
      where $Y_i^n$ represents the number of offspring the $i$th individual in the $n$th generation has. All of them are distributed as
      \begin{equation}
        \mathbb{P}(Y_i^n = k) = p_k, \; k = 0, 1, 2, 3, ...
      \end{equation}
      where $p_k$ is the probability that a parent has $k$ children. Note that if $p_0 \neq 0$, then there is positive probability that $Y_i^n = 0$ for all $i$, meaning that the population can go extinct. A sample branching process up to the second generation is shown below. 

      \begin{center}
      \begin{tikzpicture}[scale=0.8]
        \draw[fill] (0,4) circle (0.05);
        \draw[fill] (-2,2) circle (0.05);
        \draw[fill] (0,2) circle (0.05);
        \draw[fill] (2,2) circle (0.05);
        \draw[dashed] (0,4)--(-2,2);
        \draw[dashed] (0,4)--(0,2);
        \draw[dashed] (0,4)--(2,2);
        \draw[fill] (-3,0) circle (0.05);
        \draw[fill] (-1,0) circle (0.05);
        \draw[dashed] (-2,2)--(-3,0);
        \draw[dashed] (-2,2)--(-1,0);
        \draw[dashed] (2,2)--(3,0);
        \draw[dashed] (2,2)--(1,0);
        \draw[dashed] (2,2)--(2,0);
        \draw[fill] (3,0) circle (0.05);
        \draw[fill] (2,0) circle (0.05);
        \draw[fill] (1,0) circle (0.05);
        \node at (5,4) {$Z_0 = 1$};
        \node at (5,2) {$Z_1 = 3$};
        \node at (5,0) {$Z_2 = 5$};
        \draw[->] (4.3,4)--(3.5,4);
        \draw[->] (4.3,2)--(3.5,2);
        \draw[->] (4.3,0)--(3.5,0);
        \node at (-5, 3) {$Y_1^0 = 3$};
        \node at (-5, 1.5) {$Y_1^1 = 2$};
        \node at (-5, 1) {$Y_2^1 = 0$};
        \node at (-5, 0.5) {$Y_3^1 = 3$};
      \end{tikzpicture}
      \end{center}

      Suppose that the mean number of offspring of a single parent is finite. 
      \begin{equation}
        \mu = \mathbb{E}(Y) = \sum_{k=0}^\infty k \, \mathbb{P}(Y = k) = \sum_{k=0}^\infty k \, p_k < \infty
      \end{equation}
      If $Y_1$ and $Y_2$ are two independent, discrete random variables, we can define their convolution and use the fact that $\mathbb{P}(Y_i = k) = p_k$ to get
      \begin{align*}
        \mathbb{P}(Y_1 + Y_2 = k) & = \sum_j \mathbb{P}(Y_1 = k - j) \, \mathbb{P}(Y_2 = j) \\
        & = \sum_{j=0}^\infty p_{k-j} p_j, \;\; k = 0, 1, 2, ...
      \end{align*}
      This is a two-fold convolution of the sequence $\{p_k\}$ with itself, denoted
      \begin{equation}
        p_k^{*2} = \sum_{j=0}^\infty p_{k-j} \, p_j
      \end{equation}
      Extending this, we can find the $m$-fold convolution of the sequence $\{p_j\}$ with itself, represented by the sequence $\{p_j^{*m}\}$, where $p_k^{*m}$ is the $k$th term in this sequence. This gives us
      \begin{equation}
        p_k^{*n+1} = \sum_{j=0}^\infty p_{k-j} \, p_j^{*n}
      \end{equation}
      for all $n \in \mathbb{N}$. Using this, we can write down the transition probabilities for the Markov chain $Z_n$. 
      \begin{equation}
        \mathbb{P}(Z_{n+1} = k \, | \, Z_n = m) = \begin{cases}
        0 & \text{if } m = 0 \\
        p_k^{*m} & \text{if } m \geq 1, k \geq 0
        \end{cases}
      \end{equation}
      where $\mathbb{P}(Z_{n+1} = k \, | \, Z_n = m)$ represents the probability of the $n$th generation consisting of $m$ individuals producing a total of $k$ offspring for the $(n+1)$th generation. Thus, the branching process is completely determined by the distribution of $Z_0$ and the offspring distribution $\{p_k\}_{k=0}^\infty$. 

      \begin{lemma}
        Given this discrete-time branching process, let $\mu$ be the mean of the offspring distribution. Then, 
        \begin{equation}
          \mathbb{E}(Z_n \, | \, Z_0 = 1) = \mu^n
        \end{equation}
        If $\mu > 1$, the mean of $Z_n$ grows exponentially, and if $\mu_1$, the mean of $Z_n$ decreases exponentially. 
      \end{lemma}

    \subsubsection{Continuous-time Branching Process}

      A continuous time branching process $Z_t$ has very similar structure to the discrete time branching process, except that the times between branch events (for each individual) are independent exponentially distributed random variables Exp$(\lambda)$, where the parameter $\lambda> 0$ is the branching rate. It is as though each individual has an independent alarm clock which rings as a time that is Exp$(\lambda)$, independently of all other clocks. So, if there are currently $N$ individuals, then the next alarm will ring at rate $\lambda N$; that is, the time until the next ring is distributed as Exp$(\lambda N)$, since it is the minimum of $N$ independent Exp$(\lambda)$ random variables. When an individual branches (clock rings), that individual produces a random number of offspring, according to the offspring distribution $\{p_k\}$, as before. So, a continuous time branching process has the same geneological structure as the discrete time process, but the times between branch events is randomized. Consequently, whether or not the process eventually goes extinct, depends only on the offspring distribution, not on the branching rate $\lambda$. 

      Let $m_1(t) = \mathbb{E}(Z_t)$ denote the expected population size at time $t$. Then, it is a fact that $m_1(t)$ satisfies the ordinary differential equation
      \begin{equation}
        \frac{d}{d t} m_1 (t) = \lambda(\mu - 1) m_1 (t)
      \end{equation}
      where 
      \begin{equation}
        \mu = \sum_{k=1}^\infty k p_k
      \end{equation}
      is the mean of the offspring distribution. Solving this equation reveals that 
      \begin{equation}
        m_1 (t) = e^{\lambda (\mu-1) t} m_1 (0)
      \end{equation}
      If $\mu > 1$, the mean population size grows exponentially, and if $\mu < 1$, the mean population size decreases exponentially. 

    \subsubsection{Extinction Probability, Generating Functions}

      The expression for the transition probabilities of $Z_n$ (disrete case) is quite difficult to work with. Alternatively, it can be convenient to work with generating functions. 

      \begin{definition}[Generating Function]
        The \textit{generating function} for the offspring distribution is the function 
        \begin{equation}
          G(s) \equiv \sum_{k=0}^\infty p_k \, s^k = \mathbb{E}(s^Y)
        \end{equation}
        where $Y \sim \{p_k\}$ is a random variable representing the number of children produced by a given individual. Note that $G$ is a power series that simply encodes information about the offspring distribution (also a sequence) $\{p_k\}_{k=0}^\infty$. 
      \end{definition}

      \begin{theorem}[Properties]
        Properties of the generating function. 
        \begin{enumerate}
          \item The radius of convergence of $G(s)$ is at least $1$. $G(s)$ defines a continuous function on $|s| \leq 1$. 
          \item On the interval $[0,1]$, $G(s)$ is increasing and convex. If $p_0 + p_1 < 1$, then $G(s)$ is strictly convex for $s \in [0,1]$. 
          \item $G(0) = p_0$. 
          \item $G(1) = 1$. 
          \item $G^\prime(1^-) = \mu$ is the expected number of offspring of a single individual. 
        \end{enumerate}
      \end{theorem}
      \begin{proof}
        We use the fact that 
        \[\sum_{k=0}^\infty p_k = 1 \text{ and } 0 \geq p_k \geq \; \forall k = 0, 1, 2, ...\]
      \end{proof}

      \begin{theorem}
        Suppose that $Z_0 = 1$ and that $p_0 + p_1 < 1$. Then
        \begin{equation}
          \lim_{n \rightarrow \infty} \mathbb{P}(Z_n = 0) = \mathbb{P}(\text{eventual extinction}) = t
        \end{equation}
        where $t \in [0, 1]$ is the smallest non-negative root of the equation $t = G(t)$. If $\mu \leq 1$, then $t = 1$ (clearly, since the population will exponentially decrease on average). If $\mu > 1$, there is a positive probability that the population never goes extinct. 
      \end{theorem}
      \begin{proof}
        Let $t$ be the probability that an individual's descendent family tree goes extinct. That is, $t = \mathbb{P}(Z_n = 0$ for some $n \geq 1 \; | \; Z_0 = 1)$. To derive the equation $t = G(t)$, let us condition on the first generation, with $Y_1$ denoting the number of offspring of the single parent. 
        \begin{align*}
          t & = \mathbb{P}(\text{eventual extinction} \; | \; Z_0 = 1) \\
          & = \sum_{k=0}^\infty \mathbb{P}(\text{eventual extinction} \; | \; Z_0 = 1, Y_1 = k) \, \mathbb{P}(Y_1 = k \; | \; Z_0 = 1) \\
          & = \sum_{k=0}^\infty \mathbb{P}(\text{eventual extinction} \; | \;Z_0 =1, Y_1 = k) \, p_k
        \end{align*}
        That is, given that there are $k$ children of the first individual, the probability that this first individual's descendent family tree will go extinct is equal to the probability that each of the $k$ children's trees go extinct. These $k$ extinction events are independent. Therefore, 
        \begin{equation}
          \mathbb{P}(\text{eventual extinction} \; | \;Z_0 = 1, Y_1 = k) = t^k
        \end{equation}
        which implies that 
        \begin{equation}
          t = \sum_{k=0}^\infty \mathbb{P}(\text{eventual extinction} \; | \; Z_0 = 1, Y_1 = k) \, p_k = \sum_{k=0}^\infty t^k \, p_k = G(t)
        \end{equation}
        Additionally, under the hypothesis that $p_0 + p_1 < 1$, then $G(s)$ is strictly convex on $[0,1]$. Hence if $G^\prime (1) = \mu \leq 1$, the smallest non-negative root of $t = G(t)$ must be $t=1 \implies$ extinction occurs with probability 1. On the other hand, if $G^\prime (1) = \mu > 1$, then the smallest root of $t = G(t)$ occurs in the interval $[0,1)$. 
      \end{proof}

      Note that this result applies to both the discrete time case and the continuous time case. In continuous-time chains, whether or not the population goes extinct does not depend on $\lambda$, the rate at which individuals give birth. The $\lambda$ affects the time at which extinction occurs (if it occurs), but it does not affect the probability that it occurs. However, the extinction probability certainly does depend on the offspring distribution. 

      \begin{definition}[Counting Variable]
        A random variable $X$ is a \textit{counting variable} if it takes values in $\{0, 1, 2, ...\}$. 
      \end{definition}

      Note that generating functions is a mapping from $X$, the set of counting variables (all assumed to be pairwise independent) to the algebra of power series over variable $s$.
      \begin{equation}
        G: X \longrightarrow F[[s]]
      \end{equation}

      \begin{lemma}
        Let $X$ and $Y$ be two independent random counting variables, with generating functions $G_X (s) = \mathbb{E}(s^X)$ and $G_Y (s) = \mathbb{E}(s^Y)$. Then, the generating function for the random variable $Z = X + Y$ is $G_Z(s) = G_X (s) G_Y (s)$. That is, the generating function mapping $G$ is a homomorphism that maps addition to multiplication. In particular, if $X$ and $Y$ are iid, then $G_Z (s) = G_X (s)^2$. 
      \end{lemma}
      \begin{proof}
        Since $X$ and $Y$ are independent, 
        \begin{equation}
          G_Z (s) = \mathbb{E}(s^Z) = \mathbb{E}(s^{X+Y}) = \mathbb{E}(s^X s^Y) = \mathbb{E}(s^X) \mathbb{E}(s^Y) = G_X (s) G_Y (s)
        \end{equation}
      \end{proof}

      Applying this argument iteratively, we get the following lemma. 
      \begin{lemma}
        Let $N \geq 1$ be a fixed positive integer. Let $Y_1, Y_2, ..., Y_N$ be independent, identically distributed random counting variables with generating function $G_Y (s) = \mathbb{E}(s^Y)$. Then, the generating function for the sum $Z = Y_1 + ... + Y_n$ is 
        \begin{equation}
          G_Z (s) = G_Y (s)^N
        \end{equation}
      \end{lemma}

      Now, suppose that $N$ is not fixed, but another random variable. We wish to describe the distribution of the sum of a random number of random variables. 

      \begin{lemma}
        Let $Y_1, Y_2, Y_3, ...$ be a collection of independent, identically distributed random variables with generating function $G_Y (s) = \mathbb{E}(s^Y)$. Let $N$ be a random counting variable, independent of the $Y_i$. Let $N$ have generating function $G_N (s)$. Then the generating function for $Z = Y_1 + Y_2 + ... + Y_N$ is 
        \begin{equation}
          G_Z (s) = G_N \big( G_Y (s) \big)
        \end{equation}
      \end{lemma}
      \begin{proof}
        Just condition on $N = k$ 
        \begin{align*}
          G_Z (s) = \mathbb{E}(s^Z) & = \sum_{k=0}^\infty \mathbb{E}\big( s^Z \,|\, N=k\big) \, \mathbb{P}(N=k) \\
          & = \sum_{k=0}^\infty \mathbb{E}(s^{Y_1 + ... + Y_k} \,|\,N=k) \, \mathbb{P}(N=k) \\
          & = \sum_{k=0}^\infty G_Y (s)^k \, \mathbb{P}(N=k) \\
          & = \mathbb{E}\big( G_Y (s)^N \big) = G_N \big( G_Y (s) \big)
        \end{align*}
      \end{proof}

      \begin{theorem}
        Let $G(s)$ be the generating function for the offspring distribution $G(s) = \sum_{k=0}^\infty p_k s^k$. Suppose that $Z_0 = 1$ and let $G_n (s) = \mathbb{E}(s^{Z_n})$ be the generating function for the random variable $Z_n$. Then, 
        \begin{equation}
          G_{n+m} (s) = G_n \big(G_m (s)\big) = G_m \big( G_n (s) \big)
        \end{equation}
        Hence, 
        \begin{equation}
          G_n (s) = G(G(G(...(G(s))...))) \;\;\;\; \text{n-fold composition}
        \end{equation}
      \end{theorem}

      \begin{example}
        Suppose the offspring distribution is
        \begin{equation}
          p_k = q p^k, \;\; k \geq 0
        \end{equation}
        for some $p \in (0, 1)$, where $q = 1-p$. Thus, the number of children from a given parent is $Y = X - 1$, where $X \sim$ Geom$(q)$. Then, $\mathbb{E}(Y) = \frac{1}{q} - 1 = \frac{p}{q}$. With some computation, this means that
        \begin{equation}
          G(s) = \frac{q}{1- p s}
        \end{equation}
        and $t = \min \{1, \frac{q}{p}\}$. 
      \end{example}

\section{Common Distributions}

  \subsection{Multivariate Gaussians}

  Recall $X \sim \mathcal{N}(\mu, \sigma^2)$ implies that its PDF is 
  \[f_X (x) = \frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{(x - \mu)^2}{2 \sigma^2}}\]
  Now we will consider a Gaussian random \textit{vector}, which can be considered a vector of random variables  
  \[\mathbf{X} = \begin{bmatrix} X_1 \\ X_2 \\ \vdots \\ X_n \end{bmatrix}\]
  mapping from $\Omega$ to $\mathbb{R}^n$. It is not merely a vector where every $X_i$ is Gaussian, as we will show later. That is, a joint distribution that has all $n$ marginal distributions Gaussians does not make a multivariate Gaussian. 

  This measurable function $\mathbf{X}: \Omega \rightarrow \mathbb{R}^n$ induces a probability law $\mathbb{P}_X$ on $\mathcal{B}(\mathbb{R}^n)$, and the Radon-Nikodym theorem states the existence of a PDF $f_X$ such that $\mathbb{P}_X (B) = \int_B f_X \,d\lambda$.

  \subsubsection{Bivariate Gaussians}
  Let us first begin with two-variable Gaussians. 

  \begin{definition}[Standard Bivariate Gaussian RV]
  A random variable $(X, Y)$ is said to be a \textbf{standard bivariate Gaussian} if its PDF is of form
  \[f_{X, Y} (x, y) = \frac{1}{2 \pi \sqrt{1 - \rho^2}} \exp \bigg( -\frac{x^2 - 2 \rho x y + y^2}{2 (1 - \rho^2)} \bigg) \text{ for } \rho \in (-1, 1)\] 
  \end{definition}

  \begin{proposition}
  Given a standard bivariate Gaussian $(X, Y)$, 
  \begin{enumerate}
      \item $X$ and $Y$ are marginally distributed as $\mathcal{N}(0, 1)$. That is, if we integrate a variable (say, $x$) out, we will get a univariate standard Gaussian PDF of the other ($y$): 
      \[\int_{-\infty}^\infty \frac{1}{2 \pi \sqrt{1 - \rho^2}} \exp \bigg( -\frac{x^2 - 2 \rho x y + y^2}{2 (1 - \rho^2)}\bigg) \,dx = \frac{1}{\sqrt{2 \pi}} e^{-y^2 / 2}\]
      \item $\rho_{X, Y}$, the correlation coefficient of $X$ and $Y$, is equal to $\rho$. 
      \item The conditional distribution of $X$ given $Y = y$ is $X \mid Y = y \sim \mathcal{N} (\rho y, 1 - \rho^2)$. That is, 
      \[f_{X \mid Y} (x \mid y) = \frac{1}{2 \pi \sqrt{1 - \rho^2}} \exp \bigg( -\frac{x^2 - 2 \rho x y + y^2}{2 (1 - \rho^2)}\bigg) = \frac{1}{ \sqrt{2 \pi (1 - \rho^2)}} \, \exp\bigg( - \frac{(x - (\rho y))^2}{2 (1 - \rho^2)} \bigg)\]
      \item From (3), we can see that the conditional expectation $\mathbb{E}[X \mid Y = y] = \rho y$ since $X \mid Y = y$ has mean at $\rho y$. Therefore, the conditional expectation of $X$ given $Y$ (which is a random variable) is 
      \[\mathbb{E}[X \mid Y] = \rho Y\]
      i.e. $\mathbb{E}[X \mid Y]$ is a linear function of $Y$. 
  \end{enumerate}
  \end{proposition}

  The formula of the general bivariate Gaussian $\mathbf{X} = (X_1, X_2)$ PDF is messy, but we will put it here. 
  \[f_{X_1, X_2} (x, y) = \frac{\sigma_1 \sigma_2 \sqrt{1 - \rho^2}} \exp \bigg[ -\frac{1}{2} \bigg( \frac{(x_1 - \mu_1)^2}{\sigma_1^2} + \frac{(x_2 - \mu_2)^2}{\sigma_2^2} - 2 \rho \frac{(x_1 - \mu_1)}{\sigma_1} \frac{(x_2 - \mu_2)}{\sigma_2}\bigg)\bigg]\]
  It is cleaner to put it into matrix form. 
  \[f_{X_1, X_2} (x_1 , x_2) = \frac{1}{2 \pi \sqrt{\mathrm{det}(\boldsymbol{\Sigma})}} \exp \bigg( - \frac{(\mathbf{x} - \boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu})}{2} \bigg) \]
  where 
  \[\mathbf{x} = \begin{pmatrix} x_1 \\ x_2 \end{pmatrix}, \;\; \boldsymbol{\mu} = \begin{pmatrix} \mu_1 \\ mu_2 \end{pmatrix} , \;\; \boldsymbol{\Sigma} = \mathbb{E}\big[ (\mathbf{X} - \boldsymbol{\mu}) (\mathbf{X} - \boldsymbol{\mu})^T \big] = \begin{pmatrix} \mathrm{Var}(X_1) & \mathrm{Cov}(X_1, X_2) \\
  \mathrm{Cov}(X_1, X_2) & \mathrm{Var}(X_2) \end{pmatrix}\]

  Note that visually, $\boldsymbol{\Sigma}$ will determine how much the Gaussian distribution is "stretched" on one way or another. Obviously, the "peak" of the distribution will be $\boldsymbol{\mu}$. If $\boldsymbol{\Sigma} = I$, then we could visualize the Gaussian distribution as being perfectly symmetric. However, if we scale the distribution up to a certain constant (below shown $\boldsymbol{\Sigma} = I$, $\boldsymbol{\Sigma} = 0.61 I$, $\boldsymbol{\Sigma} = 2 I$), we get
  \begin{center}
      \includegraphics[scale=0.65]{img/Gaussian_Distribution.png}
  \end{center}

  Now we've made a remark before that given a multivariate distribution $\mathbf{X} = (X_1, \ldots, X_n)$, all of its marginal distributions being Gaussian does not mean that $\mathbf{X}$ is a multivariate Gaussian. We give a counterexample. 

  \begin{example}
  Let $Y_1, Y_2$ be iid random variables distributed according to the PDF 
  \[f_Y (y) = \sqrt{\frac{2}{\pi}} e^{-y^2 / 2} \text{ for } y > 0\]
  which we can interpret as a one-sided Gaussian. Let $W \sim \mathrm{Bernoulli}(\frac{1}{2})$ be independent of $Y_1$ and $Y_2$. Now, define the random variables 
  \[X_1 = W \, Y_1 \text{ and } X_2 = W \, Y_2\]
  Now note that $Y_1$ and $Y_2$ are both positive, and since $X_1$ and $X_2$ are both dependent on the same value of $W$, it is either $X_1$ and $X_2$ are both positive or both negative. So, the joint distribution of $X_1, X_2$ will be on only the 1st and 3rd quadrant with no mass on the 2nd and 4th. 
  \begin{center}
      \includegraphics[scale=0.23]{img/not_multi_Gaussian.jpg}
  \end{center}
  This is clearly not a multivariate Gaussian, even though the marginals are $X_1, X_2 \sim \mathcal{N}(0, 1)$. We could make the degenerate case that $X_1 = X_2$, which would make the image of $(X_1, X_2)$ just the line at $x_1 = x_2$, but we can think of this as a degenerate Gaussian with a singular $\boldsymbol{\Sigma}$. 
  \end{example}

  \subsubsection{Multivariate Gaussians}

  There are three equivalent definitions of multivariate Gaussians of $n$-variables. 

  \begin{definition}[Multivariate Gaussian]
  Let us have a vector-valued random variable $\mathbf{X} = (X_1 \ldots X_n)^T \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})$. 
  \begin{enumerate}
      \item $\mathbf{X}$ is a \textbf{multivariate Gaussian distribution} with mean $\boldsymbol{\mu} \in \mathbb{R}^n$ and symmetric, positive-definite covariance matrix $\boldsymbol{\Sigma} \in \mathbb{R}^{n \times n}$ if its probability density function is
      \[f_X (x) = \frac{1}{(2\pi)^{n/2} \mathrm{det}(\boldsymbol{\Sigma})^{1/2}} \exp\bigg( -\frac{1}{2} (x-\boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (x - \boldsymbol{\mu})\bigg)\]
      The covariance matrix $\boldsymbol{\Sigma}$ is the $n \times n$ matrix whose $(i, j)$th entry is $\Cov(X_i, X_j)$. That is, for any random vector $\mathbf{X}$ with mean $\boldsymbol{\mu}$, its covariance matrix 
      \[\boldsymbol{\Sigma} = \mathbb{E}\big[ (\mathbf{X} - \boldsymbol{\mu}) (\mathbf{X} - \boldsymbol{\mu})^T \big] = \mathbb{E}[\mathbf{X} \mathbf{X}^T] - \boldsymbol{\mu} \boldsymbol{\mu}^T\]
      is positive definite and symmetric, which implies by the spectral theorem we can break it down into $n$ orthogonal eigenspaces of positive eigenvalues. 

      \item $X$ is a multivariate Gaussian distribution if it can be expressed as 
      \[\mathbf{X} = \mathbf{D} \mathbf{w} + \boldsymbol{\mu}\]
      where $\mathbf{w}$ is a vector of independent $\mathcal{N}(0, 1)$ Gaussians, $\boldsymbol{\mu} \in \mathbb{R}^n$, and $\mathbf{D} \in \mathbb{R}^{n \times n}$. The mean of $\mathbf{X}$ is $\boldsymbol{\mu}$ and its covariance is $\boldsymbol{\Sigma} = \mathbf{D} \mathbf{D}^T$; $\mathbf{D}$ is called the \textbf{standard deviation matrix}. When modeling high-dimensional Gaussians, this way is most computationally feasible. 

      \item $X$ is a multivariate Gaussian distribution if for every $\mathbf{a} \in \mathbb{R}^n$, $\mathbf{a}^T \mathbf{x}$ is a Gaussian RV. This means that if we take $\mathbf{a} = \mathbf{0}$, then the entire $\mathbf{X}$ is constantly $0$, which we will take to be the degenerate Gaussian with mean, variance $0$. 
  \end{enumerate}
  The $n$ semi-axes of the $(n-1)$-dimensional isocontour ellipsoid formed by an $n$-dimensional Gaussian distribution are precisely the normalized eigenvectors of $\boldsymbol{\Sigma}$ multiplied by their eigenvalues. 
  \end{definition}


  If we let $\boldsymbol{\Sigma} = \mathbf{I}$, then this means that all the $X_i$'s are pairwise uncorrelated since $\Sigma_{ij} = \Cov (X_i, X_j) = 0$. In general, this does not mean that the $X_i$'s are independent, but for joint Gaussians, this also implies independence! 

  \begin{theorem}
  Given multivariate Gaussian $\mathbf{X} = (X_1 \ldots X_n)^T \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})$, the $X_i$'s are pairwise independent if and only if they are uncorrelated. 
  \end{theorem}
  \begin{proof}
  We can expand the PDF of $\mathbf{X}$ as 
  \begin{align*}
      f_X (x) & = \frac{1}{(2 \pi)^{n/2}} \exp \bigg( -\frac{1}{2} (x - \mu)^T (x - \mu) \bigg) \\
      & = \bigg(\frac{1}{\sqrt{2\pi}} \bigg)^n \exp \bigg( \sum_{i=1}^n -\frac{1}{2} (x_i - \mu_i)^2 \bigg) \\
      & = \prod_{i=1}^n \frac{1}{\sqrt{2\pi}} \exp \bigg( -\frac{1}{2} (x_i - \mu_i)^2 \bigg)
  \end{align*}
  which is the product of $n$ single-variable Gaussians $X_i$. Therefore this means that independence and uncorrelation are equivalent! 
  \end{proof}

  Therefore, if the nondiagonal entries of the covariance matrix are all $0$, then we know that the variables are all uncorrelated and therefore independent. 

\end{document}

\section{Continuous Functions}

    Even though we can generalize the concept of limits to functions mapping between arbitrary topological spaces $f: X \longrightarrow Y$, we define this for functions $f: E \subset \mathbb{R} \longrightarrow \mathbb{R}$. 

    \begin{definition}[Functions]
      Given a real-valued function $f: E \longrightarrow \mathbb{R}$ defined on domain $E \subset \mathbb{R}$,
      \begin{enumerate}
        \item $f$ is a \textbf{constant function} if $f(x) = A$ for all $x \in E$
        \item $f$ is called \textbf{ultimately constant} as $x \rightarrow a$ if it is constant in some deleted neighborhood $\mathring{U} (a)$, where $a$ is a limit point of $E$.
        \begin{center}
            \includegraphics[scale=0.25]{img/Ultimately_Constant_Function.PNG}
        \end{center}
        \item $f$ is \textbf{bounded}, \textbf{bounded above}, or \textbf{bounded below} respectively if there is a number $C \in \mathbb{R}$ such that $|f(x)|<C$, $f(x)<C$, or $C<f(x)$ for all $x \in E$.
        \begin{center}
            \includegraphics[scale=0.25]{img/Bounded_Three.PNG}
        \end{center}
        \item $f$ is \textbf{ultimately bounded}, \textbf{ultimately bounded above}, or \textbf{ultimately bounded below} as $x \rightarrow a$ if it is bounded, bounded above, or bounded below in some deleted neighborhood $\mathring{U}_E (a)$. 
        \begin{center}
            \includegraphics[scale=0.25]{img/Ultimately_Bounded_Three.PNG}
        \end{center}
      \end{enumerate}
    \end{definition}

    \begin{example}
      The function 
      \[f(x) = \sin{\frac{1}{x}} + x \cos{\frac{1}{x}}\]
      for $x \neq 0$ is not bounded on the domain of definition, but it is ultimately bounded as $x \rightarrow 0$. 

    \end{example}

    \begin{definition}[$\epsilon-\delta$ Definition of a Limit]
      The function $f: E \subset \mathbb{R} \longrightarrow \mathbb{R}$ tends to $A \in \mathbb{R}$ as $x$ tends to $a$, or that 
      \[\lim_{x \rightarrow a} f(x) = A\]
      if for every $\epsilon > 0$ there exists $\delta > 0$ such that 
      \[0<|x - a|<\delta \implies |f(x) - A| < \epsilon\]
      Note that we set the $0<|x-a|$ to ensure that $x \neq a$. 

      Therefore, in other words, for any arbitrarily small $\epsilon > 0$, if we can find a $\delta > 0$ such that the image of the \textbf{deleted $\delta$-neighborhood of $a$} (defined to be $\mathring{U}_\delta (a) \equiv U_\delta (a) \setminus a$) is completely within the $\epsilon$-neighborhood $U_\epsilon (A)$, then 
      \[\lim_{x \rightarrow a} f(x) = A\]
      Visually, 
      \begin{center}
          \includegraphics[scale=0.25]{img/Limit_of_Function_on_R.PNG}
      \end{center}
      In higher dimensional spaces, we have 
      \begin{center}
          \includegraphics[scale=0.25]{img/Limit_of_Function_on_Euclidean_Space.PNG}
      \end{center}
    \end{definition}

    \begin{example}[Limit of the Signum Function]
      The function sgn$: \mathbb{R} \longrightarrow \mathbb{R}$ defined
      \[\text{sgn}\,x = \begin{cases}
      1, & x > 0 \\
      0, & x = 0 \\
      -1, & x < 0
      \end{cases}\]
      has no limit as $x \rightarrow 0$. 

      First, it is ludicrous that the limit would be any number that is not $\{-1, 0, 1\}$. If we assume that $A \not\in \{-1,0,1\}$, then we can choose any arbitrarily small $\epsilon$-neighborhood of $A$ that does not include the three numbers. Clearly, there doesn't exist any $\delta>0$ such that the deleted $\delta$-neighborhood of $0$ maps to a set completely contained in the $\epsilon$-neighborhood of $A$. That is,
      \[\text{sgn}\big( \mathring{U}_\delta (0)\big) = \{-1,1\} \not\subset U_\epsilon (A)\]
      \begin{center}
          \includegraphics[scale=0.3]{img/Limit_of_Sign_Function_Ludicrous.PNG}
      \end{center}
      It doesn't even intersect the $\epsilon$-neighborhood at all. 
      \begin{enumerate}
        \item If $A = 1$, we can construct a $\epsilon$-neighborhood $V_A$ for $\epsilon = \frac{1}{2}$. Clearly, there exists no open neighborhood $U_0$ of $0$ that is entirely mapped to $V$, since $U_0$ contains both negative numbers and $0$ and hence must be mapped to $0, -1$. 
        \item Similarly, given the $(\epsilon=\frac{1}{2})$-neighborhood of $A = -1$, there exists no open neighborhood $U_0$ of $0$ that is entirely mapped to it, since $U_0$ contains both positive numbers and $0$ and hence must be mapped to $0, 1$. 
        \item Finally, given the $(\epsilon=\frac{1}{2})$-neighborhood of $A = 0$, there exists no open neighborhood $U_0$ of $0$ that is entirely mapped to it, since $U_0$ contains both positive and negative numbers and hence must be mapped to $\pm1$. 
      \end{enumerate}
      \begin{center}
          \includegraphics[scale=0.25]{img/Limit_of_Sign_Function_1_0_-1.PNG}
      \end{center}
      Therefore, the limit does not exist. 
    \end{example}

    \begin{example}[Limit of Absolute Value of Signum Function]
      We will show that 
      \[\lim_{x \rightarrow 0} |\text{sgn}\,x| = 1\]
      We construct a $\epsilon$-neighborhood $U_\epsilon (1)$ around $1$. Given this neighborhood, we can imagine choosing the deleted $\delta$-neighborhood $\mathring{U}_\delta (0)$ around $0$. Since every element in $\mathring{U}_\delta (0)$ maps to $1$, it is clearly in $U_\epsilon$. 
      \begin{center}
          \includegraphics[scale=0.25]{img/Absolute_Value_of_Signum.PNG}
      \end{center}
      In fact, for arbitrarily small $\epsilon > 0$, we can choose \textbf{any} $\delta>0$ since everything in $\mathbb{R} \setminus 0$ maps to $1$. We can visualize this in $\mathbb{R}^2$ as
      \begin{center}
          \includegraphics[scale=0.25]{img/Absolute_Value_of_Signum_2.PNG}
      \end{center}
    \end{example}

    The following lemma nicely interweaves the concepts of limits of sequences and limits of functions. It can be nice for visualization to define the limit of a function using Cauchy sequences rather than the usual $\epsilon-\delta$ definition. 

    \begin{lemma}[Cauchy Sequence Criterion of a Limit]
      The relation 
      \[\lim_{x \rightarrow a} f(x) = A\]
      holds if and only if for every sequence $\{x_i\}$ of points $x_n \in E \setminus a$ converging to $a$, the sequence $\{f(x_n)\}$ converges to $A$. 
      \begin{center}
          \includegraphics[scale=0.25]{img/Cauchy_Criterion_of_Limit_of_Function.PNG}
      \end{center}
      Note that we choose the points $x_n$ to be in the "deleted" neighborhood $E\setminus a$ (neighborhood $E$ with point $a$ removed) to force us to choose a sequence that is not
      \[a, a, a, a, a, a, a, \ldots\]
      That is, it forces us to choose different points for the sequence. 
    \end{lemma}

    \begin{theorem}[Properties of Limits]
    Given two numerical valued functions $f, g: E \subset \mathbb{R} \longrightarrow \mathbb{R}$ with a common domain where $g(x) \neq 0$ for all $x \in E$, let 
    \[\lim_{x \rightarrow a} f(x) = A, \;\;\;\;\; \lim_{x \rightarrow a} g(x) = B\]
    then, 
    \begin{align*}
        & \lim_{x \rightarrow a} (f+g)(x) = A + B \\
        & \lim_{x \rightarrow a} (cf)(x) = cA \\
        & \lim_{x \rightarrow a} (f \cdot g)(x) = A \cdot B \\
        & \lim_{x \rightarrow a} \bigg(\frac{f}{g}\bigg) (x) = \frac{A}{B}
    \end{align*}
    \end{theorem}
    \begin{proof}
    The the Cauchy sequence criterion for a limit, this theorem is an immediate consequence on the corresponding theorem on limits of sequences.
    \end{proof}

    We end this with a theorem connecting the relationship between a limit of a function as $x \rightarrow a$ and its ultimate behavior as $x \rightarrow a$. 

    \begin{theorem}
    Let $f: E \longrightarrow \mathbb{R}$ be a function. Then, 
    \begin{enumerate}
      \item $f$ is ultimately the constant $A$ as $x \rightarrow a$ implies that $\lim_{x \rightarrow a} f(x) = A$. 
      \item $\lim_{x \rightarrow a} f(x)$ implies that $f$ is ultimately bounded as $x \rightarrow a$. 
    \end{enumerate}
    \end{theorem}

  \subsection{Infinitesimal Functions}

    \begin{definition}[Infinitesimal Function]
      A function $f: E \subset \mathbb{R} \longrightarrow \mathbb{R}$ is said to be \textbf{infinitesimal} as $x \rightarrow a$ if 
      \[\lim_{x \rightarrow a} f(x) = 0\]
    \end{definition}

    \begin{lemma}[Sums, Products of Infinitesimals]
      It is clear that if $\alpha, \beta$ are infinitesimal as $x \rightarrow a$, then 
      \begin{enumerate}
        \item $\alpha + \beta$ is infinitesimal as $x \rightarrow a$
        \item $\alpha \cdot \beta$ is infinitesimal as $x \rightarrow a$
      \end{enumerate}
      Furthermore, if $\alpha$ is infinitesimal and $\beta$ is ultimately bounded as $x \rightarrow a$, then the product $\alpha \cdot \beta$ is infinitesimal as $x \rightarrow a$. 
    \end{lemma}
    \begin{proof}
    We prove all three statements. 
    \begin{enumerate}
      \item Assume that $\alpha$ and $\beta$ are infinitesimal as $x \rightarrow a$. Then, let us fix a small $\epsilon>0$. This means that for every $\frac{\epsilon}{2}$ there exists an open deleted neighborhood $\mathring{U}^\prime (a)$ such that its image $\alpha\big(\mathring{U}^\prime (a)\big)\subset U^\prime_{\epsilon/2} (0) \subset \mathbb{R}$. Additionally, for every $\frac{\epsilon}{2}$ there exists an open deleted neighborhood $\mathring{U}^{\prime\prime} (a)$ such that its image $\beta\big(\mathring{U}^{\prime\prime} (a)\big)\subset U^\prime_{\epsilon/2} (0) \subset \mathbb{R}$.
      Thus, for the deleted neighborhood 
      \[\mathring{U}(a) \subset \mathring{U}^\prime (a) \cup \mathring{U}^{\prime\prime} (a)\]
      we can see that for all $x \in \mathring{U}(a)$, 
      \[|(\alpha + \beta)(x)| = |\alpha (x) + \beta(x)| \leq |\alpha (x)| + |\beta(x)| < \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon\]
      and hence $(\alpha + \beta)\big( \mathring{U}(a)\big) \subset U_\epsilon (0)$. 
      \item This case is a special case of assertion 3. That is, every function that has a limit is ultimately bounded. 
      \item Since $\beta(x)$ is ultimately bounded, this means that there exists a constant $M$ and an open deleted neighborhood $\mathring{U}^\prime (a) \subset E$ such that for all $x \in \mathring{U}^\prime (a)$, its image is bounded: $|\beta(x)|<M$. Let us fix a small $\epsilon>0$. Then, by definition of the limit, for every $\frac{\epsilon}{M}$ there exists an open deleted neighborhood $\mathring{U}^{\prime\prime} (a)$ such that its image $\beta\big(\mathring{U}^{\prime\prime}(a)\big) \subset U_{\epsilon/M} (0) \subset \mathbb{R}$. Therefore, for the deleted neighborhood
      \[\mathring{U}(a) \subset \mathring{U}^\prime (a) \cup \mathring{U}^{\prime\prime}(a)\]
      we can see that for all $x \in \mathring{U} (a)$, 
      \[|(\alpha \cdot \beta)(x)| = |\alpha (x) \beta(x)| = |\alpha (x)| |\beta(x)| < \frac{\epsilon}{M} \cdot M = \epsilon\]
      Therefore, $(\alpha \cdot \beta)\big( \mathring{U} (a)\big) \subset U_\epsilon (0)$. 
    \end{enumerate}
    \end{proof}

    Note that in proving these properties of the limits, we have used the following fact about open deleted neighborhoods around $a$. 
    \begin{enumerate}
      \item $\mathring{U} (a)$ is not the empty set. 
      \item Given open deleted neighborhoods $\mathring{U}^\prime (a)$ and $\mathring{U}^{\prime\prime} (a)$, there exists an open deleted neighborhood in the intersections of these neighborhoods. 
      \[\mathring{U} (a) \subset \mathring{U}^\prime (a) \cup \mathring{U}^{\prime\prime} (a)\]
    \end{enumerate}
    These facts can be used to generalize the concept of limits as limits over a certain \textbf{filter base}. 

    \begin{theorem}[Representation of a Convergent Function as a Shift of its Infinitesimal]
    Given a function $f: E \subset \mathbb{R} \longrightarrow \mathbb{R}$, its limit exists and 
    \[\lim_{x \rightarrow a} f(x) = A\]
    if and only if $f$ can be represented as 
    \[f(x) = A + \alpha (x)\]
    where $\alpha$ is infinitesimal as $x \rightarrow a$. We can visualize this theorem by thinking of a function $f$ that results from a "shift" of an infinitesimal. 
    \begin{center}
        \includegraphics[scale=0.3]{img/Infinitesimal_Shift_Function.jpg}
    \end{center}
    \end{theorem}

    Finally, we reiterate some limit theorems already stated for sequences, but now corresponding to functions. Interpreting the function limit as the Cauchy sequence definition of limits renders the proofs of these theorems trivial. 

    \begin{theorem}[Behavior of Functions with Different Limits]
    If the functions $f, g: E \rightarrow \mathbb{R}$ are such that
    \[\lim_{x\rightarrow a} f(x) = A < B = \lim_{x \rightarrow a} g(x)\]
    then there exists a deleted neighborhood $U_\delta (a)$ in $E$ at each point of which $f(x) < g(x)$. 
    \end{theorem}

    \begin{theorem}[Squeeze Theorem for Limits of Functions]
    Given the functions $f, g, h: E \subset \mathbb{R} \longrightarrow \mathbb{R}$ such that
    \[f(x) \leq g(x) \leq h(x) \text{ for all } x \in E\]
    then, 
    \[\lim_{x \rightarrow a} f(x) = \lim_{x \rightarrow a} h(x) = C \implies \lim_{x \rightarrow a} g(x) = C\]
    \end{theorem}

  \subsection{Asymptotic Behavior of Functions}

    \begin{definition}[Little-O Notation]
      The function $f: E \longrightarrow \mathbb{R}$ is said to be \textbf{infinitesimal compared with the function $g: E \longrightarrow \mathbb{R}$} as $x \rightarrow a$, written (by abuse of notation) $f = o(g)$ as $x \rightarrow a$, if 
      \[\lim_{x \rightarrow a} \frac{f(x)}{g(x)} = 1\]
      or in other words, if $f/g$ is an infinitesimal function as $x \rightarrow a$. Therefore, $f = o(1)$ as $x \rightarrow a$ means that $f$ is infinitesimal as $x \rightarrow a$. 
      \begin{center}
          \includegraphics[scale=0.25]{img/Little_o_Functions.jpg}
      \end{center}
      Note that writing $f = o(g)$ is again, an abuse of notation. $f = o(g)$ is really a shorthand way of writing that $f$ is in the class of functions that is infinitesimal compared with the function $g$. 
    \end{definition}

    Intuitively, $f = o(g)$ means that the ratio between $f(x)$ and $g(x)$ will tend to infinity as $x \rightarrow a$ (this does not mean that $f$ will be infinitely greater than $g$, however!). For example, looking at the two functions $f(x) = x^2$ and $g(x) = x$, we have 
    \begin{enumerate}
      \item $x^2 = o(x)$ as $x \rightarrow 0$ (since $\frac{x^2}{x} = x$ is infinitesimal as $x \rightarrow 0$)
      \item $x = o(x^2)$ as $x \rightarrow \infty$ (since $\frac{x}{x^2} = \frac{1}{x}$ is infinitesimal as $x \rightarrow \infty$)
    \end{enumerate}
    We can visualize $g/f (x)$ tending to infinity within a neighborhood of $0$ and $f/g (x)$ tending to infinity within a neighborhood of $\infty$. 
    \begin{center}
        \includegraphics[scale=0.25]{img/Comparison_of_Quadratic_and_Linear.PNG}
    \end{center}

    \begin{definition}[Orders of Infinitesimals, Infinities]
      If $f = o(g)$ and $g$ is infinitesimal as $x \rightarrow a$, then $f$ is an \textbf{infinitesimal of higher order than $g$ as $x \rightarrow a$}. Furthermore, if $f$ and $g$ are infinite functions as $x\rightarrow a$ and $f = o(g)$ as $x \rightarrow a$, then $g$ is a \textbf{higher order infinity than $f$ as $x \rightarrow a$}. 
    \end{definition}

    \begin{definition}[Big-O Notation]
      By abuse of notation, $f = O(g)$ as $x \rightarrow a$ means that 
      \[\lim_{x \rightarrow a} \frac{f(x)}{g(x)} = \infty\]
      or in other words, $f/g$ is ultimately bounded as $x \rightarrow a$. In particular, $f = O(1)$ as $x \rightarrow a$ means that $f$ is bounded within a certain neighborhood $U(a)$ of $a$. 

      In the visual below, we can see that $f=O(g)$ as $x \rightarrow +\infty$ since the limit converges to constant $\frac{B}{A}$ which is bounded. In fact, at any other positive real number $x$, $(f/g)(x)$ is finite and is therefore bounded. However, at every neighborhood of $x = 0$, $(f/g)(x)$ is unbounded, meaning that $g \neq O(f)$ as $x \rightarrow 0$. 
      \begin{center}
          \includegraphics[scale=0.25]{img/Big_O_Functions.PNG}
      \end{center}
    \end{definition}

    \begin{definition}[Functions of Same Order]
      The functions $f$ and $g$ are of the same over as $x \rightarrow a$, written 
      \[f \asymp g \text{ as } x \rightarrow a\]
      if $f = O(g)$ and $g = O(f)$ as $x \rightarrow a$. Intuitively, this means that the ratio between $f$ and $g$ within some deleted neighborhood of $a$ is finite. 

      In the visual below, we can see that as long as $k \neq a$, $f = O(g)$ as $x \rightarrow k$ and as $x \rightarrow \infty$. In other words, the function $f/g$ becomes ultimately bounded at every other point other than $a$, and $f/g$ is unbounded within every neighborhood of $a$. When looking at $g/f$, we can see that this function is bounded for all $x \in \mathbb{R}$ and therefore $g = O(f)$ as $x \rightarrow k$ for all $k$. 
      \begin{center}
          \includegraphics[scale=0.25]{img/Functions_of_Same_Order.PNG}
      \end{center}
      Therefore, we can see that as long as $k \neq a$, $f \asymp g$ as $x \rightarrow k$.

      Note that the condition that $f$ and $g$ be of the same order as $x \rightarrow a$ is (by definition of ultimately bounded functions) equivalent to the condition that there exist $c_1, c_2 > 0$ and an open neighborhood $U (a)$ such that the relations
      \[c_1 |g(x)| \leq |f(x)| \leq c_2 |g(x)|\]
      is true for $x \in U(a)$. 
    \end{definition}

    \begin{definition}[Asymptotic Equivalence of Functions]
      For functions $f$ and $g$, if 
      \[\lim_{x \rightarrow a} \frac{f(x)}{g(x)} = 1\]
      we say that \textbf{$f$ behaves asymptotically like $g$ as $x \rightarrow a$}, or that \textbf{$f$ is equivalent to $g$ as $x \rightarrow a$}, written 
      \[f \sim g \text{ as } x \rightarrow a\]
      Moreover, $\sim$ is an equivalence relation, which means that
      \begin{enumerate}
        \item $f \sim f$ as $x \rightarrow a$
        \item $f \sim g$ as $x \rightarrow a \implies$ $g \sim f$ as $x \rightarrow a$
        \item $f \sim g$ and $g \sim h$ as $x \rightarrow a \implies f \sim h$ as $x \rightarrow a$
      \end{enumerate}
    \end{definition}

    We list a few examples in order to develop some sort of visual intuition for when two functions are asymptotically equivalent. 
    \begin{enumerate}
      \item If $f(a) = g(a) \neq 0$, then $f \sim g$ trivially since the ratio of $f$ and $g$ converges to $1$ within a neighborhood of $a$. 
      \begin{center}
          \includegraphics[scale=0.3]{img/trivial_case_equal_value.jpg}
      \end{center}
      \item When $f(a) = g(a) = 0$, it may be $f$ may be equivalent to $g$ or one function may be infinitesimally smaller than the other. 
      \begin{enumerate}
        \item When $f(x) = \sin{x}$ and $g(x) = x$, then $f \sim g$ since we see that 
        \[\lim_{x \rightarrow 0} \frac{sin{x}}{x} = 1\]
        and so $\sin{x} \sim x$ as $x \rightarrow 1$
        \begin{center}
            \includegraphics[scale=0.25]{img/x_vs_sin_x.PNG}
        \end{center}
        \item When $f(x) = x^2$ and $g(x) = x^4$, then  \[\lim_{x \rightarrow 0} \frac{x^4}{x^2} = 0\]
        and so $x^4 \not\sim x^2$. In fact, $x^4 = o(x^2)$. Therefore, since $x^4$ decreases to $0$ infinitely faster than $x^2$, they are not equivalent. 
        \begin{center}
            \includegraphics[scale=0.25]{img/x_fourth_vs_x_squared.jpg}
        \end{center}
        \item When $f(x) = x^2$ and $g(x) = x^3$, then  \[\lim_{x \rightarrow 0} \frac{x^3}{x^2} = 0\]
        and so $x^3 \not\sim x^2$. In fact, $x^3 = o(x^2)$. Therefore, since $x^4$ decreases to $0$ infinitely faster than $x^2$, they are not equivalent.  
        \begin{center}
            \includegraphics[scale=0.25]{img/x_squared_vs_x_cubed.jpg}
        \end{center}
        \item When $f(x) = x^2$ and $g(x) = 0.5x^2$, then 
        \[\lim_{x \rightarrow 0} \frac{0.5x^2}{x^2} = \frac{1}{2}\]
        and so $0.5x^2 \not\sim x^2$. Therefore, since $0.5x^2$ is always as twice as small as $x^2$, they are not equivalent. 
        \begin{center}
            \includegraphics[scale=0.25]{img/x_squared_vs_half_x_squared.PNG}
        \end{center}
      \end{enumerate}
      \item When analyzing the behavior of functions as $x \rightarrow \infty$, we can picture the two graphs of $f$ and $g$ on the plane and "zoom out" to see if the ratio of the values converge to $1$. This would mean that as $x \rightarrow \infty$, we should see the graphs overlapping more and more. For example, taking $f(x) = x^2$ and $g(x) = x^2 + 10x + 100$, we can see that the discrepancy is high around a neighborhood of $x = 0$. But as $x \rightarrow +\infty$, we get
      \[\lim_{x \rightarrow + \infty} \frac{x^2 + 10x + 100}{x^2} = 1\]
      and so the graphs look like they are overlapping. 
      \begin{center}
          \includegraphics[scale=0.25]{img/Behavior_of_Quadratics_of_Same_order_as_infinity.PNG}
      \end{center}
      Notice that even though the absolute difference $|(x^2 + 10x + 100) - x^2| = |10x + 100|$ tends to infinity, this difference increases infinitesimally compared to $f$ and $g$. 
    \end{enumerate}

    From this, we can see that if $f \sim g$ as $x \rightarrow a$, then their difference 
    \[f - g = o(g) = o(f)\]
    That is, $(f-g)(x)$ is infinitesimal compared to $g$ or $f$ (doesn't matter which one we compare it to). This leads to our next section, where we formalize this concept with absolute and relative errors. 

    \subsubsection{Approximations of Functions}
    It is useful to note that since the relation $\lim_{x \rightarrow a} \gamma(x) = 1$ is equivalent to 
    \[\gamma (x) = 1 + \alpha(x), \text{ where } \lim_{x \rightarrow a} \alpha(x) = 0\]
    the relation $f \sim g$ as $x\rightarrow a$ is equivalent to saying that
    \[\frac{f(x)}{g(x)} = \gamma(x), \text{ where } \lim_{x \rightarrow a} \gamma(x) = 1\]
    which implies 
    \[f(x) = g(x) + \alpha(x) g(x) = g(x) + o(g(x)) \text{ as } x \rightarrow a\]
    or, symmetrically, 
    \[g(x) = f(x) + \alpha(x) f(x) = f(x) + o(f(x)) \text{ as } x \rightarrow a\]
    This means that $f$ can be exactly represented by another function $g$, plus another (error) function $o(g(x))$ that is infinitesimal compared to $g$. 
    \begin{center}
        \includegraphics[scale=0.28]{img/Error_Approximation_of_Function.PNG}
    \end{center}
    \textbf{Note that it is not a sufficient condition that the error function be infinitesimal!} The error function $f-g$ must be infinitesimal \textbf{compared to $g$}! This tells us that not only does the error function decrease infinitesimally, but also is infinitesimal compared to the approximation function we already have, which is in general a much stronger claim. This representation of certain types functions will provide the foundation for differential calculus when we talk about "good" approximations for a function. 


    \begin{definition}[Relative Error]
      Since $f \sim g$ as $x \rightarrow a$ means that 
      \[f(x) = g(x) + \alpha(x) g(x) = g(x) + o(g(x))\]
      we can define the \textbf{relative error} of $g$ as an approximation of $f$ to be
      \[|\alpha(x)| = \bigg| \frac{f(x) - g(x)}{g(x)} \bigg|\]
      Clearly, since $f \sim g$, the relative error must be infinitesimal as $x \rightarrow a$. 
    \end{definition}

    We use the following lemma to check whether two functions are asymptotically equivalent. 
    \begin{lemma}
      $f \sim g$ as $x \rightarrow a$ if and only if the relative error of $g$ is infinitesimal as $x \rightarrow a$. 
    \end{lemma}

    \begin{example}
      We claim that 
      \[x^2 + x = \bigg(1 + \frac{1}{x} \bigg) x^2 \sim x^2 \text{ as } x \rightarrow \infty\]
      We see that the absolute error of this approximation 
      \[|(x^2 + x) - x^2| = |x|\]
      tends to infinity, but the relative error
      \[\frac{|x|}{x^2} = \frac{1}{|x|}\]
      tends to $0$ as $x \rightarrow \infty$. 
    \end{example}

    \begin{theorem}[Prime Number Theorem]
    Let $\pi(x)$ be the number of prime numbers strictly less than $x$. Then $\pi \sim \frac{x}{\ln{x}}$ as $x\rightarrow + \infty$, or more precisely, 
    \[\pi(x) = \frac{x}{\ln{x}} + o \bigg( \frac{x}{\ln{x}}\bigg) \text{ as } x \rightarrow +\infty\]
    \end{theorem}

    \begin{example}
      It is a fact that $\lim_{x\rightarrow 0} \frac{sin{x}}{x} = 1$, so we have $\sin{x} \sim x$ as $x \rightarrow 0$. So,
      \[\sin{x} = x + o(x) \text{ as } x \rightarrow 0\]
    \end{example}

    The following theorem proves useful when computing limits. 
    \begin{theorem}
    If $f \sim \Tilde{f}$ as $x \rightarrow a$, then 
    \[\lim_{x \rightarrow a} f(x) g(x) = \lim_{x \rightarrow a} \Tilde{f}(x) g(x)\]
    provided one of these limits exist. 
    \end{theorem}

    \begin{theorem}[Properties of $o(g)$ and $O(g)$ Functions]
    For $x \rightarrow a$, 
    \begin{enumerate}
      \item $o(f) + o(f) = o(f)$
      \item $o(f)$ is also $O(f)$
      \item $o(f) + O(f) = O(f)$
      \item $O(f) + O(f) = O(f)$
      \item If $g(x) \not\equiv 0$, then 
      \[\frac{o(f(x))}{g(x)} = o \bigg( \frac{f(x)}{g(x)} \bigg), \text{ and } \frac{O(f(x))}{g(x)} = O \bigg( \frac{f(x)}{g(x)} \bigg)\]
    \end{enumerate}
    \end{theorem}

\section{Continuous Functions}

    \begin{definition}[Continuity of a Function]
      A function $f$ is \textbf{continuous at point $a$} if for any neighborhood $V\big(f(a)\big)$ of $f(a)$, there is a neighborhood $U(a)$ of $a$ whose image under the mapping $f$ is contained in $V\big( f(a)\big)$. 

      Generalizing this, we say that a function is \textbf{(globally) continuous} if the preimage of every neighborhood in its codomain is an open set in its domain. 
    \end{definition}

    The equivalent of these statements for functions $f: \mathbb{R}^n \longrightarrow \mathbb{R}^m$ follows from the fact that any neighborhood of a point contains a symmetric neighborhood of the point. 
    \begin{center}
        \includegraphics[scale=0.3]{img/Symmetric_Neighborhood_in_Neighborhood.PNG}
    \end{center}

    \begin{lemma}[Existence of Limits of Continuous Functions]
      $f: E \longrightarrow \mathbb{R}$ is continuous at $a \in E$, where $a$ is a limit point of $E$ if and only if 
      \[\lim_{x \rightarrow a} f(x) = f(a)\]
    \end{lemma}
    \begin{proof}
    The limit equalling $f(a)$ means that, by definition, for any arbitrarily small deleted neighborhood of $f(a)$, denoted $U_{f(a)} \setminus f(a)$, its preimage will be an open neighborhood of $a$, which itself will contain an open set. 
    \end{proof}

    This also means that we can use the Cauchy limit definition to defined continuity of a function at a point. That is, for any sequence $\{a_n\}$ of point in codomain $E$ which converges to point $a$, the function $f$ is continuous at $a$ if the corresponding sequence $\{f(a_n)\}$ converges to $f(a)$.

    \begin{theorem}
    This means that the continuous functions commute with the operation of passing to the limit at a point. 
    \[\lim_{x \rightarrow a} f(x) = f\Big( \lim_{x \rightarrow a} x \Big)\]
    \end{theorem}

    \begin{lemma}[Properties of Continuous Functions]
      Let $f: \mathbb{R}^n \longrightarrow \mathbb{R}^m, \; g: \mathbb{R}^m \longrightarrow \mathbb{R}^p$ with $c \in \mathbb{R}$. 
      \begin{enumerate}
        \item $f$ continuous at $x_0 \implies$ $c f$ continous at $x_0$. 
        \item $f, g$ continuous at $x_0 \implies f + g$ continuous at $x_0$. 
        \item Let $m = 1$. $f, g$ continuous at $x_0 \implies f g$ continuous at $x_0$. 
        \item $f$ continuous at $x_0$ and $f(x) \neq 0 \forall x \in \mathbb{R}^n \implies 1 / f$ continuous at $x_0$. 
        \item If $f(x) = \big( f_1(x), f_2(x), ..., f_n(x) \big)$ coordinate-wise, then 
      \[ f \text{ continuous at } x_0 \iff f_1, f_2, ..., f_m \text{ continuous  at } x_0\]
          \item $f$ continuous at $x_0$ and $g$ continuous at $y_0 = f(x_0) \implies g \circ f$ continuous at $x_0$. 
      \end{enumerate}
    \end{lemma}
    \begin{proof}
    This is an immediate result of the equivalence of a function being continuous at point $a$ and its limit at point $a$ existing. 
    \end{proof}

  \subsection{Points of Discontinuity}

    \begin{definition}[Discontinuity]
      If the function $f: E \longrightarrow \mathbb{R}$ is not continuous at a point of $E$, then this point is called a \textbf{point of discontinuity}, or simply a \textbf{discontinuity} of $f$. 

      That is, $a$ is a point of discontinuity of $f$ if for some neighborhood $V(f(a))$ of $f(a)$, there exists no neighborhood of $a$ whose image under the mapping $f$ is contained in $V(f(a))$. 
      There are three types of discontinuities: 
      \begin{enumerate}
        \item A \textbf{removable discontinuity} is characterized by the fact that the limit $\lim_{x \rightarrow a} f(x) = A$ exists, but $A \neq f(a)$. \begin{center}
            \includegraphics[scale=0.23]{img/Removable_Discontinuity.PNG}
        \end{center}
        This means that we can modify $f$ and define a new function $\Tilde{f}: E \longrightarrow \mathbb{R}$ as
        \[\Tilde{f}(x) = \begin{cases}
        f(x), & x \in E \setminus a \\
        A, & x = a
        \end{cases}\]
        which would be continuous on $E$. 
        \item A \textbf{discontinuity of first kind}, also known as a jump/step discontinuity, is characterized by both the left and right-hand limits 
        \[\lim_{x \rightarrow a-0} f(x) \text{ and } \lim_{x \rightarrow a+0} f(x)\]
        existing, but at least one of them is not equal to the value $f(a)$ that the function assumes at $a$. 
        \begin{center}
            \includegraphics[scale=0.23]{img/Discontinuity_First.PNG}
        \end{center}
        \item A \textbf{discontinuity of second kind}, also known as an essential discontinuity, is characterized by at least one of the two limits 
        \[\lim_{x \rightarrow a-0} f(x) \text{ and } \lim_{x \rightarrow a+0} f(x)\]
        not existing. 
        \begin{center}
            \includegraphics[scale=0.23]{img/Discontinuity_Second.PNG}
        \end{center}
      \end{enumerate}
      Note that strictly speaking, a removable discontinuity is really a discontinuity of first kind, but in this context we distinguish them. 
    \end{definition}

    \begin{example}[Dirichlet Function]
      The Dirichlet function, defined
      \[\mathcal{D}(x) = \begin{cases}
      1, & \text{ if } x \in \mathbb{Q} \\
      0, & \text{ if } x \in \mathbb{R} \setminus \mathbb{Q} 
      \end{cases}\]
      is discontinuous at every point, and obviously all of its discontinuities are of second kind, since in every interval there are both rational and irrational numbers and therefore there exists no limit at any point $a \in \mathbb{R}$. 

      More specifically, given any point $a \in \mathbb{R}$, assume that $a$ is rational. We can set $\epsilon = 0.1$-neighborhood around the value $1$, but no matter how small we let $\delta$, the interval $(a - \delta, a + \delta)$ will contain both rationals and irrationals, meaning that it will map to $\{0,1\}$ always, which is not fully contained in $(0.9, 1.1)$.  
    \end{example}

    Here is a slightly more interesting example. 

    \begin{example}[Riemann Function]
      Let the Riemann function $\mathcal{R}$ be defined
      \[\mathcal{R}(x) = \begin{cases}
      \frac{1}{n}, & \text{ if } x = \frac{m}{n} \in \mathbb{Q}, \text{ where gcd}(m, n) = 1 \\
      0, & \text{ if } x \in \mathbb{R} \setminus \mathbb{Q}
      \end{cases}\]
      We first note that for any point $a \in \mathbb{R}$, any bounded neighborhood $U(a)$ of it, and any number $N \in \mathbb{N}$, the neighborhood $U(a)$ contains only a finite number of rational numbers $\mathbb{m}{n}$, where $n < N$. By shrinking the neighborhood, we can assume that the denominators of all rational numbers in the neighborhood are larger than $N$. We can visualize why this is by seeing that rational numbers with larger denominators have smaller "gaps" between them. 
      \begin{center}
          \includegraphics[scale=0.23]{img/Rationals_Spread_Apart.PNG}
      \end{center}
      Thus, at any point $x \in U(a) \setminus a$, we have 
      \[\big| \mathcal{R}(x) \big| < \frac{1}{N}\]
      and therefore
      \[\lim_{x \rightarrow a} \mathcal{R} (x) = 0\]
      at any point $a \in \mathbb{R} \setminus \mathbb{Q}$. Hence, the Riemann function is continuous at any irrational number. 
    \end{example}

  \subsection{Properties of Continuous Functions}

    \begin{theorem}[Local Properties of Continuous Functions]
    Let $f: E \longrightarrow \mathbb{R}$ be a function that is continuous at the point $a \in E$. Then, 
    \begin{enumerate}
      \item $f$ is bounded in some neighborhood $U(a)$. 
      \item If $f(a) \neq 0$, then in some neighborhood $U(a)$ all the values of the function have the same sign as $f(a)$. 
      \item If the function $g: U(a) \subset E \longrightarrow \mathbb{R}$ is defined in some neighborhood of $a$ and is continuous at $a$, then the following functions 
      \begin{align*}
          & (f + g) (x) \\
          & (f \cdot g) (x) \\
          & \bigg( \frac{f}{g} \bigg) \big( x \big) \text{ where } g(a) \neq 0
      \end{align*}
      are also defined in $U(a)$ and continuous at $a$. 
      \item If the function $g: Y \longrightarrow \mathbb{R}$ is continuous at a point $b \in Y$ and $f$ is such that $f: E \longrightarrow Y$, $f(a) = b$, and $f$ is continuous at $a$, then the composite function 
      \[g \circ f: E \longrightarrow \mathbb{R}\]
      is defined on $E$ and continuous at $a$. 
      \begin{center}
          \includegraphics[scale=0.25]{img/Composition_Function_Continuous.PNG}
      \end{center}
      This is easy to see because given the open neighborhood of $g(b)$, we know for a fact that $U_\delta (a)$ maps completely into $U_\epsilon (b)$, and that $U_\epsilon (b)$ maps completely into $U_\kappa (g(b))$ and so the composition of these mappings must mean that $U_\delta (a)$ maps completely into $U_\kappa (g(b))$. 
    \end{enumerate}
    \end{theorem}

    \begin{example}
      An algebraic polynomial 
      \[P(x) = a_0 x^n + a_1 x^{n-1} + a_2 x^{n-2} + \ldots + a_{n-1} x + a_n\]
      is a continuous function on $\mathbb{R}$. Since $f(x) = x$ and $f(x) = c$ are continuous functions, by induction on $x$, we can multiply them together to find that $f(x) = x^n$ is continuous, which implies that $a x^n$ is continuous, which implies that the sums of these functions are also continuous. 
    \end{example}

    Unlike local properties, the global property of a function is a property involving the entire domain of definition of the function. 

    \begin{theorem}[Intermediate Value Theorem]
    If a function that is continuous on a closed interval assumes values with different signs at the endpoints of the interval, then there is a point in the interval where it assumes the value $0$. 
    \begin{center}
        \includegraphics[scale=0.25]{img/IVT.PNG}
    \end{center}
    \end{theorem}
    \begin{proof}

    \end{proof}

    This following proof provides a very simple algorithm for finding the zero of the equation $f(x) = 0$ on an interval whose endpoints has values with opposite signs. 
    Note that the colloquial description of the intermediate value theorem, that is is impossible to pass continuously from positive to negative values without assuming the value $0$ along the way), assumes more than they state. That is, this theorem is actually dependent on the domain of definition: that is is a closed interval, or more generally, that it is \textbf{connected}. 

    \begin{corollary}
    If a function $f$ is continuous on an open interval and assumes values $f(a) = A, f(b) = B$, then for any number $C \in (A, B)$, there is a point $c$ between $a$ and $b$ such that $f(c) = C$. 
    \begin{center}
        \includegraphics[scale=0.25]{img/Corollary_of_IVT.PNG}
    \end{center}
    \end{corollary}

    \begin{theorem}[Weierstrass Maximum-Value Theorem]
    A function that is continuous on a closed interval is bounded in that interval, with a maximum and minimum. 
    \end{theorem}

    \subsubsection{Uniform Continuity}
    Roughly speaking, a function $f$ is uniformly continuous if it is possible to guarantee that $f(x)$ and $f(y)$ be as close to each other as we please by requiring only that $x$ and $y$ be sufficiently close to each other. 

    \begin{definition}[Uniform Continuity]
      A function $f: E \longrightarrow \mathbb{R}$ is \textbf{uniformly continuous} on a set $E \subset \mathbb{R}$ if for every $\epsilon > 0$, there exists $\delta > 0$ such that 
      \[\big| f(x_1) - f(x_2)\big| < \epsilon\]
      for all points $x_1, x_2 \in E$ such that $|x_1 - x_2| < \delta$. 

      Intuitively, uniform continuity says that given any two points $x, y$ in the domain where their distance is arbitrarily small ($\delta$ apart), we can guarantee that the distance between $f(x), f(y)$ is at maximum some arbitrarily small $\epsilon$. 

      The following visual shows the radical function $f(x) = \sqrt{x}$ defined on $\mathbb{R}^+$. We can see that it satisfies uniform continuity because the graph does not escape the top and/or bottom of the $\epsilon \times \delta$ window, no matter where the box is located on the graph. More strictly speaking, no matter what we set the $\epsilon$ (how long the box is), uniform continuity says that we can choose a sufficient $\delta$ (width of the box) such that the graph does not escape the top/bottom of the window no matter where the window is. 
      \begin{center}
          \includegraphics[scale=0.28]{img/Uniform_Continuity_Radical.PNG}
      \end{center}
      We can clearly see that the function $f(x) = 1/x$ is not uniformly continuous, since the graph escapes the $\epsilon \times \delta$ window at some point (marked in red). More strictly speaking, given any length $\epsilon$ of the window, we cannot create a thin-enough $\delta$ box that will contain the graph, since as $x \rightarrow 1$, the function becomes unbounded. 
      \begin{center}
          \includegraphics[scale=0.25]{img/Uniform_Continuity_Rational.PNG}
      \end{center}
      That is, arbitrarily thin boxes don't help when the slope is arbitrarily steep. 
    \end{definition}

    To compare uniform continuity with regular continuity, we can adapt this alternate (yet equivalent interpretation): Let there exist function $f: E \longrightarrow \mathbb{R}$. Given any $\epsilon>0$, we can choose a $\delta>0$ such that given any point $x \in E$ and $f(x)$, as long as a second point $y$ is $\delta$ away from $x$, then $f(y)$ is $\epsilon$ away from $f(x)$. This visualization would lead to there being a $2\epsilon \times 2\delta$ window around point $x$. 
    \begin{center}
        \includegraphics[scale=0.3]{img/Double_Epsilon_Delta_Uniform_Continuity.PNG}
    \end{center}
    Uniform continuity means that the box above does not change dimensions no matter where the point is (hence, the name uniform). Therefore, given a certain $\epsilon > 0$, the way we choose $\delta$ is only dependent on $\epsilon$, and so it must be a function of $\epsilon$: 
    \[\delta = \delta(\epsilon)\]
    However, in continuity, there just has to exist \textbf{some} $\delta$-neighborhood of $x$ such that its image is contained in the $\epsilon$-neighborhood of $f(x)$. There are no restrictions on the dimensions of this box; it just has to exist. 
    \begin{center}
        \includegraphics[scale=0.28]{img/Regular_Continuity_Box_Visual.PNG}
    \end{center}

    \begin{lemma}
      If $f$ is uniformly continuous on the set $E$, it is continuous at each point of that set. However, the converse is not generally true. 
    \end{lemma}

    \begin{theorem}[Cantor's Theorem on Uniform Continuity]
    A function that is continuous on a closed interval is uniformly continuous on that interval. 
    \end{theorem}


    \begin{example}
      Let $f: \mathbb{R} \longrightarrow \mathbb{R}, \; f(x) = 3x+7$. Then $f$ is uniformly continuous. Choose $\epsilon > 0$. Let $\delta = \epsilon / 3$. Choose $x, y \in \mathbb{R}$ and assume $|x-y| < \delta$. Then, 
      \[ | f(x) - f(y) | = | 3x + 7 - 3 y - 7 | = 3 |x-y| < 3 \delta = \epsilon\]
    \end{example}

    \begin{example}
      Let $f: (0, 4) \subset \mathbb{R} \longrightarrow \mathbb{R}, \; f(x) = x^2$. Then $f$ is uniformly continuous on $(0, 4)$. Choose $\epsilon > 0$. Let $\delta = \epsilon / 8$. Choose $x, y \in (0, 4)$ and assume $|x-y| < \delta$. Then, 
      \[ |f(x) - f(y)| = |x^2 - y^2| = (x+y) |x-y| < (4+4) |x-y| = 8\delta = \epsilon\]
    \end{example}

    In both examples, the function satisfied an inequality of form 
    \[ |f(x_1) - f(x_2)| \leq M |x_1 - x_2|\]
    this is called the Lipshitz inequality. 

    \subsubsection{Lipshitz Continuity}
    Lipshitz continuity is a strong form of uniform continuity for functions. Intuitively, a Lipshitz continuous function is limited in how fast it can change (by the Lipshitz constant). 

    \begin{definition}[Lipshitz Continuous Function]
      Given $f: E \subset \mathbb{R} \longrightarrow \mathbb{R}$, $f$ is \textbf{Lipshitz continuous} if there exists a positive real constant $M$ such that for all real $x, y \in E$, 
      \[\big| f(x) - f(y) \big| \leq M \big| x - y \big|\]
      The corresponding $M$ is called the \textbf{Lipshitz constant}, and the smallest constant $M$ satisfying this inequality is called the \textbf{best Lipshitz constant}. 

      Note that Lipshitz continuity pops up as a very natural extension of uniform continuity. The inequality above just means that given an $\epsilon$, we can choose a $\delta$ such that a linear multiple of $\delta$ is always greater than $\epsilon$. This means that Lipshitz continuity is just uniform continuity such that the $\delta$ function is linear:  
      \[\delta = \delta(\epsilon) = \frac{1}{M} \epsilon\]
      \begin{center}
          \includegraphics[scale=0.25]{img/Lipshitz_Continuity.jpg}
      \end{center}
    \end{definition}

    Another way to interpret uniform continuity is by seeing that the derivative of $f$ is bounded by the slope $M$. 
    \begin{center}
        \includegraphics[scale=0.3]{img/Lipshitz_Continuity_Slope_Bound.PNG}
    \end{center}
    This slope bound implies that for every pair of points on the graph of this function, the absolute value of the slope of the line connecting them is not greater than $M'$. The smallest $M'$ is the best Lipshitz constant. 

    \begin{definition}[Bi-Lipshitz Continuity]
      A function $f: E \subset \mathbb{R}$ is \textbf{Bi-Lipshitz continuous} if there exists constant $M\geq 1$ such that for all real $x, y \in E$, 
      \[ \frac{1}{M} |x - y| \leq |f(x) - f(y)| \leq M |x - y|\]
      A visual of this map is shown, where the function $f$ must always land in the shaded green area. 
      \begin{center}
          \includegraphics[scale=0.25]{img/BiLipshitz_Map.PNG}
      \end{center}
      It immediately follows that for $x \neq y$, $ |f(x) - f(y)|$ cannot equal $0$, which means that a bilipshitz map is injective. A bilipshitz map is really just Lipshitz map with its inverse also being Lipshitz. 
    \end{definition}

    \begin{proposition}
    A bilipshitz map $f$ is a homeomorphism onto its image. 
    \end{proposition}

    \subsubsection{Inverse Function Theorem}

    We begin by introducing this intuitive lemma. 
    \begin{lemma}
      A continuous mapping $f: E \longrightarrow \mathbb{R}$ of a closed interval $E = [a,b]$ into $\mathbb{R}$ is injective if and only if the function $f$ is strictly monotonic on $[a,b]$. 

      Furthermore, every strictly monotonic function $f: X \subset \mathbb{R} \longrightarrow \mathbb{R}$ (for arbitrary $X$) has an inverse 
      \[f^{-1}: f(X) \subset \mathbb{R} \longrightarrow \mathbb{R}\]
      with the same kind of monotonicity on $f(X)$ that $f$ has on $X$. 
    \end{lemma}

    \begin{lemma}[Criterion for Continuity of a Monotonic Function]
      A monotonic function $f: E \longrightarrow \mathbb{R}$ defined on a closed interval $E = [a,b]$ is continuous if and only if its set of values $f(E)$ is the closed interval with endpoints $f(a)$ and $f(b)$. 

      Note that both conditions imply that there are no points of discontinuities in the graph of $f$. 
    \end{lemma}


    \begin{theorem}[Inverse Function Theorem]
    A function $f: X \longrightarrow \mathbb{R}$ that is strictly monotonic on a set $X \subset \mathbb{R}$ has an inverse $f^{-1}: Y \longrightarrow \mathbb{R}$ defined on the set $Y = f(X)$ of values of $f$. The function $f^{-1}: Y \longrightarrow \mathbb{R}$ is monotonic and has the same type of monotonicity on $Y$ that $f$ has on $X$. 
    \begin{center}
        \includegraphics[scale=0.25]{img/Inverse_Function_Theorem_Analysis.PNG}
    \end{center}
    If in addition, $X$ is a closed interval $[a,b]$ and $f$ is continuous on $X$, then the set $Y = f(X)$ is the closed interval with endpoints $f(a)$ and $f(b)$ and the function $f^{-1}: Y \longrightarrow \mathbb{R}$ is continuous on it.
    \end{theorem}

    \begin{example}
      The function $f(x) = \sin{x}$ is increasing and continuous on the closed interval $\big[ -\frac{\pi}{2}, \frac{\pi}{2} \big]$. Hence, the restriction to the closed interval $\big[ -\frac{\pi}{2}, \frac{\pi}{2} \big]$ has an inverse $x = f^{-1}(y)$, which is denoted by 
      \[x = \arcsin{y}\]
      This function is defined on the closed interval $\big[- \sin\big(-\frac{\pi}{2}\big), \sin\big(-\frac{\pi}{2}\big) \big] = [-1,1]$ and increases continuously from $-\frac{\pi}{2}$ to $\frac{\pi}{2}$. 
      \begin{center}
          \includegraphics[scale=0.25]{img/Inverse_Function_Theorem_Sin.PNG}
      \end{center}
    \end{example}


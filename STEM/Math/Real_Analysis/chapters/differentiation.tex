\section{Differentiation}

  \subsection{Functions Differentiable at a Point}

    \begin{definition}[Differentiable Function]
      A function $f: E \subset \mathbb{R} \longrightarrow \mathbb{R}$ is \textbf{differentiable} at a given point $x$ (that is a limit point of $E$) if there exists a linear function $h \mapsto df(x) h$ (called the \textbf{differential of $f$}) and an infinitesimal $\alpha (x;h) = o(h)$ as $h \rightarrow 0$, such that
      \[f(x + h) - f(x) = df(x) (h) + \alpha (x; h)\]
      Note that $x$ is fixed; what we are really interested here is the $h$ value. Furthermore, 
      \begin{enumerate}
        \item $\Delta x(h) \equiv (x + h) - x = h$ is called the \textbf{increment of the argument}
        \item $\Delta f(x;h) \equiv f(x + h) - f(x)$ is called the \textbf{increment of the function} 
      \end{enumerate}
      They are often denoted (inappropriately) by the symbols $\Delta x$ and $\Delta f(x)$ representing functions of $h$. The differential and the infinitesimal can be visualized below. 
      \begin{center}
          \includegraphics[scale=0.25]{img/Differential_Diagram.PNG}
      \end{center}
    \end{definition}

    \begin{definition}[Derivative]
      Given function $f: E \subset \mathbb{R} \longrightarrow \mathbb{R}$, the number
      \[f^\prime (x) = \lim_{h \rightarrow 0} \frac{f(x+h) - f(x)}{h}\]
      is called the \textbf{derivative} of the function $f$ at $x$. It can be visualized as the sequence of the slopes of the secant lines converging onto the slope of the black tangent line as shown. 
      \begin{center}
          \includegraphics[scale=0.27]{img/Tangent_Lines_Converging.PNG}
      \end{center}
      This equality can also be written in the equivalent form: 
      \[\frac{f(x+h) - f(x)}{h} = f^\prime (x) + \alpha (h)\]
      where $\alpha$ is infinitesimal as $h \rightarrow 0$. This also also equivalent to:
      \[f(x+h) - f(x) = f^\prime (x) h + o (h)\]
      where the error term $o(h) \rightarrow 0$ as $h \rightarrow 0$. 
    \end{definition}

    Note that we have defined the differentiability of a function at a point and the existence of its derivative at a point completely separately. But it turns out that the existence of this arbitrary number $f^\prime (x)$ we call the "derivative," defined
    \[f^\prime (x) = \lim_{h \rightarrow 0} \frac{f(x + h) - f(x)}{h}\]
    actually has an equivalent form of 
    \[f(x + h) - f(x) = f^\prime (x) h + o(h)\]
    But since $f^\prime(x)$ is in $\mathbb{R}$, the function $h \mapsto f^\prime (x) h$ is linear and $o(h)$ is infinitesimal, so it is in the form 
    \[f(x + h) - f(x) = df (x) (h) + \alpha(x; h)\]
    which, by definition, means that it is differentiable! Therefore, we have determined the equivalence between the differentiability of a function at a point and the existence of its derivative at the same point. Furthermore, this function $h \mapsto f^\prime (x) h$ is precisely the differential of $f$, meaning that
    \[df (x) (h) = f^\prime (x) h\]
    Furthermore, 
    \[\Delta f(x; h) - df(x)(h) = \alpha (x; h)\]
    and $\alpha(x;h) = o (h)$ as $h \rightarrow 0$, or in other words, the difference between the increment of the function and the value of the function $df(x)$ in $h$ is an infinitesimal of higher order than the first in $h$. For this reason, we say that the differential is the \textbf{principal linear part of the increment of the function}. 

    In particular, if $f(x) \equiv x$, then we have $f^\prime (x) \equiv 1$ and 
    \[dx (h) = 1 \cdot h = h\]
    Substituting this equality into $df(x) (h) = f^\prime (x) h$, we get
    \[df (x) (h) = f^\prime (x) \,dx (h)\]
    or without the input parameter $h$, 
    \[df(x) = f^\prime (x) \,dx\]
    Note that this is an equality between two functions of $h$. From this, we obtain the familiar \textbf{Leibniz notation} of the derivative: 
    \[\frac{df (x) (h)}{dx(h)} = f^\prime (x) \iff \frac{df(x)}{dx} = f^\prime (x)\]
    That is, the function $\frac{df(x)}{dx}$, which is the ratio of the functions $df(x)$ and $dx$, is constant and equals $f^\prime (x)$. 

  \subsection{Tangent Line: Geometric Meaning of the Derivative, Differential}

    Let us try to construct successive approximations to an arbitrary function $f: E \longrightarrow \mathbb{R}$ at a given limit point $x_0$. That is, we find a function $g$ such that
    \[f = g + o(g)\]
    Depending on what $g$ is, we can construct better approximations of $f$. 

    \subsubsection{Constant Approximation}
    The 0th order approximation is when $g$ is a constant. That is, $g \equiv c_0$ for some $c_0 \in \mathbb{R}$. This means
    \[f(x) = c_0 + o(c_0) = c_0 + o(1) \text{ as } x \rightarrow x_0\]
    More precisely, we want this difference $f(x) - c_0$ to be $o(1)$ as $x \rightarrow x_0$, which means that it is simply infinitesimal. Visualizing this, we can see that given a constant approximation (labeled in blue) to a function at $x_0$, its error term (labeled in green) is in fact, infinitesimal. All this boils down to the fact that 
    \[\lim_{x \rightarrow x_0} f(x) = c_0\]
    If the function is continuous at $x_0$, then 
    \[\lim_{x \rightarrow x_0} f(x) = f(x_0)\]
    and naturally $c_0 = f(x_0)$. Both the continuous (left) and noncontinuous case (right) is shown, but in most cases, we will assume continuity. 
    \begin{center}
        \includegraphics[scale=0.3]{img/Constant_Approximation_Continuous_Noncontinuous_case.PNG}
    \end{center}

    \subsubsection{Linear Approximation}
    The 1st order approximation is a linear function that approximates $f$ as
    \[f(x) = c_0 + c_1(x - x_0) + o(x - x_0) \text{ as } x \rightarrow x_0\]
    Following the previous logic, assuming $f$ continuous means that $c_0 = f(x_0)$. Furthermore, as $x \rightarrow x_0$
    \begin{align*}
        f(x) = c_0 + c_1(x - x_0) + o(x - x_0) & \implies c_1 = \frac{f(x) - c_0 - o(x - x_0)}{x - x_0} \\
        & \implies c_1 = \frac{f(x) - c_0}{x - x_0} - \frac{o(x - x_0)}{x - x_0}\\
        & \implies c_1 = \frac{f(x) - c_0}{x - x_0} - o(1) \\
        & \implies c_1 = \lim_{x \rightarrow x_0} \frac{f(x) - c_0}{x - x_0} = f^\prime (x_0)
    \end{align*}
    But this just means that $f^\prime (x_0) = c_1$, Note that before, we have proved the equivalence of the existence of a derivative at $x_0$ with differentiability at $x_0$ (which itself means that there exists a linear approximation $df(x)(h)$ that is a function of $h$). Here, we have created a linear approximation with respect to $x = x_0 + h$, rather than $h$ (shifted the function). 

    Therefore, the function 
    \[\alpha (x) = f(x_0) + f^\prime (x_0) (x - x_0)\]
    provides the best linear approximation to the function $f$ in a neighborhod of $x_0$ in the sense that for any other function $\beta(x)$ of the form 
    \[\beta(x) = c_0 + c_1 (x - x_0)\]
    we have $f(x) - \beta(x) \neq o(x - x_0)$ as $x \rightarrow x_0$. The graph of the function $\alpha$ is the straight line
    \[y - f(x_0) = f^\prime (x_0) (x - x_0)\]

    This leads to the definition of our familiar tangent line. 

    \begin{definition}[Tangent Line]
      If a function $f: E \longrightarrow \mathbb{R}$ is differentiable at a point $x_0 \in E$, the line defined by
      \[y - f(x_0) = f^\prime (x_0) (x - x_0)\]
      is called the \textbf{tangent} to the graph of $f$ at the point $(x_0, f(x_0))$. 
    \end{definition}

    \subsubsection{Higher Order Approximations}
    We can continue this pattern to get a quadratic approximation of $f$ in the form
    \[f(x) = c_0 + c_1 (x - x_0) + c_2 (x - x_0)^2 + o\big((x - x_0)^2 \big) \text{ as } x \rightarrow x_0\]
    As we have done in the previous subsection, we can derive (assuming continuity of $f$) $c_0 = f(x_0), c_1 = f^\prime (x_0)$. To derive what $c_2$ should be, we see that the equation above implies
    \[c_2 = \frac{f(x) - c_0 - c_1 (x - x_0) - o\big((x - x_0)^2 \big)}{(x - x_0)^2} = \frac{f(x) - c_0 - c_1 (x - x_0)}{(x - x_0)^2} - o(1)\]
    which means
    \[c_2 = \lim_{x \rightarrow x_0} \frac{f(x) - c_0 - c_1 (x - x_0)}{(x - x_0)^2}\]
    Extending this, if we are seeking a polynomial $P_n(x_0; x) = c_0 + c_1 (x - x_0) + \ldots + c_n (x - x_0)^n$ such that
    \[f(x) = c_0 + c_1 (x - x_0) + \ldots + c_n (x - x_0)^n + o\big((x - x_0)^n\big) \text{ as } x \rightarrow x_0\]
    we would find 
    \begin{align*}
        c_0 & = \lim_{x \rightarrow x_0} f(x) \\
        c_1 & = \lim_{x \rightarrow x_0} \frac{f(x) - c_0}{x - x_0} \\
        c_2 & = \lim_{x \rightarrow x_0} \frac{f(x) - c_0 - c_1 (x - x_0)}{(x - x_0)^2} \\
        \ldots & = \ldots \\
        c_n & = \lim_{x \rightarrow x_0} \frac{f(x) - (c_0 + \ldots + c_{n-1}(x - x_0)^{n-1})}{(x - x_0)^n}
    \end{align*}

    We formalize the order of these approximations by analyzing their error bound. 

    \begin{definition}[nth Order Contact]
      If $f, g: E \longrightarrow \mathbb{R}$ are continuous at point $x_0$ and $(f - g) (x) = o\big( (x - x_0)^n \big)$ as $x \rightarrow x_0$, then we say that $f$ and $g$ have \textbf{$n$th order contact at $x_0$}, or more precisely, \textbf{contact of order at least $n$}. 

      The following visual shows approximations $g$ of an arbitrary function $f$ that have $0$th (left), $1$st (middle), and $2$nd (right) order contact at $x_0$. 
      \begin{center}
          \includegraphics[scale=0.25]{img/nth_order_contact.PNG}
      \end{center}
    \end{definition}

    \subsubsection{The Real Tangent Space}

    Tangent spaces. 

    \begin{definition}[Tangent Space]
      Given function $f: E \longrightarrow \mathbb{R}$ and a point $x_0 \in E$, the increment of the argument $h = x - x_0$ can be regarded as a vector attached to the point $x_0$ and defining the transition from $x_0$ to $x_0 + h$. $h$ is called a \textbf{tangent vector}, and the set of all such vectors as $T_{x_0} \mathbb{R}$. Similarly, we denote $T_{y_0} \mathbb{R}$ the set of all displacement vectors from the point $y_0$ along the $y$-axis. 
      \begin{center}
          \includegraphics[scale=0.25]{img/Tangent_Space_1_dimensional_in_R.PNG}
      \end{center}
      Then, we can see that the differential is a mapping
      \[df(x_0): T_{x_0} \mathbb{R} \longrightarrow T_{f(x_0)} \mathbb{R}\]
      Note that that there are two functions to pay attention to here: 
      \begin{enumerate}
        \item The true increment of $f$, defined $h \mapsto f(x_0 + h) - f(x_0) = \Delta f(x_0; h)$ (labeled in green). 
        \item The differential $h \mapsto f^\prime (x_0) h = df(x_0) (h)$, which gives the increment of the tangent to the graph for increment $h$ in the argument (labeled in red). 
      \end{enumerate}
    \end{definition}

    \begin{example}
      Let $f(x) = \sin{x}$. Then we will show that $f^\prime (x) = \cos{x}$. 
      \begin{align*}
          \lim_{h \rightarrow 0} \frac{\sin{(x+h)} - \sin(x)}{h} & = \lim_{h \rightarrow 0} \frac{2 \sin \big( \frac{h}{2} \big) \cos \big( x + \frac{h}{2} \big)}{h} \\
          & = \lim_{h \rightarrow 0} \cos \Big( x + \frac{h}{2} \Big) \cdot \lim_{h\rightarrow 0} \frac{\sin\big( \frac{h}{2}\big)}{\big(\frac{h}{2}\big)} = \cos(x)
      \end{align*}
      Here, we have used the theorem on the limit of a product, the continuity of the function $\cos(x)$, the equivalence $\sin{t} \sim t$ as $t \rightarrow 0$, and the theorem on the limit of a composite function. 
    \end{example}

    \begin{example}
      We will show that $\cos^\prime (x) = - \sin(x)$. 
      \begin{align*}
          \lim_{h\rightarrow 0} \frac{\cos(x+h) - \cos(x)}{h} & = \lim_{h \rightarrow 0} \frac{-2 \sin \big(\frac{h}{2}\big) \, \sin \big( x + \frac{h}{2}\big)}{h} \\
          & = - \lim_{h\rightarrow 0} \sin \Big( x + \frac{h}{2} \Big) \cdot \lim_{h\rightarrow0} \frac{\sin\big(\frac{h}{2} \big)}{\big( \frac{h}{2} \big)} = -\sin(x)
      \end{align*}
    \end{example}

  \subsection[Rules of Differentation over R]{Rules of Differentiation over \(\mathbb{R}\)}

    \subsubsection{Basic Properties; Derivatives of Composite, Inverse Functions}

    \begin{theorem}[Arithmetic Properties of Differentiation over $\mathbb{R}$]
    If functions $f, g: E \longrightarrow \mathbb{R}$ are differentiable at a point $x \in E$, then 
    \begin{enumerate}
      \item their sum is differentiable at $x$, and 
      \[d(f+g) (x) = df(x) + dg(x) \iff (f+g)^\prime (x) = (f^\prime + g^\prime) (x)\]
      \item their product is differentiable at $x$, and 
      \[d (f \cdot g) (x) = g(x) df(x) + f(x) dg(x) \iff (f \cdot g)^\prime (x) = f^\prime (x) \cdot g(x) + f(x) \cdot g^\prime (x)\]
      \item their quotient is differentiable at $x$ if $g(x) \neq 0$, and 
      \[d \Big( \frac{f}{g} \Big) (x) =  \frac{g(x) df(x) - f(x) dg(x)}{g^2 (x)} \iff \bigg(\frac{f}{g}\bigg)^\prime (x) = \frac{f^\prime (x) g(x) - f(x) g^\prime (x)}{g^2 (x)}\]
    \end{enumerate}
    It is clear that $c\cdot df(x) = d (cf)(x)$, it is clear that the derivative is a linear operator from the space of all functions differentiable at $x_0$ the space of all functions. 
    \end{theorem}
    \begin{proof}
    Since $f$ and $g$ are differentiable at $x$, there exists the differential $df(x)(h) = f^\prime (x) h$ and $dg(x) = g^\prime (x) h$ where
    \begin{align*}
        f(x + h) & = f(x) + df(x)(h) + o(h) = f(x) + f^\prime (x) h + o(h) \\
        g(x + h) & = g(x) + dg(x)(h) + o(h) = g(x) + g^\prime (x) h + o(h) 
    \end{align*}
    From this relation, we can clearly see that a certain property of the differential automatically implies the same property of the derivative. (Remember that $f^\prime (x)$ and $g^\prime(x)$ are not functions! They are scalars defined on fixed point $x$.) 
    \begin{enumerate}
      \item Even though this derivation may be a bit long, every step is included to minimize ambiguity. 
      \begin{align*}
          (f + g)(x + h) - (f + g)(x) & = \big( f(x + h) + g(x + h)\big) - \big( f(x) + g(x)\big) \\
          & = \big( f(x + h) - f(x)\big) + \big(g(x + h) - g(x) \big) \\
          & = \big( df(x)(h) + o(h)\big) + \big( dg(x)(h) + o(h)\big) \\
          & = \big(f^\prime (x) h + o(h)\big) + \big( g^\prime (x) (h) + o(h)\big) \\
          & = \big(f^\prime(x) + g^\prime (x)\big) h + o(h) \\
          & = (f^\prime + g^\prime)(x)(h) + o(h) \\
          & = d (f + g)(x) h + o(h)
      \end{align*}
      \item For the product rule, we have
      \begin{align*}
          (f \cdot g) (x + h) & - (f \cdot g)(x) = f(x+h)g(x+h) - f(x) g(x) \\
          & = \big(f(x) + df(x) (h) + o(h)\big)\big(g(x) + dg(x)(h) + o(h)\big) - f(x) g(x) \\
          & = \big(f(x) + f^\prime (x) h + o(h)\big)\big(g(x) + g^\prime (x) h + o(h)\big) - f(x) g(x)
      \end{align*}
      Expanding this gives 
      \begin{multline*}
          \big(f^\prime (x) g(x) + f(x) g^\prime(x)\big) h + \big(f(x) + g(x)\big) o(h) + \\ f^\prime (x) g^\prime (x) h^2 + \big(f^\prime (x) + g^\prime (x) \big) h o(h) + \big(o(h)\big)^2
      \end{multline*}
      but note that since $f(x), g(x), f^\prime(x), g^\prime (x)$ are constants, we see that 
      \begin{enumerate}
        \item $\big(f(x) + g(x)\big) o(h) = o(h)$ because 
        \[\lim_{h \rightarrow 0} \frac{\big(f(x) + g(x)\big) o(h)}{h} = \big(f(x) + g(x)\big) \lim_{h \rightarrow 0} \frac{o(h)}{h} = 0\]
        \item $f^\prime(x) g^\prime (x) h^2 = o(h)$ since
        \[\lim_{h \rightarrow 0} \frac{f^\prime(x) g^\prime (x) h^2}{h} = f^\prime(x) g^\prime (x) \lim_{h \rightarrow 0}  h = 0\]
        \item $\big(f^\prime(x) + g^\prime(x)\big) h o(h) = o(h)$ because 
        \[\lim_{h \rightarrow 0} \frac{\big(f^\prime(x) + g^\prime(x)\big) h o(h)}{h} = \big(f^\prime(x) + g^\prime(x)\big) \lim_{h \rightarrow 0} o(h) = 0\]
        In fact, this term is of $o(h^2)$. 
        \item We can see that $(o(h))^2 = o(h)$ since 
        \[\lim_{h \rightarrow 0} \frac{(o(h))^2}{h} = \lim_{h \rightarrow 0} \frac{o(h)}{h} \cdot \lim_{h \rightarrow 0} o(h) = 0 \cdot 0 = 0\]
        In fact, $(o(h))^2 = o(h^2)$. 
      \end{enumerate}
      Therefore, the above simplifies to 
      \[(f \cdot g)(x + h) - (f \cdot g) (x) = \big(f^\prime(x) g(x) + f(x) g^\prime (x)\big) h + o(h)\]
      But this means that the differential (best approximation) $d(f \cdot g) (x)$ must be 
      \[(f \cdot g)^\prime (x) (h) = (f \cdot g)^\prime (x) h = \big(f^\prime(x) g(x) + f(x) g^\prime (x)\big) h\]
      \item Since the function $g(x) \neq 0$ at point $x$, then by continuity we can assume that there exists a neighborhood $U(x)$ where the image of that neighborhood does not vanish. That is, we can guarantee that $g(x + h) \neq 0$ for sufficiently small values of $h$. We assume $h$ is small in the following computations. 
      \begin{align*}
          \bigg(\frac{f}{g}\bigg) (x + h) - \bigg( \frac{f}{g}\bigg) (x) & = \frac{f(x + h)}{g(x + h)} - \frac{f(x)}{g(x)} \\
          & = \frac{1}{g(x)g(x + h)} \big( f(x + h) g(x) - f(x) g(x + h)\big) \\
          & = \bigg(\frac{1}{g^2(x)} + o(1)\bigg) \Big( \big(f(x) + f^\prime(x) h + o(h)\big) g(x) \\
          & \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;- f(x) \big( g(x) + g^\prime(x) h + o(h)\big)\Big) \\
          & = \bigg(\frac{1}{g^2(x)} + o(1)\bigg) \Big(\big(f^\prime(x) g(x) - f(x) g^\prime(x)\big) h + o(h)\Big) \\
          & = \frac{f^\prime(x) g(x) - f(x) g^\prime(x)}{g^2 (x)} h + o(h)
      \end{align*}
      Note that here we have used the continuity of $g$ at the point $x$ and the fact that $g(x) \neq 0$ to deduce that
      \[\lim_{h \rightarrow 0} \frac{1}{g(x) g(x + h)} = \frac{1}{g^2(x)} \iff \frac{1}{g(x) + g(x + h)} = \frac{1}{g^2(x)} + o(1)\]
      where $o(1)$ is infinitesimal as $h \rightarrow 0$. 
    \end{enumerate}
    \end{proof}

    \begin{theorem}[Chain Rule for Composite Functions over $\mathbb{R}$]
    Let there be functions $f: E_1 \subset \mathbb{R} \longrightarrow E_2 \subset \mathbb{R}$ is differentiable at a point $x \in E_1$ and the function $g: E_2 \subset \mathbb{R} \longrightarrow \mathbb{R}$ is differentiable at point $y = f(x) \in E_2$, with respective differentials 
    \begin{align*}
        df(x)& : T_x \mathbb{R} \longrightarrow T_y \mathbb{R} \\
        dg(y)& : T_y\mathbb{R} \longrightarrow T_{g(y)} \mathbb{R}
    \end{align*}
    Then the composite function $g \circ f: E_1 \longrightarrow \mathbb{R}$ is differentiable at $x$, and $d(g \circ f)(x): T_x \mathbb{R} \longrightarrow T_{g \circ f(x)} \mathbb{R}$ is
    \[d(g \circ f)(x) = d g(y) \circ d f(x) \iff (g \circ f)^\prime (x) = g^\prime \big( f(x) \big) \circ f^\prime (x)\]
    \end{theorem}
    \begin{proof}
    We will denote the increment of the argument with the variables $h$ and $t$. Then, by differentiability of $f$ and $g$, we have
    \begin{align*}
        f(x + h) - f(x) & = f^\prime (x) h + o(h) \text{ as } h \rightarrow 0 \\
        g(y + t) - g(y) & = g^\prime(y) t + o(t) \text{ as } t \rightarrow 0
    \end{align*}
    Since the function $o(t)$ can be represented as $o(t) = \gamma(t) t$, where $\gamma = o(1)$ and hence is infinitesimal as $t \rightarrow 0$, meaning that we can assume $\gamma(0) = 0$ (since $o(t)$ is defined for $t = 0$). 

    We can think of the displacement of $x$ as like a chain reaction: As$x \mapsto x + h$, $f(x) \mapsto f(x + h)$, which we could interpret as $y \mapsto y + t$ and hence means that $g(y) \mapsto g(y + t)$. So, setting $f(x) = y$ and $f(x + h) = y + t$, by differentiability and hence continuity of $f$ at point $x$, we can conclude that $t \rightarrow 0$ as $h \rightarrow 0$. So, we have
    \[\gamma\big(f(x+h) - f(x)\big) = \gamma\big( (y+t) - y\big) = \gamma(t) = \alpha(h) \rightarrow 0 \text{ as } h \rightarrow 0\]
    Thus, we get 
    \begin{align*}
        o(t) = \gamma(t) t & = \gamma\big( f(x + h) - f(x)\big)\big( f(x + h) - f(x)\big) \\
        & = \alpha(h) \big(f^\prime(x) h + o(h)\big) \\
        & = \alpha(h) f^\prime(x) h + \alpha(h) o(h) \\
        & = o(h) + o(h) = o(h) \text{ as } h \rightarrow 0 \\
        (g \circ f)(x + h) - (g \circ f)(x) & = g\big(f(x + h)\big) - g\big(f(x)\big) \\
        & = g (y + t) - g(y) \\
        & = g^\prime (y) t + o(t) \\
        & = g^\prime \Big(f(x)\big) \big(f(x + h) - f(x)\big) + o\big( f(x + h) - f(x)\big) \\
        & = g^\prime \big(f(x)\big) \big(f^\prime (x) h + o(h)\big) + o\big( f(x + h)\big) - f(x)\big) \\
        & = g^\prime\big( f(x) \big) \big( f^\prime (x) h\big) + g^\prime \big( f(x)\big) \big(o(h)\big) + o\big(f(x + h) - f(x)\big) 
    \end{align*}
    Since $g^\prime\big(f(x)\big) \big( o(h)\big)$ is really just a constant multiplied by a function that is $o(h)$, it is $o(h)$. $o\big( f(x + h) - f(x) \big)$. As for $o\big(f(x + h) - f(x)\big)$, we see that since $f(x + h) - f(x) = t$, a function that is $o\big(f(x + h) - f(x)\big)$ becomes infinitesimal compared to $t$ as $t \rightarrow 0$. As already stated before, we have
    \[o\big(f(x + h) - f(x) \big) = o(h) \text{ as } h \rightarrow 0\]
    and thus, we proved that
    \begin{align*}
        (g \circ f)(x + h) - (g \circ f)(x) & = g^\prime (y) f^\prime (x) h + o(h) \\
        & = \big(dg(y) \circ df(x)\big) (h) + o(h)
    \end{align*}
    \end{proof}

    \begin{theorem}[Differentiation of Inverse Functions over $\mathbb{R}$]
    Let $E_1, E_2 \subset \mathbb{R}$, and $f: E_1 \longrightarrow E_2$ and $f^{-1}: E_2 \longrightarrow E_1$ be mutually inverse and continuous at points $x_0 \in E_1$ and $f(x_0) = y_0 \in E_2$. If $f$ is differentiable at $x_0$ and $f^\prime(x_0) \neq 0$, then $f^{-1}$ also differentiable at the point $y_0$, and 
    \[\big(f^{-1}\big)^{-1} (y_0) = \big(f^\prime (x_0)\big)^{-1} \iff df^{-1} (y_0) = \big(df(x_0)\big)^{-1}\]
    \begin{center}
        \includegraphics[scale=0.25]{img/Real_Analysis_Differentiation_Inverse_Functions.PNG}
    \end{center}
    Note that if we knew in advance that $f^{-1}$ was differentiable at $y_0$ (which is a stronger hypothesis), we can find immediately by the identity 
    \[(f^{-1} \circ f) (x) = x\]  
    and the theorem on the differentiation of a composite function that
    \[(f^{-1})^\prime (y_0) \cdot f^\prime (x_0) = 1\]
    \end{theorem}

    Note that if the hypothesis was satisfied, but $f^\prime (x_0) = 0$, then $f^{-1}$ would not be differentiable since it would have an undefined differential. 
    \begin{center}
        \includegraphics[scale=0.3]{img/Inverse_Function_Differentiation_Derivative_Zero_problem.PNG}
    \end{center}
    \subsubsection{Higher-Order Derivatives}

    \begin{definition}[Global Derivative Function]
      If function $f: E \longrightarrow \mathbb{R}$ is differentiable at every point $x \in E$, then a new function $f^\prime: E \longrightarrow \mathbb{R}$, whose value at a point $x \in E$ equals the derivative $f^\prime(x)$ of the function $f$ at the point. 
    \end{definition}

    \begin{definition}[Second, Nth Derivative]
      This function $f^\prime$ may itself have a derivative $(f^\prime)^\prime : E \longrightarrow \mathbb{R}$, called the \textbf{second derivative} of the original function $f$, denoted 
      \[f^{\prime\prime} (x), \;\;\; \frac{d^2 f(x)}{dx^2}\]
      By induction, if the derivative $f^{(n-1)} (x)$ of order $n-1$ of $f$ has been defined, then the \textbf{derivative of order $n$} is defined by the formula
      \[f^{(n)} (x) \equiv \big(f^{(n-1)}\big)^\prime (x)\]
      The set of function $f: E \longrightarrow \mathbb{R}$ having continuous derivatives up to order $n$ inclusive is denoted $C^{n} (E, \mathbb{R})$. 
    \end{definition}

    \begin{lemma}[Leibniz' Formula]
      Let $u(x)$ and $v(x)$ be functions having derivatives up to order $n$ inclusive on a common set $E$. Then, 
      \[(uv)^{(n)} = \sum_{m = 0}^n \binom{n}{m} u^{(n-m)} v^{(m)}\]
      This means that given a polynomial $P_n (x) = c_0 + c_1 (x - x_0) + \ldots + c_n (x - x_0)^n$, then 
      \begin{align*}
          P_n(x_0) & = 0 \\
          P_n^\prime (x_0) & = 1! c_1 \\
          P_n^{\prime\prime} (x_0) & = 2! c_2 \\
          \ldots & = \ldots \\
          P_n^{(n)} (x_0) & = n! c_n \\
          P_n^{(k)} (x_0) & = 0 \text{ for } k > n
      \end{align*}
      and thus the polynomial $P_n (x)$ can be written as
      \[P_n (x) = P_n^{(0)} (x_0) + \frac{1}{1!} P_n^{(1)} (x_0) (x-x_0) + \frac{1}{2!} P_n^{(2)} (x_0) (x-x_0)^2 + \ldots + \frac{1}{n!} P_n^{(n)} (x_0) (x-x_0)^n\]
    \end{lemma}

  \subsection{Theorems of Differential Calculus}

    \subsubsection{Fermant's Lemma, Rolle's Theorem}

    \begin{definition}[Local Extrema]
      A point $x_0 \in E \subset \mathbb{R}$ is called a \textbf{local maximum} (resp. \textbf{local minimum}) and the value of a function $f: E \longrightarrow \mathbb{R}$ at that point a \textbf{local maximum value} (resp. \textbf{local minimum value}) if there exists a neighborhood $U_E (x_0)$ of $x_0$ in $E$ such that at any point $x \in U_E (x_0)$ we have 
      \[f(x) \leq f(x_0), \big( \text{resp. } f(x) \geq f(x_0) \big)\]
      If this is a strict inequality
      \[f(x) < f(x_0), \big( \text{resp. } f(x) > f(x_0) \big)\]
      then $x_0$ is called a \textbf{strict local maximum} (resp. \textbf{strict local minimum}). 
    \end{definition}

    \begin{definition}[Interior Extrema]
      An extremum $x_0 \in E$ of the function $f: E \longrightarrow \mathbb{R}$ is called an \textbf{interior extremum} if $x_0$ is not on the boundary of $E$, or more rigorously, $x_0$ is a limit point of both sets $E_- = \{x \in E \;|\; x < x_0\}$ and $E_+ = \{ x\in E\;|\; x > x_0\}$. For example, the graphs below are interior extrema at $x_0, x_0^*$. 
      \begin{center}
          \includegraphics[scale=0.25]{img/Interior_Extrema.PNG}
      \end{center}
      But the following graphs show extrema (at $x_0$) that are not interior extrema. 
      \begin{center}
          \includegraphics[scale=0.25]{img/Non_Interior_Extrema.PNG}
      \end{center}
    \end{definition}

    \begin{lemma}[Fermant]
      If a function $f: E \longrightarrow \mathbb{R}$ is differentiable at an interior extrememum $x_0 \in E$, then its derivative at $x_0$ is $0$. That is, 
      \[f^\prime (x_0) = 0\]
      Thus, this lemma gives a necessary condition for an interior extremum of a differentiable function. But for non-iterior extrema, it is generally not true that $f^\prime(x_0) = 0$ and so the converse does not hold (labeled in green). 
      \begin{center}
          \includegraphics[scale=0.25]{img/Fermant_Condition_for_Extrema.PNG}
      \end{center}
    \end{lemma}
    \begin{proof}
    By definition of differentiability at $x_0$ we get
    \begin{align*}
        f(x_0 + h) - f(x_0) & = f^\prime (x_0) h + o(h) \\
        & = f^\prime(x_0) h + \alpha (x_0; h) h \\
        & = \big(f^\prime (x_0) + \alpha(x_0; h)\big) h
    \end{align*}
    where we know that $o(h)$ can be written as $o(1) h$ for some infinitesimal $o(1)$ as $h \rightarrow 0$. If $f^\prime (x_0) \neq 0$, then for $h$ sufficiently close to $0$ the quantity $f^\prime(x_0) + \alpha(x_0; h)$ would have the same sign as $f^\prime (x_0)$, since $\alpha(x_0; h) \rightarrow 0$ as $h \rightarrow 0$. But the value of $h$ can be both positive or negative, given that $x_0$ is an interior extremum. This contradiction must imply that $f^\prime (x_0) = 0$. 
    \end{proof}

    Geometrically, Fermant's lemma is obvious, since it asserts that at an extremum of a differentiable function, the tangent to its graph is horizontal. Physically, this lemma means that in motion along a line the velocity must be zero at the instant then the direction reverses. 

    \begin{theorem}[Rolle's Theorem]
    If a function $f: [a, b] \longrightarrow \mathbb{R}$ is continuous on a closed interval $[a,b]$ and differentiable on the open interval $(a, b)$ and $f(a) = f(b)$, then there exists a point $\zeta \in (a, b)$ such that $f^\prime (\zeta) = 0$. 
    \begin{center}
        \includegraphics[scale=0.25]{img/Analysis_Rolles_Theorem.PNG}
    \end{center}
    \end{theorem}

    \subsubsection{Mean Value Theorem, Cauchy's Finite-Increment Theorem}

    The following theorem is extremely useful in studying numerical valued functions. 

    \begin{theorem}[Mean Value Theorem]
    If a function $f: [a,b] \longrightarrow \mathbb{R}$ is continuous on a closed interval $[a,b]$ and differentiable on the open interval $(a, b)$, there exists a point $\zeta \in (a, b)$ such that 
    \[f^\prime (\zeta) = \frac{f(b) - f(a)}{b - a} \iff f(b) - f(a) = f^\prime (\zeta) (b-a)\]
    Geometrically, this means that there exists a tangent line somewhere at $\zeta \in (a, b)$ that is parallel the secant line connecting the two points $\big(a, f(a)\big)$ and $\big( b, f(b)\big)$. 
    \begin{center}
        \includegraphics[scale=0.25]{img/Analysis_Mean_Value_Theorem_Diagram.PNG}
    \end{center}
    \end{theorem}

    Some remarks: 
    \begin{enumerate}
      \item Physically, if $x$ is interpreted as time and $f(b) - f(a)$ as the amount of displacement over the time $b-a$ of a particle moving along the line, this theorem says that the velocity $f^\prime (x)$ of the particle at some time $\zeta \in (a, b)$ is such that if the particle had moved with constant velocity $f^\prime (\zeta)$ over the whole time interval, it would have been displaced by the same amount $f(b) - f(a)$. We call $f^\prime (\zeta)$ the \textbf{average velocity} over the time interval $[a, b]$. 
      \item Note that the Mean Value Theorem is important in that it connects the increment of a function over a finite interval with the derivative of the function on that interval. Up to now, we have characterized only the local (infinitesimal) increment of a function in terms of the derivative or differential at a given point. MVT connects the increment of a function over a \textbf{finite} interval with the derivative of the function. 
    \end{enumerate}

    The MVT actually leads to multiple useful corollaries. 

    \begin{corollary}[Derivative of a Monotonic Function]
    If the derivative of a function is nonnegative (resp. positive) at every point of an open interval, then the function is nondecreasing (resp. increasing) on that interval. 
    \end{corollary}
    \begin{proof}
    If $x_1 < x_2$ are two points of the interval, then the MVT
    \[f(x_2) - f(x_1) = f^\prime (\zeta) (x_2 - x_1)\]
    shows that the sign of the left hand side must equal that of the right. 
    \end{proof}

    \begin{corollary}[Derivative of a Constant Function]
    A function that is continuous on a closed interval $[a,b]$ is constant on it if and only if its derivative equals $0$ at every point of the interval $[a,b]$ or the open interval $(a, b)$. 

    Therefore, if the derivatives $f_1^\prime (x)$ and $f_2^\prime (x)$ of two functions $f_1 (x)$ and $f_2 (x)$ are equal on some interval (that is, $f_1^\prime (x) = f_2^\prime (x)$ on the interval), then the difference
    \[(f_1 - f_2) (x) = f_1 (x) - f_2 (x)\]
    is constant. 
    \end{corollary}
    \begin{proof}
    Given constant function $f$, the MVT equation 
    \[0 = f(x_2) - f(x_1) = f^\prime (\zeta) (x_2 - x_1)\]
    implies that $f^\prime (\zeta) = 0$ for all $x_1, x_2 \in E$. It follows that by the arithmetic properties of the derivative, given two functions $f_1, f_2$ with the same derivative on an interval, the derivative of their difference $(f_1 - f_2)^\prime = 0$, and therefore must be constant on that interval. 
    \end{proof}

    The following proposition is a useful generalization of Lagrange's theorem. 
    \begin{theorem}[Cauchy's Finite-Increment Theorem]
    Let $x = x(t)$ and $y = y(t)$ be functions that are continuous on a closed interval $[\alpha, \beta]$ and differentiable on the open interval $(\alpha, \beta)$. Then, there exists a point $\tau \in [\alpha, \beta]$ such that
    \[x^\prime (\tau) \big( y(\beta) - y (\alpha)\big) = y^\prime (\tau) \big( x(\beta) - x(\alpha)\big)\]
    If in addition $x^\prime (t) \neq 0$ for each $t \in (\alpha, \beta)$, then $x(\alpha) \neq x(\beta)$ and we have the equality 
    \[\frac{y(\beta) - y(\alpha)}{x(\beta) - x(\alpha)} = \frac{y^\prime (\tau)}{x^\prime (\tau)}\]
    \end{theorem}

    \subsubsection{Taylor's Formula}
    From the following results one may deduce that the more derivatives of two functions coincide (including the derivative of the $0$th order) at a point, the better these functions approximate each other in a neighborhood of that point. Using Leibniz's rule, approximations up to a certain degree at a point can be expressed as a polynomial 
    \[P_n (x_0; x) = P_n (x_0) + \frac{P_n^\prime (x_0)}{1!} (x-x_0) + ... + \frac{P_n^{(n)} (x_0)}{n!} (x-x_0)^n\]
    where each coefficient of the polynomial 

    \begin{definition}[Taylor Polynomial]
      If a function $f:E \longrightarrow \mathbb{R}$ has derivatives of all orders $n \in \mathbb{N}$ at a point $x_0$, the unique series
      \[P_n (x_0; x) = f(x_0) + \frac{f^\prime (x_0)}{1!} (x-x_0) + ... + \frac{f^{(n)} (x_0)}{n!} (x-x_0)^n\]
      is the \textbf{Taylor polynomial of order $n$ of $f(x)$ at $x_0$}. We can see that the derivatives of $f$ and $P_n$ coincide up to order $n$. 
    \end{definition}

    \begin{definition}[Analytic Functions]
      We cannot assume that the Taylor series of an infinitely differentiable function converges to the function $f$ within a neighborhood $U(x_0)$, nor can we assume that it converges at all! These types of "nice" functions that have a Taylor approximation within the neighborhood of $x_0$ are called \textbf{analytic functions} and can be written in the form 
      \[f(x) =  f(x_0) + \frac{f^\prime (x_0)}{1!} (x-x_0) + ... + \frac{f^{(n)} (x_0)}{n!} (x-x_0)^n + r_n (x_0; x)\]
      where $r$ is called the \textbf{remainder term}. 
    \end{definition}

    \begin{example}[Infinitely Differentiable, Non-Analytic Function]
      A example of a non-analytic function is
      \[f(x) = \begin{cases}
      e^{-1/x^2} & \text{ if } x \neq 0 \\
      0 & \text{ if } x = 0
      \end{cases}\]
      which looks like
      \begin{center}
          \includegraphics[scale=0.25]{img/Infinitely_Differentiable_Non_Analytic_Function.PNG}
      \end{center}
      One can verify that the derivative $f^{(k)} (0) = 0$ for all $k$ and hence the Taylor series is identically equal to $0$, while $f(x) \neq 0$ if $x \neq 0$. 
    \end{example}

    The relationship between these different conditions is nicely summarized in the figure. 
    \begin{center}
    \begin{tikzpicture}
        \draw (-7.5,0) rectangle (7.5, 4);
        \draw[fill=lightgray] (-6.5, 0.5) rectangle (6.5, 3);
        \draw[fill=white] (-5.5, 1) rectangle (5.5, 2);
        \node[above] at (0, 1) {Taylor series converges to $f$ at $x_0 \iff f$ is analytic};
        \node[above] at (0, 2) {Taylor series converges at $x_0$};
        \node[above] at (0, 3) {$f$ infinitely differentiable at $x_0 \iff $ Taylor series of $f$ exists at $x_0$};
    \end{tikzpicture}
    \end{center}

    The following lemma proves why Taylor Polynomials are considered a "good" approximations to analytic functions. 

    \begin{lemma}[Infinitesimality of Functions with Vanishing Derivative up to Order $n$]
      Given a function $\varphi: E \longrightarrow \mathbb{R}$ defined on a closed interval $E$ with endpoint $x_0$, let its derivatives vanish up to order $n$ at $x_0$. That is
      \[\varphi(x_0) = \varphi^\prime (x_0) = \ldots = \varphi^{(n)} (x_0) = 0\]
      Then, $\varphi = o\big((x - x_0)^n\big)$ as $x \rightarrow x_0$. 
    \end{lemma}
    \begin{proof}
    We prove by induction. For $n = 1$, the definition of differentiability states that 
    \[\varphi(x) = \varphi^(x_0) + \varphi^\prime (x - x_0) + o(x - x_0) \text{ as } x \rightarrow x_0\]
    and so we have proved that 
    \[\varphi(x_0) = \varphi^\prime (x_0) = 0 \implies \varphi(x) = o(x - x_0) \text{ as } x \rightarrow x_0\]
    Now, suppose this assertion has been proved for order $n = k - 1 \geq 1$. That is, we have shown that 
    \[\varphi(x_0) = \ldots = \varphi^{(k-1)}(x_0) = 0 \implies \varphi= o\big((x - x_0)^{k-1}\big) \text{ as } x \rightarrow x_0\]
    Then we must show that this is valid for order $n = k \geq 2$. Assume that 
    \[\varphi(x_0) = \varphi^\prime (x_0) = \ldots = \varphi^{(k)} (x_0) = 0\]
    We can see that this is equivalent to
    \[(\varphi^\prime)^\prime (x_0) = (\varphi^\prime)^{(2)} (x_0) = \ldots = (\varphi^\prime)^{(k-1)} = 0\]
    and therefore by the induction assumption, we have
    \[\varphi^\prime = o\big( (x - x_0)^{k-1}\big) \text{ as } x \rightarrow x_0\]
    which means that we can put it in form 
    \[\varphi(x) = \alpha (x) (x - x_0)^{k-1} \text{ so that } \lim_{x \rightarrow x_0} \varphi(x) = \lim_{x \rightarrow x_0} \alpha(x) = 0 \]

    From the mean value theorem and substituting what we have above, we get 
    \begin{align*}
        \varphi(x) = \varphi(x) - \varphi(x_0) & = \varphi^\prime(\zeta) (x - x_0) \\
        & = \varphi (\zeta) (\zeta - x_0)^{k-1} (x - x_0)
    \end{align*}
    where $\zeta \in (x_0, x)$. However, this implies that $|\zeta - x_0| < |x - x_0|$, and thus, as $x \rightarrow x_0$, $\zeta \rightarrow x_0$, which then makes $\alpha(\zeta) \rightarrow 0$. Since
    \[|\varphi (x)| \leq |\alpha(\zeta)| |x - x_0|^{k-1} |x - x_0| = |\alpha(\zeta)| |x - x_0|^k\]
    This means that $\varphi(x)$ is bounded by function $|\alpha(\zeta)| |x - x_0|^k$, which is $o\big((x-x_0)^k\big)$, and so 
    \[\varphi = o\big( (x - x_0)^k \big) \text{ as } x \rightarrow x_0\]
    By induction, this works for all orders $n$. 
    \end{proof}

    \begin{theorem}[Peano's Form of the Remainder]
    Given analytic function $f: E \longrightarrow \mathbb{R}$, a point $x_0 \in E$, and its $n$th order Taylor polynomial $P_n (x_0; x)$ around $x_0$, $P_n$ is a "good" approximation of $f$ in the fact that its error term is $o\big((x - x_0)^n\big)$. That is, 
    \[f(x) = P_n (x_0; x) + o\big((x - x_0)^n \big) \text{ as } x \rightarrow x_0\]
    This equation where $r_n (x; x_0) = o\big((x - x_0)^n\big)$ is called the \textbf{Peano's form of the remainder}. 
    \end{theorem}
    \begin{proof}
    Since the Taylor polynomial $P_n (x_0; x)$ is constructed from the requirement that its derivatives up to order $n$ inclusive must coincide with the corresponding derivatives of $f$ at $x_0$, it follows that
    \[r_n (x_0; x_0) \equiv f^{(k)} (x_0) - P_n^{(k)} (x_0; x_0) = 0 \text{ for } k = 0, 1, \ldots, n\]
    Using the previous lemma, a this means that $r_n (x; x_0) = o\big((x - x_0)^n\big)$ as $x \rightarrow x_0$. 
    \end{proof}

    \begin{theorem}[Lagrange Form of the Remainder]
    If $f: E \longrightarrow \mathbb{R}$ has derivatives of order $n+1$ on the open interval with endpoints $x_0$ and $x$, then 
    \[f(x) = f(x_0) + \frac{f^\prime (x_0)}{1!} (x - x_0) + \ldots + \frac{f^{(n)}(x_0)}{n!} (x - x_0)^n + r_n (x; x_0)\]
    where 
    \[r_n (x; x_0) = \frac{f^{(n+1)} (\zeta)}{(n+1)!} (x - x_0)^{n+1}\]
    This form is called \textbf{Taylor's formula with the Lagrange form of the remainder}. Furthermore, this form says that if $f^{(n+1)} (x)$ is bounded in a neighborhood of $x_0$, it also implies the formula
    \[f(x) = f(x_0) + \frac{f^\prime (x_0)}{1!} (x - x_0) + \ldots + \frac{f^{(n)} (x_0)}{n!} (x - x_0)^n + O\big( (x - x_0)^{n+1} \big)\]
    Therefore, we can use this boundedness of $f^{(n+1)}$ to find the maximum error bound 
    \[|r_n (x; x_0)|\]
    of $P_n (x; x_0)$. 
    \end{theorem}
    \begin{proof}
    It is a direct result from the lemma. This is actually a generalization of the mean value theorem but for higher orders. 
    \end{proof}

    \begin{corollary}[Table of Asymptotic Formulas for Elementary Functions]
    We write the Maclaurin series (Taylor series around $x = 0$) for elementary functions. Note that these error terms are $O(x^{n+1})$ (bounded compared to $x^{n+1}$) and $o(x^n)$ (infinitesimal compared to $x^n$). 
    \begin{align*}
        e^x & = 1 + \frac{1}{1!} x + \frac{1}{2!} x^2 + \ldots + \frac{1}{n!} x^n + O(x^{n+1}) \\
        \cos{x} & = 1 - \frac{1}{2!} x^2 + \frac{1}{4!}x^4 - \ldots + \frac{(-1)^n}{(2n)!} x^{2n} + O(x^{2n+2}) \\
        \sin{x} & = x - \frac{1}{3!} x^3 + \frac{1}{5!}x^5 - \ldots + \frac{(-1)^n}{(2n+1)!} x^{2n+1} + O(x^{2n+3}) \\
        \cosh{x} & = 1 + \frac{1}{2!} x^2 + \frac{1}{4!} x^4 + \ldots + \frac{1}{(2n)!} x^{2n} + O(x^{2n+2}) \\
        \sinh{x} & = x + \frac{1}{3!} x^3 + \frac{1}{5!} x^5 + \ldots + \frac{1}{(2n+1)!} x^{2n+1} + O(x^{2n+3}) \\
        \ln{(1+x)} & = x - \frac{1}{2}x^2 + \frac{1}{3} x^3 - \ldots + \frac{(-1)^n}{n} x^n + O(x^{n+1}) \\
        (1 + x)^\alpha & = 1 + \frac{\alpha}{1!} x + \frac{\alpha(\alpha-1)}{2!} x^2 + \ldots + \frac{\alpha (\alpha-1) \ldots (\alpha - n + 1)}{n!} x^n + O(x^{n+1})
    \end{align*}
    \end{corollary}

  \subsection{The Study of Functions using Differential Calculus}

    \subsubsection{Conditions for Monotonicity of Functions}
    We can now connect the concepts of derivatives and monotonicity. 

    \begin{theorem}[Derivative $\implies$ Monotonicity]
    Given function $f: E \longrightarrow \mathbb{R}$ that is differentiable on an open interval $(a, b) = E$, 
    \begin{align*}
        f^\prime (x) > 0 & \implies f \text{ is increasing} \\
        f^\prime (x) \geq 0 & \iff f \text{ is nondecreasing} \\
        f^\prime (x) \equiv 0 & \iff f \text{ is constant} \\
        f^\prime (x) \leq 0 & \iff \text{ is nonincreasing} \\
        f^\prime (x) < 0 & \implies f \text{ is decreasing} 
    \end{align*}
    Note that if $f$ is strictly increasing (resp. decreasing), we cannot determine that $f^\prime(x) \geq 0$ (resp. $f^\prime (x) \leq 0$). For example, take the function $f(x) = x^3$, which is strictly increasing, but has derivative $f^\prime (0) = 0$ at $x = 0$. 
    \begin{center}
        \includegraphics[scale=0.25]{img/Monotonicity_Counterexample_x3.PNG}
    \end{center}
    It is clearly strictly increasing within a neighborhood $U(0)$, so we can see that
    \begin{align*}
        f \text{ is increasing} & \implies f^\prime (x) \geq 0 \\
        f \text{ is decreasing} & \implies f^\prime (x) \leq 0
    \end{align*}
    \end{theorem}


    \subsubsection{Conditions for Extrema of Functions}
    Similarly, we can connect the concepts of extrema and derivatives. 

    \begin{theorem}[First Derivative Test]
    Let function $f: E \longrightarrow \mathbb{R}$ be defined in a neighborhood $U(x_0)$ of point $x_0$, which is continuous at $x_0$ and differentiable in $\mathring{U}(x_0)$, a deleted neighborhood of $x_0$. (Note that this is broader hypothesis than just assuming that $f$ be differentiable at $x_0$.) Let
    \[\mathring{U}^- (x_0) \equiv \{x \in U(x_0) \;|\; x < x_0\}, \;\; \mathring{U}^+ (x_0) \equiv \{x \in U(x_0) \;|\; x > x_0\}\]
    That is, $\mathring{U}^- (x_0)$ is the left portion of $\mathring{U}(x_0)$ and $\mathring{U}^+ (x_0)$ is the right portion of $\mathring{U}(x_0)$. Then, 
    \begin{enumerate}
      \item $(x_0, f(x_0))$ is strict local minimum if $f^\prime(x) < 0$ in $\mathring{U}^- (x_0)$ and $f^\prime (x) > 0$ in $\mathring{U}^+ (x_0)$. 
      \begin{center}
          \includegraphics[scale=0.25]{img/Strict_Local_minimum.PNG}
      \end{center}
      \item $(x_0, f(x_0))$ is strict local maximum if $f^\prime(x) > 0$ in $\mathring{U}^- (x_0)$ and $f^\prime (x) < 0$ in $\mathring{U}^+ (x_0)$. 
      \begin{center}
          \includegraphics[scale=0.25]{img/Strict_Local_Maximum.PNG}
      \end{center}
      \item $(x_0, f(x_0))$ has no extremum at $x_0$ if $f^\prime (x) > 0$ in both $\mathring{U}^- (x_0), \mathring{U}^+ (x_0)$, or if $f^\prime(x)< 0$ in both $\mathring{U}^- (x_0), \mathring{U}^+ (x_0)$. 
      \begin{center}
          \includegraphics[scale=0.25]{img/No_Extremum.PNG}
      \end{center}
    \end{enumerate}
    \end{theorem}

    Note that if there is a discontinuity at a point $x_0$, then this theorem does not apply. For example, $(x_0, f(x_0))$ in the graph below is a local minimum, even though the derivatives to the left of $x_0$ are positive and those to the right of $x_0$ are negative (within neighborhood $U(x_0)$). Similarly, $(x_0, g(x_0))$ is a local maximum, even though the derivative to the left and to the right of $x_0$ are both positive. 
    \begin{center}
        \includegraphics[scale=0.3]{img/Theorem_not_apply_if_Discontinuity.PNG}
    \end{center}

    \begin{proposition}[2nd, $n$th Derivative Test]
    Let function $f: E \longrightarrow \mathbb{R}$ be defined on a neighborhood $U(x_0)$ of $x_0$ has derivatives of order up to $n$ inclusive at $x_0$. If its derivatives up to the $(n-1)$th order vanishes 
    \[f^\prime (x_0) = f^{\prime\prime} (x_0) ... = f^{(n-1)} (x_0) = 0\]
    but the $n$th derivative at $x_0$ does \textbf{not} vanish
    \[f^{(n)} (x_0) \neq 0\]
    then 
    \begin{enumerate}
      \item $n$ is odd $\implies$ there is no local extremum at $x_0$ 
      \item $n$ is even $\implies$ there is a local extremum at $x_0$
      \begin{enumerate}
        \item $f^{(n)} (x_0) > 0 \implies$ it is a strict local minimum
        \item $f^{(n)} (x_0) < 0 \implies$ it is a strict local maximum
      \end{enumerate}
    \end{enumerate}
    \end{proposition}

    \subsubsection{Important Algebraic Inequalities}

    We also introduce various inequalities that may be useful for producing future results. The following lemmas can be proved with elementary algebra. 

    \begin{lemma}[Young's Inequalities]
      If $a>0$ and $b>0$, and the numbers $p$ and $p$ are such that $p \neq 0, 1$ and $q \neq 0, 1$, and $\frac{1}{p} + \frac{1}{q} = 1$, then 
      \begin{align*}
          a^{\frac{1}{p}} b^{\frac{1}{q}} \leq \frac{1}{p} a + \frac{1}{q} b \text{  if } p > 1 \\
          a^{\frac{1}{p}} b^{\frac{1}{q}} \geq \frac{1}{p} a + \frac{1}{q} b \text{  if } p < 1
      \end{align*}
      and equality holds in both cases if and only if $a = b$. 
    \end{lemma}

    \begin{lemma}[Holder's Inequalities]
      Let $x_i \geq 0, y_i \geq 0$ for $i = 1, 2, ..., n$, and let $\frac{1}{p} + \frac{1}{q} = 1$. Then, 
      \begin{align*}
          &\sum_{i=1}^n x_i y_i \leq \bigg( \sum_{i=1} x_i^p \bigg)^{\frac{1}{p}} \, \bigg( \sum_{i=1} y_i^q \bigg)^{\frac{1}{q}} \text{  for } p > 1 \\
          &\sum_{i=1}^n x_i y_i \geq \bigg( \sum_{i=1} x_i^p \bigg)^{\frac{1}{p}} \, \bigg( \sum_{i=1} y_i^q \bigg)^{\frac{1}{q}} \text{  for } p < 1, p \neq 0
      \end{align*}
    \end{lemma}

    \begin{lemma}[Minkowski's Inequalities]
      Let $x_i \geq 0, y_i \geq 0$ for $i = 1, 2, ... ,n$. Then, 
      \begin{align*}
          \bigg( \sum_{i=1}^n (x_i + y_i)^p \bigg)^{\frac{1}{p}} & \leq \bigg( \sum_{i=1}^n x_i^p \bigg)^\frac{1}{p} + \bigg( \sum_{i=1}^n y_i^p \bigg)^{\frac{1}{p}} \text{  when } p > 1 \\
          \bigg( \sum_{i=1}^n (x_i + y_i)^p \bigg)^{\frac{1}{p}} & \geq \bigg( \sum_{i=1}^n x_i^p \bigg)^\frac{1}{p} + \bigg( \sum_{i=1}^n y_i^p \bigg)^{\frac{1}{p}} \text{  when } p < 1, p \neq 0
      \end{align*}
    \end{lemma}

    \subsubsection{Conditions for a Function to be Convex}
    \begin{definition}[Convex, Concave Functions]
      A function $f: (a, b) \longrightarrow \mathbb{R}$ defined on an open interval $(a, b) \subset \mathbb{R}$ is \textbf{convex} if the inequality
      \[f( \alpha_1 x_1 + \alpha_2 x_2) \leq \alpha_1 f(x_1) + \alpha_2 f(x_2)\]
      holds and \textbf{concave}, or \textbf{convex upward}, if the inequality 
      \[f( \alpha_1 x_1 + \alpha_2 x_2) \geq \alpha_1 f(x_1) + \alpha_2 f(x_2)\]
      holds for all pairs of points $x_1, x_2 \in (a, b)$ and any numbers $\alpha_1, \alpha_2 \geq 0$ such that $\alpha_1 + \alpha_2 = 1$. If this inequality is strict whenever $x_1 \neq x_2$ and $\alpha_1 \alpha_2 \neq 0$, the function is said to be \textbf{strictly convex} and \textbf{strictly concave}, respectively. 

      Visually, this just means that given any two points $a, b$, the graph of a convex function (left) in $(a, b)$ always lies underneath the secant line formed by the two points and the graph of a concave function (right) in $(a, b)$ lies over the secant line formed by the two points. 
      \begin{center}
          \includegraphics[scale=0.25]{img/Convex_Concave_Functions_Secant.PNG}
      \end{center}
      However, this is only a visual aid. In reality, it is actually not only the secant line formed by the two endpoints, but every pairs of points within that interval. For example, even though the secant line $l$ formed by the endpoints $a, b$ is above the whole graph in $(a, b)$, $f$ is not convex over $(a, b)$ since the secant line formed by points $\alpha, \beta$ do not lie completely underneath $f$. 
      \begin{center}
          \includegraphics[scale=0.25]{img/Multiple_Secant_Lines_Convex_Clarification.PNG}
      \end{center}
    \end{definition}

    The following is also another equivalent condition for a function to be convex over $(a, b)$. 

    \begin{proposition}
    A function $f: (a, b) \longrightarrow \mathbb{R}$ that is differentiable on the open interval $(a, b)$ is convex on $(a, b)$ if and only if its graph contains no points below any tangent drawn to it.
    \begin{center}
        \includegraphics[scale=0.25]{img/Convex_Function_Over_Tangent_Line.PNG}
    \end{center}
    \end{proposition}

    \begin{theorem}[2nd Derivatives of Convex Functions]
    Given a function $f: (a, b) \longrightarrow \mathbb{R}$ that is differentiable in its domain, 
    \begin{enumerate}
      \item $f$ is convex $\iff f^\prime$ is nondecreasing on $(a, b) \iff f^{\prime\prime} \geq 0$ on $(a, b)$ 
      \item $f$ is strictly convex $\iff f^\prime$ is increasing on $(a, b) \iff f^{\prime\prime} > 0$ on $(a, b)$ 
      \item $f$ is concave $\iff f^\prime$ is nonincreasing on $(a, b) \iff f^{\prime\prime} \leq 0$ on $(a, b)$ 
      \item $f$ is strictly concave $\iff f^\prime$ is decreasing on $(a, b) \iff f^{\prime\prime} < 0$ on $(a, b)$ 
    \end{enumerate}
    \end{theorem}

    \begin{definition}[Inflection Point]
      Let $f: E \longrightarrow \mathbb{R}$ be a function defined and differentiable on a neighborhood $U(x_0)$. If the function is convex downward (resp. upward) on the set $\mathring{U}^- (x_0) = \{x \in U(x_0) \;|\; x < x_0\}$ and convex upward (resp. downward) on $\mathring{U}^+ (x_0) = \{x \in U(x_0)\;|\; x > x_0\}$, then the point 
      \[\big( x_0, f(x_0) \big)\]
      is called a \textbf{inflection point of the graph}. 
      \begin{center}
          \includegraphics[scale=0.25]{img/Inflection_Point_Analysis.PNG}
      \end{center}
    \end{definition}

    \begin{proposition}[Jensen's Inequality]
    If $f: (a, b) \longrightarrow \mathbb{R}$ is a convex function, $x_1, ..., x_n$ are points of $(a, b)$, and $\alpha_1, ..., \alpha_n$ are nonnegative numbers such that $\alpha_1 + ... + \alpha_n = 1$, then 
    \[f(\alpha_1 x_1 + ... + \alpha_n x_n) \leq \alpha_1 f(x_1) + ... + \alpha_n f(x_n)\]
    \end{proposition}

    \subsubsection{L'Hopital's Rule}

    \begin{theorem}[L'Hopital's Rule]
    Let $c$ be an extended real number (i.e. $c \in \mathbb{R} \cup \{+\infty, -\infty\}$ and let $(a, b)$ be an open interval containing $c$ (for a two-sided limit) or an open interval with endpoint $c$ (for a one-sided limit, or a limit at infinity if $c$ is infinite). Assume that $f$ and $g$ are assumed to be differentiable on $(a, b) \setminus c$, and additionally $g^\prime (x) \neq 0$ on $(a, b) \setminus c$. If either 
    \[\lim_{x \rightarrow c} f(x) = \lim_{x \rightarrow c} g(x) = 0 \text{ or } \lim_{x \rightarrow c} |f(x)| = \lim_{x \rightarrow c} |g(x)| = \infty\]
    then 
    \[\lim_{x \rightarrow c} \frac{f(x)}{g(x)} = \lim_{x \rightarrow c} \frac{f^\prime (x)}{g^\prime (x)}\]
    L'Hopital's rule can be stated colloquially, but not quite accurately, as follows: \textbf{The limit of a ratio of functions equals the limit of the ratio of their derivatives if their derivatives exist.}
    \end{theorem}

    \begin{example}
      Let $f(x) = \sin{x}$ and $g(x) = -0.5x$. Then, the function 
      \[h(x) = \frac{f(x)}{g(x)} = \frac{\sin{x}}{-0.5x}\]
      is clearly undefined at $x = 0$. 
      \begin{center}
          \includegraphics[scale=0.25]{img/LHopital_Example_1.PNG}
      \end{center}
      However, we can solve the limit using L'Hopital's rule to get
      \[\lim_{x \rightarrow 0} \frac{\sin{x}}{-0.5x} = \lim_{x \rightarrow 0} \frac{\cos{x}}{-0.5} = -2\]
      Therefore, $h: \mathbb{R} \setminus 0 \longrightarrow \mathbb{R}$ can be completed to continuous function on all of $\mathbb{R}$ by defining the extension: 
      \[H(x) \equiv \begin{cases}
      h(x), & x \neq 0 \\
      -2, & x = 0
      \end{cases}\]
      \begin{center}
      \includegraphics[scale=0.25]{img/LHopital_Example_2.PNG}
      \end{center}
    \end{example}

  \subsection{Complex Analysis: An Introduction}

    Just as the equation $x^2 = 2$ has no solutions in the domain $\mathbb{Q}$ of rational numbers, the equation $x^2 = -1$ has no solutions in the domain $\mathbb{R}$ of real numbers. Just as we adjoin the symbol $\sqrt{2}$ as a solution of $x^2 = 2$ and connect it with rational numbers to get new numbers of the form 
    \[r_1 + r_2 \sqrt{2}, \;\;\; r_1, r_2 \in \mathbb{Q}\]
    we introduce the symbol $i$ as a solution of $x^2 = -1$ and attach this number to real numbers. 

    One feature of this enlargement of the field $\mathbb{R}$ of real numbers into the resulting field $\mathbb{C}$ of complex numbers, every algebraic equation with real or complex coefficients now has a solution. 

    \subsubsection[Algebraic Extension of Field R]{Algebraic Extension of Field $\mathbb{R}$}
    We introduce the number $i$, called the \textbf{imaginary unit}, such that $i^2 = -1$. We may multiply real numbers $y$ to $i$ to get $yi$, and we can add real numbers to such numbers, to get numbers of the form 
    \[x + yi, \;\;\; x, y \in \mathbb{R}\]
    We then define all objects of the form $x + iy$ as the \textbf{complex numbers}, with addition defined
    \[(x_1 + i y_1) + (x_2 + i y_2) \equiv (x_1 + x_2) + i (y_1 + y_2)\]
    and multiplication defined
    \[(x_1 + i y_1) \cdot (x_2 + i y_2) \equiv (x_1 x_2 - y_1 y_2) + i (x_1 y_2 + x_2 y_1)\]
    As expected, this makes $+$ and $\cdot$ commutative operations. Furthermore, two complex numbers $z = x_1 + i y_1$ and $w = x_2 + i y_2$ are equal if and only if $x_1 = x_2$ and $y_1 = y_2$. 

    One nontrivial property of field $\mathbb{C}$ is that every element $z \in \mathbb{C}$ has a multiplicative inverse $z^{-1}$. To find this, we must define the following. 

    \begin{definition}[Complex Conjugate]
      Given complex number $z = x + i y$, its \textbf{complex conjugate} is 
      \[\overline{z} = \overline{x + iy} = x - iy\]
      Note that 
      \[z \cdot \overline{z} = x^2 + y^2 \neq 0 \text{ iff } z \neq 0\]
    \end{definition}

    Thus, given $z$, 
    \[z^{-1} = \frac{1}{z \cdot \overline{z}} \cdot \overline{z} \iff (x + yi)^{-1} = \frac{x}{x^2 + y^2} - i \frac{y}{x^2 + y^2}\]

    \subsubsection[Geometric Interpretation of C]{Geometric Interpretation of $\mathbb{C}$}
    Once the algebraic operations $+$ and $\cdot$ has been introduced, the symbol $i$ is no longer needed. That is, we can define a new set $\mathbb{R}^2 = \mathbb{R} \times \mathbb{R}$ with the operations $+_\mathbb{R}, \cdot_\mathbb{R} : \mathbb{R}^2 \times \mathbb{R}^2 \longrightarrow \mathbb{R}^2$ defined
    \begin{align*}
        (x_1, y_1) +_\mathbb{R} (x_2, y_2) & \equiv (x_1 + x_2, y_1 + y_2) \\
        (x_1, y_1) \cdot_\mathbb{R} (x_2, y_2) & \equiv (x_1 x_2 - y_1 y_2, x_1 y_2 + x_2 y_1)
    \end{align*}
    We can check that this new set $(\mathbb{R}^2, +_\mathbb{R}, \cdot_{\mathbb{R}})$ is isomorphic to $(\mathbb{C}, +, \cdot)$ as fields, and therefore one can identify complex numbers with vectors $z = (x, y)$ of the plane $\mathbb{R}^2$, where $x = \text{Re}\,z$ is called the \textbf{real part} and $y = \text{Im}\,z$ is called the \textbf{imaginary part}. 

    \begin{definition}[Norm, Metric of $\mathbb{C}$]
      Moreover, the isomorphism
      \[\gamma: \mathbb{C} \longrightarrow \mathbb{R}^2, \;\; \gamma(x + yi) = (x, y)\]
      induces additional structures on $\mathbb{C}$, such as the norm and metric. 
      \begin{enumerate}
        \item The norm of $z = x + iy \in \mathbb{C}$ is defined as the norm of $\gamma(z) = (x, y) \in \mathbb{R}^2$. That is, 
        \[|z| = |x + yi| = |(x, y)| = \sqrt{x^2 + y^2}\]
        Or more simply, 
        \[|z| = z \cdot \overline{z}\]
        \item The metric of two complex numbers $z_1, z_2 \in \mathbb{C}$ is defined
        \[|z_1 - z_2| = |(x_1, y_1) - (x_2, y_2)| = |(x_1 - x_2, y_1 - y_2)| = \sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}\]
        Or more simply, 
        \[|z_1 - z_2| = (z_1 - z_2) \cdot \overline{(z_1 - z_2)}\]
      \end{enumerate}
    \end{definition}

    \begin{definition}[Polar Coordinates of $\mathbb{C}$]
      Given the basis transformation of polar coordinates $(r, \varphi) \mapsto p(r, \varphi) = (x, y)$ where 
      \[p\begin{pmatrix} r \\ \varphi \end{pmatrix} = \begin{pmatrix}
      r \cos{\varphi} \\ r \sin{\varphi} 
      \end{pmatrix} = \begin{pmatrix} x \\ y \end{pmatrix}\]
      the isomorphism $\mathbb{C} \simeq \mathbb{R}^2$ induces a similar polar transformation in $\mathbb{C}$
      \[\rho = \gamma^{-1} \circ p \circ \gamma: \mathbb{C}_{(r, \theta)} \longrightarrow \mathbb{C}_{(x, y)}, \;\;\rho(r + \theta i) = r \cos{\theta} + r \sin{\theta} i = x + y i\]
      as shown in the commutative diagram. 
      \[\begin{tikzcd}
          \mathbb{C}_{(r, \theta)} \arrow{d}{\gamma} \arrow{r}{\rho} & \mathbb{C}_{(x, y)} \arrow{d}{\gamma} \\
          \mathbb{R}^2_{(r, \theta)} \arrow{r}{p} & \mathbb{R}^2_{(x, y)}
        \end{tikzcd}\]
      Therefore, we can write 
      \[z = r ( \cos{\varphi} + i \sin{\varphi})\]
      where $r = |z|$ is called the \textbf{magnitude} of $z$, and $\varphi = \text{Arg}\,z$ is called the \textbf{argument} of $z$. 
    \end{definition}

    \begin{lemma}[Multiplication of Complex Numbers in Polar Form]
      It turns out that multiplication is a lot easier in polar coordinates than in rectangular ones: 
      \begin{align*}
          z_1 \cdot z_2 & = (r_1 \cos{\varphi_1} + i r_1 \sin{\varphi_1})(r_2 \cos{\varphi_2} + i r_2 \sin{\varphi_2}) \\
          & = \ldots \\
          & = r_1 r_2 \big(\cos{(\varphi_1 + \varphi_2)} + i \sin{(\varphi_1 + \varphi_2)}
      \end{align*}
      \begin{center}
          \includegraphics[scale=0.25]{img/Multiplication_Complex_Polar_Form.jpg}
      \end{center}
    \end{lemma}

    \begin{theorem}[De Moivre's Formula]
    By induction using the previous lemma, we get 
    \[z = r ( \cos{\varphi} + i \sin{\varphi}) \implies z^n = r^n (\cos{n\varphi} + i \sin{n \varphi})\]
    \end{theorem}

    \begin{corollary}[Roots of Unity]
    The $n$ complex solutions of the equation 
    \[z^n = a\]
    where $a = \rho (\cos{\psi} + i \sin{\psi})$ is 
    \[z_k = \sqrt[n]{\rho} \bigg( \cos\Big(\frac{\psi + 2\pi k}{n} \Big) + i \sin\Big(\frac{\psi + 2\pi k}{n}\Big) \bigg), \;\;\;\;\; k = 0, 1, 2, \ldots, n-1\]
    Moreover, if $a = 1$, then the $n$ complex solutions are called the \textbf{$n$th roots of unity}, defined
    \[z_k = \cos\Big(\frac{2\pi k}{n}\Big) + i \sin\Big(\frac{2\pi k}{n}\Big), \;\;\;\;\; k = 0, 1, 2, \ldots, n-1\]
    which shows that the $n$th roots of unity are at the vertices of a regular $n$-sided polygon inscribed in the unit circle, with one vertex at $1$, within the complex plane. The $5$th and $6$th roots of unity are shown below. 
    \begin{center}
        \includegraphics[scale=0.27]{img/5th_6th_Roots_of_Unity.PNG}
    \end{center}
    \end{corollary}

    Finally, we can visualize certain transformations in $\mathbb{C}$. For a fixed $b \in \mathbb{C}$, the sum $z + b$ cam be interpreted as the mapping of $\mathbb{C}$ onto itself given by the formula 
    \[z \mapsto z + b\]
    This mapping is a translation of the plane by the vector $b$. 
    \begin{center}
        \includegraphics[scale=0.25]{img/Translation_in_Complex_Plane.jpg}
    \end{center}
    Visualizing multiplication is a bit harder. Given a 
    \[a = |a| (\cos{\varphi} + i \sin{\varphi}) \neq 0\]
    the product $az$ can be interpreted as the mapping of $\mathbb{C}$ onto itself given by the formula
    \[z \mapsto az\]
    which is the composition of a dilation by a factor of $|a|$ and a rotation through the angle $\varphi \in \text{Arg}\,a$. 
    \begin{center}
        \includegraphics[scale=0.3]{img/Multiplication_in_Complex_Plane.jpg}
    \end{center}

    \subsubsection[Sequences and Series in C]{Sequences and Series in $\mathbb{C}$}
    Our previous construction of a metric within $\mathbb{C}$ enables to define the $\epsilon$-neighborhood of a number $z_0 \in \mathbb{C}$ as the set
    \[U_\epsilon (z_0) \equiv \{z \in \mathbb{C}\;|\; |z - z_0| < \epsilon\}\]
    which can be visualized as an open disk of radius $\epsilon$ in $\mathbb{R}^2$ centered at point $(x_0, y_0)$ if $z_0 = x_0 + i y_0$. 
    \begin{center}
        \includegraphics[scale=0.25]{img/Epsilon_Neighborhood_in_C.jpg}
    \end{center}

    \begin{definition}[Convergence of a Sequence in $\mathbb{C}$]
      A sequence $\{z_n\}$ of complex numbers \textbf{converges} to $z_0 \in \mathbb{C}$ if and only if 
      \[\lim_{n \rightarrow \infty} |z_n - z_0| = 0\]
      It is clear from the inequality
      \[\max\{|x_n - x_0|, |y_n - y_0|\} \leq |z_n - z_0| \leq |x_n - x_0| + |y_n - y_0|\]
      that a sequence of complex numbers converges if and only if the sequences of its real and imaginary parts of the terms of the sequence both converge. That is, 
      \[\{z_n\} \text{ converges} \iff \{\text{Re}\,z\} \text{ and } \{\text{Im}\,z\} \text{ converges}\]
    \end{definition}

    \begin{lemma}[Convergence of Cauchy Sequences over $\mathbb{C}$]
      A sequence of complex numbers $\{z_n\}$ is called a \textbf{Cauchy sequence} if for every $\epsilon>0$ there exists an index $N \in \mathbb{N}$ such that
      \[|z_n - z_m|<\epsilon \text{ for all } n, m > N\]
      It is also clear that 
      \[\{z_n\} \text{ is Cauchy} \iff \{\text{Re}\,z\} \text{ and } \{\text{Im}\,z\} \text{ is Cauchy}\]
      and using the Cauchy criterion for sequences of real numbers, we can easily see that a sequence of complex numbers converges if and only if it is a Cauchy sequence. 
    \end{lemma}

    \begin{lemma}[Convergence of Cauchy Series over $\mathbb{C}$]
      Interpreting the sum of a series of complex numbers
      \[z_1 + z_2 + \ldots + z_n + \ldots\]
      as the limit of the sequence its partial sums $\{s_n\}$, where $s_n = z_1 + \ldots z_n$ as $n \rightarrow \infty$, we can see that the series converges if and only if for every $\epsilon > 0$ there exists a $N \in \mathbb{N}$ such that 
      \[|z_m + \ldots + z_n| < \epsilon\]
      for any natural numbers $n \geq m > N$. 
    \end{lemma}

    \begin{definition}[Absolute Convergence of $\mathbb{C}$]
      A series $z_1 + \ldots + z_n + \ldots$ of complex numbers is \textbf{absolutely convergent} if the series
      \[|z_1| + |z_2| + \ldots + |z_n| + \ldots\]
      converges. Clearly, is a series converges absolutely, then it converges due to the inequality
      \[|z_m + \ldots + z_n| \leq |z_m| + \ldots + |z_n|\]
    \end{definition}

    \begin{example}
      The following complex series converges because they converges absolutely. That is, 
      \begin{align*}
          1 + \frac{1}{1!}|z| + \frac{1}{2!}|z^2| + \ldots \text{ converges } \forall \; \mathbb{C} & \implies 1 + \frac{1}{1!}z + \frac{1}{2!}z^2 + \ldots \text{ converges } \forall \; \mathbb{C} \\
          |z| + \frac{1}{3!}|z|^3 + \frac{1}{5!}|z|^5 + \ldots \text{ converges } \forall \; \mathbb{C}  & \implies z - \frac{1}{3!} z^3 + \frac{1}{5!}z^5 + \ldots \text{ converges } \forall \; \mathbb{C} \\
          1 + \frac{1}{2!}|z|^2 + \frac{1}{4!} |z|^4 + \ldots \text{ converges }  \forall \; \mathbb{C} & \implies 1 - \frac{1}{2!}z^2 + \frac{1}{4!} z^4 - \ldots \text{ converges }  \forall \; \mathbb{C} 
      \end{align*}
    \end{example}

    \begin{definition}[Complex Power Series]
      Series of the form 
      \[\sum_{n=0}^\infty c_n (z - z_0)^n = c_0 + c_1 (z - z_0) + \ldots + c_n (z - z_0) + \ldots\]
      are called \textbf{complex power series}, or \textbf{power series over $\mathbb{C}$}. 
    \end{definition}

    But a power series is quite useless unless we know the domain in which is converges (again, note that it is not always guaranteed to converge onto the function $f$ if its power series expansion does converge at all). To develop more sophisticated tests of convergence of a complex power series, we introduce the complex analogue of the root test for real power series. 

    \begin{theorem}[Cauchy-Hadamard Theorem]
    The complex power series 
    \[c_0 + c_1 (z - z_0) + \ldots + c_n (z - z_0) + \ldots\]
    converges inside the disk $|z - z_0| < R$ with center at $z_0$ and radius given by the formula
    \[R = \frac{1}{\varlimsup_{n \rightarrow \infty} \sqrt[n]{|c_n|}} = \frac{1}{\lim_{n \rightarrow \infty} \sup{\sqrt[n]{|c_n|}}}\]
    Where $\varlimsup$ denotes the superior limit. Furthermore, 
    \begin{enumerate}
      \item the power series diverges at any point exterior to the disk. 
      \item the power series converges absolutely at any point interior to the disk. 
      \item the power series is indeterminate at any point on the boundary of the disk. 
    \end{enumerate}
    Note that in the degenerate case when $R = 0$, the series converges only at the point $z = z_0$. 
    \end{theorem}

    \begin{corollary}[Abel's First Theorem on Power Series]
    If the power series 
    \[c_0 + c_1 (z - z_0) + \ldots + c_n (z - z_0) + \ldots\]
    converges at some value $z^*$, then it converges absolutely for any value of $z$ satisfying
    \[|z - z_0| < |z^* - z_0|\]
    The values of $z$ satisfying the inequality above can be intuitively visualized as the following region. 
    \begin{center}
        \includegraphics[scale=0.25]{img/Abels_First_Theorem.PNG}
    \end{center}
    \end{corollary}

    \begin{theorem}[Product of Absolutely Convergent Series]
    Let $a_1 + a_2 + \ldots$ and $b_1 + b_2 + \ldots$ be an absolutely convergent series such that
    \[\sum_{i=1}^\infty a_i = A \text{ and } \sum_{j=1}^\infty b_j = B\]
    Then, the Cauchy product of the two series 
    \[\bigg( \sum_{i=1}^\infty a_i \bigg) \cdot \bigg( \sum_{j=1}^\infty b_j \bigg) = \sum_{k=0}^\infty c_k = A B, \text{ where } c_k = \sum_{l=0}^k a_l b_{k-l}\]

    $a_1 b_1 + a_2 b_2 + \ldots$ is absolutely convergent and 
    \[\sum_{i = 1}^\infty a_i b_i = A B\]
    \end{theorem}
    \begin{proof}
    To be done. 
    \end{proof}

      \begin{example}[Convergence of the Cauchy Product of Absolutely Convergent Complex Series]
      The two series 
      \[\sum_{n = 0}^\infty \frac{1}{n!} a^n \text{ and } \sum_{m = 0}^\infty \frac{1}{m!} b^m\]
      converges absolutely. Therefore, we can see that their Cauchy product can be nicely represented by grouping together all monomials of the form $a^n b^m$ having the same total degree $m + n = k$. 
      \[\bigg( \sum_{n = 0}^\infty \frac{1}{n!} a^n \bigg) \cdot \bigg( \sum_{m = 0}^\infty \frac{1}{m!} b^m \bigg) = \sum_{k=0}^\infty \bigg(\sum_{n+m=k} \frac{1}{n!} a^n \frac{1}{m!} b^m \bigg)\]
      But we can simplify 
      \[\sum_{m + n = k} \frac{1}{n! m!} a^n b^m = \frac{1}{k!} \sum_{n=0}^k \frac{k!}{n! (k-n)!} a^n b^{k-n} = \frac{1}{k!} (a + b)^k\]
      and therefore we find that 
      \[\bigg( \sum_{n = 0}^\infty \frac{1}{n!} a^n \bigg) \cdot \bigg( \sum_{m = 0}^\infty \frac{1}{m!} b^m \bigg) = \sum_{k=0}^\infty \frac{1}{k!} (a + b)^k\]
    \end{example}

      \subsubsection{Euler's Formula}

      \begin{definition}[Complex Taylor Expansions of Transcendental Functions]
        Since we have determined absolute convergence, and therefore convergence, of all these series in all of $\mathbb{C}$, it is natural to extend the definitions of 
        \[\exp, \cos, \sin: \mathbb{R} \longrightarrow \mathbb{R}\]
        to the complex field 
        \[\exp, \cos, \sin: \mathbb{C} \longrightarrow \mathbb{C}\]
        by defining them as 
        \begin{align*}
            e^z & \equiv 1 + \frac{1}{1!}z + \frac{1}{2!} z^2 + \frac{1}{3!} z^3 + \ldots \\
            \cos{z} & \equiv 1 - \frac{1}{2!} z^2 + \frac{1}{4!} z^4 - \frac{1}{6!} z^6 + \ldots \\
            \sin{z} & \equiv z - \frac{1}{3!} z^3 + \frac{1}{5!} z^5 - \frac{1}{7!} z^7 + \ldots
        \end{align*}
        Notice that even in the complex field, $\cos{z}$ is an even function and $\sin{z}$ is an odd function. 
        \begin{align*}
            \cos(-z) & = \cos(z) \\
            \sin(-z) & = -\sin(z)
        \end{align*}
      \end{definition}

      In fact, the last example in the previous subsection just proves the following. 

      \begin{lemma}[Exponential Map as a Group Homomorphism]
        The exponential map $\exp: \mathbb{C} \longrightarrow \mathbb{C}\setminus \{0\}$ satisfies the following
        \[\exp(z_1 + z_2) = \exp(z_1) \cdot \exp (z_2)\]
        That is, $\exp$ is a group homomorphism from $(\mathbb{C}, +)$ to $(\mathbb{C} \setminus \{0\}, \cdot)$. 
      \end{lemma}

      \begin{definition}[Euler's Formula]
        By making the substitution $z = yi$ in the series expansion of $e^z$ (where $y$ is an arbitrary complex number), we get 
        \begin{align*}
            e^{iy} & = 1 + \frac{1}{1!} (iy) + \frac{1}{2!}(iy)^2 + \frac{1}{3!} (iy)^3 + \frac{1}{4!} (iy)^4 + \ldots \\
            & = \bigg(1 - \frac{1}{2} y^2 + \frac{1}{4!} y^4 - \ldots \bigg) + i \bigg(\frac{1}{1!} y - \frac{1}{3!} y^3 + \frac{1}{5!} y^5 - \ldots \bigg)
        \end{align*}
        which brings us the identity
        \[e^{iy} = \cos{y} + i \sin{y}\]
      \end{definition}

      Since $\cos$ is even and $\sin$ is odd, we can add the two identities
      \begin{align*}
          e^{iz} & = \cos{z} + i \sin{z} \\
          e^{-iz} & = \cos{z} - i \sin{z} 
      \end{align*}
      to get 
      \begin{align*}
          \cos{z} & = \frac{1}{2}\big( e^{iz} + e^{-iz} \big) \\
          \sin{z} & = \frac{1}{2i} \big( e^{iz} - e^{-iz} \big)
      \end{align*}
      This gives us a very elegant connection between these three transcendental functions. 

      \begin{definition}[Hyperbolic Functions]
        Likewise, the following series are convergent (since they are absolutely convergent) and therefore we can define the extension of $\cosh$ and $\sinh$ into the complex field as 
        \begin{align*}
            \cosh{z} & \equiv 1 + \frac{1}{2!} z^2 + \frac{1}{4!} z^4 + \frac{1}{6!} z^6 + \ldots \\
            \sinh{z} & \equiv z + \frac{1}{3!} z^3 + \frac{1}{5!} z^5 + \frac{1}{7!} z^7 + \ldots 
        \end{align*}
        The following identities immediately follow
        \begin{align*}
            \cosh{z} & = \frac{1}{2} \big( e^z + e^{-z} \big) \\
            \sinh{z} & = \frac{1}{2} \big( e^{z} - e^{-z}\big) 
        \end{align*}
      \end{definition}

      \begin{lemma}[Trigonometric, Hyperbolic Identities over $\mathbb{C}$]
        Common identities, which are exactly the same as their real analogues, are listed. 
        \begin{enumerate}
          \item $\cos^2{z} + \sin^2 {z} = 1$
          \item $\cosh^2{z} - \sinh^2{z} = 1$ 
          \item $e^{i(z_1 + z_2)} = (\cos{z_1} \cos{z_2} - \sin{z_1} \sin{z_2}) + i (\sin{z_1} \cos{z_2} + \cos{z_1} \sin{z_2})$
          \item $\cos{(z_1 + z_2)} = \cos{z_1} \cos{z_2} - \sin{z_1} \sin{z_2}$
          \item $\sin{(z_1 + z_2)} = \sin{z_1} \cos{z_2} + \cos{z_1} \sin{z_2}$
          \item $\cosh{z} = \cos{iz}$ 
          \item $\sinh{z} = -i \sin{iz}$
        \end{enumerate}
      \end{lemma}

      However, to obtain even such geometrically obvious facts as the equality
      \[\sin{\pi} = 0 \text{ or } \cos{z + 2\pi} = \cos{z}\]
      from the power series definitions of $\cos$ and $\sin$ is extremely difficult. What the properties actually do is present the remarkable unity of these seemingly different trigonometric and hyperbolic functions, which would have been impossible to detect without going into the domain of complex numbers. 

      If we just take the following identities
      \begin{align*}
          \cos{x} & = \cos{(x + 2 \pi)} \\
          \sin{x} & = \sin{(x + 2\pi)} \\
          \cos{0} & = 1 \\
          \sin{0} & = 0
      \end{align*}
      then we get the following identity. 

      \begin{theorem}[Euler's Identity]
      The following relation is true. 
      \[e^{i\pi} + 1 = 0\]
      which immediately implies 
      \[\exp(z + 2\pi i) = \exp{z}\]
      That is, the exponential function is a periodic function on $\mathbb{C}$ with the purely imaginary period $T = 2 \pi i$. 
      \end{theorem}

      \begin{corollary}[Trigonometric Notation of Complex Number]
      With Euler's formula and the periodic relation of $\exp{z}$, the trigonometric form of a complex number can be presented as
      \[z = r(\cos{\varphi} + i \sin{\varphi}) = r e^{i \varphi}\]
      We can rewrite DeMoivre's formula as
      \[z^n = r^n e^{n \varphi i}\]
      \end{corollary}

      \subsubsection{Visualizing Complex Functions}

        TBD

      \subsubsection{Continuity, Differentiability, Analyticity of Complex Functions}
      The definitions of continuity and differentiability are the same, just under a different field. 

      \begin{definition}[Limit of a Complex Function]
        The function $f: E \subset \mathbb{C} \longrightarrow \mathbb{C}$ tends to $A \in \mathbb{C}$ as $z \rightarrow a$, or that
        \[\lim_{z \rightarrow a} f(z) = A\]
        if for every $\epsilon > 0$ there exists a $\delta > 0$ such that
        \[0<|z - a|<\delta \implies |f(z) - A|<\epsilon\]
        Note that we set $0<|z - a|$ to ensure that $z \neq a$. 

        Therefore, in other words, for any arbitrarily small $\epsilon>0$, we can find a $\delta > 0$ such that the image of the deleted $\delta$-neighborhood of $a$, denoted $\mathring{U}_\delta (a)$), is completely within the $\epsilon$-neighborhood $U_\epsilon (A)$. 
        \begin{center}
            \includegraphics[scale=0.25]{img/Limit_of_Complex_Function.PNG}
        \end{center}
      \end{definition}

      \begin{definition}[Continuity of a Complex Function]
        A function $f: E \subset \mathbb{C} \longrightarrow \mathbb{C}$ is \textbf{continuous} at a point $z_0 \in E$ if for any neighborhood $U(f(z_0))$ there exists a neighborhood $U(z_0)$ such that its image is contained in $U(f(z_0))$. In short, 
        \[\lim_{z \longrightarrow z_0} f(z) = f(z_0)\]
        \begin{center}
          \includegraphics[scale=0.25]{img/Continuity_of_Complex_Function.PNG}
        \end{center}
      \end{definition}

      \begin{definition}[Differentiability of a Complex Function]
        The \textbf{derivative} of a function $f: E \subset \mathbb{C} \longrightarrow \mathbb{C}$ is defined
        \[f^\prime (z_0) = \lim_{z \rightarrow z_0} \frac{f(z) - f(z_0)}{z - z_0}\]
        if this limit exists. $f$ \textbf{differentiable} at $x_0$ means that a differential function 
        \[df(z_0): T_{z_0} \mathbb{C} \longrightarrow T_{f(z_0)} \mathbb{C}, \;\;\; h \mapsto df(z_0)(h)\]
        exists such that
        \[f(z) = f(z_0) + df(z_0)(h) + o(h)\]
        where $h = z - z_0$ is the increment of the argument. Just like the real case, it turns out that $df(z_0)(h) = f^\prime (z_0) h$, and 
        \[f(z) - f(z_0) = f^\prime(z_0) (z - z_0) + o(z - z_0)\]
        which elegantly weaves together the two concepts of differentiability and the derivative. 

        Visualizing this, we can see that for whatever function $f: \mathbb{C} \longrightarrow \mathbb{C}$ there is a linear function that transforms the entire space as such at $z_0$ (along with a given point $z_0 \in \mathbb{C}$), 
        \begin{center}
            \includegraphics[scale=0.25]{img/Differential_of_Complex_Valued_Function.PNG}
        \end{center}
        The differential $df(z_0)$ at the point $z_0$ is a linear mapping that "best" approximates $f$, with an error of $o(h) = o(z - z_0)$. 
      \end{definition}

      \begin{lemma}[Arithmetic Properties of Differentiation over $\mathbb{C}$]
        If functions $f, g: E \subset \mathbb{C} \longrightarrow \mathbb{C}$ are differentiable at a point $z \in E$, then 
        \begin{enumerate}
          \item their sum is differentiable at $z$, and 
          \[d(f + g)(z) = df(z) + dg(z) \iff (f + g)^\prime (z) = (f^\prime + g^\prime)(z)\]
          \item their product is differentiable at $z$, and 
          \[d(f \cdot g) (z) = g(z) df(z) + f(z) dg(z) \iff (f \cdot g)^\prime (z) = f^\prime (z) g(z) + f(z) \cdot g^\prime (z)\]
          \item their quotient is differentiable at $z$ if $g(z) \neq 0$, and 
          \[d \bigg( \frac{f}{g}\bigg) (z) = \frac{g(z) df(z) - f(z) dg(z)}{g^2 (z)} \iff \bigg(\frac{f}{g}\bigg)^\prime (z) = \frac{f^\prime (z) g(z) - f(z) g^\prime (z)}{g^2 (z)}\]
        \end{enumerate}
        Just like the real case, the operation of taking the derivative is a linear operator. 
      \end{lemma}

      \begin{lemma}[Chain Rule for Composite Functions over $\mathbb{C}$]
        Let there be functions $f: E_1 \subset \mathbb{C} \longrightarrow \mathbb{C}$ differentiable at point $z \in E_1$ and $g: E_2 \subset \mathbb{C} \longrightarrow \mathbb{C}$ differentiable at point $w = f(z) \in E_2$, with respective differentials 
        \begin{align*}
            df(z) & : T_z \mathbb{C} \longrightarrow T_w \mathbb{C} \\
            dg(w) & : T_w \mathbb{C} \longrightarrow T_{g(w)} \mathbb{C}
        \end{align*}
        Then, the composite function $g \circ f: E_1 \longrightarrow \mathbb{C}$ is differentiable at $z$, and $d(g \circ f)(z): T_z \mathbb{C} \longrightarrow T_{g \circ f(z)} \mathbb{C}$ is
        \[d(g \circ f) (z) = dg(w) \circ df(z) \iff (g \circ f)^\prime (z) = g^\prime \big(f(z)\big) \circ f^\prime (z)\]
      \end{lemma}

      \subsubsection{Power Series Representation of a Function}
      \begin{definition}[Holomorphic Function]
        If function $f: E \subset \mathbb{C} \longrightarrow \mathbb{C}$ is (complex) differentiable at a point $z_0 \in E$, then $f$ is said to be \textbf{holomorphic at $z_0$}. 
      \end{definition}

      We recall the diagram that summarizes the conditions of differetiability and analyticity of a function $f$ over the field $\mathbb{R}$. 
      \begin{center}
      \begin{tikzpicture}
          \draw (-7.5,0) rectangle (7.5, 4);
          \draw[fill=lightgray] (-6.5, 0.5) rectangle (6.5, 3);
          \draw[fill=white] (-5.5, 1) rectangle (5.5, 2);
          \node[above] at (0, 1) {Taylor series converges to $f$ at $x_0 \iff f$ is analytic};
          \node[above] at (0, 2) {Taylor series converges at $x_0$};
          \node[above] at (0, 3) {$f$ infinitely differentiable at $x_0 \iff $ Taylor series of $f$ exists at $x_0$};
      \end{tikzpicture}
      \end{center}
      In the theory of functions of a complex variable we actually have a remarkable theorem that does not have an analogue for functions over $\mathbb{R}$. 

      \begin{theorem}[Analyticity of Differentiable Functions over $\mathbb{C}$]
      If a function $f: E \subset \mathbb{C} \longrightarrow \mathbb{C}$ is differentiable in a neighborhood of a point $z_0 \in E$, then it is analytic at that point. In other words, 
      \[f \text{ is holomorphic at } z_0 \implies f \text{ is analytic at } z_0\]
      This means that the conditions in the diagram above all are equivalent! Visually, 
      \begin{center}
      \begin{tikzpicture}
          \draw (-6.5,1) rectangle (6.5, 5); 
          \node[above] at (0,4) {$f$ is differentiable at $z_0 \iff f$ is holomorphic at $z_0$};
          \node at (0,3.8) {$\Updownarrow$};
          \node at (0,2.8) {$\Updownarrow$};
          \node at (0,1.8) {$\Updownarrow$};
          \node[above] at (0, 1) {Taylor series converges to $f$ at $z_0 \iff f$ is analytic};
          \node[above] at (0, 2) {Taylor series converges at $z_0$};
          \node[above] at (0, 3) {$f$ infinitely differentiable at $z_0 \iff $ Taylor series of $f$ exists at $z_0$};
      \end{tikzpicture}
      \end{center}
      This is certainly an amazing fact, since it then follows from the theorem that if a function $f(z)$ has one derivative $f^\prime (z)$ in a neighborhood of a point, it also has derivatives of all orders in that neighborhood. 
      \end{theorem}

      \subsubsection[Algebraic Closedness of the Field C]{Algebraic Closedness of the Field $\mathbb{C}$}

      \begin{definition}[Algebraically Closed Field]
        A field $\mathbb{F}$ is \textbf{algebraically closed} if every nonconstant polynomial in $\mathbb{F}[x]$ (the polynomial ring with coefficients in $\mathbb{F}$) has a root in $\mathbb{F}$. 
      \end{definition}

      \begin{theorem}[Fundamental Theorem of Algebra]
      $\mathbb{C}$ is algebraically closed. That is, every polynomial 
      \[P(z) \equiv c_0 + c_1 z + c_2 z^2 + \ldots + c_n z^n\]
      of degree $n\geq 1$ with complex coefficients $c_i \in \mathbb{C}$ ($i = 0, 1, \ldots, n$) has a root in $\mathbb{C}$. This immediately implies that every polynomial $P(z)$ admits a representation (unique up to the order of the factors) in the form 
      \[P(z) = c_n (z - z_1) (z - z_2) \ldots (z - z_n)\]
      where $z_1, \ldots, z_n \in \mathbb{C}$ not necessarily all distinct. 
      \end{theorem}

      We can also prove the interesting property about zeroes of polynomials in $\mathbb{R}[x]$. 

      \begin{corollary}[Complex Conjugate Roots of Real Polynomials]
      Given a polynomial with real coefficients
      \[P(z) \equiv a_0 + a_1 z + a_2 z^2 + \ldots + a_n z^n\]
      $P$, as we know, does not always have real roots (e.g. $P(x) = x^2 + 1$). However, we state that
      \[\text{if } P(z_0) = 0, \text{ then } P(\overline{z}_0) = 0\]
      Therefore, every polynomial $P$ with real coefficients can be expanded as a product of linear and quadratic polynomial with real coefficients. 
      \end{corollary}
      \begin{proof}
      We can see from the properties of complex numbers that
      \begin{align*}
          \overline{(z_1 + z_2)} & = \overline{z_1} + \overline{z_2} \\
          \overline{(z_1 \cdot z_2)} & = \overline{(r_1 e^{i \varphi_1} \cdot r_2 e^{i \varphi_2})} \\
          & = \overline{r_1 r_2 e^{i(\varphi_1 + \varphi_2)}} = r_1 r_2 e^{-i(\varphi_1 + \varphi_2)} \\
          & = r_1 e^{-i\varphi_1} \cdot r_2 e^{-i \varphi_2} = \overline{z}_1 \cdot \overline{z}_2
      \end{align*}
      Thus, if $P(z_0) = 0$, then 
      \[0 = \overline{P(z_0)} = \overline{a_0 + \ldots + a_n z_0^n} = \overline{a}_0 + \ldots + \overline{a}_n \overline{z}_0^n  = a_0 + \ldots + a_n \overline{z}_0^n = P(\overline{z}_0)\]
      and thus $P(\overline{z}_0) = 0$. 
      \end{proof}

    \subsection{Primitives}

      \begin{definition}[Primitive]
        A function $F(x)$ is a \textbf{primitive} of a function $f(x)$ on an interval if $F$ is differentiable on the interval and satisfies the equation 
        \[F^\prime (x) = f(x)\]
        or equivalently, if their respective differentials satisfy
        \[d F(x) = f(x) \,dx\]
      \end{definition}

      \begin{lemma}
        If $F_1(x)$ and $F_2 (x)$ are two primitives of $f(x)$ on the same interval, then the difference $(F_1 - F_2)(x)$ is constant on that interval. 
      \end{lemma}

    \begin{example}
    Both 
    \[F_1(x) \equiv \arctan{x} \text{ and } F_2(x) \equiv \arccot{\frac{1}{x}}\]
    are primitives of $f(x) = \frac{1}{1 + x^2}$. Indeed, we can see by direct calculation that in the domain $\mathbb{R} \setminus 0$, 
    \[F_1 (x) - F_2 (x) = \arctan{x} - \arccot{\frac{1}{x}} = \begin{cases}
    0, & x > 0 \\
    -\pi, & x < 0
    \end{cases}\]
    which is supported by the lemma. 
    \end{example}

      Notice how given a function $f(x)$, the operation of finding its differential, denoted with $d$, gives us a new function of $h$, called the differential 
      \[df(x)(h)\]
      Similarly, the operation of finding a primitive of function $f(x)$, denoted with the symbol $\int$, gives us a new function. 

      \begin{definition}[Indefinite Integration]
        The operation of finding a primitive of a certain function $f(x)$ is called \textbf{indefinite integration}, and the mathematical notation 
        \[\int f(x) \,dx\]
        is called the \textbf{indefinite integral of $f(x)$} on a given interval ($f$ called the \textbf{integrand} and $f(x)\,dx$ called the \textbf{differential form}). 
        \begin{enumerate}
          \item It immediately follows from the lemma that if $F(x)$ is any particular primitive of $f(x)$ on the interval, then on that interval 
          \[\int f(x) \,dx = F(x) + C\]
          \item If $F^\prime (x) = f(x)$ (that is, $F$ is a primitive of $f$ on some interval), then we have
          \[d \int f(x)\,dx = d F(x) = F^\prime (x) \,dx \]
          \item It also follows that 
          \[\int d F(x) = \int F^\prime (x)\,dx = F(x) + C\]
        \end{enumerate}
      \end{definition}

      \begin{theorem}[Basic Methods of Indefinite Integration]
      The definition of the indefinite integral has three basic properties that can be used to solve indefinite integrals. 
      \begin{enumerate}
        \item Linearity of the indefinite integral.
        \[\int \big( \alpha u(x) + \beta v(x)\big) \, dx = \alpha \int u(x)\,dx + \beta \int v(x)\,dx + C\]
        \item Integration by parts. 
        \[\int (u v)^\prime \,dx = \int u^\prime (x) v(x) \,dx + \int u(x) v^\prime (x) \,dx + C\]
        \item Change of Variable, or $U$-substitution. Given that $F^\prime (x) = f(x)$ on an interval $I_x$ and $\varphi: I_t \longrightarrow I_x$ is a $C^1$ mapping of interval $I_t$ into $I_x$, then
        \[\int (f \circ \varphi) (t) \varphi^\prime (t) \,dt = (F \circ \varphi)(t) + C\]
      \end{enumerate}
      \end{theorem}


\section{Sequences of Functions}

  We now motivate this section by introducing double limits. 

  \begin{example}[Double]
    Consider the double sequence $\big( \frac{1}{m + n} \big)_{m, n \in \mathbb{N}}$. We can compute the limit as both $n, m \to +\infty$ in many ways. 
    \begin{enumerate}
      \item We can first set $m \to +\infty$, then $n \to +\infty$. 
      \item We can first set $n \to +\infty$, then $m \to +\infty$. 
      \item We might want to take $n$ twice as slow as $m$. 
    \end{enumerate}
    All of these converge to the same value of $0$, so there is no problem. 
  \end{example}

  In this case, we are considering a double sequence in $\mathbb{R}$. However, if we fix one value, then it becomes a sequence of functions, and we already have established that classes of functions form a vector space. In general, interchanging limits are not allowed. 

  \begin{example}
    Consider the slightly different sequence $\big( \frac{m}{m + n} \big)_{m, n}$. 
    \begin{enumerate}
      \item If $m >> n$, i.e. take the sequence of values $(10^k, k)$, then this will approach $1$. 
      \item If $n >> m$, i.e take the sequence of values $(k, 10^k)$, then this will approach $0$. 
      \item In intermediate cases, you can in fact get any number between $0$ and $1$. 
    \end{enumerate}
  \end{example}

  Therefore, in general, 
  \begin{equation}
    \lim_{n \to \infty} \lim_{m \to \infty} a_{mn} \neq \lim_{n \to \infty} \lim_{m \to \infty} a_{nm}
  \end{equation}
  To get equality, we need to make a stronger assumption than simple existence of limits. 

  \begin{definition}[Pointwise Convergence]
    Let $E \subset \mathbb{R}$ and $f, f_n : E \to \mathbb{R}$. We say that $f_n \to f$ \textbf{pointwise} if 
    \begin{equation}
      \lim_{n \to \infty} f_n (x) = f(x) \text{ for all } x \in E
    \end{equation}
  \end{definition}

  \begin{example}
    Let $f_n (x) = x/n$. Then $f_n \to 0$ pointwise, where $0$ is the $0$ function. This is true since for every fixed $x$, we can set $n$ so large that $x/n < \epsilon$ for any $\epsilon$. 
  \end{example}

  \begin{example}
    Let $f_n (x) : [0, 1] \to \mathbb{R}$  defined $f_n (x) = x^n$. Then, 
    \begin{align}
      0 \leq x < 1 & \implies x^n \to 0 \text{ as } n \to \infty \\ 
      x = 1 & \implies x^n \to 1 \text{ as } n \to \infty
    \end{align}
    So, 
    \begin{equation}
      f_n \to f^\ast (x) \coloneqq \begin{cases} 
        0 & \text{ if } x < 1 \\ 
        1 & \text{ if } x = 1
      \end{cases}
    \end{equation}
    Note that all $f_n$ are continuous but $f^\ast$ is discontinuous since 
    \begin{equation}
      \lim_{x \to 1} \lim_{n \to \infty} f_n (x) = 0 \neq 1 = \lim_{n \to \infty} \lim_{x \to 1} f_n (x)
    \end{equation}
  \end{example}

  \begin{example}
    Consider the function $f_n: [0, 1] \to \mathbb{R}$ defined by $f_n (0) = f_n (1/n) = 0$ and $f_n (1/2n) = 2n$, with everything else linearly interpolated. Then $f_n \to 0$ since it is constantly $0$ at $0$ and for every $x > 0$, there exists $1/N < x$ and so $f_n (x) = 0$ for all $n \geq N$. However, $\int_0^1 f_n (x) \,dx = 1$, so 
    \begin{equation}
      \lim_{n \to \infty} \int_0^1 f_n (x)\,dx \neq \int_0^1 \lim_{n \to \infty} f_n (x)\,dx
    \end{equation}
    So integration is not continuous with respect to the topology induced by pointwise convergence. 
  \end{example}

  The problem is not in the construction of the limits or the integral, but with the pointwise convergence. With pointwise convergence, 
  \begin{enumerate}
    \item we cannot exchange limits 
    \item does not preserve continuity 
    \item cannot exchange integrals 
    \item cannot exchange sums
  \end{enumerate}

  \begin{example}
    Review. 
    \begin{equation}
      \bigg( 1 - \frac{1}{n} \bigg)^n \to e^{-1}
    \end{equation}
  \end{example}

  But with \textit{uniform convergence}, we can do all this. 

\subsection{Uniform Convergence}

  \begin{definition}[Uniform Convergence]
    Given $f_n : E \to \mathbb{R}$ of bounded functions, $(f_n)$ is said to \textbf{converge uniformly} to a bounded function $f: E \to \mathbb{R}$ if $\forall \epsilon > 0$, there exists $N \in \mathbb{N}$ s.t. 
    \begin{equation}
      n \geq N \implies |f_n (x) - f(x) | < \epsilon \text{ for all } x \in E
    \end{equation}
    the ``for all $x \in E$'' is the uniform part, which is similar to uniform continuity. 
  \end{definition}

  Generally, to prove uniform convergence, you will need to find that $|f_n (x) - f(x)|$ is bounded by something that is independent of $x$, and it goes to $0$ as $n \to \infty$. 

  \begin{example}
    We have uniform convergence
    \begin{equation}
      \frac{\sin(e^n x)}{n} \to 0 \text{ since } \bigg| \frac{\sin(e^n x)}{n} \leq \frac{1}{n} \to 0
    \end{equation}
  \end{example}

  \begin{example}
    $f_n (x) = x^n$ does not converge uniformly to \textit{any} function in $[0, 1]$. It suffices to find a sequence $(y_n) \subset [0, 1]$ s.t. $f_n (y_n) - f(y_n)| \not\to 0$ as $n \to \infty$. Take $y_n = 1 - \frac{1}{n}$. Then $f_n (y_n) \to \frac{1}{e} \neq 0$, but $f(y_n) \to 1$. 
  \end{example}

  \begin{example}
    The triangle functions do not converge uniformly since $f_n$ is unbounded, i.e. $f_n (\frac{1}{2n}) = 2n$, while $f_n ( \frac{1}{n}) = 0$. So we can construct two sequences that  
    \begin{equation}
      (1/2n) \to \infty, (1/n) \to 0 \text{ as } n \to \infty
    \end{equation}
  \end{example}

  \begin{theorem}[Uniform Convergence Implies Pointwise Convergence]
    Uniform convergence implies pointwise convergence. 
  \end{theorem} 

  \begin{theorem}
    We claim the following. 
    \begin{equation}
      f_n \to f \text{ uniformly on } E \iff \lim_{n \to \infty} \sup_{x \in E} |f_n (x) - f(x)| = 0
    \end{equation}
  \end{theorem}

  \begin{definition}[Uniformly Cauchy]
    A sequence $f_n: E \to \mathbb{R}$ is called \textbf{uniformly Cauchy} if $\forall \epsilon > 0$, there exists $N \in \mathbb{N}$ s.t. $\forall n, m \geq N$, 
    \begin{equation}
      |f_n (x) - f_m (x)| < \epsilon \text{ for all } x \in E
    \end{equation}
  \end{definition}

  \begin{lemma} 
    $(f_n)$ converges uniformly iff $(f_n)$ is uniformly Cauchy. 
  \end{lemma}

  Continuity. 

  Integration. 

  Differentiability. 

  Note that uniform convergence may not be met due to some counterexamples. In general, there are 3 ways that uniform convergence can fail to happen. 

  \begin{enumerate}
    \item \textit{Concentration}. Note that $x^n$ as $n \to \infty$ almost converges except at one point. 
    \item \textit{Translation}. Consider $f_n (x) = \sin(x - n)$. Then by increasing $n$ we are shifting it to $+\infty$. 
    \item \textit{Oscillation}. Consider $f_n (x) = \sin(nx)$. As $n$ increases the function oscillates widely. This is sort of like the worst.\footnote{It turns out that this is the same as (2) under the Fourier transform.} 
  \end{enumerate}

  \begin{lemma} 
    If $f: [0, 1] \to \mathbb{R}$ is continuouusly differentiable on $[0, 1]$, then 
    \begin{equation}
      \lim_{n \to \infty} \int_0^1 f(x) \sin(nx) \,dx = 0
    \end{equation} 
    This means that as $n \to \infty$, the integral becomes small. 
  \end{lemma}
  \begin{proof}
    
  \end{proof}

  Now we prove a stronger version. 

  \begin{theorem}
    For every $f \in C([0, 1])$, 
    \begin{equation}
      \lim_{n \to \infty} \int_0^1 f(x)\, \sin(nx) \,dx = 0
    \end{equation}
  \end{theorem}
  \begin{proof}
    Let $\epsilon > 0$. Then by the Weierstrass approximation theorem\footnote{aka, the set of polynomials is dense in the set of continuous functions with the supremum metric. Remember polynomial interpolation, which is for a finite number of points. This is a little different.}, there exists a polynomial $p: [0, 1] \to \mathbb{R}$ for which 
    \begin{equation}
      \sup_{x \in [0, 1]} | p(x) - f(x)| < \epsilon 
    \end{equation}
    Now consider 
    \begin{align}
      \bigg| \int_0^1 f(x) \, \sin(nx) \,dx \bigg| & \leq \bigg| \int_0^1 p(x) \sin(nx) \,dx \bigg| + \bigg| \int_0^1 (f(x) - p(x)) \sin(nx) \,dx \bigg| \\
                                                   & \leq \bigg| \int_0^1 p(x) \sin(nx) \,dx \bigg| + \epsilon \\
                                                   & \leq 2 \epsilon 
    \end{align}
    where the first inequality is the triangle inequality, the second is due to the Weierstrass approximation theorem, and the third is due to $p(x)$ being infinitely differentiable, and so by the lemma above it is $\leq \epsilon$. 

  \end{proof}

  \begin{theorem}
    
  \end{theorem}
  \begin{proof}
    Suppose for the sake of contradiction that $\sin(n_k x) \to g(x)$ uniformly for subsequence $(n_k)_k$. Then $g(x)$ must be continuous on $[0, 1]$. Then 
    \begin{equation} 
      \int_a^b g(x)^2 \,dx = \lim_{n \to \infty} \int_0^1 g(x) \sin(n_k x) \,dx = 0
    \end{equation}
    due to the theorem, which implies that $g = 0$. But since $g(nx) = \pm 1$ for some $x$ for all $n$, we have a contradiction. 
  \end{proof}

  We would like uniform convergence, so we want conditions to avoid lack of uniform convergence. Keep in mind to counterexamples. To avoid translation, work with compact space, or if not compact, have the functions decay uniformly. To avoid oscillation, we can bound the derivative, which is a restriction on each function $|f_n^\prime (x)| \leq M$. The Cauchy criterion is too much. To avoid going to infinity, just bound $f$: $|f_n (x)| \leq M$ for all $n \in \mathbb{N}, x \in X$. 


\subsection{Equicontinuous Families} 

  The bounding of derivatives can be a bit strong. We aren't always working with differentiable functions, so we introduce a similar concept and introduce the Arzela-Ascolli theorem. 

  \begin{definition}[Equicontinuous Family]
    A family of functions $\mathcal{F}$ on $E$ is said to be \textbf{equicontinuous} if $\forall \epsilon > 0$ there exists a $\delta > 0$ s.t. 
    \begin{equation}
      |x - y| < \delta \implies |f(x) - f(y)| < \epsilon
    \end{equation}
    for all $x, y \in E, f \in \mathcal{F}$. So this doesn't even depend on $f$. 
  \end{definition}

  \begin{example}
    Fix $M \geq 0$. Then 
    \begin{equation}
      \mathcal{F}_n \coloneqq \{f : [0, 1] \to \mathbb{R} \mid |f^\prime (x)| \leq M \} 
    \end{equation}
    is an equicontinuous family. For any $f \in \mathcal{F}$, the MVT $|f(x) - f(y)| = |f^\prime (c) (x - y)|$ for some $c \in (x, y)$. But since $f^\prime (c)$ is bounded by $M$, take $\delta = \epsilon/M$. 
  \end{example}

  \begin{example}
    $\mathcal{F} = \{\sin(nx)\}_{n \in \mathbb{N}}$ is not equicontinuous on $[0, 1]$ since 
    \begin{equation}
      \bigg| \sin\Big( n \frac{\pi}{2n} \Big) - \sin \Big( n \frac{\pi}{n} \Big) \bigg| = 1
    \end{equation}
    for all $n$. So setting $x_n = \frac{\pi}{2n}, y_n = \frac{\pi}{n}$, we have $d(x_n, y_n) \to 0$ while $d(f(x_n), f(y_n)) \geq 1$. So this is not equicontinuous. 
  \end{example} 

  Now the Ascolli's theorem gives us conditions to get rid of translation, oscillations, and infinity. To prove the second statement, we will need a lemma, so we state it now, along with providing a neat trick for constructing sequences. 

  \begin{lemma} 
    Let $(f_n)$ be a sequence of functions on $[0, 1]$ that's uniformly bounded. Let $\{q_m\}_{m=1}^\infty$ be a countable set of numbers in $[0, 1]$. Then $\exists$ a subsequence $(f_{n_k})$ for which $f_{n_k} (q_m)$ is convergent for all $m \in \mathbb{N}$. 
  \end{lemma}
  \begin{proof}
    Intuitively, if we find a sequence of functions, we want to look at each point---say $1$---and look at $(f_n (1))_n$. $(f_n (1))$ is bounded and so contains a convergent subsequence $(f_{n_k} (1))_k$. Now with this subsequence, we look at $(f_{n_k} (0))_k$ which is bounded and therefore $(f_{n_{k_j}}(0))_j$ converges, and $(f_{n_{k_{j}}}(1))_j$ must converge as a subsequence of convergent $(f_{n_k} (1))_k$. Now do this for all $q$'s, and we get a single subsequence that converges for all of them. 

    For ease of notation, let $f_{ij}$ denote the $j$th term of the $i$th subsequence. Then there exists $(f_{n, 1})_n$ s.t. $(f_{n, 1} (q_1))_n$ converges. Take a subsequence $f_{n, 2}$ of $f_{n, 1}$ s.t. $(f_{n, 2} (q_2))_n$ converges. Given $(f_{n, k})_n$, find a subsequence of it, called $(f_{n, k+1})_n$ for which $(f_{n, k+1} (q_{k+1}))_n$ converges. Now $(f_{n, n})_n$ is a subsequence of the original one ($n$th term of $n$th subsequence) for which $(f_{q, n})_n$ is eventually a subsequence of $(f_{n, j})_n$ for any fixed $j$. 
  \end{proof} 

  As an intuitive example, suppose 
  \begin{enumerate}
    \item $(f_{2n} (q_1))$ converges 
    \item $(f_{3n} (q_2))$ converges 
    \item $(f_{5n} (q_3))$ converges 
    \item $(f_{7n} (q_4))$ converges 
  \end{enumerate}
  So combining the first two, we have that $(f_{6n}(q_i))$ converges for $i = 1, 2$. Continuing on, $(f_{30n} (q_i))$ converges for $i = 1, 2, 3$. But you can't do this infinitely. So if you want a single subsequence s.t. all the sequences converges, we can do 
  \begin{equation}
    f_2, f_6, f_{30}, f_{210}, f_{2310}, \ldots
  \end{equation}
  Since 
  \begin{enumerate}
    \item if you take out $f_2$, it is a subsequence of $(f_{3n})$ which converges for $q_2$, and 
    \item if you also take out $f_{6}$, it is a subsequence of $(f_{6n})$ which converges for $q_1, q_2$, and 
    \item if you also take out $f_{30}$, it is a subsequence of $(f_{30n})$ which converges for $q_1, q_2, q_3$
  \end{enumerate}
  so $f_{n_k} (q_i)$ converges for all $i$. 

  \begin{theorem}[Arzela-Ascolli's Theorem]
    We claim the following. 
    \begin{enumerate}
      \item If a sequence of continuous functions $f_n: [0, 1] \to \mathbb{R}$ converges uniformly, then they form an equicontinuous family. 
      \item If a sequence of functions $f_n: [0, 1] \to \mathbb{R}$ is equicontinuous (and so uniformly bounded), then it has a uniformly convergent subsequence. 
    \end{enumerate}
  \end{theorem}
  \begin{proof}
    Assume $(f_n)$ is uniformly convergent. Then it is uniformly Cauchy. To prove equicontinuity, given a $\epsilon > 0$ we need to find a $\delta > 0$ for all the functions. Since $f_n$ is uniformly Cauchy, $\exists N$ s.t. if $n \geq N$, then 
    \begin{equation}
      \sup_{x \in [0, 1]} | f_n (x) - f_N (x) | < \epsilon/3
    \end{equation} 
    Consider the first $N$ functions $f_1, \ldots, f_N$. They are all continuous on a compact set and so uniformly continuous. So for each $f_i$, there exists a $\delta_i$ s.t. $|x - y| < \delta_i \implies |f_i (x) - f_i (y)| < \epsilon$. So take $\delta = \frac{1}{3} \min_i \delta_i > 0$. So for all $1 \leq i \leq N$, 
    \begin{equation}
      |x - y| < \delta \implies |f_i (x) - f_i (y)| < \epsilon/3
    \end{equation} 
    and for $n \geq N$, 
    \begin{equation}
      |f_n (x) - f_n (y)| \leq \underbrace{|f_n (x) - f_N (x)}_{< \epsilon/3} + \underbrace{f_N (x) - f_N (y)}_{< \epsilon/3} + \underbrace{f_N (y) - f_n (y)}_{< \epsilon/3} < \epsilon
    \end{equation} 
    For the second part, let $E = \mathbb{Q} \cap [0, 1]$. It is a good thing that $E$ is dense in $[0, 1]$. Let $(f_n)$ be an equicontinuous on $[0, 1]$ and uniformly bounded. Due to the lemma, there exists a $(f_{n_k})_k$ so that the $f_{n_k}$ converges pointwise on $E$ (since $E$ is countable). We will not use equicontinuity of $(f_{n_k})_k$ to prove it's uniformly Cauchy on $[0, 1]$, which will imply that it's convergent. To make notation easier we will call $f_{n_k} = g_k$. Let $\epsilon > 0$. Since $g_k$ is equicontinuous, $\exists \delta > 0$ s.t. 
    \begin{equation}
      |x - y| < \delta \implies |g_k (x) - g_k (y)| < \epsilon
    \end{equation} 
    Since $E = \{q_1, q_2, \ldots \}$ is dense in $[0, 1]$, $\{B_\delta (q_i) \}_{i=1}^\infty$ is an open cover of $[0, 1]$. Since $[0, 1]$ is compact, there exists a finite subcover 
    \begin{equation}
      [0, 1] \subset \bigcup_{j=1}^N B_{\delta} (q_{i_j})
    \end{equation}
    Since $(g_k (q_{i_j}))$ converges for each $1 \leq j \leq N$, there exists $M_j$ s.t. 
    \begin{equation}
      n, m \geq M_j \implies |g_n (q_{i_j} - g_m (q_{i_j}))| < \epsilon
    \end{equation}
    Take $M = \max_j M_j$. Now if $m, n \geq M$, given $x \in [0, 1]$ $\exists q_i$ with $1 \leq i \leq N$ so that $x \in B_\delta (q_i)$, and so 
    \begin{align}
      |g_n (x) - g_m (x)| & \leq \underbrace{| g_n (x) - g_n (q_i)|}_{< \epsilon} + \underbrace{| g_n (q_i) - g_m (q_i)|}_{< \epsilon} + \underbrace{|g_m (q_i) - g_m (x)|}_{< \epsilon} < 3\epsilon
    \end{align}
    where the first and third inequalities come from equicontinuity, and the middle come from convergence on $E$. So by setting $\delta/3$ we are done. 
  \end{proof}

  \begin{example}
    An important application is in the existence of minimizers/maximizers for optimization problems involving functions. To minimize 
    \begin{equation}
      J(f) = \int_0^1 \sqrt{1 + f^\prime (t)} \,dt 
    \end{equation}
    is the length of the curve of $f: [0, 1] \to \mathbb{R}$. To minimize the length of the curve, we must search over a set of functions. So to use EVT, you must know what the compact subsets of functions. 
  \end{example}

  Ascolli's theorem exactly characterizes these compact subsets. These compact subsets of function spaces is the closure of equicontinuous functions. 

  \begin{corollary}
    A set of functions $K \subset C([0, 1])$ is compact iff it is, under the supremum metric $\sup_{x \in [0, 1]}$, 
    \begin{enumerate}
      \item closed 
      \item bounded 
      \item equicontinuous
    \end{enumerate}
    The first two are needed for finite dimensions. The third condition is for function spaces. 
  \end{corollary}

\subsection{Approximation of the Identity} 

  \begin{definition}[Approximation of the Identity]
    A family of functions $\{\varphi_\epsilon\}$ parameterized by $\epsilon$ is called an \textbf{approximation of the identity} if 
    \begin{enumerate}
      \item $\int_{-\infty}^{\infty} \varphi_\epsilon (y) \,dy = 1$ 
      \item $\lim_{\epsilon \to 0} \int_{|y| > \delta} \varphi_\epsilon (y) \,dy = 0$ for all $\delta > 0$ 
      \item $\varphi_\epsilon \geq 0$.\footnote{This condition is flexible, but it makes things a bit easier for now.}
    \end{enumerate}
  \end{definition}

  \begin{example}
    Consider the functions $f_\epsilon$ satisfying $f(-\epsilon) = f(\epsilon) = 0$, $f(0) = 1/\epsilon$, and everything in between linearly interpolated. Then this is an approximation of the identity. 
  \end{example}

  \begin{example}
    Take any $\varphi \geq 0$ s.t. $\int_{-\infty}^\infty \varphi = 1$ and define 
    \begin{equation}
      \varphi_\epsilon = \frac{1}{\epsilon} \varphi \bigg( \frac{x}{\epsilon} \bigg)
    \end{equation}
    which we can think of as squeezing the function horizontally to $0$ and making the amplitude very large. Then we see that 
    \begin{equation}
      \int_{-\infty}^{\infty} \varphi_\epsilon (y) \,dy = \int_{-\infty}^{\infty} \frac{1}{\epsilon} \varphi \bigg(\frac{y}{\epsilon} \bigg) = \int_{-\infty}^\infty \varphi(x) \,dx = 1
    \end{equation}
    Fix $\delta > 0$. Then 
    \begin{equation}
      \int_\delta^{\infty} \varphi_\epsilon (x) \,dx = \int_\delta^\infty \frac{1}{\epsilon} \varphi \bigg( \frac{x}{\epsilon} \bigg) \,dx = \int_{\delta/\epsilon}^\infty \varphi(y) \,dy \to 0 \text{ as } \epsilon \to 0
    \end{equation}
    Since $\delta$ is fixed, we have $\delta/\epsilon \to +\infty$ as $\epsilon \to 0$, and so this integral of the tail above converges to $0$.  
  \end{example} 

  The approximation of the identity (AoI) has an amazing property. 

  \begin{theorem}
    Let $\{\varphi_\epsilon\}$ be an AoI. Assume $f: \mathbb{R} \to \mathbb{R}$ is bounded and continuous. Then 
    \begin{equation}
      \lim_{\epsilon \to 0} \int_{-\infty}^\infty \varphi_\epsilon (y) \, f(y) \,dy = f(0)
    \end{equation}

    \begin{figure}[H]
      \centering 
      \caption{The triangle has area $1$. Now if you integrate the product of $\varphi_\epsilon (y)$ and $f(y)$, it's like taking the product and multiplying by $f$. But as $\epsilon \to 0$, the triangle's area is $1$, and at the end you just multiply by $f(0)$. } 
      \label{fig:aoi}
    \end{figure}
  \end{theorem}
  \begin{proof}
    We have 
    \begin{align}
      \bigg| \int_{-\infty}^\infty \varphi_\epsilon (y) \, f(y) \,dy - f(0) \bigg| & = \bigg| \int_{-\infty}^\infty \varphi_\epsilon (y) \, \big( f(y) - f(0) \big) \,dy \bigg| \\ 
                                                                                   & \leq \bigg| \int_{-\delta}^{\delta} \varphi_\epsilon (y) \big( f(y) - f(0) \big) \,dy \bigg| + \bigg| \int_{|y| > \delta} \varphi_\epsilon (y) \big( f(y) - f(0) \big) \,dy \bigg| \\ 
                                                                                   & \leq \sup_{y \in [-\delta, \delta]} |f(y) - f(0)| + 2 M \int_{|y| > \delta} \varphi_\epsilon (y) \,dy 
    \end{align}
    where the final step follows from the left integral is less than $\sup_{[-\delta, \delta]} |f(y) - f(0)|$, and for the right integral, we have $f(y) - f(0) \leq 2 \sup |f| \leq 2 M$. Since you don't know the limit exists, you take the limsup, 
    \begin{equation}
      \limsup_{\epsilon \to 0} \bigg| \int_{-\infty}^\infty \varphi_\epsilon (y) \, f(y) \,dy - f(0) \bigg| \leq \sup_{y \in [-\delta, \delta]} |f(y) - f(0)| \to 0 \text{ as } \delta \to 0
    \end{equation}
  \end{proof}

  \begin{corollary}[Convolution]
    If $\{f_\epsilon\}$ is an AoI with $f$ bounded and continuous and $x \in \mathbb{R}$, we have 
    \begin{equation}
      \lim_{\epsilon \to 0} \int_{-\infty}^\infty \varphi_\epsilon (y) \, f(x - y) \,dy  = f(x)
    \end{equation}
    This is called the \textbf{convolution} of $f$ with $\varphi_\epsilon$. 
  \end{corollary}

  \begin{definition}[Dirac Delta Function]
    $\varphi_\epsilon \to \delta_0$ as $\epsilon \to 0$, the Dirac delta function. This is a limit. 
  \end{definition}

  \begin{theorem}
    Consider the functions 
    \begin{equation}
      \varphi_n (x) = \begin{cases} 
        c_n (1 - x^2)^n & \text{ if } x \in [-1, 1] \\ 
        0 & \text{ else }
      \end{cases}, \qquad c_n = \bigg( \int_{-1}^1 (1 - x^2)^n \,dx \bigg)^{-1}
    \end{equation}
    Then, 
    \begin{equation}
      \int_{-1}^1 \varphi_n (x) = \int_{-\infty}^\infty \varphi_n (x) \,dx = 1
    \end{equation}
    so $\{\varphi_n\}$ is an approximation of the identity. 
  \end{theorem}
  \begin{proof}
    Note that $c_n$ is chosen such that $\int_{-\infty}^\infty \varphi_n (x) \,dx = 1$ and $\varphi_n \geq 0$. We want to show 
    \begin{equation}
      \int_{|x| > \delta} \varphi_n (x) \,dx \to 0 \text{ as } n \to \infty \text{ for all } \delta > 0
    \end{equation}
    We claim that $c_n \leq 10 \sqrt{n}$. since we wish to upper bound the multiplicative inverse of an integral, it suffices to lower bound the inverse---i.e. the integral itself. 
    \begin{align}
      \int_{-1}^1 (1 - x^2)^n \,dx = 2 \int_0^1 (1 - x^2)^n \,dx & \geq 2 \int_0^{1/\sqrt{n}} (1 - x^2)^n \,dx \\
                                                                 & \geq 2 \int_0^{1/\sqrt{n}} (1 - nx^2) \,dx 
    \end{align}
    where the last inequality follows from the binomial inequality $(1 - s)^n \geq 1 - sn$. Therefore, the final integral is now computable, so it equals 
    \begin{equation}
      '' = 2 \bigg(x - \frac{nx^3}{3}\bigg) \bigg|_0^{1/\sqrt{n}} = \frac{4}{3} \frac{1}{\sqrt{n}} \implies c_n \leq \bigg( \frac{4}{3} \frac{1}{\sqrt{n}})^{-1} < \sqrt{n}
    \end{equation}  
    Now we have 
    \begin{align}
      \int_{|x| > \delta} \varphi_n (x) \,dx & = 2 \int_\delta^{+\infty} \varphi_n (x) \,dx \\ 
                                             & = 2 \int_\delta^1 c_n (1 - x^2)^n \,dx \\ 
                                             & \leq 20 \sqrt{n} \int_\delta^1 (1 - x^2)^n \,dx \\ 
                                             & \leq 20 \sqrt{n} (1 - \delta^2)^n \to 0 \text{ as } n \to \infty 
    \end{align} 
    Note that we could have claim that the bound be $c_n \leq (10\sqrt{n})^{1000}$ and this would still be true. 
  \end{proof} 

  \begin{theorem}[Stone-Weierstrass Theorem]
    Let $f \in C([a, b])$. Then $\forall \epsilon > 0$, $\exists$ a polynomial $p$ s.t. 
    \begin{equation}
      d(f, p) \coloneqq \sup_{x \in [a, b]} d\big( f(x), p(x) ) < \epsilon
    \end{equation}
    This is equivalent to saying that $\mathbb{R}[x]$ is dense in $C([a, b])$, or that for any $f \in C([a, b])$, $\exists$ sequence $(p_n)$ of polynomials s.t. $p_n \to p$ uniformly on $[a, b]$. 
  \end{theorem}
  \begin{proof}
    By translation and dilation, it suffices to take $[a, b] = [-1, 1]$. This is because translation/dilations are automorphisms (?). It also suffices to consider only $f$ for which $f(-1) > 0$ and $f(1) = k$ for some number $k$. This is because we can always replace $f$ with $\Tilde{f}$ defined by 
    \begin{equation}
      \Tilde{f}(x) \coloneqq f(x) - \bigg( \frac{(1 + x) f(1) + (1 - x)f(-1)}{2} \bigg)
    \end{equation} 
    Let's extend $f$ by $0$ outside of $[-1, 1]$. $f$ is now a bounded continuous function $\mathbb{R}$ implying that $f$ is uniformly continuous. Then we can take the integral. 
    \begin{equation}
      \varphi_n (x) = \begin{cases} 
        \frac{(1 - x^2)^n}{\int (1 - x^2)^n \,dx} & \text{ if } x \in [-1, 1] \\ 
        0 & \text{ else} 
      \end{cases} 
    \end{equation}
    Since $f$ is bounded, uniformly continuous on $\mathbb{R}$, and since $\{\varphi\}$ is an approximation of the identity, 
    \begin{equation}
      \int_{-\infty}^\infty \varphi_n (t) f(x - t) \,dt \to f(x)
    \end{equation} 
    and so $p_n$ is defined on $[-\frac{1}{2}, \frac{1}{2}]$ with $p_n \to f$ uniformly. 
  \end{proof}

  Now we can use the exact same strategy to prove convergence of Fourier series. 

  \begin{definition}[$L^2$ Inner Product]
    The $L^2$ inner product is defined on $C([a, b])$ as 
    \begin{equation}
      \langle f, g \rangle \coloneqq \frac{1}{b - a} \int_a^b f(x) \overline{g(x)} \,dx 
    \end{equation}
    where $f, g$ are complex valued. 
  \end{definition}

  We know that orthonormal bases behave nicely. We present one particularly important one. 

  \begin{lemma} 
    The functions $\{e^{inx}\}_{n \in \mathbb{Z}}$ are orthonormal in $C([0, 2\pi])$.
  \end{lemma} 
  \begin{proof}
    We have for $n, m \in \mathbb{Z}$
    \begin{equation}
      \langle e^{inx}, e^{imx} \rangle = \frac{1}{2\pi} \int_0^{2\pi} e^{inx} \overline{e^{imx}} \,dx = \frac{1}{2\pi} \int_0^{2\pi} e^{inx} e^{-imx} \,dx = \frac{1}{2\pi} \int_0^{2\pi} e^{i(n-m) x} \,dx
    \end{equation}
    So if $n = m$, then $\langle f, g \rangle = 1$. If not, then 
    \begin{equation}
      \langle f, g \rangle = \frac{1}{2 \pi i (n - m)} e^{i (n - m) x} \bigg|_0^{2\pi} = 0 
    \end{equation}
    and we are done. 
  \end{proof}

  \begin{lemma}
    Define 
    \begin{equation}
      \varphi_N (x) = \sum_{k = -N}^{N} e^{i k x}
    \end{equation}
    Then, the family $\{\varphi_N\}_{N = 0}^\infty$ forms an AoI. This is called a \textit{generalized AoI}. 
  \end{lemma}

  With this, we can prove that any sufficiently smooth (i.e. $C^1$) functions can be approximated with Fourier series. 


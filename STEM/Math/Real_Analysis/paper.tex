\documentclass{article}

% packages
  % basic stuff for rendering math
  \usepackage[letterpaper, top=1in, bottom=1in, left=1in, right=1in]{geometry}
  \usepackage[utf8]{inputenc}
  \usepackage[english]{babel}
  \usepackage{amsmath} 
  \usepackage{amssymb}

  % extra math symbols and utilities
  \usepackage{mathtools}        % for extra stuff like \coloneqq
  \usepackage{mathrsfs}         % for extra stuff like \mathsrc{}
  \usepackage{centernot}        % for the centernot arrow 
  \usepackage{bm}               % for better boldsymbol/mathbf 
  \usepackage{enumitem}         % better control over enumerate, itemize
  \usepackage{hyperref}         % for hypertext linking
  \usepackage{fancyvrb}          % for better verbatim environments
  \usepackage{newverbs}         % for texttt{}
  \usepackage{xcolor}           % for colored text 
  \usepackage{listings}         % to include code
  \usepackage{lstautogobble}    % helper package for code
  \usepackage{parcolumns}       % for side by side columns for two column code
  

  % page layout
  \usepackage{fancyhdr}         % for headers and footers 
  \usepackage{lastpage}         % to include last page number in footer 
  \usepackage{parskip}          % for no indentation and space between paragraphs    
  \usepackage[T1]{fontenc}      % to include \textbackslash
  \usepackage{footnote}
  \usepackage{etoolbox}

  % for custom environments
  \usepackage{tcolorbox}        % for better colored boxes in custom environments
  \tcbuselibrary{breakable}     % to allow tcolorboxes to break across pages

  % figures
  \usepackage{pgfplots}
  \pgfplotsset{compat=1.18}
  \usepackage{float}            % for [H] figure placement
  \usepackage{tikz}
  \usepackage{tikz-cd}
  \usepackage{circuitikz}
  \usetikzlibrary{arrows}
  \usetikzlibrary{positioning}
  \usetikzlibrary{calc}
  \usepackage{graphicx}
  \usepackage{algorithmic}
  \usepackage{caption} 
  \usepackage{subcaption}
  \captionsetup{font=small}

  % for tabular stuff 
  \usepackage{dcolumn}

  \usepackage[nottoc]{tocbibind}
  \pdfsuppresswarningpagegroup=1
  \hfuzz=5.002pt                % ignore overfull hbox badness warnings below this limit

% New and replaced operators
  \DeclareMathOperator{\im}{Im}
  \DeclareMathOperator{\Div}{div}
  \DeclareMathOperator{\curl}{curl}
  \DeclareMathOperator{\Int}{Int}
  \DeclareMathOperator{\Id}{Id}
  \DeclareMathOperator{\Lie}{Lie}
  \DeclareMathOperator{\Hom}{Hom}
  \DeclareMathOperator{\Alt}{Alt}
  \DeclareMathOperator{\rank}{rank}
  \DeclareMathOperator{\conv}{conv}
  \DeclareMathOperator{\aff}{aff}
  \DeclareMathOperator{\arccot}{arccot}

% Custom Environments
  \newtcolorbox[auto counter, number within=section]{question}[1][]
  {
    colframe = orange!25,
    colback  = orange!10,
    coltitle = orange!20!black,  
    breakable, 
    title = \textbf{Question \thetcbcounter ~(#1)}
  }

  \newtcolorbox[auto counter, number within=section]{exercise}[1][]
  {
    colframe = teal!25,
    colback  = teal!10,
    coltitle = teal!20!black,  
    breakable, 
    title = \textbf{Exercise \thetcbcounter ~(#1)}
  }
  \newtcolorbox[auto counter, number within=section]{solution}[1][]
  {
    colframe = violet!25,
    colback  = violet!10,
    coltitle = violet!20!black,  
    breakable, 
    title = \textbf{Solution \thetcbcounter}
  }
  \newtcolorbox[auto counter, number within=section]{lemma}[1][]
  {
    colframe = red!25,
    colback  = red!10,
    coltitle = red!20!black,  
    breakable, 
    title = \textbf{Lemma \thetcbcounter ~(#1)}
  }
  \newtcolorbox[auto counter, number within=section]{theorem}[1][]
  {
    colframe = red!25,
    colback  = red!10,
    coltitle = red!20!black,  
    breakable, 
    title = \textbf{Theorem \thetcbcounter ~(#1)}
  } 
  \newtcolorbox[auto counter, number within=section]{proposition}[1][]
  {
    colframe = red!25,
    colback  = red!10,
    coltitle = red!20!black,  
    breakable, 
    title = \textbf{Proposition \thetcbcounter ~(#1)}
  } 
  \newtcolorbox[auto counter, number within=section]{corollary}[1][]
  {
    colframe = red!25,
    colback  = red!10,
    coltitle = red!20!black,  
    breakable, 
    title = \textbf{Corollary \thetcbcounter ~(#1)}
  } 
  \newtcolorbox[auto counter, number within=section]{proof}[1][]
  {
    colframe = orange!25,
    colback  = orange!10,
    coltitle = orange!20!black,  
    breakable, 
    title = \textbf{Proof. \thetcbcounter ~(#1)}
  } 
  \newtcolorbox[auto counter, number within=section]{definition}[1][]
  {
  colframe = yellow!25,
  colback  = yellow!10,
  coltitle = yellow!20!black,  
  breakable, 
  title = \textbf{Definition \thetcbcounter ~(#1)}
  } 
  \newtcolorbox[auto counter, number within=section]{example}[1][]
  {
    colframe = blue!25,
    colback  = blue!10,
    coltitle = blue!20!black,  
    breakable, 
    title = \textbf{Example \thetcbcounter ~(#1)}
  } 

  \BeforeBeginEnvironment{example}{\savenotes}
  \AfterEndEnvironment{example}{\spewnotes}
  \BeforeBeginEnvironment{lemma}{\savenotes}
  \AfterEndEnvironment{lemma}{\spewnotes}
  \BeforeBeginEnvironment{theorem}{\savenotes}
  \AfterEndEnvironment{theorem}{\spewnotes}
  \BeforeBeginEnvironment{corollary}{\savenotes}
  \AfterEndEnvironment{corollary}{\spewnotes}
  \BeforeBeginEnvironment{proposition}{\savenotes}
  \AfterEndEnvironment{proposition}{\spewnotes}
  \BeforeBeginEnvironment{definition}{\savenotes}
  \AfterEndEnvironment{definition}{\spewnotes}
  \BeforeBeginEnvironment{exercise}{\savenotes}
  \AfterEndEnvironment{exercise}{\spewnotes}
  \BeforeBeginEnvironment{proof}{\savenotes}
  \AfterEndEnvironment{proof}{\spewnotes}
  \BeforeBeginEnvironment{solution}{\savenotes}
  \AfterEndEnvironment{solution}{\spewnotes}
  \BeforeBeginEnvironment{question}{\savenotes}
  \AfterEndEnvironment{question}{\spewnotes}

  \definecolor{dkgreen}{rgb}{0,0.6,0}
  \definecolor{gray}{rgb}{0.5,0.5,0.5}
  \definecolor{mauve}{rgb}{0.58,0,0.82}
  \definecolor{darkblue}{rgb}{0,0,139}
  \definecolor{lightgray}{gray}{0.93}

% Page style
  \pagestyle{fancy}
  \fancyhead[L]{Real Analysis}
  \fancyhead[C]{Muchang Bahng}
  \fancyhead[R]{Spring 2025} 
  \fancyfoot[C]{\thepage / \pageref{LastPage}}
  \renewcommand{\footrulewidth}{0.4pt}          % the footer line should be 0.4pt wide
  \renewcommand{\thispagestyle}[1]{}  % needed to include headers in title page

\begin{document}

\title{Real Analysis}
\author{Muchang Bahng}
\date{Spring 2025}

\maketitle
\tableofcontents
\pagebreak 

  Let's first talk about why we need analysis in general in the first place. Algebra allows us to define certain algebraic structures, which are essentially sets with operations. These operations are defined to have a finite number of arguments. For example, let's take a look at the negation $x \mapsto -x$ and the addition $x, y \mapsto x + y$ operations in a group $G$. We can compose these operations up to any finite length $n$, removing the parentheses due to associativity, but note that the ``sum'' below is not a single operation. It is a composition of $n-1$ operations. 
  \begin{align}
    & x_1 + x_2 + \ldots + x_n \in G  \\
    & -(-(\ldots(-x))) \in G
  \end{align} 
  This is still well defined due to closure, but what if we wanted to do this an infinite number of times? 
  \begin{align}
    x_1 + x_2 + \ldots & = ? \\
    \ldots(-(-x)) & = ? 
  \end{align} 
  For someone who has learned about sequences and series in high school, this may not be a big jump in logic, but it is. The objects above are not even well-defined and trying to define them with algebraic tools is equivalent to the famous Zeno's paradox. So we simply need to add more tools in order to define these new mathematical objects, which we call \textit{series}. To define series, we need to first define sequences. Can we do this with algebra? Yes, since we can simply model it as a function. 

  \begin{definition}[Sequence]
    A sequence is a function $f: \mathbb{N} \rightarrow X$. We usually denote a sequence by writing out the first few terms of the sequence, followed by an ellipsis. 
    \begin{equation}
      a_1 = f(1), a_2 = f(2), \ldots
    \end{equation}
    or as an indexed set over the naturals $\{a_i\}_{i \in \mathbb{N}}$. 
  \end{definition}

  Therefore, we can consider series as a sequence of finite sums, each element which is well-defined. 
  \begin{equation}
    x_1, x_1 + x_2, x_1 + x_2 + x_3, \ldots
  \end{equation} 
  For any $n \in \mathbb{N}$, we can get the value of $a_n = \sum_{i=1}^n a_i$, but can we say something about the limiting behavior of $a_n$? That is, maybe we can just slap a value $x$ onto this series such that it doesn't ``break'' any of the rules we have in the finite sense. Unfortunately, it is not possible to define such values for all series, but it is possible for some of them, which we call \textit{convergent series}. To rigorously determine which ones are convergent and which ones are not, we need the tools of topology and analysis. Defining the concept of sequences that model infinitely composed operations is what allows us to define differentiation and integration. 

  Great, we've motivated the need for analysis, but before jumping straight into real analysis, let's talk about what analysis in general works with. It studies functions of the form $f: X \rightarrow Y$, and minimally both $X, Y$ must be \textit{Banach spaces}, i.e. complete normed vector spaces over some field $\mathbb{F}$. Almost all flavors of analysis, including real ($\mathbb{R}$), complex ($\mathbb{C}$), multivariate ($\mathbb{R}^n$), p-adic, and functional (infinite-dimensional Banach spaces) analysis require \textit{at least} a Banach space structure. Why are Banach spaces so great? Well if we were to define convergence in $X$ or $Y$, then it only makes sense to talk about convergence with respect to a topology. So $X, Y$ must at least be topological spaces. It would also be bad if we were to take a sequence in $X$ and find out that it converges to some element outside of $X$. Therefore, we want a notion of \textit{completeness} in the sense that all sequences that ``get closer,'' i.e. Cauchy sequences, actually converge in $X$. Unfortunately, while convergence of sequences is preserved under homeomorphisms (and is thus a topological property), convergence of Cauchy sequences is not.\footnote{Consider the sequence $a_n = 1/(n+1)$ in $(0, 1)$ and the map $f(x) = 1/x$ to the set $(1, +\infty)$. $a_n$ is Cauchy but $f(a_n)$ is not.} Furthermore, the notion of uniform convergence is a metric space property, not a topological one. Therefore, the concept of distances is crucial to the construction of analysis. As for the norm, I'm still not sure why we need this.\footnote{Aspinwall and Ng told me this, but I'm not sure why. The Frechet derivative seems like it can be purely defined with a metric. } 

  But in college courses such as real and complex analysis, why do we say we work over the \textit{fields} $\mathbb{R}$ and $\mathbb{C}$ rather than the Banach spaces $\mathbb{R}$ and $\mathbb{C}$? This is because of the following theorem. 

  \begin{theorem}
    Every field $\mathbb{F}$ is a $1$-dimensional vector space over itself. 
  \end{theorem}

  Therefore, when we talk about the \textit{field} $\mathbb{R}$, we are really treating it as a vector space $\mathbb{R}$ over the field $\mathbb{R}$.\footnote{Thanks to Prof. Lenny Ng for clarifying this.} Every other structure beyond this is a ``bonus'' property that gives us extra tools to prove stronger properties. The most notable is the total ordering on $\mathbb{R}$, which allows us to define upper/lower bounds and other real-analysis specific theorems like the intermediate value theorem or the mean value theorem. Other structures include the inner product or the measure. 

  Now that we've taken in the big picture, for each type of analysis, we should construct the underlying relevant Banach space. At the very least, we can with the tools of set theory and algebra define the rationals $\mathbb{Q}$ as an ordered field over the quotient space $\mathbb{Z} \times \mathbb{Z} / \sim$. Furthermore, $\mathbb{Q}$ itself is a normed vector space (over $\mathbb{Q}$)\footnote{Note that while we define the norm and metric to usually map to $\mathbb{R}^+$, $\mathbb{R}$ isn't even defined yet and so to avoid circular definitions, we define the norm on the rationals to have codomain $\mathbb{Q}$. } and the only thing we need now is completeness. 
  \begin{enumerate}
    \item If the norm on $\mathbb{Q}$ is defined as the normal absolute value (Euclidean norm), completing it gives $\mathbb{R}$ as an ordered field which also has a compatible order as that of $\mathbb{Q}$. We study functions mapping to and from $\mathbb{R}$ with \textit{single-variable real analysis}. 
    \item If we take the \textit{p-adic} norm, then completing it with respect to this gives the \textit{p-adic numbers}, which also forms a field but loses the ordering. We deal with functions over the p-adics with \textit{p-adic analysis}. 
    \item We can construct $\mathbb{C}$ by taking $\mathbb{R}^2$ and endowing it with a bit more structure. We get \textit{complex analysis}. 
    \item We can construct $\mathbb{R}^n$ and $\mathbb{C}^n$ by easily defining its vector space structure and then endowing it with a norm, and showing that it is complete with respect to the norm-induced metric. This is known as \textit{multivariate analysis}. 
    \item With all these defined, we can define Banach function spaces like $L^p$ and perform analysis on operators $f: L^p \rightarrow L^q$. This is \textit{functional analysis}. 
  \end{enumerate}
  What we have talked about so far was Cauchy completeness, but there is a different type of completeness called \textit{Dedekind completeness}, also equivalently known as the \textit{least-upper-bound (LUB) property}, defined only on ordered sets (with no other structure). It turns out that in an ordered field, the two forms of completeness are equivalent.\footnote{Actually, this is not true. Dedekind completeness is equivalent to Cauchy completeness plus the Archimedean property. An example of a Cauchy-complete non Archimidean field is the field $F$ of rational functions over $\mathbb{R}$, with positive cone consisting of those functions $f/g$ such that the leading coefficients of $f, g$ have the same algebraic sign. The Cauchy completion of this into the equivalence classes of Cauchy sequences in $F$ results in a non-Archimedean field. } Therefore, many real analysis textbooks tend to use Dedekind completeness when constructing the reals, but Cauchy completeness is in a sense more ``fundamental.'' We will go through both independent constructions of $\mathbb{R}$ involving both types of completeness since both are used in future theorems. 

  \begin{enumerate}
    \item \textit{Construction from Cauchy Sequences}. We verify that $\mathbb{Q}$ is a field and endow it with the standard Euclidean metric $d(x, y) = |x - y|$. We can then construct a new quotient space $S$ of Cauchy sequences in $\mathbb{Q}$, define all the ordered field operations/relations, and finally show that $S$ satisfies Cauchy completeness. Most would end here and claim that this is $\mathbb{R}$, but we must also prove the Archimedean property with this order. Once done, now we can truly claim $S = \mathbb{R}$. 

    \item \textit{Construction from Dedekind Cuts}. We verify that $\mathbb{Q}$ is a field, put an order on it, and verify that it is an ordered field. We then construct a new set $D$ of \textit{Dedekind cuts} from $\mathbb{Q}$, define the compatible ordered field operations/relations, and show that this new set $D$ satisfies the least-upper bound property. We claim that $D = \mathbb{R}$. 
  \end{enumerate} 

\section{Fields and Banach Spaces} 

  \subsection{The Rationals}

    \subsubsection{Field Properties}  

      \begin{definition}[Field]
        A \textbf{field} is an algebraic structure $(\mathbb{F}, +, \cdot)$ where 
        \begin{enumerate}
          \item $\mathbb{F}$ is an abelian group under $+$, with $0$ being the \textit{additive identity}. 
          \item $\mathbb{F}$ is an abelian group under $\cdot$, with $1$ being the \textit{multiplicative identity}. 
          \item It connects the two operations through the \textit{distributive property}.
          \begin{equation}
            x \cdot (y + z) = x \cdot y + x \cdot z
          \end{equation}
        \end{enumerate}
      \end{definition} 

      \begin{lemma}[Left = Right Distributivity]
        Left and right distributivity are equivalent. 
        \begin{equation}
          x \cdot (y + z) = (y + z) \cdot x
        \end{equation}
      \end{lemma} 
      \begin{proof}
        \begin{align}
          x \cdot (y + z) & = x \cdot y + x \cdot z && \tag{Distributive} \\
                          & = y \cdot x + z \cdot x && \tag{Commutative} \\
                          & = (y + z) \cdot x && \tag{Distributive} 
        \end{align}
      \end{proof} 

      \begin{lemma}[Operations]
        A field satisfies. 
        \begin{enumerate}
          \item $0 \cdot x = 0$.  
        \end{enumerate}
      \end{lemma}

      Now that we've reviewed some fields, let's construct $\mathbb{Q}$ from $\mathbb{Z}$ and verify it's a field. 

      \begin{definition}[Rationals]
        Given the ordered ring of integers $(\mathbb{Z}, +_{\mathbb{Z}}, \times_{\mathbb{Z}}, \leq_{\mathbb{Z}})$ the \textbf{rational numbers} $(\mathbb{Q}, +_{\mathbb{Q}}, \times_{\mathbb{Q}})$ are defined as such. 
        \begin{enumerate}
          \item $\mathbb{Q}$ is the quotient space on $\mathbb{Z} \times \mathbb{Z} \setminus \{0\}$ with the equivalence relation $\sim$ 
          \begin{equation}
            (a, b) \sim (c, d) \iff a \times_{\mathbb{Z}} d = b \times_{\mathbb{Z}} c
          \end{equation} 
          We denote this class as $(a, b)$, where $b > 0$, since if $b < 0$, we know that $(-a, -b)$ are also in this order. 

          \item The additive and multiplicative identities are 
          \begin{equation}
            0_{\mathbb{Q}} \coloneqq (0_{\mathbb{Z}}, a), \;\;\; 1_{\mathbb{Q}} \coloneqq (a, a)
          \end{equation}

          \item Addition on $\mathbb{Q}$ is defined 
          \begin{equation}
            (a, b) +_{\mathbb{Q}} (c, d) \coloneqq \big( (a \times_{\mathbb{Z}} d) +_{\mathbb{Z}} (b \times_{\mathbb{Z}} c), b \times_{\mathbb{Z}} d \big) 
          \end{equation}

          \item The additive inverse is defined 
          \begin{equation}
            -(a, b) \coloneqq (-a, b)
          \end{equation}

          \item Multiplication on $\mathbb{Q}$ is defined 
          \begin{equation}
            (a, b) \times_{\mathbb{Q}} (c, d) \coloneqq \big( a \times_{\mathbb{Z}} c, b \times_{\mathbb{Z}} d \big)
          \end{equation} 

          \item The multiplicative inverse is defined 
          \begin{equation}
            (a, b)^{-1} \coloneqq (b, a)
          \end{equation}
        \end{enumerate}
      \end{definition}

      \begin{theorem}
        $\mathbb{Q}$ is a field. 
      \end{theorem} 
      \begin{proof}
        We do a few things. 
        \begin{enumerate}
          \item Verify the additive identity. 
          \item Verify the multiplicative identity. 
          \item Additive inverse is actually an inverse. 
          \item Multiplicative inverse is actually an inverse. 
          \item Addition is commutative. 
          \item Addition is associative. 
          \item Multiplication is commutative. 
          \item Multiplication is associative. 
          \item Multiplication distributes over addition. 
        \end{enumerate}
      \end{proof} 

      We have successfully defined the rationals, but now these are almost completely separate elements. We know that all integers are rational numbers, and so to show that the rationals are an extension of $\mathbb{Z}$ we want to identify a \textit{canonical injection} $\iota: \mathbb{Z} \rightarrow \mathbb{Q}$. This can't just be any canonical injection; it must preserve the algebraic structure between the two sets and must therefore be a \textit{ring homomorphism}. 

      \begin{theorem}[Canonical Injection of $\mathbb{Z}$ to $\mathbb{Q}$ is a Ring Homomorphism]
        Let us define the canonical injection $\iota: \mathbb{Z} \rightarrow \mathbb{Q}$ to be $\iota(a) = (a, 1)$. This is a ring homomorphism. 
      \end{theorem}
      \begin{proof}
        
      \end{proof} 

    \subsubsection{Ordered Field Properties} 

      Great, so we have established that $\mathbb{Q}$ is a field. The next property we want to formalize is order. 

      \begin{definition}[Partial, Total/Linear Order]
        A \textbf{partial order} on a set $X$ is a relation $\leq$ such that for any elements $x, y \in X$, 
        \begin{enumerate}
          \item Reflexive: $x \leq x$ 
          \item Antisymmetric: $x \leq y, y \leq x \implies x = y$
          \item Transitivity: $x \leq y, y \leq z \implies x \leq z$
        \end{enumerate}
        Note that when we say $x \leq y$, this means "$x$ is related to $y$" (but does not necessarily mean that $y$ is related to $x$), or "$x$ is less than or equal to $y$." A set $X$ with a partial order is called a partially ordered set. 

        Additionally, given elements $x, y$ of partially order set $X$, if either $x \leq y$ or $y \leq x$, then $x$ and $y$ are \textbf{comparable}. Otherwise, they are \textbf{incomparable}. A partial order in which every pair of elements is comparable is called a \textbf{total order}, or \textbf{linear order}. Note that from this $\leq$ relation, we can similarly define 
        \begin{enumerate}
          \item $\leq$: less than or equal to 
          \item $\geq$: greater than or equal to 
          \item $<$: strictly less than ($x < y$ iff $x\leq y, x \neq y$)
          \item $>$: strictly greater than ($x > y$ iff $x \geq y, x \neq y$)
        \end{enumerate}
      \end{definition} 

      \begin{example}[Partially Ordered Sets]
        We list some examples of partially ordered sets. 
        \begin{enumerate}
          \item The real numbers ordered by the standard "less-than-or-equal" relation $\leq$ (totally ordered set as well). 
          \item The set of subsets of a given set $X$ ordered by inclusion. That is, the power set $2^X$ with the partial order $\subseteq$ is partially ordered. 
          \item The set of natural numbers equipped with the relation of divisibility. 
          \item The set of subspaces of a vector space ordered by inclusion. 
          \item For a partially ordered set $P$, the sequence space containing all sequences of elements from $P$, where sequence $a$ precedes sequence $b$ if every item in $a$ precedes the corresponding item in $b$. 
        \end{enumerate}
      \end{example} 

      We now want to define the natural ordering of the rationals. There are countless ways to do it, but I just take the difference and claim that it is greater than $0$. 

      \begin{theorem}[Order on Rationals]
        The order $\leq_{\mathbb{Q}}$ defined on the rationals as 
        \begin{equation}
          (a, b) \leq_{\mathbb{Q}} (c, d) \iff ad \leq_{\mathbb{Z}} bc
        \end{equation}
        is a total order. Remember that we have defined $b, d > 0$. 
      \end{theorem}
      \begin{proof}
        We prove the three properties. 
        \begin{enumerate}
          \item Reflexive. 
          \begin{equation}
            (a, b) \leq_{\mathbb{Q}} (a, b) \iff ab \leq_{\mathbb{Z}} ab
          \end{equation} 

          \item Antisymmetric. 
          \begin{align}
            (a, b) \leq_{\mathbb{Q}} (c, d) & \implies ad \leq_{\mathbb{Z}} bc
            (c, d) \leq_{\mathbb{Q}} (a, b) & \implies bc \leq_{\mathbb{Z}} ad
          \end{align} 
          This implies that both $ad = bc$, which by definition means that they are in the same equivalence class. 

          \item Transitivity. Assume that $(a, b) \leq (c, d)$ and $(c, d) \leq (e, f)$. Then, we notice that $b, d, f > 0$ and therefore by the ordered ring property\footnote{If $a \leq b$ and $0 \leq c$, then $ac \leq bc$.} of $\mathbb{Z}$, we have 
          \begin{align}
            (a, b) \leq_{\mathbb{Q}} (c, d) & \implies ad \leq_{\mathbb{Z}} bc \implies adf \leq_{\mathbb{Z}} bcf \\ 
            (c, d) \leq_{\mathbb{Q}} (e, f) & \implies cf \leq_{\mathbb{Z}} de \implies bcf \leq_{\mathbb{Z}} bde
          \end{align}
          Therefore from transitivity of the ordering on $\mathbb{Z}$ we have $adf \leq bde$. By the ordered ring property\footnote{If $a \leq b$, then $a + c \leq b + c$.}  we have $0 \leq bde - adf = d(be - af)$. But notice that $d > 0$ from our definition of rationals, and therefore it must be the case that $0 \leq be - af \implies af \leq_{\mathbb{Z}} be$, whicb by definition means $(a, b) \leq_{\mathbb{Q}} (e, f)$. 
        \end{enumerate}
      \end{proof} 

      As soon as we define an order the concept of extrema and bounds are well defined. Let's define them too. 

      \begin{definition}[Extrema, Bounds]
        Given a partially ordered set $X$ and some subset $S \subset X$. 
        \begin{enumerate}
          \item $x \in X$ is an \textbf{upper bound} of $S$ if $x \geq y$ for all $y \in S$. 
          \item $x \in X$ is a \textbf{lower bound} of $S$ if $x \leq y$ for all $y \in S$. 
          \item $x \in X$ is a \textbf{maximum} of $S$ if $x$ is an upper bound and $x \in S$. 
          \item $x \in X$ is a \textbf{minimum} of $S$ if $x$ is a lower bound and $x \in S$. 
          \item $x \in X$ is a \textbf{supremum}, or \textbf{least upper bound}, of $S$ if $x$ is the minimum of the set of all upper bounds of $S$. 
          \item $x \in X$ is a \textbf{infimum}, or \textbf{greatest lower bound}, of $S$ if $x$ is the maximum of the set of all lower bounds of $S$. 
        \end{enumerate}
        The main difference between the supremum/infimum and maximum/minimum is that the supremum/infimum accounts for limit points of the subset $S$. 
      \end{definition}

      Note that given a set, we can really put whatever order we want on it. However, consider the field with the following order. 
      \begin{equation}
        \mathbb{F} = \{0, 1\}, \; 0 < 1
      \end{equation} 
      This does not behave well with respect to its operations because for example if we have $0 < 1$, then adding the same element to both sides should preserve the ordering. But this is not the case since $0 + 1 = 1 > 1 + 1 = 0$. While it may be easy to define an order, we would like it to be an ordered field. 

      \begin{definition}[Ordered Field]
        An \textbf{ordered field} is a field that has an order satisfying 
        \begin{enumerate}
          \item $y < z \implies x + y < x + z$ for all $x \in \mathbb{F}$. 
          \item $x > 0, y > 0 \implies xy > 0$. 
        \end{enumerate}
      \end{definition}

      \begin{theorem}[Finite Fields]
        There are no finite ordered fields. 
      \end{theorem} 

      \begin{theorem}[Properties]
        In an ordered field, 
        \begin{enumerate}
          \item $x > 0 \implies -x < 0$. 
          \item $x \neq 0 \implies x^2 > 0$. 
          \item If $x > 0$, then $y < z \implies xy < xz$. 
        \end{enumerate}
      \end{theorem} 

      As we have hinted, the rationals is an ordered field. 

      \begin{theorem}
        $\mathbb{Q}$ is an ordered field. 
      \end{theorem} 
      \begin{proof}
        
      \end{proof} 

      Not only is it an ordered field, but it also is consistent with the ordering on $\mathbb{Z}$! It's nice how all these properties seem to fit together. 

      \begin{theorem}[Preservation of Order]
        The canonical injection $\iota$ preserves order. That is, for $a, b \in \mathbb{Z}$, 
        \begin{equation}
          a < b \iff \iota(a) < \iota(b)
        \end{equation}
      \end{theorem}
      \begin{proof}
        
      \end{proof}

      Note that an order can be used to generate an order topology, which we will define below. 

      \begin{definition}[Order Topology on $\mathbb{Q}$]
        The order topology on $\mathbb{Q}$ is the topology generated by the set $\mathscr{B}$ of all open intervals 
        \begin{equation}
          (a, b) \coloneqq \{ x \in \mathbb{Q} \mid a < x < b\}
        \end{equation}
      \end{definition}

    \subsubsection{Norm} 

      Note that we can also define a norm on the rationals with just the order and algebraic properties. 

      \begin{theorem}[Norm on $\mathbb{Q}$] 
        The following is indeed a norm on $\mathbb{Q}$. 
        \begin{equation}
          |x| \coloneqq \begin{cases} x & \text{ if } x \geq 0 \\ -x & \text{ if } x < 0 \end{cases}
        \end{equation} 
      \end{theorem} 

      It is well known that the metric induced by any norm is indeed a metric. Therefore we state the metric as a definition. 

      \begin{definition}[Metric on $\mathbb{Q}$]
        The Euclidean metric on $\mathbb{Q}$ is defined 
        \begin{equation}
          d(x, y) \coloneqq |x - y| = \begin{cases} x - y & \text{ if } x \geq y \\ y - x & \text{ if } x < y \end{cases}
        \end{equation}
      \end{definition}

      Thus we get to what we want: the induced topology of open balls. 
      Again, since we know from point-set topology that metric topologies are indeed topologies, we will state this as a definition rather than a theorem.  

      \begin{definition}[Open-Ball Topology on $\mathbb{Q}$]
        The Euclidean topology on $\mathbb{Q}$ is the topology generated by the set $\mathscr{B}$ of all open balls
        \begin{equation}
          B(x, r) \coloneqq \{ y \in \mathbb{Q} \mid |x - y| < r \}
        \end{equation} 
      \end{definition}

      Note that this is the same topology as the order topology. This should however be proved. 

      \begin{theorem}[Metric and Order Topologies on $\mathbb{Q}$]
        The metric and order topologies on $\mathbb{Q}$ are the same topologies. 
      \end{theorem}
      \begin{proof}
        
      \end{proof}

  \subsection{The Reals}

      By constructing the topology of $\mathbb{Q}$ earlier, we can talk about convergence. The first question to ask (if you were the first person inventing the reals) is ``how do I know that there exists some other numbers at all?'' The first clue is trying to find the side length of a square with area $2$. As we see, this number is not rational. 

      \begin{theorem}[$\sqrt{2}$ is Not Rational]
        There exists no $x \in \mathbb{Q}$ s.t. $x^2 = 2$. 
      \end{theorem}
      \begin{proof}
        
      \end{proof} 

      We can ``imagine'' that a square with area $2$ certainly exists, but the fact that its side length is undefined is certainly unsettling. I don't know about you, but I would try to ``invent'' $\sqrt{2}$. We can maybe do this in multiple ways. 
      \begin{enumerate}
        \item I write out the decimal expansion one by one, which gives our first exposure to sequences. 
        \begin{equation}
          1, 1.4, 1.41, 1.414, \ldots
        \end{equation} 
        It is clear that on $\mathbb{Q}$, this sequence does not converge. Our intuition tells that that if the terms get closer and closer to each other, they must be getting closer and closer to \textit{something}, though that something is not in $\mathbb{Q}$. This motivates the definition for \textit{Cauchy completeness}. 

        \item I would write out maybe some nested intervals so that $\sqrt{2}$ \textit{must}  lie within each interval. 
        \begin{equation}
          [1, 2] \supset [1.4, 1.5] \supset [1.41, 1.42] \supset \ldots 
        \end{equation}
        This motivates the definition of \textit{nested-interval completeness}. 

        \item I would define the set of all rationals such that $x^2 < 2$, and try to define $\sqrt{2}$ as the max or supremum of this set. We will quickly find that neither the max nor the supremum exists in $\mathbb{Q}$, and this motivates the definition for \textit{Dedekind completeness}. 
      \end{enumerate}

      All three of these methods points at the same intuition that there should not be any "gaps" or "missing points" in the set that we will construct to be $\mathbb{R}$. This contrasts with the rational numbers, whose corresponding number line has a "gap" at each irrational value. 

    \subsubsection{Dedekind Completeness} 

      \begin{definition}[Least Upper Bound Property]
        A totally ordered algebraic field $\mathbb{F}$ (must it be a field?) is complete if every nonempty set of $F$ having an upper bound must have a least upper bound (supremum) in $F$. 
      \end{definition}

      \begin{definition}[Dedekind Cut]
        A \textbf{Dedekind cut} is a partition of the rational numbers into two sets $A$ and $B$, such that all elements of $A$ are less than all elements of $B$, and $A$ contains no greatest element. The set $B$ may or may not have a smallest element among the rationals. 
        \begin{enumerate}
        \item If $B$ has a smallest among the rationals, the cut corresponds to that rational, or in other words, the cut is generated by the rational number. 
        \begin{center}
          \includegraphics[scale=0.25]{img/Dedekind_Cut_Rational.PNG}
        \end{center}
        In the visual above, this Dedekind cut of $\mathbb{Q}$ is generated by $4/3$. 
        \item Otherwise, that cut defines a unique irrational number which, loosely speaking, fills the "gap" between $A$ and $B$. An irrational cut is equated to an irrational number which is in neither set. 
        \begin{center}
          \includegraphics[scale=0.25]{img/Dedekind_Cut_Irrational.PNG}
        \end{center}
        In the visual above, this Dedekind cut of $\mathbb{Q}$ is generated by $\sqrt{2}$, which is not in $\mathbb{Q}$. 
        \end{enumerate}
        Note that Dedekind cuts can be generalized from the rational numbers to any totally ordered set by defining the partition $A$ and $B$, where every element in $A$ is less than every element in $B$, and $A$ contains no greatest element. 
      \end{definition}

      \begin{definition}[Dedekind Completeness]
        A totally ordered algebraic field $F$ is complete if every Dedekind cut of $F$ is generated by an element of $F$. 
      \end{definition} 

      \begin{theorem}
        $\mathbb{Q}$ is not Dedekind-complete. 
      \end{theorem}
      \begin{proof}
        
      \end{proof}

      Therefore, we construct $\mathbb{R}$ with these Dedekind cuts. By construction, this new set $\mathbb{R}$ is Dedekind complete. 

      \begin{theorem}
        Dedekind completeness implies Archimedean property. 
      \end{theorem}

    \subsubsection{Cauchy Completeness} 

      \begin{definition}[Cauchy Sequence]
        A sequence $a_n$ in a metric space $(X, d)$ is a \textbf{Cauchy sequence} if for every $\epsilon > 0$, there exists an $N$ s.t. 
        \begin{equation}
          d(a_i, a_j) < \epsilon
        \end{equation}
        for every $i, j > N$. We call this \textbf{Cauchy convergence}. 
      \end{definition}

      Note that it is not sufficient to say that a sequence is Cauchy by claiming that each term becomes arbitrarily close to the preceding term. That is, 
      \begin{equation}
        \lim_{n \rightarrow \infty} |x_{n+1} - x_{n}| = 0
      \end{equation}
      For example, look at the sequence 
      \begin{equation}
        a_n = \sqrt{n} \implies a_{n+1} - a_{n} = \frac{1}{\sqrt{n+1} + \sqrt{n}} < \frac{1}{2\sqrt{n}}
      \end{equation}
      However, it is clear that $a_n$ gets arbitrarily large, meaning that a finite interval can contain at most a finite number of terms in $\{a_n\}$. 

      It is trivial that convergence implies Cauchy convergence, but the other direction is not true. Therefore, we would like to work in a space where these two are equivalent, and this is called completeness. 

      \begin{definition}[Cauchy Completeness]
        A metric space $(X, d)$ is complete if every Cauchy sequence in that space converges to an element in $X$. 
      \end{definition} 

      \begin{theorem}
        $\mathbb{Q}$ is not Cauchy-complete. 
      \end{theorem}
      \begin{proof}
        
      \end{proof}

      Therefore, we can construct the reals as equivalence classes over Cauchy sequences. By construction, the reals are Cauchy complete. 

    \subsubsection{Nested Intervals Theorem}

      \begin{lemma}[Nested Interval Completeness of $\mathbb{R}$]
        $\mathbb{R}$ is Nested Interval complete. 
      \end{lemma}

      We can also see that $\mathbb{Q}$ is not nested interval complete since the sequence, derived from the digits of $\pi$, 
      \[[3,4] \supset [3.1, 3.2] \supset [3.14, 3.15] \supset [3.141, 3.142] \supset \ldots\]
      is a nested sequence of closed intervals in the rational numbers whose intersection is empty in $\mathbb{Q}$. 

    \subsubsection{Nested Intervals Completeness} 

      The final way we prove is using nested-intervals completeness.  

      \begin{definition}[Nested Interval Completeness, Cantor's Intersection Theorem]
        Let $F$ be a totally ordered algebraic field. Let $I_n= [a_n, b_n]$ ($a_n < b_n$) be a sequence of closed intervals, and suppose that these intervals are nested in the sense that
        \[I_1 \supset I_2 \supset I_3 \supset \ldots\]
        where 
        \[\lim_{n \rightarrow + \infty} b_n - a_n = 0\]
        $F$ is complete if the intersection of all of these intervals $I_n$ contains exactly one point. That is, 
        \[\bigcup_{n=1}^\infty I_n \in F\]
      \end{definition}

      Note that defining nested intervals requires only an ordered field. One may look at this and try to ask if this is a specific instance of the following conjecture: The intersection of a nested sequence of nonempty closed sets in a topological space has exactly 1 point. This claim may not even make sense, actually. If we define nested in terms of proper subsets, then for a finite topological space a sequence cannot exist since we will run out of open sets and so this claim is vacuously true and false. If we allow $S_n = S_{n+1}$ then we can just select $X \supset X \supset \ldots$, which is obviously not true. However, a slightly weaker claim is that every proper nested non-empty closed sets has a non-empty intersection is a consequence of compactness. 

      \begin{theorem}
        $\mathbb{Q}$ is not nested-interval complete. 
      \end{theorem}
      \begin{proof}
      \end{proof}

      Therefore, we can complete $\mathbb{Q}$. It turns out that this is equivalent to the construction using Dedekind cuts, and by definition this new set is nested interval complete. However, like Cauchy completeness, this actually does not imply the Archimedean property. 

    \subsubsection{Properties of the Real Line} 

      Now that we have completed it, we can define the real numbers. 

      \begin{definition}[The Real Numbers]
        The \textbf{set of real numbers}, denoted $\mathbb{R}$, is a totally ordered complete algebraic field. 
      \end{definition} 

      It seems that the real numbers is \textit{any} set that satisfies the definition above. Does this mean that there are multiple real number lines? 

      \begin{example}[Multiple Reals?]
        For example, let us construct three distinct sets satisfying these axioms: 
        \begin{enumerate}
          \item A line $\mathbb{L}$ with $+$ associated with the translation of $\mathbb{L}$ along itself and $\cdot$ associated with the "stretching/compressing" of the line around the additive origin $0$. 
          \item An uncountable list of numbers with possibly infinite decimal points, known as the decimal number system. 
          \begin{equation}
            \ldots, -2.583\ldots, \ldots , 0, \ldots, 1.2343\ldots, \ldots, \sqrt{2}, \ldots
          \end{equation}
          \item A circle with a point removed, with addition and multiplication defined similarly as the line. 
        \end{enumerate}
      \end{example}

      We will show that there is only one set, up to isomorphism, that satisfies all these properties. 

      \begin{theorem}[Uniqueness]
        $\mathbb{R}$ is unique up to field isomorphism. That is, if two individuals construct sets  $\mathbb{R}_A$ and $\mathbb{R}_B$ that satisfy these properties, then 
        \begin{equation}
          \mathbb{R}_A \simeq \mathbb{R}_B
        \end{equation}
      \end{theorem} 

      The second new property is that the reals are uncountable. 

      \begin{theorem}[Cantor's Diagonalization]
        Uncountability. 
      \end{theorem} 

      \begin{theorem}[Archimidean Property]
        
      \end{theorem}

      Provide examples of ordered, Cauchy-complete fields that are not Archimedean. 

      \begin{theorem}[Existence of Nth Roots]
        Is this true for any complete field? 
      \end{theorem}

      \begin{theorem}[Denseness]
        
      \end{theorem}
     
  \subsection{Complex Numbers} 

    Great! We have officially constructed the reals, and we can finally feel satisfied about defining metrics, norms, and inner products as mappings to the codomain $\mathbb{R}$. The next field that will be particularly important is the complex numbers. 

    \subsubsection{Direct Construction from Rationals?}

    \subsubsection{Properties of the Complex Numbers} 

  \subsection{Euclidean Space} 

    Congratulations! We have rigorously constructed both the reals and complex numbers, and this becomes the cornerstone to construct other fundamental sets. Now we consider spaces of the form $\mathbb{R}^n$ or $\mathbb{C}^n$, which we call \textit{Euclidean spaces}, and construct them. This is actually quite easy since we understand linear algebra. 

\section{Euclidean Topology} 

    With the construction of the real line and the real space, the extra properties of completeness, norm, and order (for the real line) allows us to restate these topological properties in terms of these ``higher-order'' properties. It also proves much more results than for general topological spaces. Therefore, the next few sections will focus on reiterating the topological properties of $\mathbb{R}$ and $\mathbb{R}^n$ (this can be done slightly more generally for metric spaces, but we talk about this in point-set topology). In this section, we will restate the notion of open sets, limit points, compactness, connectedness, and separability. Then we can continue in the next section sequences and their limits, and after that we describe continuity. Once this is done, we can focus constructing the derivative and integral, which are unique to Banach spaces. 

  \subsection{Open Sets} 

    It is well-known that the set of open-balls of a metric space $(X, d)$ is indeed a topology, which we prove in point-set topology. We will narrow our scope to $\mathbb{R}^n$ and just reprove this.  

    \begin{theorem}[Euclidean Topology]
      Let $\tau_{\mathbb{R}}$ (which we denote as $\tau$) be the set of subsets $S$ of $(\mathbb{R}^n, || \cdot ||)$ satisfying the property that if $x \in S$, then there exists an open $\epsilon$-ball $B(x, \epsilon)$ s.t. $B \subset S$. $\tau$ is a topology of $\mathbb{R}$. 
    \end{theorem} 
    \begin{proof}
      
    \end{proof}

    \begin{theorem}[Equivalence to Open Ball Topology]
      $\tau$ is equal to the topology $\tau^\prime$ generated by the basis $\mathscr{B}$ of open balls 
      \begin{equation} 
        B(x, r) \coloneqq \{ y \in \mathbb{R}^n \mid ||x - y|| < r\}
      \end{equation}
    \end{theorem} 
    \begin{proof}
      
    \end{proof} 

    Now that we have defined the Euclidean topology, we will prove that the features of topological objects can be reduced to features in $\mathbb{R}^n$. 

    \begin{definition}[Open Neighborhood]
      An \textbf{open neighborhood} of $x \in \mathbb{R}^n$ is an open set containing $x$. 
    \end{definition}

    \begin{theorem}
      
    \end{theorem}

  \subsection{Limit Points} 

    \begin{definition}[Limit Point]
      A point $p \in \mathbb{R}^n$ is a \textbf{limit point} of $S \subset \mathbb{R}^n$ if every open neighborhood of $p$ has a nontrivial intersection with $X$. 
    \end{definition}

  \subsection{Compactness}

    The general notion of compactness\footnote{According to Terry Tao, a compact set is "small," in the sense that it is easy to deal with. While this may sound counterintuitive at first, since $[0,1]$ is considered compact while $(0,1)$, a subset of $[0,1]$, is considered noncompact. More generally, a set that is compact may be large in area and complicated, but the fact that it is compact means we can interact with it in a finite way using open sets, the building blocks of topology. That finite collection of open sets makes it possible to account for all the points in a set in a finite way. This is easily noticed, since functions defined over compact sets have more controlled behavior than those defined over noncompact sets. Similarly, classifying noncompact spaces are more difficult and less satisfying. } for topological spaces is not needed for analysis. Rather, we make use of the following theorem which allows us to focus on the compactness of subsets in Euclidean spaces $\mathbb{R}^n$. 

    \begin{theorem}[Heine-Borel Theorem] 
    A subset $S$ of Euclidean space $\mathbb{R}^n$ is compact if and only if it is closed and bounded. 
    \end{theorem}

    \begin{example}
      An open set in $\mathbb{R}^2$ is not compact. Take the open rectangle $ R = (0,1)^2 \subset \mathbb{R}^2$. There exists an infinite cover of $R$
      \[R = \bigcup_{n=0}^\infty \big(0,1\big) \times \bigg( 0, \frac{ 2^{n+1} - 1}{2^{n+1}} \bigg) \]
      that does not have a finite subcover. 
    \end{example}

    Clearly, the limit point of an open set is its boundary points. Note that a sequence of points can also have a limit point. 

    \begin{theorem}[Bolzano-Weierstrass Theorem]
      Every bounded infinite sequence in $\mathbb{R}^n$ has an accumulation point. That is, there exists a point $p \in \mathbb{R}^n$ such that every open neighborhood $U_p$ contains an infinite subset of the sequence. 
    \end{theorem}
    \begin{proof}
      The fact that the infinite sequence is bounded means that there exists some closed subset $I \in \mathbb{R}^n$ that contains all point of the sequence. By definition $I$ is compact, so by the Heine-Borel theorem, every cover of $I$ has a finite subcover. 

      Now, assume that there exists an infinite sequence in $I$ that is not convergent, i.e. has no limit point. Then, each point $x_i \in I$ would have a neighborhood $U(x_i)$ containing at most a finite number of points in the sequence. We can define $I$ such that the union of the neighborhoods is a cover of $I$. That is, 
      \[I \subset \bigcup_{i=1}^\infty U(x_i)\]
      However, since every $U(x_i)$ contains at most a finite number of points, we must have an infinite open neighborhoods to cover $I \implies$ we cannot have a finite subcover. This contradicts the fact that $I$ is compact. 
    \end{proof}

  \subsection{Connectedness}

  \subsection{Separability}

\end{document}

\section{Sequences}

  \subsection{Sequences, Basic Properties}

      \begin{definition}[Sequence]
        A function $f: \mathbb{N} \longrightarrow X$ is a \textbf{sequence}, denoted $\{a_n\} = a_1, a_2, a_3, ...$. Even though we can just let $X$ be ordered, we simplify and assume that $X = \mathbb{R}$ from now on. Let $A$ be some real number. 
        \begin{enumerate}
          \item $\{x_n\}$ is a \textbf{constant sequence} if $a_i = A$ for all $i$
          \item $\{x_n\}$ is an \textbf{ultimately constant sequence} if $a_i = A$ for all $i > N$ for some $N \in \mathbb{N}$. If $A = 0$, then $\{x_n\}$ is \textbf{finary}.
          \item $\{x_n\}$ is \textbf{bounded} if there exists $M$ such that $|x_n| < M$ for all $n \in \mathbb{N}$
        \end{enumerate}
        A \textbf{subsequence} of $\{ a_n\}$ is a sequence $\{a_{\gamma_k}\}$, where $\{\gamma_k\}$ is a strictly increasing infinite subset of $\mathbb{N}$. 
      \end{definition}

      \begin{definition}[Sequence Space]
        The set of all real-valued sequences, denoted $\mathbb{R}^\mathbb{N}$, is an infinite dimensional vector space over $\mathbb{R}$, where addition and scalar multiplication of sequences are defined component-wise. 
        \begin{align*}
            &\{x_n\} + \{y_n\} = \{x_n + y_n\}\\
            &\; c \{x_n\} = \{c x_n\}, \; c \in \mathbb{R}
        \end{align*}
        $X^\mathbb{N}$ is also equivalent to the function space of elements $f: \mathbb{N} \longrightarrow \mathbb{R}$. We can equip this space this additional operations. The product and quotient of sequences is defined component-wise. That is, given two numerical sequences $\{x_n\}$ and $\{y_n\}$ over $\mathbb{R}$, 
        \begin{enumerate}
          \item $\{x_n\} \cdot \{y_n\} = \{(x_n \cdot y_n)\}$
          \item $\{x_n\} / \{y_n\} = \Big\{ \Big(\frac{x_n}{y_n}\Big) \Big\}$ which, of course, is defined only when $y_n \neq 0$ for all $n \in \mathbb{N}$. 
        \end{enumerate}
      \end{definition}

      \begin{definition}[Limit of a Sequence]
        A number $A \in \mathbb{R}$ is called the \textbf{limit of the sequence} $\{x_n\}$, written 
        \[ \lim_{n \rightarrow \infty} x_n = A,\]
        if for every neighborhood $U_A$ there exists an index $N$ such that 
        \[x_n \in U_A \text{ for all } n > N\]
        Equivalently, $A$ is the limit of $\{x_n\}$ if for every $\epsilon>0$, there exists an index $N$ such that
        \[ |x_n - A| < \epsilon \text{ for all } n > N\]
        If $A$ is the limit of $\{x_n\}$, then we say that $\{x_n\}$ \textbf{converges} to $A$. If the limit of $\{x_n\}$ is not well defined or finite, then we say that $\{x_n\}$ is \textbf{divergent}. 

        There are two ways we can visualize the limit of a sequence. The first way is to visualize the sequence on $\mathbb{N} \times X$ and imagine the various open neighborhoods of the limit $A$ getting smaller and smaller, albeit containing an infinite number of elements of $\{x_n\}$. For example, when $X = \mathbb{R}$, we have
        \begin{center}
          \includegraphics[scale=0.25]{img/Limit_Sequence_R1.PNG}
        \end{center}
        The other way to visualize it is to just imagine the codomain space $X$ and the sequence as a collection of ordered points in $X$. We can then imagine an open neighborhood of limit $A$ getting smaller and smaller, albeit containing an infinite number of elements of $\{x_n\}$. When $X = \mathbb{R}^2$, we have
        \begin{center}
            \includegraphics[scale=0.25]{img/Limit_Sequence_R2.PNG}
        \end{center}
      \end{definition}

      \begin{theorem}[Properties of Limits]
        Given that $\{x_n\}, \{y_n\}$ are numerical sequences with $y_n \neq 0$ for all $n$, and let 
        \[\lim_{n \rightarrow \infty} x_n = A, \;\;\;\;\;\; \lim_{n \rightarrow \infty} y_n = B \neq 0\]
        then, 
        \begin{align*}
            & \lim_{n\rightarrow \infty} (x_n + y_n) = A + B \\
            & \lim_{n \rightarrow \infty} (c x_n) = c A \\
            & \lim_{n \rightarrow \infty} (x_n \cdot y_n) = A \cdot B \\
            & \lim_{n \rightarrow \infty} \frac{x_n}{y_n} = \frac{A}{B}
        \end{align*}
        It immediately follows that the set of all convergent sequences in $\mathbb{R}^\mathbb{N}$ is a subspace of $\mathbb{R}^\mathbb{N}$. 
      \end{theorem}
      \begin{proof}
        Assume that 
        \[\lim_{n \rightarrow \infty} x_n = A \text{ and } \lim_{n \rightarrow \infty} y_n = B \neq 0\]
        This means that for every $\epsilon > 0$, there exists $N_1, N_2 \in \mathbb{N}$ such that
        \begin{align*}
            |x_n - A| &< \epsilon \text{ for all } n > N_1 \\
            |y_n - B| &< \epsilon \text{ for all } n > N_2
        \end{align*}
        Therefore, for a given $\epsilon$, we wish to prove that there exists a $N$ such that for all $n > N$, 
        \begin{align*}
            1. & |(x_n + y_n) - (A+B)| < \epsilon \\
            2. & |c x_n - cA| < \epsilon \\
            3. & |(x_n y_n) - (AB)| < \epsilon \\
            4. & \bigg| \frac{x_n}{y_n} - \frac{A}{B} \bigg| < \epsilon
        \end{align*}
        \begin{enumerate}
          \item By the triangle inequality, we can see that
          \[|(x_n + y_n) - (A+B)| = |x_n - A| + |y_n - B| \]
          Since we can choose the error between $x_n$ and $A$ for $n > N_1$, and $y_n$ and $B$ for $n>N_2$ as small as we want, we set it to $\epsilon/2$. Then, we have
          \[|(x_n + y_n) - (A+B)| = |x_n - A| + |y_n - B| < \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon\]
          for all $n> N = \max\{N_1, N_2\}$. Therefore, for a given $\epsilon$, there exists an $N$ such that 
          \[|(x_n + y_n) - (A+B)| < \epsilon \text{ for all } n > N\]
          \item This proof is easy. For a given $\epsilon$, we choose the error to be $\frac{\epsilon}{c}$.
          \[|x_n - A| < \frac{\epsilon}{c} \text{ for all } n >N_1\]
          Then, there exists natural number $N_1$ such that
          \[|c x_n - c A| < c |x_n - A| =  c \frac{\epsilon}{c} = \epsilon \text{ for all } n > N_1\]
          \item We first observe that since the limit of $\{y_n\}$ exists, it must be bounded by a value, say $B$. That is, 
          \[|y_n| < Y \text{ for all } n \in \mathbb{N}\]
          Then, we see that
          \begin{align*}
              |x_n y_n - AB| & = |(x_n y_n - Ay_n) + (Ay_n - AB)| \\
              & < |x_n y_n - A y_n| + |A y_n - AB| \\
              & = |y_n| |x_n - A| + |A| |y_n - B|
          \end{align*}
          Suppose $\epsilon > 0$ is given. Then, we can set the error bounds freely; there exists $N_1, N_2 \in \mathbb{N}$ such that 
          \begin{align*}
              |x_n - A| < \frac{\epsilon}{2Y} \text{ for all } n > N_1 \\
              |y_n - B| < \frac{\epsilon}{2|A|} \text{ for all } n > N_2
          \end{align*}
          Then, we can see that 
          \[|x_n y_n - AB| \leq |y_n||x_n -A| + |A| |y_n - B| < Y \cdot \frac{\epsilon}{2Y} + |A| \frac{\epsilon}{2|A|} = \epsilon\]
          for all $n> N = \max\{N_1, N_2\}$.
          \item We use the estimate
          \[\bigg| \frac{A}{B} - \frac{x_n}{y_n} \bigg| = \frac{|x_n| |y_n - B| + |y_n||x_n - A|}{y_n^2} \cdot \frac{1}{1 - \delta(y_n)}, \;\;\; \delta(y_n) = \frac{|y_n - B|}{|y_n|}\]
          For a given $\epsilon > 0$, we find natural numbers $N_1, N_2$ such that 
          \begin{align*}
              |x_n - A| & < \min\Big\{ 1, \frac{\epsilon|B|}{8} \Big\} \text{ for all } n > N_1 \\
              |y_n - B| & < \min \Big\{ \frac{|B|}{4}, \frac{\epsilon B^2}{16(|A| + 1)} \Big\} \text{ for all } n > N_2 
          \end{align*}
          From this we can deduce that 
          \[|x_n| = |x_n - A + A| < |x_n - A| + |A| < |A| + 1\]
          and
          \begin{align*}
              |B| & = |y_n + B - y_n| < |y_n| + |B - y_n| \\
              \implies & |y_n| > |B| - |y_n - B| > |B| - \frac{|B|}{4} > \frac{|B|}{2} \\
              \implies & \frac{1}{|y_n|} < \frac{2}{|B|} \\
              \implies & 0 < \delta(y_n) = \frac{|y_n - B|}{|y_n|} < \frac{|B|/4}{|B|/2} = \frac{1}{2} \\
              \implies & 1 - \delta(y_n) > \frac{1}{2}\\
              \implies & 0 < \frac{1}{1 - \delta(y_n)} < 2
          \end{align*}
          So, we can substitute 
          \begin{align*}
              |x_n| \cdot \frac{1}{y_n^2} \cdot |y_n - B| & < (|A| + 1) \cdot \frac{4}{B^2} \cdot \frac{\epsilon \cdot B^2}{16(|A|+1)} = \frac{\epsilon}{4} \\
              \bigg|\frac{1}{y_n} \bigg| \cdot |x_n - A| & < \frac{2}{|B|} \cdot \frac{\epsilon |B|}{8} = \frac{\epsilon}{4} 
          \end{align*}
          into the final equation to get
          \[\bigg| \frac{A}{B} - \frac{x_n}{y_n}\bigg| < \epsilon \text{ for all } n > N= \max\{N_1, N_2\}\]
        \end{enumerate}
      \end{proof}

      Interestingly, we can interpret the limit as a mapping itself
      \[\lim: \mathbb{R}^\mathbb{N}_C \longrightarrow \mathbb{R}\]
      The properties of the limit imply that 
      \begin{enumerate}
        \item $\lim$ is a linear mapping between vector spaces, an element of Hom$(\mathbb{R}^\mathbb{N}_C, \mathbb{R})$
        \item $\lim$ is a multiplicative group homomorphism from $\mathbb{R}^\mathbb{N}_C$ to $\mathbb{R}$
      \end{enumerate}

      \begin{theorem}
        Given convergent sequences $\{x_n\}$ and $\{y_n\}$, if 
        \[ \lim_{n \rightarrow \infty} x_n < \lim_{n \rightarrow \infty} y_n\]
        then there exists an index $N \in \mathbb{N}$ such that $x_n < y_n$ for all $n > N$. 
      \end{theorem}

      \begin{theorem}[Squeeze Theorem for Sequences]
        Given sequences $\{x_n\}, \{y_n\}, \{z_n\}$ such that 
        \[x_n \leq y_n \leq z_n\]
        for all $n > N$, if $\{x_n\}$ and $\{z_n\}$ both converge to the same limit, then the sequence $\{y_n\}$ also converges to that limit. That is, 
        \[\lim_{n \rightarrow \infty} x_n = \lim_{n \rightarrow \infty} z_n = A \implies \lim_{n \rightarrow \infty} y_n = A\]
        This is quite easy to visualize. For convenience, we use the extension of the sequences $\{x_n\}, \{y_n\}, \{z_n\}$ onto the real numbers. 
        \begin{center}
            \includegraphics[scale=0.25]{img/Sequence_Squeeze_Theorem.PNG}
        \end{center}
      \end{theorem}

    \subsubsection{Divergent Sequences}

      Note that while a convergent sequence can be visualized quite easily by the Cauchy convergence criterion, there are many way in which a sequence can be divergent. 
      \begin{enumerate}
        \item Increasing/decreasing indefinitely
        \item Oscillating between two constant values
        \item Oscillating between a value tending to $+\infty$ and a value tending to $-\infty$
        \item Many other classes of divergence
      \end{enumerate}

      \begin{definition}[Sequence Tending to Infinity]
        The sequence $\{x_n\}$ \textbf{tends to positive infinity} if for each number $c$ there exists $N \in \mathbb{N}$ such that $x_n > c$ for all $n > N$. It is denoted 
        \[x_n \rightarrow + \infty \text{ or } \lim_{n \rightarrow \infty} x_n = + \infty\]
        We define sequences that \textbf{tend to negative infinity} similarly. 
        \begin{center}
            \includegraphics[scale=0.25]{img/Positive_Negative_Infinity.PNG}
        \end{center}
        And $\{x_n\}$ \textbf{tends to infinity} if for each $c$ there exists $N \in \mathbb{N}$ such that $|x_n| > c$ for all $n > N$, which is written 
        \[x_n \rightarrow \infty\]
        \begin{center}
            \includegraphics[scale=0.25]{img/Tending_to_Infinity.PNG}
        \end{center}
      \end{definition}

      Note that 
      \[x_n \rightarrow +\infty \text{ or } x_n \rightarrow -\infty \implies x_n \rightarrow \infty\]
      but the converse is not necessarily true. The simple example is the sequence $x_n = (-1)^n n$. Also, it is important to know that a sequence may be unbounded and yet not tend to $+\infty$, $-\infty$, or $\infty$. 

      \begin{example}[Unbounded Sequence that Doesn't tend to $\infty$]
        The sequence $x_n = n^{(-1)^n}$ is divergent yet does not tend to positive infinity, negative infinity, nor infinity. 
        \begin{center}
            \includegraphics[scale=0.25]{img/Neither_Divergent_Sequence.PNG}
        \end{center}
      \end{example}

    \subsubsection{Monotonic Sequences}

      \begin{definition}[Monotonic Sequences]
        A sequence $\{x_n\}$ is \textbf{increasing} if $x_{n+1} > x_n$ for all $n$. Similarly, it is \textbf{nondecreasing} if $x_{n+1} \geq x_n$, \textbf{decreasing} if $x_{n+1} < x_n$, and \textbf{nonincreasing} if $x_{n+1} \leq x_n$. Sequences of these types are called \textbf{monotonic}. 
      \end{definition}


      \begin{lemma}[Convergence Criterion for Monotonic Sequences]
        In order for a nondecreasing (nonincreasing) sequence to be convergent, it is necessary and sufficient that it is bounded above (or below). 
      \end{lemma}

      \begin{theorem}[Bolzano-Weierstrass Theorem]
      Every bounded sequence in $\mathbb{R}^n$ contains a convergent subsequence. 
      \end{theorem} 
      \begin{proof}
      It suffices to prove that there exists a monotonic sequence within a bounded sequence $\{x_n\}$. 
      \end{proof}

      \begin{corollary}
      From each sequence of real numbers there exists either a convergent subsequence or a subsequence tending to infinity. 
      \end{corollary}

      This allows us to measure the convergence or divergence of subsequences inside a more complicated sequence. We further define additional tools to do this. 

      \begin{example}
        We claim that 
        \[\lim_{n\rightarrow \infty} \frac{n}{q^n} = 0 \text{ if } q>1\]
      \end{example}
      \begin{proof}
      Since $x_n = \frac{n}{q^n} \implies x_{n+1} = \frac{n+1}{nq} x_n$ for $n \in \mathbb{N}$. Since 
      \[\lim_{n\rightarrow \infty} \frac{n+1}{nq} = \lim_{n \rightarrow \infty} \bigg(1 + \frac{1}{n}\bigg) \frac{1}{q} = \lim_{n\rightarrow \infty} \bigg( 1 + \frac{1}{n} \bigg) \cdot \lim_{n\rightarrow \infty} \frac{1}{q} = 1 \cdot \frac{1}{q} = \frac{1}{q} < 1\]
      there exists an index $N$ such that $\frac{n+1}{nq} < 1$ for $n>N$. Thus, we have 
      \[x_n > x_{n+1} = x_n \cdot \frac{n+1}{nq} \text{ for } n > N\]
      which means that the sequence will be monotonically decreasing from index $N$ on. The terms of the sequence
      \[x_{N+1} > x_{N+2} > x_{N+3} > \ldots\]
      are positive (bounded below) and are monotonically decreasing, so it must have a limit. 

      Finding the actual limit is easy. Let $x = \lim_{n \rightarrow \infty} x_n$. It follows from the relation $x_{n+1} = \frac{n+1}{nq} x_n$ that
      \[x = \lim_{n\rightarrow \infty} \big(x_{n+1}\big) = \lim_{n \rightarrow \infty} \bigg(\frac{n+1}{nq} x_n \bigg) = \lim_{n \rightarrow \infty} \frac{n+1}{nq} \cdot \lim_{n \rightarrow \infty} x_n = \frac{1}{q} x\]
      which implies that $\big( 1 - \frac{1}{q}\big) = 0 \implies x = 0$.
      \end{proof}

      \begin{example}
        We claim that
        \[\lim_{n\rightarrow \infty} \sqrt[n]{n} = 1\]
      \end{example}

    \subsubsection{The Number e}

    \subsubsection{Partial, Inferior, Superior Limits}

      \begin{definition}[Partial Limits]
        The \textbf{partial limit} of a sequence $\{x_n\}$ is the limit of any of its subsequence.  
        \begin{center}
            \includegraphics[scale=0.26]{img/Partial_Limit.PNG}
        \end{center}
        Two (out of the many) partial limits of the sequence above is $+\infty$ and $0$. 
      \end{definition}

      \begin{definition}[Inferior, Superior Limits]
        The \textbf{inferior limit} and \textbf{superior limit} of a sequence $\{x_k\}$ are defined as follows, and they can be shown to be the smallest and largest partial limits of the sequence. That is, 
        \begin{align*}
            \varliminf_{k \rightarrow \infty} x_k & \equiv \lim_{n \rightarrow \infty} \inf_{k \geq n}{x_k} = \min\{ \lim_{r \rightarrow \infty} y_r\}\\
            \varlimsup_{k \rightarrow \infty} x_k & \equiv \lim_{n \rightarrow \infty} \sup_{k \geq n}{x_k} = \max\{ \lim_{r \rightarrow \infty} y_r\}
        \end{align*}
        where $\{y_r\}$ is any subsequence of $\{x_n\}$. Despite the definition, it isn't too difficult to visualize this. For example, take a look at the superior and inferior limits of the divergent sequence below.
        \begin{center}
            \includegraphics[scale=0.5]{img/Lim_sup_example.png}
        \end{center}
        In order to find the superior limit, we first look the whole sequence in $\mathbb{N}$ and find the supremum. We now "decrease" our domain from $\mathbb{N}$ to $\{2, 3, \ldots\}$, then $\{3, 4, \ldots\}$, then $\{4, 5, \ldots\}$ and so on, continuing to label the supremum of the sequence. The limit of this sequence of supremums is the superior limit. Informally, the superior limit tells us what the supremum of the "end terms" of $\{x_n\}$ will be, and similarly for the inferior limit. 

        The second property of superior and inferior limits is that they represent the greatest and least possible partial limit of a sequence. For example, the six red lines marked in the middle (along with infinitely many others) are viable partial limits because one can choose a subsequence such that all of its points after a certain $n$ lie in some $\epsilon$-neighborhood of the limit. 
        \begin{center}
            \includegraphics[scale=0.12]{img/Sup_Inf_Limits_as_Limit_Bounds.jpeg}
        \end{center}
        Therefore, the superior and inferior limits represent some sort of "bound" on the sequence in the long run. That is, on the long run, the terms of the sequence $\{x_n\}$ cannot be greater than its superior limit and cannot be less than its inferior limit. With this interpretation, the following theorem should be clear. 
      \end{definition}

      \begin{theorem}
      A sequence has a limit or tends to $\pm \infty$ if and only if its inferior and superior limits are the same. 
      \end{theorem}

      \begin{corollary}
      A sequence converges if and only if every subsequence of it converges. 
      \end{corollary}

  \subsection{Real Series}

      Defining limits and convergence for series can be painstaking... unless we construct series as sequences. 

      \begin{definition}[Series over $\mathbb{R}$]
        Given a sequence of real numbers $\{a_n\}$, the \textbf{series} of $\{a_n\}$ is defined
        \[s = \sum_{k=1}^\infty a_k\]
        The series can be interpreted as the sequence of partial sums $\{s_n\}$, where
        \[s_n = \sum_{k=1}^n a_k\]
        is the \textbf{$n$th partial term of the series}. Therefore, we can interpret the sum of the series $s$ as the limit of $\{s_n\}$. 
        \[\lim_{n \rightarrow \infty} s_n = s\]
        If the sequence $\{s_n\}$ converges, the series is \textbf{convergent} and \textbf{divergent} otherwise. 
      \end{definition}

      It can be visualized as the Riemann sums of the smooth extension of the function representing $\{a_n\}$, as shown below, where the $a_n$'s represent the height of each bar and the $s_n$'s represent the sums of the area of each bar. 
      \begin{center}
          \includegraphics[scale=0.3]{img/Series_as_Riemann_Sums.PNG}
      \end{center}
      Note that we are really just defining a series as an ordered pair $(\{a_n\}, \{s_n\})$ of sequences connected by the relation 
      \[s_n = \sum_{k=1}^n a_k \text{ for all } n \in \mathbb{N}\]

      Since the convergence of a series is equivalent to convergence of its sequence of partial sums, applying the Cauchy convergence criterion to the sequence $\{s_n\}$ leads to the following theorem. 

      \begin{theorem}[Cauchy Convergence Criterion for Series]
      The series $a_1 + \ldots + a_n + \ldots$ converges if and only if for every $\epsilon > 0$ there exists $N \in \mathbb{N}$ such that for all $m \geq n > N$, 
      \[|a_n + \ldots + a_m| < \epsilon\]
      \end{theorem}

      \begin{definition}[Cauchy Product of Real Series]

      \end{definition}

      \begin{corollary}[nth Term Test]
      A necessary (but not sufficient) condition for convergence of the series $a_1 + \ldots a_n + \ldots$ is that the terms tend to $0$ as $n \rightarrow \infty$. That is, it is necessary that
      \[\lim_{n\rightarrow \infty} a_n = 0\]
      \end{corollary}
      \begin{proof}
      It suffices to set $m = n$ in the Cauchy convergence criterion. This would mean that for every $\epsilon > 0$ there exists a $N \in \mathbb{N}$ such that 
      \[|a_n| = |a_n - 0| < \epsilon \text{ for all } n > N\]
      which, by definition, means that $\{a_n\}$ converges to $0$. 
      \end{proof}

      \begin{example}[Geometric Series]
        The series 
        \[1 + q + q^2 + \ldots + q^n + \ldots\]
        is called the \textbf{geometric series}. 

        Since $|q^n| = |q|^n$, we have $|q^n| \geq 1$ when $|q| \geq 1$. So, if $|q| \geq 1$, the terms $q^n$ does not converge to $0$ and the Cauchy convergence criterion is not met. 

        Now, suppose $|q|<1$. Then, 
        \[s_n = 1 + q + \ldots + q^{n-1} = \frac{1 - q^n}{1-q}\]
        which implies that
        \[\lim_{n\rightarrow \infty} s_n = \frac{1}{1-q}\]
        since $\lim_{n\rightarrow \infty} q^n = 0$ if $|q|<1$. This, the series converges to if and only if $|q|<1$, and its sum is $\frac{1}{1-q}$. 
      \end{example}

      \begin{example}[Harmonic Series]
        The series 
        \[1 + \frac{1}{2} + \frac{1}{3} + \ldots + \frac{1}{n} + \ldots\]
        is called the \textbf{harmonic series}, since each term from the second on is the harmonic mean of the two terms on either side of it. Clearly, 
        \[\lim_{n \rightarrow \infty} a_n = \lim_{n \rightarrow \infty} \frac{1}{n} = 0\]
        but the sequence of partial sums $s_n$ diverges, and thus the harmonic series diverges. 
      \end{example}

    \subsubsection{Convergence Tests}

      \begin{definition}[Absolute Convergence]
        The series $\sum_{n=1}^\infty a_n$ is \textbf{absolutely convergent} if the series 
        \[\sum_{n=1}^\infty |a_n|\]
        converges. Clearly, every absolutely convergent series because 
        \[\bigg|\sum_{n=1}^\infty a_n \bigg| \leq \sum_{n=1}^\infty |a_n|\]
      \end{definition}

      \begin{theorem}[Direct Comparison Test]
      Let $\sum_{n=1}^\infty a_n$ and $\sum_{n=1}^\infty b_n$ be 2 series with nonnegative terms. If there exists an index $N \in \mathbb{N}$ such that $a_n \leq b_n$ for all $n >N$, then 
      \begin{align*}
          \sum_{n=1}^\infty b_n \text{ convergent } & \implies \sum_{n=1}^\infty a_n \text{ convergent } \\
          \sum_{n=1}^\infty a_n \text{ divergent } & \implies \sum_{n=1}^\infty b_n \text{ divergent }
      \end{align*}
      \end{theorem}

      \begin{theorem}[Limit Comparison Test]
      Suppose the limit 
      \[\lim_{n\rightarrow \infty} \bigg| \frac{a_{n+1}}{a_n} \bigg| = \alpha\]
      exists for the series $\sum_{n=1}^\infty a_n$. Then, 
      \begin{align*}
          \alpha < 1 & \implies \sum_{n=1}^\infty a_n \text{ converges absolutely} \\
          \alpha > 1 & \implies \sum_{n=1}^\infty a_n \text{ diverges} \\
          \alpha = 1 & \implies \text{ Inconclusive}
      \end{align*}
      \end{theorem}

      \begin{theorem}[Root Test]
      Let $\sum_{n=1}^\infty$ be a given series and 
      \[\alpha = \limsup_{n\rightarrow \infty} \sqrt[n]{|a_n|}\]
      Then, the following are true
      \begin{align*}
          \alpha < 1 & \implies \sum_{n=1}^\infty \text{ converges absolutely} \\
          \alpha > 1 & \implies \sum_{n=1}^\infty \text{ diverges} \\
          \alpha = 1 & \implies \text{ Inconclusive} 
      \end{align*}
      \end{theorem}

      \begin{theorem}[Weierstrass M-test for Absolute Convergence]
      Let $\sum_{n=1}^\infty$ and $\sum_{n=1}^\infty b_n$ be series. Suppose there exists an index $N \in \mathbb{N}$ such that $|a_n| \leq b_n$ for all $n>N$. Then, 
      \[\sum_{n=1}^\infty b_n \text{ converges } \implies \sum_{n=1}^\infty a_n \text{ converges absolutely}\]
      \end{theorem}

      The following theorem, while obvious, has interesting consequences. 

      \begin{theorem}[Cauchy]
      If $a_1 \geq a_2 \geq \ldots \geq 0$, the series $\sum_{n=1}^\infty a_n$ converges if and only if the series 
      \[\sum_{k=0}^\infty 2^k a_{2^k} = a_1 + 2 a_2 + 4a_4 + 8a_8 + \ldots \]
      converges. 
      \end{theorem}
      \begin{proof}
      Letting $A_k = a_1 + a_2 + \ldots + a_k$ and $S_n = a_1 + 2a_2 + \ldots + 2^n a_{2^n}$, it is clear that by adding up the inequalities
      \begin{align*}
          & a_2 \leq a_2 \leq a_1 \\
          & 2a_4 \leq a_3 + a_4 \leq 2a_2 \\
          & 4a_8 \leq a_5 + a_6 + a_7 + a_8 \leq 4a_4 \\
          & \ldots \\
          & 2^n a_{2^{n+1}} \leq a_{2^n + 1} + \ldots + a_{2^{n+1}} \leq 2^n a_{2^n}, 
      \end{align*}
      we get
      \[\frac{1}{2}(S_{n+1} - a_1) \leq A_{2^{n+1}} - a_1 \leq S_n\]
      Since the sequences $\{A_k\}$ and $\{S_k\}$ are nondecreasing, and hence from the inequalities we can conclude that they are either both bounded above (which means that they are both convergent since it is a bounded, nondecreasing series) or both unbounded above (which means that they are both divergent since they are nondecreasing and unbounded). 
      \end{proof}

      \begin{corollary}[p-series Test]
      The series 
      \[\sum_{n=1}^\infty \frac{1}{n^p}\]
      converges for $p>1$ and diverges for $p \leq 1$. 
      \end{corollary}
      \begin{proof}
      Suppose $p\geq 0$. By the previous theorem, the series converges or diverges simultaneously with the series 
      \[\sum_{k=0}^\infty 2^k \frac{1}{(2^k)^p} = \sum_{k=0}^\infty (2^{1-p})^k\]
      which is really just a geometric series. A necessary and sufficient condition for the convergence of this series is that $2^{1-p} < 1$, that is, $p>1$. 

      Now suppose $p \leq 0$. The series is then clearly divergent since all of the terms are larger than $1$. 
      \end{proof}

    \subsubsection{Representation of Euler's Number as a Series}

\section{Continuous Functions}

    Even though we can generalize the concept of limits to functions mapping between arbitrary topological spaces $f: X \longrightarrow Y$, we define this for functions $f: E \subset \mathbb{R} \longrightarrow \mathbb{R}$. 

    \begin{definition}[Functions]
      Given a real-valued function $f: E \longrightarrow \mathbb{R}$ defined on domain $E \subset \mathbb{R}$,
      \begin{enumerate}
        \item $f$ is a \textbf{constant function} if $f(x) = A$ for all $x \in E$
        \item $f$ is called \textbf{ultimately constant} as $x \rightarrow a$ if it is constant in some deleted neighborhood $\mathring{U} (a)$, where $a$ is a limit point of $E$.
        \begin{center}
            \includegraphics[scale=0.25]{img/Ultimately_Constant_Function.PNG}
        \end{center}
        \item $f$ is \textbf{bounded}, \textbf{bounded above}, or \textbf{bounded below} respectively if there is a number $C \in \mathbb{R}$ such that $|f(x)|<C$, $f(x)<C$, or $C<f(x)$ for all $x \in E$.
        \begin{center}
            \includegraphics[scale=0.25]{img/Bounded_Three.PNG}
        \end{center}
        \item $f$ is \textbf{ultimately bounded}, \textbf{ultimately bounded above}, or \textbf{ultimately bounded below} as $x \rightarrow a$ if it is bounded, bounded above, or bounded below in some deleted neighborhood $\mathring{U}_E (a)$. 
        \begin{center}
            \includegraphics[scale=0.25]{img/Ultimately_Bounded_Three.PNG}
        \end{center}
      \end{enumerate}
    \end{definition}

    \begin{example}
      The function 
      \[f(x) = \sin{\frac{1}{x}} + x \cos{\frac{1}{x}}\]
      for $x \neq 0$ is not bounded on the domain of definition, but it is ultimately bounded as $x \rightarrow 0$. 

    \end{example}

    \begin{definition}[$\epsilon-\delta$ Definition of a Limit]
      The function $f: E \subset \mathbb{R} \longrightarrow \mathbb{R}$ tends to $A \in \mathbb{R}$ as $x$ tends to $a$, or that 
      \[\lim_{x \rightarrow a} f(x) = A\]
      if for every $\epsilon > 0$ there exists $\delta > 0$ such that 
      \[0<|x - a|<\delta \implies |f(x) - A| < \epsilon\]
      Note that we set the $0<|x-a|$ to ensure that $x \neq a$. 

      Therefore, in other words, for any arbitrarily small $\epsilon > 0$, if we can find a $\delta > 0$ such that the image of the \textbf{deleted $\delta$-neighborhood of $a$} (defined to be $\mathring{U}_\delta (a) \equiv U_\delta (a) \setminus a$) is completely within the $\epsilon$-neighborhood $U_\epsilon (A)$, then 
      \[\lim_{x \rightarrow a} f(x) = A\]
      Visually, 
      \begin{center}
          \includegraphics[scale=0.25]{img/Limit_of_Function_on_R.PNG}
      \end{center}
      In higher dimensional spaces, we have 
      \begin{center}
          \includegraphics[scale=0.25]{img/Limit_of_Function_on_Euclidean_Space.PNG}
      \end{center}
    \end{definition}

    \begin{example}[Limit of the Signum Function]
      The function sgn$: \mathbb{R} \longrightarrow \mathbb{R}$ defined
      \[\text{sgn}\,x = \begin{cases}
      1, & x > 0 \\
      0, & x = 0 \\
      -1, & x < 0
      \end{cases}\]
      has no limit as $x \rightarrow 0$. 

      First, it is ludicrous that the limit would be any number that is not $\{-1, 0, 1\}$. If we assume that $A \not\in \{-1,0,1\}$, then we can choose any arbitrarily small $\epsilon$-neighborhood of $A$ that does not include the three numbers. Clearly, there doesn't exist any $\delta>0$ such that the deleted $\delta$-neighborhood of $0$ maps to a set completely contained in the $\epsilon$-neighborhood of $A$. That is,
      \[\text{sgn}\big( \mathring{U}_\delta (0)\big) = \{-1,1\} \not\subset U_\epsilon (A)\]
      \begin{center}
          \includegraphics[scale=0.3]{img/Limit_of_Sign_Function_Ludicrous.PNG}
      \end{center}
      It doesn't even intersect the $\epsilon$-neighborhood at all. 
      \begin{enumerate}
        \item If $A = 1$, we can construct a $\epsilon$-neighborhood $V_A$ for $\epsilon = \frac{1}{2}$. Clearly, there exists no open neighborhood $U_0$ of $0$ that is entirely mapped to $V$, since $U_0$ contains both negative numbers and $0$ and hence must be mapped to $0, -1$. 
        \item Similarly, given the $(\epsilon=\frac{1}{2})$-neighborhood of $A = -1$, there exists no open neighborhood $U_0$ of $0$ that is entirely mapped to it, since $U_0$ contains both positive numbers and $0$ and hence must be mapped to $0, 1$. 
        \item Finally, given the $(\epsilon=\frac{1}{2})$-neighborhood of $A = 0$, there exists no open neighborhood $U_0$ of $0$ that is entirely mapped to it, since $U_0$ contains both positive and negative numbers and hence must be mapped to $\pm1$. 
      \end{enumerate}
      \begin{center}
          \includegraphics[scale=0.25]{img/Limit_of_Sign_Function_1_0_-1.PNG}
      \end{center}
      Therefore, the limit does not exist. 
    \end{example}

    \begin{example}[Limit of Absolute Value of Signum Function]
      We will show that 
      \[\lim_{x \rightarrow 0} |\text{sgn}\,x| = 1\]
      We construct a $\epsilon$-neighborhood $U_\epsilon (1)$ around $1$. Given this neighborhood, we can imagine choosing the deleted $\delta$-neighborhood $\mathring{U}_\delta (0)$ around $0$. Since every element in $\mathring{U}_\delta (0)$ maps to $1$, it is clearly in $U_\epsilon$. 
      \begin{center}
          \includegraphics[scale=0.25]{img/Absolute_Value_of_Signum.PNG}
      \end{center}
      In fact, for arbitrarily small $\epsilon > 0$, we can choose \textbf{any} $\delta>0$ since everything in $\mathbb{R} \setminus 0$ maps to $1$. We can visualize this in $\mathbb{R}^2$ as
      \begin{center}
          \includegraphics[scale=0.25]{img/Absolute_Value_of_Signum_2.PNG}
      \end{center}
    \end{example}

    The following lemma nicely interweaves the concepts of limits of sequences and limits of functions. It can be nice for visualization to define the limit of a function using Cauchy sequences rather than the usual $\epsilon-\delta$ definition. 

    \begin{lemma}[Cauchy Sequence Criterion of a Limit]
      The relation 
      \[\lim_{x \rightarrow a} f(x) = A\]
      holds if and only if for every sequence $\{x_i\}$ of points $x_n \in E \setminus a$ converging to $a$, the sequence $\{f(x_n)\}$ converges to $A$. 
      \begin{center}
          \includegraphics[scale=0.25]{img/Cauchy_Criterion_of_Limit_of_Function.PNG}
      \end{center}
      Note that we choose the points $x_n$ to be in the "deleted" neighborhood $E\setminus a$ (neighborhood $E$ with point $a$ removed) to force us to choose a sequence that is not
      \[a, a, a, a, a, a, a, \ldots\]
      That is, it forces us to choose different points for the sequence. 
    \end{lemma}

    \begin{theorem}[Properties of Limits]
    Given two numerical valued functions $f, g: E \subset \mathbb{R} \longrightarrow \mathbb{R}$ with a common domain where $g(x) \neq 0$ for all $x \in E$, let 
    \[\lim_{x \rightarrow a} f(x) = A, \;\;\;\;\; \lim_{x \rightarrow a} g(x) = B\]
    then, 
    \begin{align*}
        & \lim_{x \rightarrow a} (f+g)(x) = A + B \\
        & \lim_{x \rightarrow a} (cf)(x) = cA \\
        & \lim_{x \rightarrow a} (f \cdot g)(x) = A \cdot B \\
        & \lim_{x \rightarrow a} \bigg(\frac{f}{g}\bigg) (x) = \frac{A}{B}
    \end{align*}
    \end{theorem}
    \begin{proof}
    The the Cauchy sequence criterion for a limit, this theorem is an immediate consequence on the corresponding theorem on limits of sequences.
    \end{proof}

    We end this with a theorem connecting the relationship between a limit of a function as $x \rightarrow a$ and its ultimate behavior as $x \rightarrow a$. 

    \begin{theorem}
    Let $f: E \longrightarrow \mathbb{R}$ be a function. Then, 
    \begin{enumerate}
      \item $f$ is ultimately the constant $A$ as $x \rightarrow a$ implies that $\lim_{x \rightarrow a} f(x) = A$. 
      \item $\lim_{x \rightarrow a} f(x)$ implies that $f$ is ultimately bounded as $x \rightarrow a$. 
    \end{enumerate}
    \end{theorem}

  \subsection{Infinitesimal Functions}

    \begin{definition}[Infinitesimal Function]
      A function $f: E \subset \mathbb{R} \longrightarrow \mathbb{R}$ is said to be \textbf{infinitesimal} as $x \rightarrow a$ if 
      \[\lim_{x \rightarrow a} f(x) = 0\]
    \end{definition}

    \begin{lemma}[Sums, Products of Infinitesimals]
      It is clear that if $\alpha, \beta$ are infinitesimal as $x \rightarrow a$, then 
      \begin{enumerate}
        \item $\alpha + \beta$ is infinitesimal as $x \rightarrow a$
        \item $\alpha \cdot \beta$ is infinitesimal as $x \rightarrow a$
      \end{enumerate}
      Furthermore, if $\alpha$ is infinitesimal and $\beta$ is ultimately bounded as $x \rightarrow a$, then the product $\alpha \cdot \beta$ is infinitesimal as $x \rightarrow a$. 
    \end{lemma}
    \begin{proof}
    We prove all three statements. 
    \begin{enumerate}
      \item Assume that $\alpha$ and $\beta$ are infinitesimal as $x \rightarrow a$. Then, let us fix a small $\epsilon>0$. This means that for every $\frac{\epsilon}{2}$ there exists an open deleted neighborhood $\mathring{U}^\prime (a)$ such that its image $\alpha\big(\mathring{U}^\prime (a)\big)\subset U^\prime_{\epsilon/2} (0) \subset \mathbb{R}$. Additionally, for every $\frac{\epsilon}{2}$ there exists an open deleted neighborhood $\mathring{U}^{\prime\prime} (a)$ such that its image $\beta\big(\mathring{U}^{\prime\prime} (a)\big)\subset U^\prime_{\epsilon/2} (0) \subset \mathbb{R}$.
      Thus, for the deleted neighborhood 
      \[\mathring{U}(a) \subset \mathring{U}^\prime (a) \cup \mathring{U}^{\prime\prime} (a)\]
      we can see that for all $x \in \mathring{U}(a)$, 
      \[|(\alpha + \beta)(x)| = |\alpha (x) + \beta(x)| \leq |\alpha (x)| + |\beta(x)| < \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon\]
      and hence $(\alpha + \beta)\big( \mathring{U}(a)\big) \subset U_\epsilon (0)$. 
      \item This case is a special case of assertion 3. That is, every function that has a limit is ultimately bounded. 
      \item Since $\beta(x)$ is ultimately bounded, this means that there exists a constant $M$ and an open deleted neighborhood $\mathring{U}^\prime (a) \subset E$ such that for all $x \in \mathring{U}^\prime (a)$, its image is bounded: $|\beta(x)|<M$. Let us fix a small $\epsilon>0$. Then, by definition of the limit, for every $\frac{\epsilon}{M}$ there exists an open deleted neighborhood $\mathring{U}^{\prime\prime} (a)$ such that its image $\beta\big(\mathring{U}^{\prime\prime}(a)\big) \subset U_{\epsilon/M} (0) \subset \mathbb{R}$. Therefore, for the deleted neighborhood
      \[\mathring{U}(a) \subset \mathring{U}^\prime (a) \cup \mathring{U}^{\prime\prime}(a)\]
      we can see that for all $x \in \mathring{U} (a)$, 
      \[|(\alpha \cdot \beta)(x)| = |\alpha (x) \beta(x)| = |\alpha (x)| |\beta(x)| < \frac{\epsilon}{M} \cdot M = \epsilon\]
      Therefore, $(\alpha \cdot \beta)\big( \mathring{U} (a)\big) \subset U_\epsilon (0)$. 
    \end{enumerate}
    \end{proof}

    Note that in proving these properties of the limits, we have used the following fact about open deleted neighborhoods around $a$. 
    \begin{enumerate}
      \item $\mathring{U} (a)$ is not the empty set. 
      \item Given open deleted neighborhoods $\mathring{U}^\prime (a)$ and $\mathring{U}^{\prime\prime} (a)$, there exists an open deleted neighborhood in the intersections of these neighborhoods. 
      \[\mathring{U} (a) \subset \mathring{U}^\prime (a) \cup \mathring{U}^{\prime\prime} (a)\]
    \end{enumerate}
    These facts can be used to generalize the concept of limits as limits over a certain \textbf{filter base}. 

    \begin{theorem}[Representation of a Convergent Function as a Shift of its Infinitesimal]
    Given a function $f: E \subset \mathbb{R} \longrightarrow \mathbb{R}$, its limit exists and 
    \[\lim_{x \rightarrow a} f(x) = A\]
    if and only if $f$ can be represented as 
    \[f(x) = A + \alpha (x)\]
    where $\alpha$ is infinitesimal as $x \rightarrow a$. We can visualize this theorem by thinking of a function $f$ that results from a "shift" of an infinitesimal. 
    \begin{center}
        \includegraphics[scale=0.3]{img/Infinitesimal_Shift_Function.jpg}
    \end{center}
    \end{theorem}

    Finally, we reiterate some limit theorems already stated for sequences, but now corresponding to functions. Interpreting the function limit as the Cauchy sequence definition of limits renders the proofs of these theorems trivial. 

    \begin{theorem}[Behavior of Functions with Different Limits]
    If the functions $f, g: E \rightarrow \mathbb{R}$ are such that
    \[\lim_{x\rightarrow a} f(x) = A < B = \lim_{x \rightarrow a} g(x)\]
    then there exists a deleted neighborhood $U_\delta (a)$ in $E$ at each point of which $f(x) < g(x)$. 
    \end{theorem}

    \begin{theorem}[Squeeze Theorem for Limits of Functions]
    Given the functions $f, g, h: E \subset \mathbb{R} \longrightarrow \mathbb{R}$ such that
    \[f(x) \leq g(x) \leq h(x) \text{ for all } x \in E\]
    then, 
    \[\lim_{x \rightarrow a} f(x) = \lim_{x \rightarrow a} h(x) = C \implies \lim_{x \rightarrow a} g(x) = C\]
    \end{theorem}

  \subsection{Asymptotic Behavior of Functions}

    \begin{definition}[Little-O Notation]
      The function $f: E \longrightarrow \mathbb{R}$ is said to be \textbf{infinitesimal compared with the function $g: E \longrightarrow \mathbb{R}$} as $x \rightarrow a$, written (by abuse of notation) $f = o(g)$ as $x \rightarrow a$, if 
      \[\lim_{x \rightarrow a} \frac{f(x)}{g(x)} = 1\]
      or in other words, if $f/g$ is an infinitesimal function as $x \rightarrow a$. Therefore, $f = o(1)$ as $x \rightarrow a$ means that $f$ is infinitesimal as $x \rightarrow a$. 
      \begin{center}
          \includegraphics[scale=0.25]{img/Little_o_Functions.jpg}
      \end{center}
      Note that writing $f = o(g)$ is again, an abuse of notation. $f = o(g)$ is really a shorthand way of writing that $f$ is in the class of functions that is infinitesimal compared with the function $g$. 
    \end{definition}

    Intuitively, $f = o(g)$ means that the ratio between $f(x)$ and $g(x)$ will tend to infinity as $x \rightarrow a$ (this does not mean that $f$ will be infinitely greater than $g$, however!). For example, looking at the two functions $f(x) = x^2$ and $g(x) = x$, we have 
    \begin{enumerate}
      \item $x^2 = o(x)$ as $x \rightarrow 0$ (since $\frac{x^2}{x} = x$ is infinitesimal as $x \rightarrow 0$)
      \item $x = o(x^2)$ as $x \rightarrow \infty$ (since $\frac{x}{x^2} = \frac{1}{x}$ is infinitesimal as $x \rightarrow \infty$)
    \end{enumerate}
    We can visualize $g/f (x)$ tending to infinity within a neighborhood of $0$ and $f/g (x)$ tending to infinity within a neighborhood of $\infty$. 
    \begin{center}
        \includegraphics[scale=0.25]{img/Comparison_of_Quadratic_and_Linear.PNG}
    \end{center}

    \begin{definition}[Orders of Infinitesimals, Infinities]
      If $f = o(g)$ and $g$ is infinitesimal as $x \rightarrow a$, then $f$ is an \textbf{infinitesimal of higher order than $g$ as $x \rightarrow a$}. Furthermore, if $f$ and $g$ are infinite functions as $x\rightarrow a$ and $f = o(g)$ as $x \rightarrow a$, then $g$ is a \textbf{higher order infinity than $f$ as $x \rightarrow a$}. 
    \end{definition}

    \begin{definition}[Big-O Notation]
      By abuse of notation, $f = O(g)$ as $x \rightarrow a$ means that 
      \[\lim_{x \rightarrow a} \frac{f(x)}{g(x)} = \infty\]
      or in other words, $f/g$ is ultimately bounded as $x \rightarrow a$. In particular, $f = O(1)$ as $x \rightarrow a$ means that $f$ is bounded within a certain neighborhood $U(a)$ of $a$. 

      In the visual below, we can see that $f=O(g)$ as $x \rightarrow +\infty$ since the limit converges to constant $\frac{B}{A}$ which is bounded. In fact, at any other positive real number $x$, $(f/g)(x)$ is finite and is therefore bounded. However, at every neighborhood of $x = 0$, $(f/g)(x)$ is unbounded, meaning that $g \neq O(f)$ as $x \rightarrow 0$. 
      \begin{center}
          \includegraphics[scale=0.25]{img/Big_O_Functions.PNG}
      \end{center}
    \end{definition}

    \begin{definition}[Functions of Same Order]
      The functions $f$ and $g$ are of the same over as $x \rightarrow a$, written 
      \[f \asymp g \text{ as } x \rightarrow a\]
      if $f = O(g)$ and $g = O(f)$ as $x \rightarrow a$. Intuitively, this means that the ratio between $f$ and $g$ within some deleted neighborhood of $a$ is finite. 

      In the visual below, we can see that as long as $k \neq a$, $f = O(g)$ as $x \rightarrow k$ and as $x \rightarrow \infty$. In other words, the function $f/g$ becomes ultimately bounded at every other point other than $a$, and $f/g$ is unbounded within every neighborhood of $a$. When looking at $g/f$, we can see that this function is bounded for all $x \in \mathbb{R}$ and therefore $g = O(f)$ as $x \rightarrow k$ for all $k$. 
      \begin{center}
          \includegraphics[scale=0.25]{img/Functions_of_Same_Order.PNG}
      \end{center}
      Therefore, we can see that as long as $k \neq a$, $f \asymp g$ as $x \rightarrow k$.

      Note that the condition that $f$ and $g$ be of the same order as $x \rightarrow a$ is (by definition of ultimately bounded functions) equivalent to the condition that there exist $c_1, c_2 > 0$ and an open neighborhood $U (a)$ such that the relations
      \[c_1 |g(x)| \leq |f(x)| \leq c_2 |g(x)|\]
      is true for $x \in U(a)$. 
    \end{definition}

    \begin{definition}[Asymptotic Equivalence of Functions]
      For functions $f$ and $g$, if 
      \[\lim_{x \rightarrow a} \frac{f(x)}{g(x)} = 1\]
      we say that \textbf{$f$ behaves asymptotically like $g$ as $x \rightarrow a$}, or that \textbf{$f$ is equivalent to $g$ as $x \rightarrow a$}, written 
      \[f \sim g \text{ as } x \rightarrow a\]
      Moreover, $\sim$ is an equivalence relation, which means that
      \begin{enumerate}
        \item $f \sim f$ as $x \rightarrow a$
        \item $f \sim g$ as $x \rightarrow a \implies$ $g \sim f$ as $x \rightarrow a$
        \item $f \sim g$ and $g \sim h$ as $x \rightarrow a \implies f \sim h$ as $x \rightarrow a$
      \end{enumerate}
    \end{definition}

    We list a few examples in order to develop some sort of visual intuition for when two functions are asymptotically equivalent. 
    \begin{enumerate}
      \item If $f(a) = g(a) \neq 0$, then $f \sim g$ trivially since the ratio of $f$ and $g$ converges to $1$ within a neighborhood of $a$. 
      \begin{center}
          \includegraphics[scale=0.3]{img/trivial_case_equal_value.jpg}
      \end{center}
      \item When $f(a) = g(a) = 0$, it may be $f$ may be equivalent to $g$ or one function may be infinitesimally smaller than the other. 
      \begin{enumerate}
        \item When $f(x) = \sin{x}$ and $g(x) = x$, then $f \sim g$ since we see that 
        \[\lim_{x \rightarrow 0} \frac{sin{x}}{x} = 1\]
        and so $\sin{x} \sim x$ as $x \rightarrow 1$
        \begin{center}
            \includegraphics[scale=0.25]{img/x_vs_sin_x.PNG}
        \end{center}
        \item When $f(x) = x^2$ and $g(x) = x^4$, then  \[\lim_{x \rightarrow 0} \frac{x^4}{x^2} = 0\]
        and so $x^4 \not\sim x^2$. In fact, $x^4 = o(x^2)$. Therefore, since $x^4$ decreases to $0$ infinitely faster than $x^2$, they are not equivalent. 
        \begin{center}
            \includegraphics[scale=0.25]{img/x_fourth_vs_x_squared.jpg}
        \end{center}
        \item When $f(x) = x^2$ and $g(x) = x^3$, then  \[\lim_{x \rightarrow 0} \frac{x^3}{x^2} = 0\]
        and so $x^3 \not\sim x^2$. In fact, $x^3 = o(x^2)$. Therefore, since $x^4$ decreases to $0$ infinitely faster than $x^2$, they are not equivalent.  
        \begin{center}
            \includegraphics[scale=0.25]{img/x_squared_vs_x_cubed.jpg}
        \end{center}
        \item When $f(x) = x^2$ and $g(x) = 0.5x^2$, then 
        \[\lim_{x \rightarrow 0} \frac{0.5x^2}{x^2} = \frac{1}{2}\]
        and so $0.5x^2 \not\sim x^2$. Therefore, since $0.5x^2$ is always as twice as small as $x^2$, they are not equivalent. 
        \begin{center}
            \includegraphics[scale=0.25]{img/x_squared_vs_half_x_squared.PNG}
        \end{center}
      \end{enumerate}
      \item When analyzing the behavior of functions as $x \rightarrow \infty$, we can picture the two graphs of $f$ and $g$ on the plane and "zoom out" to see if the ratio of the values converge to $1$. This would mean that as $x \rightarrow \infty$, we should see the graphs overlapping more and more. For example, taking $f(x) = x^2$ and $g(x) = x^2 + 10x + 100$, we can see that the discrepancy is high around a neighborhood of $x = 0$. But as $x \rightarrow +\infty$, we get
      \[\lim_{x \rightarrow + \infty} \frac{x^2 + 10x + 100}{x^2} = 1\]
      and so the graphs look like they are overlapping. 
      \begin{center}
          \includegraphics[scale=0.25]{img/Behavior_of_Quadratics_of_Same_order_as_infinity.PNG}
      \end{center}
      Notice that even though the absolute difference $|(x^2 + 10x + 100) - x^2| = |10x + 100|$ tends to infinity, this difference increases infinitesimally compared to $f$ and $g$. 
    \end{enumerate}

    From this, we can see that if $f \sim g$ as $x \rightarrow a$, then their difference 
    \[f - g = o(g) = o(f)\]
    That is, $(f-g)(x)$ is infinitesimal compared to $g$ or $f$ (doesn't matter which one we compare it to). This leads to our next section, where we formalize this concept with absolute and relative errors. 

    \subsubsection{Approximations of Functions}
    It is useful to note that since the relation $\lim_{x \rightarrow a} \gamma(x) = 1$ is equivalent to 
    \[\gamma (x) = 1 + \alpha(x), \text{ where } \lim_{x \rightarrow a} \alpha(x) = 0\]
    the relation $f \sim g$ as $x\rightarrow a$ is equivalent to saying that
    \[\frac{f(x)}{g(x)} = \gamma(x), \text{ where } \lim_{x \rightarrow a} \gamma(x) = 1\]
    which implies 
    \[f(x) = g(x) + \alpha(x) g(x) = g(x) + o(g(x)) \text{ as } x \rightarrow a\]
    or, symmetrically, 
    \[g(x) = f(x) + \alpha(x) f(x) = f(x) + o(f(x)) \text{ as } x \rightarrow a\]
    This means that $f$ can be exactly represented by another function $g$, plus another (error) function $o(g(x))$ that is infinitesimal compared to $g$. 
    \begin{center}
        \includegraphics[scale=0.28]{img/Error_Approximation_of_Function.PNG}
    \end{center}
    \textbf{Note that it is not a sufficient condition that the error function be infinitesimal!} The error function $f-g$ must be infinitesimal \textbf{compared to $g$}! This tells us that not only does the error function decrease infinitesimally, but also is infinitesimal compared to the approximation function we already have, which is in general a much stronger claim. This representation of certain types functions will provide the foundation for differential calculus when we talk about "good" approximations for a function. 


    \begin{definition}[Relative Error]
      Since $f \sim g$ as $x \rightarrow a$ means that 
      \[f(x) = g(x) + \alpha(x) g(x) = g(x) + o(g(x))\]
      we can define the \textbf{relative error} of $g$ as an approximation of $f$ to be
      \[|\alpha(x)| = \bigg| \frac{f(x) - g(x)}{g(x)} \bigg|\]
      Clearly, since $f \sim g$, the relative error must be infinitesimal as $x \rightarrow a$. 
    \end{definition}

    We use the following lemma to check whether two functions are asymptotically equivalent. 
    \begin{lemma}
      $f \sim g$ as $x \rightarrow a$ if and only if the relative error of $g$ is infinitesimal as $x \rightarrow a$. 
    \end{lemma}

    \begin{example}
      We claim that 
      \[x^2 + x = \bigg(1 + \frac{1}{x} \bigg) x^2 \sim x^2 \text{ as } x \rightarrow \infty\]
      We see that the absolute error of this approximation 
      \[|(x^2 + x) - x^2| = |x|\]
      tends to infinity, but the relative error
      \[\frac{|x|}{x^2} = \frac{1}{|x|}\]
      tends to $0$ as $x \rightarrow \infty$. 
    \end{example}

    \begin{theorem}[Prime Number Theorem]
    Let $\pi(x)$ be the number of prime numbers strictly less than $x$. Then $\pi \sim \frac{x}{\ln{x}}$ as $x\rightarrow + \infty$, or more precisely, 
    \[\pi(x) = \frac{x}{\ln{x}} + o \bigg( \frac{x}{\ln{x}}\bigg) \text{ as } x \rightarrow +\infty\]
    \end{theorem}

    \begin{example}
      It is a fact that $\lim_{x\rightarrow 0} \frac{sin{x}}{x} = 1$, so we have $\sin{x} \sim x$ as $x \rightarrow 0$. So,
      \[\sin{x} = x + o(x) \text{ as } x \rightarrow 0\]
    \end{example}

    The following theorem proves useful when computing limits. 
    \begin{theorem}
    If $f \sim \Tilde{f}$ as $x \rightarrow a$, then 
    \[\lim_{x \rightarrow a} f(x) g(x) = \lim_{x \rightarrow a} \Tilde{f}(x) g(x)\]
    provided one of these limits exist. 
    \end{theorem}

    \begin{theorem}[Properties of $o(g)$ and $O(g)$ Functions]
    For $x \rightarrow a$, 
    \begin{enumerate}
      \item $o(f) + o(f) = o(f)$
      \item $o(f)$ is also $O(f)$
      \item $o(f) + O(f) = O(f)$
      \item $O(f) + O(f) = O(f)$
      \item If $g(x) \not\equiv 0$, then 
      \[\frac{o(f(x))}{g(x)} = o \bigg( \frac{f(x)}{g(x)} \bigg), \text{ and } \frac{O(f(x))}{g(x)} = O \bigg( \frac{f(x)}{g(x)} \bigg)\]
    \end{enumerate}
    \end{theorem}

\section{Continuous Functions}

    \begin{definition}[Continuity of a Function]
      A function $f$ is \textbf{continuous at point $a$} if for any neighborhood $V\big(f(a)\big)$ of $f(a)$, there is a neighborhood $U(a)$ of $a$ whose image under the mapping $f$ is contained in $V\big( f(a)\big)$. 

      Generalizing this, we say that a function is \textbf{(globally) continuous} if the preimage of every neighborhood in its codomain is an open set in its domain. 
    \end{definition}

    The equivalent of these statements for functions $f: \mathbb{R}^n \longrightarrow \mathbb{R}^m$ follows from the fact that any neighborhood of a point contains a symmetric neighborhood of the point. 
    \begin{center}
        \includegraphics[scale=0.3]{img/Symmetric_Neighborhood_in_Neighborhood.PNG}
    \end{center}

    \begin{lemma}[Existence of Limits of Continuous Functions]
      $f: E \longrightarrow \mathbb{R}$ is continuous at $a \in E$, where $a$ is a limit point of $E$ if and only if 
      \[\lim_{x \rightarrow a} f(x) = f(a)\]
    \end{lemma}
    \begin{proof}
    The limit equalling $f(a)$ means that, by definition, for any arbitrarily small deleted neighborhood of $f(a)$, denoted $U_{f(a)} \setminus f(a)$, its preimage will be an open neighborhood of $a$, which itself will contain an open set. 
    \end{proof}

    This also means that we can use the Cauchy limit definition to defined continuity of a function at a point. That is, for any sequence $\{a_n\}$ of point in codomain $E$ which converges to point $a$, the function $f$ is continuous at $a$ if the corresponding sequence $\{f(a_n)\}$ converges to $f(a)$.

    \begin{theorem}
    This means that the continuous functions commute with the operation of passing to the limit at a point. 
    \[\lim_{x \rightarrow a} f(x) = f\Big( \lim_{x \rightarrow a} x \Big)\]
    \end{theorem}

    \begin{lemma}[Properties of Continuous Functions]
      Let $f: \mathbb{R}^n \longrightarrow \mathbb{R}^m, \; g: \mathbb{R}^m \longrightarrow \mathbb{R}^p$ with $c \in \mathbb{R}$. 
      \begin{enumerate}
        \item $f$ continuous at $x_0 \implies$ $c f$ continous at $x_0$. 
        \item $f, g$ continuous at $x_0 \implies f + g$ continuous at $x_0$. 
        \item Let $m = 1$. $f, g$ continuous at $x_0 \implies f g$ continuous at $x_0$. 
        \item $f$ continuous at $x_0$ and $f(x) \neq 0 \forall x \in \mathbb{R}^n \implies 1 / f$ continuous at $x_0$. 
        \item If $f(x) = \big( f_1(x), f_2(x), ..., f_n(x) \big)$ coordinate-wise, then 
      \[ f \text{ continuous at } x_0 \iff f_1, f_2, ..., f_m \text{ continuous  at } x_0\]
          \item $f$ continuous at $x_0$ and $g$ continuous at $y_0 = f(x_0) \implies g \circ f$ continuous at $x_0$. 
      \end{enumerate}
    \end{lemma}
    \begin{proof}
    This is an immediate result of the equivalence of a function being continuous at point $a$ and its limit at point $a$ existing. 
    \end{proof}

  \subsection{Points of Discontinuity}

    \begin{definition}[Discontinuity]
      If the function $f: E \longrightarrow \mathbb{R}$ is not continuous at a point of $E$, then this point is called a \textbf{point of discontinuity}, or simply a \textbf{discontinuity} of $f$. 

      That is, $a$ is a point of discontinuity of $f$ if for some neighborhood $V(f(a))$ of $f(a)$, there exists no neighborhood of $a$ whose image under the mapping $f$ is contained in $V(f(a))$. 
      There are three types of discontinuities: 
      \begin{enumerate}
        \item A \textbf{removable discontinuity} is characterized by the fact that the limit $\lim_{x \rightarrow a} f(x) = A$ exists, but $A \neq f(a)$. \begin{center}
            \includegraphics[scale=0.23]{img/Removable_Discontinuity.PNG}
        \end{center}
        This means that we can modify $f$ and define a new function $\Tilde{f}: E \longrightarrow \mathbb{R}$ as
        \[\Tilde{f}(x) = \begin{cases}
        f(x), & x \in E \setminus a \\
        A, & x = a
        \end{cases}\]
        which would be continuous on $E$. 
        \item A \textbf{discontinuity of first kind}, also known as a jump/step discontinuity, is characterized by both the left and right-hand limits 
        \[\lim_{x \rightarrow a-0} f(x) \text{ and } \lim_{x \rightarrow a+0} f(x)\]
        existing, but at least one of them is not equal to the value $f(a)$ that the function assumes at $a$. 
        \begin{center}
            \includegraphics[scale=0.23]{img/Discontinuity_First.PNG}
        \end{center}
        \item A \textbf{discontinuity of second kind}, also known as an essential discontinuity, is characterized by at least one of the two limits 
        \[\lim_{x \rightarrow a-0} f(x) \text{ and } \lim_{x \rightarrow a+0} f(x)\]
        not existing. 
        \begin{center}
            \includegraphics[scale=0.23]{img/Discontinuity_Second.PNG}
        \end{center}
      \end{enumerate}
      Note that strictly speaking, a removable discontinuity is really a discontinuity of first kind, but in this context we distinguish them. 
    \end{definition}

    \begin{example}[Dirichlet Function]
      The Dirichlet function, defined
      \[\mathcal{D}(x) = \begin{cases}
      1, & \text{ if } x \in \mathbb{Q} \\
      0, & \text{ if } x \in \mathbb{R} \setminus \mathbb{Q} 
      \end{cases}\]
      is discontinuous at every point, and obviously all of its discontinuities are of second kind, since in every interval there are both rational and irrational numbers and therefore there exists no limit at any point $a \in \mathbb{R}$. 

      More specifically, given any point $a \in \mathbb{R}$, assume that $a$ is rational. We can set $\epsilon = 0.1$-neighborhood around the value $1$, but no matter how small we let $\delta$, the interval $(a - \delta, a + \delta)$ will contain both rationals and irrationals, meaning that it will map to $\{0,1\}$ always, which is not fully contained in $(0.9, 1.1)$.  
    \end{example}

    Here is a slightly more interesting example. 

    \begin{example}[Riemann Function]
      Let the Riemann function $\mathcal{R}$ be defined
      \[\mathcal{R}(x) = \begin{cases}
      \frac{1}{n}, & \text{ if } x = \frac{m}{n} \in \mathbb{Q}, \text{ where gcd}(m, n) = 1 \\
      0, & \text{ if } x \in \mathbb{R} \setminus \mathbb{Q}
      \end{cases}\]
      We first note that for any point $a \in \mathbb{R}$, any bounded neighborhood $U(a)$ of it, and any number $N \in \mathbb{N}$, the neighborhood $U(a)$ contains only a finite number of rational numbers $\mathbb{m}{n}$, where $n < N$. By shrinking the neighborhood, we can assume that the denominators of all rational numbers in the neighborhood are larger than $N$. We can visualize why this is by seeing that rational numbers with larger denominators have smaller "gaps" between them. 
      \begin{center}
          \includegraphics[scale=0.23]{img/Rationals_Spread_Apart.PNG}
      \end{center}
      Thus, at any point $x \in U(a) \setminus a$, we have 
      \[\big| \mathcal{R}(x) \big| < \frac{1}{N}\]
      and therefore
      \[\lim_{x \rightarrow a} \mathcal{R} (x) = 0\]
      at any point $a \in \mathbb{R} \setminus \mathbb{Q}$. Hence, the Riemann function is continuous at any irrational number. 
    \end{example}

  \subsection{Properties of Continuous Functions}

    \begin{theorem}[Local Properties of Continuous Functions]
    Let $f: E \longrightarrow \mathbb{R}$ be a function that is continuous at the point $a \in E$. Then, 
    \begin{enumerate}
      \item $f$ is bounded in some neighborhood $U(a)$. 
      \item If $f(a) \neq 0$, then in some neighborhood $U(a)$ all the values of the function have the same sign as $f(a)$. 
      \item If the function $g: U(a) \subset E \longrightarrow \mathbb{R}$ is defined in some neighborhood of $a$ and is continuous at $a$, then the following functions 
      \begin{align*}
          & (f + g) (x) \\
          & (f \cdot g) (x) \\
          & \bigg( \frac{f}{g} \bigg) \big( x \big) \text{ where } g(a) \neq 0
      \end{align*}
      are also defined in $U(a)$ and continuous at $a$. 
      \item If the function $g: Y \longrightarrow \mathbb{R}$ is continuous at a point $b \in Y$ and $f$ is such that $f: E \longrightarrow Y$, $f(a) = b$, and $f$ is continuous at $a$, then the composite function 
      \[g \circ f: E \longrightarrow \mathbb{R}\]
      is defined on $E$ and continuous at $a$. 
      \begin{center}
          \includegraphics[scale=0.25]{img/Composition_Function_Continuous.PNG}
      \end{center}
      This is easy to see because given the open neighborhood of $g(b)$, we know for a fact that $U_\delta (a)$ maps completely into $U_\epsilon (b)$, and that $U_\epsilon (b)$ maps completely into $U_\kappa (g(b))$ and so the composition of these mappings must mean that $U_\delta (a)$ maps completely into $U_\kappa (g(b))$. 
    \end{enumerate}
    \end{theorem}

    \begin{example}
      An algebraic polynomial 
      \[P(x) = a_0 x^n + a_1 x^{n-1} + a_2 x^{n-2} + \ldots + a_{n-1} x + a_n\]
      is a continuous function on $\mathbb{R}$. Since $f(x) = x$ and $f(x) = c$ are continuous functions, by induction on $x$, we can multiply them together to find that $f(x) = x^n$ is continuous, which implies that $a x^n$ is continuous, which implies that the sums of these functions are also continuous. 
    \end{example}

    Unlike local properties, the global property of a function is a property involving the entire domain of definition of the function. 

    \begin{theorem}[Intermediate Value Theorem]
    If a function that is continuous on a closed interval assumes values with different signs at the endpoints of the interval, then there is a point in the interval where it assumes the value $0$. 
    \begin{center}
        \includegraphics[scale=0.25]{img/IVT.PNG}
    \end{center}
    \end{theorem}
    \begin{proof}

    \end{proof}

    This following proof provides a very simple algorithm for finding the zero of the equation $f(x) = 0$ on an interval whose endpoints has values with opposite signs. 
    Note that the colloquial description of the intermediate value theorem, that is is impossible to pass continuously from positive to negative values without assuming the value $0$ along the way), assumes more than they state. That is, this theorem is actually dependent on the domain of definition: that is is a closed interval, or more generally, that it is \textbf{connected}. 

    \begin{corollary}
    If a function $f$ is continuous on an open interval and assumes values $f(a) = A, f(b) = B$, then for any number $C \in (A, B)$, there is a point $c$ between $a$ and $b$ such that $f(c) = C$. 
    \begin{center}
        \includegraphics[scale=0.25]{img/Corollary_of_IVT.PNG}
    \end{center}
    \end{corollary}

    \begin{theorem}[Weierstrass Maximum-Value Theorem]
    A function that is continuous on a closed interval is bounded in that interval, with a maximum and minimum. 
    \end{theorem}

    \subsubsection{Uniform Continuity}
    Roughly speaking, a function $f$ is uniformly continuous if it is possible to guarantee that $f(x)$ and $f(y)$ be as close to each other as we please by requiring only that $x$ and $y$ be sufficiently close to each other. 

    \begin{definition}[Uniform Continuity]
      A function $f: E \longrightarrow \mathbb{R}$ is \textbf{uniformly continuous} on a set $E \subset \mathbb{R}$ if for every $\epsilon > 0$, there exists $\delta > 0$ such that 
      \[\big| f(x_1) - f(x_2)\big| < \epsilon\]
      for all points $x_1, x_2 \in E$ such that $|x_1 - x_2| < \delta$. 

      Intuitively, uniform continuity says that given any two points $x, y$ in the domain where their distance is arbitrarily small ($\delta$ apart), we can guarantee that the distance between $f(x), f(y)$ is at maximum some arbitrarily small $\epsilon$. 

      The following visual shows the radical function $f(x) = \sqrt{x}$ defined on $\mathbb{R}^+$. We can see that it satisfies uniform continuity because the graph does not escape the top and/or bottom of the $\epsilon \times \delta$ window, no matter where the box is located on the graph. More strictly speaking, no matter what we set the $\epsilon$ (how long the box is), uniform continuity says that we can choose a sufficient $\delta$ (width of the box) such that the graph does not escape the top/bottom of the window no matter where the window is. 
      \begin{center}
          \includegraphics[scale=0.28]{img/Uniform_Continuity_Radical.PNG}
      \end{center}
      We can clearly see that the function $f(x) = 1/x$ is not uniformly continuous, since the graph escapes the $\epsilon \times \delta$ window at some point (marked in red). More strictly speaking, given any length $\epsilon$ of the window, we cannot create a thin-enough $\delta$ box that will contain the graph, since as $x \rightarrow 1$, the function becomes unbounded. 
      \begin{center}
          \includegraphics[scale=0.25]{img/Uniform_Continuity_Rational.PNG}
      \end{center}
      That is, arbitrarily thin boxes don't help when the slope is arbitrarily steep. 
    \end{definition}

    To compare uniform continuity with regular continuity, we can adapt this alternate (yet equivalent interpretation): Let there exist function $f: E \longrightarrow \mathbb{R}$. Given any $\epsilon>0$, we can choose a $\delta>0$ such that given any point $x \in E$ and $f(x)$, as long as a second point $y$ is $\delta$ away from $x$, then $f(y)$ is $\epsilon$ away from $f(x)$. This visualization would lead to there being a $2\epsilon \times 2\delta$ window around point $x$. 
    \begin{center}
        \includegraphics[scale=0.3]{img/Double_Epsilon_Delta_Uniform_Continuity.PNG}
    \end{center}
    Uniform continuity means that the box above does not change dimensions no matter where the point is (hence, the name uniform). Therefore, given a certain $\epsilon > 0$, the way we choose $\delta$ is only dependent on $\epsilon$, and so it must be a function of $\epsilon$: 
    \[\delta = \delta(\epsilon)\]
    However, in continuity, there just has to exist \textbf{some} $\delta$-neighborhood of $x$ such that its image is contained in the $\epsilon$-neighborhood of $f(x)$. There are no restrictions on the dimensions of this box; it just has to exist. 
    \begin{center}
        \includegraphics[scale=0.28]{img/Regular_Continuity_Box_Visual.PNG}
    \end{center}

    \begin{lemma}
      If $f$ is uniformly continuous on the set $E$, it is continuous at each point of that set. However, the converse is not generally true. 
    \end{lemma}

    \begin{theorem}[Cantor's Theorem on Uniform Continuity]
    A function that is continuous on a closed interval is uniformly continuous on that interval. 
    \end{theorem}


    \begin{example}
      Let $f: \mathbb{R} \longrightarrow \mathbb{R}, \; f(x) = 3x+7$. Then $f$ is uniformly continuous. Choose $\epsilon > 0$. Let $\delta = \epsilon / 3$. Choose $x, y \in \mathbb{R}$ and assume $|x-y| < \delta$. Then, 
      \[ | f(x) - f(y) | = | 3x + 7 - 3 y - 7 | = 3 |x-y| < 3 \delta = \epsilon\]
    \end{example}

    \begin{example}
      Let $f: (0, 4) \subset \mathbb{R} \longrightarrow \mathbb{R}, \; f(x) = x^2$. Then $f$ is uniformly continuous on $(0, 4)$. Choose $\epsilon > 0$. Let $\delta = \epsilon / 8$. Choose $x, y \in (0, 4)$ and assume $|x-y| < \delta$. Then, 
      \[ |f(x) - f(y)| = |x^2 - y^2| = (x+y) |x-y| < (4+4) |x-y| = 8\delta = \epsilon\]
    \end{example}

    In both examples, the function satisfied an inequality of form 
    \[ |f(x_1) - f(x_2)| \leq M |x_1 - x_2|\]
    this is called the Lipshitz inequality. 

    \subsubsection{Lipshitz Continuity}
    Lipshitz continuity is a strong form of uniform continuity for functions. Intuitively, a Lipshitz continuous function is limited in how fast it can change (by the Lipshitz constant). 

    \begin{definition}[Lipshitz Continuous Function]
      Given $f: E \subset \mathbb{R} \longrightarrow \mathbb{R}$, $f$ is \textbf{Lipshitz continuous} if there exists a positive real constant $M$ such that for all real $x, y \in E$, 
      \[\big| f(x) - f(y) \big| \leq M \big| x - y \big|\]
      The corresponding $M$ is called the \textbf{Lipshitz constant}, and the smallest constant $M$ satisfying this inequality is called the \textbf{best Lipshitz constant}. 

      Note that Lipshitz continuity pops up as a very natural extension of uniform continuity. The inequality above just means that given an $\epsilon$, we can choose a $\delta$ such that a linear multiple of $\delta$ is always greater than $\epsilon$. This means that Lipshitz continuity is just uniform continuity such that the $\delta$ function is linear:  
      \[\delta = \delta(\epsilon) = \frac{1}{M} \epsilon\]
      \begin{center}
          \includegraphics[scale=0.25]{img/Lipshitz_Continuity.jpg}
      \end{center}
    \end{definition}

    Another way to interpret uniform continuity is by seeing that the derivative of $f$ is bounded by the slope $M$. 
    \begin{center}
        \includegraphics[scale=0.3]{img/Lipshitz_Continuity_Slope_Bound.PNG}
    \end{center}
    This slope bound implies that for every pair of points on the graph of this function, the absolute value of the slope of the line connecting them is not greater than $M'$. The smallest $M'$ is the best Lipshitz constant. 

    \begin{definition}[Bi-Lipshitz Continuity]
      A function $f: E \subset \mathbb{R}$ is \textbf{Bi-Lipshitz continuous} if there exists constant $M\geq 1$ such that for all real $x, y \in E$, 
      \[ \frac{1}{M} |x - y| \leq |f(x) - f(y)| \leq M |x - y|\]
      A visual of this map is shown, where the function $f$ must always land in the shaded green area. 
      \begin{center}
          \includegraphics[scale=0.25]{img/BiLipshitz_Map.PNG}
      \end{center}
      It immediately follows that for $x \neq y$, $ |f(x) - f(y)|$ cannot equal $0$, which means that a bilipshitz map is injective. A bilipshitz map is really just Lipshitz map with its inverse also being Lipshitz. 
    \end{definition}

    \begin{proposition}
    A bilipshitz map $f$ is a homeomorphism onto its image. 
    \end{proposition}

    \subsubsection{Inverse Function Theorem}

    We begin by introducing this intuitive lemma. 
    \begin{lemma}
      A continuous mapping $f: E \longrightarrow \mathbb{R}$ of a closed interval $E = [a,b]$ into $\mathbb{R}$ is injective if and only if the function $f$ is strictly monotonic on $[a,b]$. 

      Furthermore, every strictly monotonic function $f: X \subset \mathbb{R} \longrightarrow \mathbb{R}$ (for arbitrary $X$) has an inverse 
      \[f^{-1}: f(X) \subset \mathbb{R} \longrightarrow \mathbb{R}\]
      with the same kind of monotonicity on $f(X)$ that $f$ has on $X$. 
    \end{lemma}

    \begin{lemma}[Criterion for Continuity of a Monotonic Function]
      A monotonic function $f: E \longrightarrow \mathbb{R}$ defined on a closed interval $E = [a,b]$ is continuous if and only if its set of values $f(E)$ is the closed interval with endpoints $f(a)$ and $f(b)$. 

      Note that both conditions imply that there are no points of discontinuities in the graph of $f$. 
    \end{lemma}


    \begin{theorem}[Inverse Function Theorem]
    A function $f: X \longrightarrow \mathbb{R}$ that is strictly monotonic on a set $X \subset \mathbb{R}$ has an inverse $f^{-1}: Y \longrightarrow \mathbb{R}$ defined on the set $Y = f(X)$ of values of $f$. The function $f^{-1}: Y \longrightarrow \mathbb{R}$ is monotonic and has the same type of monotonicity on $Y$ that $f$ has on $X$. 
    \begin{center}
        \includegraphics[scale=0.25]{img/Inverse_Function_Theorem_Analysis.PNG}
    \end{center}
    If in addition, $X$ is a closed interval $[a,b]$ and $f$ is continuous on $X$, then the set $Y = f(X)$ is the closed interval with endpoints $f(a)$ and $f(b)$ and the function $f^{-1}: Y \longrightarrow \mathbb{R}$ is continuous on it.
    \end{theorem}

    \begin{example}
      The function $f(x) = \sin{x}$ is increasing and continuous on the closed interval $\big[ -\frac{\pi}{2}, \frac{\pi}{2} \big]$. Hence, the restriction to the closed interval $\big[ -\frac{\pi}{2}, \frac{\pi}{2} \big]$ has an inverse $x = f^{-1}(y)$, which is denoted by 
      \[x = \arcsin{y}\]
      This function is defined on the closed interval $\big[- \sin\big(-\frac{\pi}{2}\big), \sin\big(-\frac{\pi}{2}\big) \big] = [-1,1]$ and increases continuously from $-\frac{\pi}{2}$ to $\frac{\pi}{2}$. 
      \begin{center}
          \includegraphics[scale=0.25]{img/Inverse_Function_Theorem_Sin.PNG}
      \end{center}
    \end{example}

\section{Differential Calculus}

  \subsection{Functions Differentiable at a Point}

    \begin{definition}[Differentiable Function]
      A function $f: E \subset \mathbb{R} \longrightarrow \mathbb{R}$ is \textbf{differentiable} at a given point $x$ (that is a limit point of $E$) if there exists a linear function $h \mapsto df(x) h$ (called the \textbf{differential of $f$}) and an infinitesimal $\alpha (x;h) = o(h)$ as $h \rightarrow 0$, such that
      \[f(x + h) - f(x) = df(x) (h) + \alpha (x; h)\]
      Note that $x$ is fixed; what we are really interested here is the $h$ value. Furthermore, 
      \begin{enumerate}
        \item $\Delta x(h) \equiv (x + h) - x = h$ is called the \textbf{increment of the argument}
        \item $\Delta f(x;h) \equiv f(x + h) - f(x)$ is called the \textbf{increment of the function} 
      \end{enumerate}
      They are often denoted (inappropriately) by the symbols $\Delta x$ and $\Delta f(x)$ representing functions of $h$. The differential and the infinitesimal can be visualized below. 
      \begin{center}
          \includegraphics[scale=0.25]{img/Differential_Diagram.PNG}
      \end{center}
    \end{definition}

    \begin{definition}[Derivative]
      Given function $f: E \subset \mathbb{R} \longrightarrow \mathbb{R}$, the number
      \[f^\prime (x) = \lim_{h \rightarrow 0} \frac{f(x+h) - f(x)}{h}\]
      is called the \textbf{derivative} of the function $f$ at $x$. It can be visualized as the sequence of the slopes of the secant lines converging onto the slope of the black tangent line as shown. 
      \begin{center}
          \includegraphics[scale=0.27]{img/Tangent_Lines_Converging.PNG}
      \end{center}
      This equality can also be written in the equivalent form: 
      \[\frac{f(x+h) - f(x)}{h} = f^\prime (x) + \alpha (h)\]
      where $\alpha$ is infinitesimal as $h \rightarrow 0$. This also also equivalent to:
      \[f(x+h) - f(x) = f^\prime (x) h + o (h)\]
      where the error term $o(h) \rightarrow 0$ as $h \rightarrow 0$. 
    \end{definition}

    Note that we have defined the differentiability of a function at a point and the existence of its derivative at a point completely separately. But it turns out that the existence of this arbitrary number $f^\prime (x)$ we call the "derivative," defined
    \[f^\prime (x) = \lim_{h \rightarrow 0} \frac{f(x + h) - f(x)}{h}\]
    actually has an equivalent form of 
    \[f(x + h) - f(x) = f^\prime (x) h + o(h)\]
    But since $f^\prime(x)$ is in $\mathbb{R}$, the function $h \mapsto f^\prime (x) h$ is linear and $o(h)$ is infinitesimal, so it is in the form 
    \[f(x + h) - f(x) = df (x) (h) + \alpha(x; h)\]
    which, by definition, means that it is differentiable! Therefore, we have determined the equivalence between the differentiability of a function at a point and the existence of its derivative at the same point. Furthermore, this function $h \mapsto f^\prime (x) h$ is precisely the differential of $f$, meaning that
    \[df (x) (h) = f^\prime (x) h\]
    Furthermore, 
    \[\Delta f(x; h) - df(x)(h) = \alpha (x; h)\]
    and $\alpha(x;h) = o (h)$ as $h \rightarrow 0$, or in other words, the difference between the increment of the function and the value of the function $df(x)$ in $h$ is an infinitesimal of higher order than the first in $h$. For this reason, we say that the differential is the \textbf{principal linear part of the increment of the function}. 

    In particular, if $f(x) \equiv x$, then we have $f^\prime (x) \equiv 1$ and 
    \[dx (h) = 1 \cdot h = h\]
    Substituting this equality into $df(x) (h) = f^\prime (x) h$, we get
    \[df (x) (h) = f^\prime (x) \,dx (h)\]
    or without the input parameter $h$, 
    \[df(x) = f^\prime (x) \,dx\]
    Note that this is an equality between two functions of $h$. From this, we obtain the familiar \textbf{Leibniz notation} of the derivative: 
    \[\frac{df (x) (h)}{dx(h)} = f^\prime (x) \iff \frac{df(x)}{dx} = f^\prime (x)\]
    That is, the function $\frac{df(x)}{dx}$, which is the ratio of the functions $df(x)$ and $dx$, is constant and equals $f^\prime (x)$. 

  \subsection{Tangent Line: Geometric Meaning of the Derivative, Differential}

    Let us try to construct successive approximations to an arbitrary function $f: E \longrightarrow \mathbb{R}$ at a given limit point $x_0$. That is, we find a function $g$ such that
    \[f = g + o(g)\]
    Depending on what $g$ is, we can construct better approximations of $f$. 

    \subsubsection{Constant Approximation}
    The 0th order approximation is when $g$ is a constant. That is, $g \equiv c_0$ for some $c_0 \in \mathbb{R}$. This means
    \[f(x) = c_0 + o(c_0) = c_0 + o(1) \text{ as } x \rightarrow x_0\]
    More precisely, we want this difference $f(x) - c_0$ to be $o(1)$ as $x \rightarrow x_0$, which means that it is simply infinitesimal. Visualizing this, we can see that given a constant approximation (labeled in blue) to a function at $x_0$, its error term (labeled in green) is in fact, infinitesimal. All this boils down to the fact that 
    \[\lim_{x \rightarrow x_0} f(x) = c_0\]
    If the function is continuous at $x_0$, then 
    \[\lim_{x \rightarrow x_0} f(x) = f(x_0)\]
    and naturally $c_0 = f(x_0)$. Both the continuous (left) and noncontinuous case (right) is shown, but in most cases, we will assume continuity. 
    \begin{center}
        \includegraphics[scale=0.3]{img/Constant_Approximation_Continuous_Noncontinuous_case.PNG}
    \end{center}

    \subsubsection{Linear Approximation}
    The 1st order approximation is a linear function that approximates $f$ as
    \[f(x) = c_0 + c_1(x - x_0) + o(x - x_0) \text{ as } x \rightarrow x_0\]
    Following the previous logic, assuming $f$ continuous means that $c_0 = f(x_0)$. Furthermore, as $x \rightarrow x_0$
    \begin{align*}
        f(x) = c_0 + c_1(x - x_0) + o(x - x_0) & \implies c_1 = \frac{f(x) - c_0 - o(x - x_0)}{x - x_0} \\
        & \implies c_1 = \frac{f(x) - c_0}{x - x_0} - \frac{o(x - x_0)}{x - x_0}\\
        & \implies c_1 = \frac{f(x) - c_0}{x - x_0} - o(1) \\
        & \implies c_1 = \lim_{x \rightarrow x_0} \frac{f(x) - c_0}{x - x_0} = f^\prime (x_0)
    \end{align*}
    But this just means that $f^\prime (x_0) = c_1$, Note that before, we have proved the equivalence of the existence of a derivative at $x_0$ with differentiability at $x_0$ (which itself means that there exists a linear approximation $df(x)(h)$ that is a function of $h$). Here, we have created a linear approximation with respect to $x = x_0 + h$, rather than $h$ (shifted the function). 

    Therefore, the function 
    \[\alpha (x) = f(x_0) + f^\prime (x_0) (x - x_0)\]
    provides the best linear approximation to the function $f$ in a neighborhod of $x_0$ in the sense that for any other function $\beta(x)$ of the form 
    \[\beta(x) = c_0 + c_1 (x - x_0)\]
    we have $f(x) - \beta(x) \neq o(x - x_0)$ as $x \rightarrow x_0$. The graph of the function $\alpha$ is the straight line
    \[y - f(x_0) = f^\prime (x_0) (x - x_0)\]

    This leads to the definition of our familiar tangent line. 

    \begin{definition}[Tangent Line]
      If a function $f: E \longrightarrow \mathbb{R}$ is differentiable at a point $x_0 \in E$, the line defined by
      \[y - f(x_0) = f^\prime (x_0) (x - x_0)\]
      is called the \textbf{tangent} to the graph of $f$ at the point $(x_0, f(x_0))$. 
    \end{definition}

    \subsubsection{Higher Order Approximations}
    We can continue this pattern to get a quadratic approximation of $f$ in the form
    \[f(x) = c_0 + c_1 (x - x_0) + c_2 (x - x_0)^2 + o\big((x - x_0)^2 \big) \text{ as } x \rightarrow x_0\]
    As we have done in the previous subsection, we can derive (assuming continuity of $f$) $c_0 = f(x_0), c_1 = f^\prime (x_0)$. To derive what $c_2$ should be, we see that the equation above implies
    \[c_2 = \frac{f(x) - c_0 - c_1 (x - x_0) - o\big((x - x_0)^2 \big)}{(x - x_0)^2} = \frac{f(x) - c_0 - c_1 (x - x_0)}{(x - x_0)^2} - o(1)\]
    which means
    \[c_2 = \lim_{x \rightarrow x_0} \frac{f(x) - c_0 - c_1 (x - x_0)}{(x - x_0)^2}\]
    Extending this, if we are seeking a polynomial $P_n(x_0; x) = c_0 + c_1 (x - x_0) + \ldots + c_n (x - x_0)^n$ such that
    \[f(x) = c_0 + c_1 (x - x_0) + \ldots + c_n (x - x_0)^n + o\big((x - x_0)^n\big) \text{ as } x \rightarrow x_0\]
    we would find 
    \begin{align*}
        c_0 & = \lim_{x \rightarrow x_0} f(x) \\
        c_1 & = \lim_{x \rightarrow x_0} \frac{f(x) - c_0}{x - x_0} \\
        c_2 & = \lim_{x \rightarrow x_0} \frac{f(x) - c_0 - c_1 (x - x_0)}{(x - x_0)^2} \\
        \ldots & = \ldots \\
        c_n & = \lim_{x \rightarrow x_0} \frac{f(x) - (c_0 + \ldots + c_{n-1}(x - x_0)^{n-1})}{(x - x_0)^n}
    \end{align*}

    We formalize the order of these approximations by analyzing their error bound. 

    \begin{definition}[nth Order Contact]
      If $f, g: E \longrightarrow \mathbb{R}$ are continuous at point $x_0$ and $(f - g) (x) = o\big( (x - x_0)^n \big)$ as $x \rightarrow x_0$, then we say that $f$ and $g$ have \textbf{$n$th order contact at $x_0$}, or more precisely, \textbf{contact of order at least $n$}. 

      The following visual shows approximations $g$ of an arbitrary function $f$ that have $0$th (left), $1$st (middle), and $2$nd (right) order contact at $x_0$. 
      \begin{center}
          \includegraphics[scale=0.25]{img/nth_order_contact.PNG}
      \end{center}
    \end{definition}

    \subsubsection{The Real Tangent Space}

    Tangent spaces. 

    \begin{definition}[Tangent Space]
      Given function $f: E \longrightarrow \mathbb{R}$ and a point $x_0 \in E$, the increment of the argument $h = x - x_0$ can be regarded as a vector attached to the point $x_0$ and defining the transition from $x_0$ to $x_0 + h$. $h$ is called a \textbf{tangent vector}, and the set of all such vectors as $T_{x_0} \mathbb{R}$. Similarly, we denote $T_{y_0} \mathbb{R}$ the set of all displacement vectors from the point $y_0$ along the $y$-axis. 
      \begin{center}
          \includegraphics[scale=0.25]{img/Tangent_Space_1_dimensional_in_R.PNG}
      \end{center}
      Then, we can see that the differential is a mapping
      \[df(x_0): T_{x_0} \mathbb{R} \longrightarrow T_{f(x_0)} \mathbb{R}\]
      Note that that there are two functions to pay attention to here: 
      \begin{enumerate}
        \item The true increment of $f$, defined $h \mapsto f(x_0 + h) - f(x_0) = \Delta f(x_0; h)$ (labeled in green). 
        \item The differential $h \mapsto f^\prime (x_0) h = df(x_0) (h)$, which gives the increment of the tangent to the graph for increment $h$ in the argument (labeled in red). 
      \end{enumerate}
    \end{definition}

    \begin{example}
      Let $f(x) = \sin{x}$. Then we will show that $f^\prime (x) = \cos{x}$. 
      \begin{align*}
          \lim_{h \rightarrow 0} \frac{\sin{(x+h)} - \sin(x)}{h} & = \lim_{h \rightarrow 0} \frac{2 \sin \big( \frac{h}{2} \big) \cos \big( x + \frac{h}{2} \big)}{h} \\
          & = \lim_{h \rightarrow 0} \cos \Big( x + \frac{h}{2} \Big) \cdot \lim_{h\rightarrow 0} \frac{\sin\big( \frac{h}{2}\big)}{\big(\frac{h}{2}\big)} = \cos(x)
      \end{align*}
      Here, we have used the theorem on the limit of a product, the continuity of the function $\cos(x)$, the equivalence $\sin{t} \sim t$ as $t \rightarrow 0$, and the theorem on the limit of a composite function. 
    \end{example}

    \begin{example}
      We will show that $\cos^\prime (x) = - \sin(x)$. 
      \begin{align*}
          \lim_{h\rightarrow 0} \frac{\cos(x+h) - \cos(x)}{h} & = \lim_{h \rightarrow 0} \frac{-2 \sin \big(\frac{h}{2}\big) \, \sin \big( x + \frac{h}{2}\big)}{h} \\
          & = - \lim_{h\rightarrow 0} \sin \Big( x + \frac{h}{2} \Big) \cdot \lim_{h\rightarrow0} \frac{\sin\big(\frac{h}{2} \big)}{\big( \frac{h}{2} \big)} = -\sin(x)
      \end{align*}
    \end{example}

  \subsection[Rules of Differentation over R]{Rules of Differentiation over \(\mathbb{R}\)}

    \subsubsection{Basic Properties; Derivatives of Composite, Inverse Functions}

    \begin{theorem}[Arithmetic Properties of Differentiation over $\mathbb{R}$]
    If functions $f, g: E \longrightarrow \mathbb{R}$ are differentiable at a point $x \in E$, then 
    \begin{enumerate}
      \item their sum is differentiable at $x$, and 
      \[d(f+g) (x) = df(x) + dg(x) \iff (f+g)^\prime (x) = (f^\prime + g^\prime) (x)\]
      \item their product is differentiable at $x$, and 
      \[d (f \cdot g) (x) = g(x) df(x) + f(x) dg(x) \iff (f \cdot g)^\prime (x) = f^\prime (x) \cdot g(x) + f(x) \cdot g^\prime (x)\]
      \item their quotient is differentiable at $x$ if $g(x) \neq 0$, and 
      \[d \Big( \frac{f}{g} \Big) (x) =  \frac{g(x) df(x) - f(x) dg(x)}{g^2 (x)} \iff \bigg(\frac{f}{g}\bigg)^\prime (x) = \frac{f^\prime (x) g(x) - f(x) g^\prime (x)}{g^2 (x)}\]
    \end{enumerate}
    It is clear that $c\cdot df(x) = d (cf)(x)$, it is clear that the derivative is a linear operator from the space of all functions differentiable at $x_0$ the space of all functions. 
    \end{theorem}
    \begin{proof}
    Since $f$ and $g$ are differentiable at $x$, there exists the differential $df(x)(h) = f^\prime (x) h$ and $dg(x) = g^\prime (x) h$ where
    \begin{align*}
        f(x + h) & = f(x) + df(x)(h) + o(h) = f(x) + f^\prime (x) h + o(h) \\
        g(x + h) & = g(x) + dg(x)(h) + o(h) = g(x) + g^\prime (x) h + o(h) 
    \end{align*}
    From this relation, we can clearly see that a certain property of the differential automatically implies the same property of the derivative. (Remember that $f^\prime (x)$ and $g^\prime(x)$ are not functions! They are scalars defined on fixed point $x$.) 
    \begin{enumerate}
      \item Even though this derivation may be a bit long, every step is included to minimize ambiguity. 
      \begin{align*}
          (f + g)(x + h) - (f + g)(x) & = \big( f(x + h) + g(x + h)\big) - \big( f(x) + g(x)\big) \\
          & = \big( f(x + h) - f(x)\big) + \big(g(x + h) - g(x) \big) \\
          & = \big( df(x)(h) + o(h)\big) + \big( dg(x)(h) + o(h)\big) \\
          & = \big(f^\prime (x) h + o(h)\big) + \big( g^\prime (x) (h) + o(h)\big) \\
          & = \big(f^\prime(x) + g^\prime (x)\big) h + o(h) \\
          & = (f^\prime + g^\prime)(x)(h) + o(h) \\
          & = d (f + g)(x) h + o(h)
      \end{align*}
      \item For the product rule, we have
      \begin{align*}
          (f \cdot g) (x + h) & - (f \cdot g)(x) = f(x+h)g(x+h) - f(x) g(x) \\
          & = \big(f(x) + df(x) (h) + o(h)\big)\big(g(x) + dg(x)(h) + o(h)\big) - f(x) g(x) \\
          & = \big(f(x) + f^\prime (x) h + o(h)\big)\big(g(x) + g^\prime (x) h + o(h)\big) - f(x) g(x)
      \end{align*}
      Expanding this gives 
      \begin{multline*}
          \big(f^\prime (x) g(x) + f(x) g^\prime(x)\big) h + \big(f(x) + g(x)\big) o(h) + \\ f^\prime (x) g^\prime (x) h^2 + \big(f^\prime (x) + g^\prime (x) \big) h o(h) + \big(o(h)\big)^2
      \end{multline*}
      but note that since $f(x), g(x), f^\prime(x), g^\prime (x)$ are constants, we see that 
      \begin{enumerate}
        \item $\big(f(x) + g(x)\big) o(h) = o(h)$ because 
        \[\lim_{h \rightarrow 0} \frac{\big(f(x) + g(x)\big) o(h)}{h} = \big(f(x) + g(x)\big) \lim_{h \rightarrow 0} \frac{o(h)}{h} = 0\]
        \item $f^\prime(x) g^\prime (x) h^2 = o(h)$ since
        \[\lim_{h \rightarrow 0} \frac{f^\prime(x) g^\prime (x) h^2}{h} = f^\prime(x) g^\prime (x) \lim_{h \rightarrow 0}  h = 0\]
        \item $\big(f^\prime(x) + g^\prime(x)\big) h o(h) = o(h)$ because 
        \[\lim_{h \rightarrow 0} \frac{\big(f^\prime(x) + g^\prime(x)\big) h o(h)}{h} = \big(f^\prime(x) + g^\prime(x)\big) \lim_{h \rightarrow 0} o(h) = 0\]
        In fact, this term is of $o(h^2)$. 
        \item We can see that $(o(h))^2 = o(h)$ since 
        \[\lim_{h \rightarrow 0} \frac{(o(h))^2}{h} = \lim_{h \rightarrow 0} \frac{o(h)}{h} \cdot \lim_{h \rightarrow 0} o(h) = 0 \cdot 0 = 0\]
        In fact, $(o(h))^2 = o(h^2)$. 
      \end{enumerate}
      Therefore, the above simplifies to 
      \[(f \cdot g)(x + h) - (f \cdot g) (x) = \big(f^\prime(x) g(x) + f(x) g^\prime (x)\big) h + o(h)\]
      But this means that the differential (best approximation) $d(f \cdot g) (x)$ must be 
      \[(f \cdot g)^\prime (x) (h) = (f \cdot g)^\prime (x) h = \big(f^\prime(x) g(x) + f(x) g^\prime (x)\big) h\]
      \item Since the function $g(x) \neq 0$ at point $x$, then by continuity we can assume that there exists a neighborhood $U(x)$ where the image of that neighborhood does not vanish. That is, we can guarantee that $g(x + h) \neq 0$ for sufficiently small values of $h$. We assume $h$ is small in the following computations. 
      \begin{align*}
          \bigg(\frac{f}{g}\bigg) (x + h) - \bigg( \frac{f}{g}\bigg) (x) & = \frac{f(x + h)}{g(x + h)} - \frac{f(x)}{g(x)} \\
          & = \frac{1}{g(x)g(x + h)} \big( f(x + h) g(x) - f(x) g(x + h)\big) \\
          & = \bigg(\frac{1}{g^2(x)} + o(1)\bigg) \Big( \big(f(x) + f^\prime(x) h + o(h)\big) g(x) \\
          & \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;- f(x) \big( g(x) + g^\prime(x) h + o(h)\big)\Big) \\
          & = \bigg(\frac{1}{g^2(x)} + o(1)\bigg) \Big(\big(f^\prime(x) g(x) - f(x) g^\prime(x)\big) h + o(h)\Big) \\
          & = \frac{f^\prime(x) g(x) - f(x) g^\prime(x)}{g^2 (x)} h + o(h)
      \end{align*}
      Note that here we have used the continuity of $g$ at the point $x$ and the fact that $g(x) \neq 0$ to deduce that
      \[\lim_{h \rightarrow 0} \frac{1}{g(x) g(x + h)} = \frac{1}{g^2(x)} \iff \frac{1}{g(x) + g(x + h)} = \frac{1}{g^2(x)} + o(1)\]
      where $o(1)$ is infinitesimal as $h \rightarrow 0$. 
    \end{enumerate}
    \end{proof}

    \begin{theorem}[Chain Rule for Composite Functions over $\mathbb{R}$]
    Let there be functions $f: E_1 \subset \mathbb{R} \longrightarrow E_2 \subset \mathbb{R}$ is differentiable at a point $x \in E_1$ and the function $g: E_2 \subset \mathbb{R} \longrightarrow \mathbb{R}$ is differentiable at point $y = f(x) \in E_2$, with respective differentials 
    \begin{align*}
        df(x)& : T_x \mathbb{R} \longrightarrow T_y \mathbb{R} \\
        dg(y)& : T_y\mathbb{R} \longrightarrow T_{g(y)} \mathbb{R}
    \end{align*}
    Then the composite function $g \circ f: E_1 \longrightarrow \mathbb{R}$ is differentiable at $x$, and $d(g \circ f)(x): T_x \mathbb{R} \longrightarrow T_{g \circ f(x)} \mathbb{R}$ is
    \[d(g \circ f)(x) = d g(y) \circ d f(x) \iff (g \circ f)^\prime (x) = g^\prime \big( f(x) \big) \circ f^\prime (x)\]
    \end{theorem}
    \begin{proof}
    We will denote the increment of the argument with the variables $h$ and $t$. Then, by differentiability of $f$ and $g$, we have
    \begin{align*}
        f(x + h) - f(x) & = f^\prime (x) h + o(h) \text{ as } h \rightarrow 0 \\
        g(y + t) - g(y) & = g^\prime(y) t + o(t) \text{ as } t \rightarrow 0
    \end{align*}
    Since the function $o(t)$ can be represented as $o(t) = \gamma(t) t$, where $\gamma = o(1)$ and hence is infinitesimal as $t \rightarrow 0$, meaning that we can assume $\gamma(0) = 0$ (since $o(t)$ is defined for $t = 0$). 

    We can think of the displacement of $x$ as like a chain reaction: As$x \mapsto x + h$, $f(x) \mapsto f(x + h)$, which we could interpret as $y \mapsto y + t$ and hence means that $g(y) \mapsto g(y + t)$. So, setting $f(x) = y$ and $f(x + h) = y + t$, by differentiability and hence continuity of $f$ at point $x$, we can conclude that $t \rightarrow 0$ as $h \rightarrow 0$. So, we have
    \[\gamma\big(f(x+h) - f(x)\big) = \gamma\big( (y+t) - y\big) = \gamma(t) = \alpha(h) \rightarrow 0 \text{ as } h \rightarrow 0\]
    Thus, we get 
    \begin{align*}
        o(t) = \gamma(t) t & = \gamma\big( f(x + h) - f(x)\big)\big( f(x + h) - f(x)\big) \\
        & = \alpha(h) \big(f^\prime(x) h + o(h)\big) \\
        & = \alpha(h) f^\prime(x) h + \alpha(h) o(h) \\
        & = o(h) + o(h) = o(h) \text{ as } h \rightarrow 0 \\
        (g \circ f)(x + h) - (g \circ f)(x) & = g\big(f(x + h)\big) - g\big(f(x)\big) \\
        & = g (y + t) - g(y) \\
        & = g^\prime (y) t + o(t) \\
        & = g^\prime \Big(f(x)\big) \big(f(x + h) - f(x)\big) + o\big( f(x + h) - f(x)\big) \\
        & = g^\prime \big(f(x)\big) \big(f^\prime (x) h + o(h)\big) + o\big( f(x + h)\big) - f(x)\big) \\
        & = g^\prime\big( f(x) \big) \big( f^\prime (x) h\big) + g^\prime \big( f(x)\big) \big(o(h)\big) + o\big(f(x + h) - f(x)\big) 
    \end{align*}
    Since $g^\prime\big(f(x)\big) \big( o(h)\big)$ is really just a constant multiplied by a function that is $o(h)$, it is $o(h)$. $o\big( f(x + h) - f(x) \big)$. As for $o\big(f(x + h) - f(x)\big)$, we see that since $f(x + h) - f(x) = t$, a function that is $o\big(f(x + h) - f(x)\big)$ becomes infinitesimal compared to $t$ as $t \rightarrow 0$. As already stated before, we have
    \[o\big(f(x + h) - f(x) \big) = o(h) \text{ as } h \rightarrow 0\]
    and thus, we proved that
    \begin{align*}
        (g \circ f)(x + h) - (g \circ f)(x) & = g^\prime (y) f^\prime (x) h + o(h) \\
        & = \big(dg(y) \circ df(x)\big) (h) + o(h)
    \end{align*}
    \end{proof}

    \begin{theorem}[Differentiation of Inverse Functions over $\mathbb{R}$]
    Let $E_1, E_2 \subset \mathbb{R}$, and $f: E_1 \longrightarrow E_2$ and $f^{-1}: E_2 \longrightarrow E_1$ be mutually inverse and continuous at points $x_0 \in E_1$ and $f(x_0) = y_0 \in E_2$. If $f$ is differentiable at $x_0$ and $f^\prime(x_0) \neq 0$, then $f^{-1}$ also differentiable at the point $y_0$, and 
    \[\big(f^{-1}\big)^{-1} (y_0) = \big(f^\prime (x_0)\big)^{-1} \iff df^{-1} (y_0) = \big(df(x_0)\big)^{-1}\]
    \begin{center}
        \includegraphics[scale=0.25]{img/Real_Analysis_Differentiation_Inverse_Functions.PNG}
    \end{center}
    Note that if we knew in advance that $f^{-1}$ was differentiable at $y_0$ (which is a stronger hypothesis), we can find immediately by the identity 
    \[(f^{-1} \circ f) (x) = x\]  
    and the theorem on the differentiation of a composite function that
    \[(f^{-1})^\prime (y_0) \cdot f^\prime (x_0) = 1\]
    \end{theorem}

    Note that if the hypothesis was satisfied, but $f^\prime (x_0) = 0$, then $f^{-1}$ would not be differentiable since it would have an undefined differential. 
    \begin{center}
        \includegraphics[scale=0.3]{img/Inverse_Function_Differentiation_Derivative_Zero_problem.PNG}
    \end{center}
    \subsubsection{Higher-Order Derivatives}

    \begin{definition}[Global Derivative Function]
      If function $f: E \longrightarrow \mathbb{R}$ is differentiable at every point $x \in E$, then a new function $f^\prime: E \longrightarrow \mathbb{R}$, whose value at a point $x \in E$ equals the derivative $f^\prime(x)$ of the function $f$ at the point. 
    \end{definition}

    \begin{definition}[Second, Nth Derivative]
      This function $f^\prime$ may itself have a derivative $(f^\prime)^\prime : E \longrightarrow \mathbb{R}$, called the \textbf{second derivative} of the original function $f$, denoted 
      \[f^{\prime\prime} (x), \;\;\; \frac{d^2 f(x)}{dx^2}\]
      By induction, if the derivative $f^{(n-1)} (x)$ of order $n-1$ of $f$ has been defined, then the \textbf{derivative of order $n$} is defined by the formula
      \[f^{(n)} (x) \equiv \big(f^{(n-1)}\big)^\prime (x)\]
      The set of function $f: E \longrightarrow \mathbb{R}$ having continuous derivatives up to order $n$ inclusive is denoted $C^{n} (E, \mathbb{R})$. 
    \end{definition}

    \begin{lemma}[Leibniz' Formula]
      Let $u(x)$ and $v(x)$ be functions having derivatives up to order $n$ inclusive on a common set $E$. Then, 
      \[(uv)^{(n)} = \sum_{m = 0}^n \binom{n}{m} u^{(n-m)} v^{(m)}\]
      This means that given a polynomial $P_n (x) = c_0 + c_1 (x - x_0) + \ldots + c_n (x - x_0)^n$, then 
      \begin{align*}
          P_n(x_0) & = 0 \\
          P_n^\prime (x_0) & = 1! c_1 \\
          P_n^{\prime\prime} (x_0) & = 2! c_2 \\
          \ldots & = \ldots \\
          P_n^{(n)} (x_0) & = n! c_n \\
          P_n^{(k)} (x_0) & = 0 \text{ for } k > n
      \end{align*}
      and thus the polynomial $P_n (x)$ can be written as
      \[P_n (x) = P_n^{(0)} (x_0) + \frac{1}{1!} P_n^{(1)} (x_0) (x-x_0) + \frac{1}{2!} P_n^{(2)} (x_0) (x-x_0)^2 + \ldots + \frac{1}{n!} P_n^{(n)} (x_0) (x-x_0)^n\]
    \end{lemma}

  \subsection{Theorems of Differential Calculus}

    \subsubsection{Fermant's Lemma, Rolle's Theorem}

    \begin{definition}[Local Extrema]
      A point $x_0 \in E \subset \mathbb{R}$ is called a \textbf{local maximum} (resp. \textbf{local minimum}) and the value of a function $f: E \longrightarrow \mathbb{R}$ at that point a \textbf{local maximum value} (resp. \textbf{local minimum value}) if there exists a neighborhood $U_E (x_0)$ of $x_0$ in $E$ such that at any point $x \in U_E (x_0)$ we have 
      \[f(x) \leq f(x_0), \big( \text{resp. } f(x) \geq f(x_0) \big)\]
      If this is a strict inequality
      \[f(x) < f(x_0), \big( \text{resp. } f(x) > f(x_0) \big)\]
      then $x_0$ is called a \textbf{strict local maximum} (resp. \textbf{strict local minimum}). 
    \end{definition}

    \begin{definition}[Interior Extrema]
      An extremum $x_0 \in E$ of the function $f: E \longrightarrow \mathbb{R}$ is called an \textbf{interior extremum} if $x_0$ is not on the boundary of $E$, or more rigorously, $x_0$ is a limit point of both sets $E_- = \{x \in E \;|\; x < x_0\}$ and $E_+ = \{ x\in E\;|\; x > x_0\}$. For example, the graphs below are interior extrema at $x_0, x_0^*$. 
      \begin{center}
          \includegraphics[scale=0.25]{img/Interior_Extrema.PNG}
      \end{center}
      But the following graphs show extrema (at $x_0$) that are not interior extrema. 
      \begin{center}
          \includegraphics[scale=0.25]{img/Non_Interior_Extrema.PNG}
      \end{center}
    \end{definition}

    \begin{lemma}[Fermant]
      If a function $f: E \longrightarrow \mathbb{R}$ is differentiable at an interior extrememum $x_0 \in E$, then its derivative at $x_0$ is $0$. That is, 
      \[f^\prime (x_0) = 0\]
      Thus, this lemma gives a necessary condition for an interior extremum of a differentiable function. But for non-iterior extrema, it is generally not true that $f^\prime(x_0) = 0$ and so the converse does not hold (labeled in green). 
      \begin{center}
          \includegraphics[scale=0.25]{img/Fermant_Condition_for_Extrema.PNG}
      \end{center}
    \end{lemma}
    \begin{proof}
    By definition of differentiability at $x_0$ we get
    \begin{align*}
        f(x_0 + h) - f(x_0) & = f^\prime (x_0) h + o(h) \\
        & = f^\prime(x_0) h + \alpha (x_0; h) h \\
        & = \big(f^\prime (x_0) + \alpha(x_0; h)\big) h
    \end{align*}
    where we know that $o(h)$ can be written as $o(1) h$ for some infinitesimal $o(1)$ as $h \rightarrow 0$. If $f^\prime (x_0) \neq 0$, then for $h$ sufficiently close to $0$ the quantity $f^\prime(x_0) + \alpha(x_0; h)$ would have the same sign as $f^\prime (x_0)$, since $\alpha(x_0; h) \rightarrow 0$ as $h \rightarrow 0$. But the value of $h$ can be both positive or negative, given that $x_0$ is an interior extremum. This contradiction must imply that $f^\prime (x_0) = 0$. 
    \end{proof}

    Geometrically, Fermant's lemma is obvious, since it asserts that at an extremum of a differentiable function, the tangent to its graph is horizontal. Physically, this lemma means that in motion along a line the velocity must be zero at the instant then the direction reverses. 

    \begin{theorem}[Rolle's Theorem]
    If a function $f: [a, b] \longrightarrow \mathbb{R}$ is continuous on a closed interval $[a,b]$ and differentiable on the open interval $(a, b)$ and $f(a) = f(b)$, then there exists a point $\zeta \in (a, b)$ such that $f^\prime (\zeta) = 0$. 
    \begin{center}
        \includegraphics[scale=0.25]{img/Analysis_Rolles_Theorem.PNG}
    \end{center}
    \end{theorem}

    \subsubsection{Mean Value Theorem, Cauchy's Finite-Increment Theorem}

    The following theorem is extremely useful in studying numerical valued functions. 

    \begin{theorem}[Mean Value Theorem]
    If a function $f: [a,b] \longrightarrow \mathbb{R}$ is continuous on a closed interval $[a,b]$ and differentiable on the open interval $(a, b)$, there exists a point $\zeta \in (a, b)$ such that 
    \[f^\prime (\zeta) = \frac{f(b) - f(a)}{b - a} \iff f(b) - f(a) = f^\prime (\zeta) (b-a)\]
    Geometrically, this means that there exists a tangent line somewhere at $\zeta \in (a, b)$ that is parallel the secant line connecting the two points $\big(a, f(a)\big)$ and $\big( b, f(b)\big)$. 
    \begin{center}
        \includegraphics[scale=0.25]{img/Analysis_Mean_Value_Theorem_Diagram.PNG}
    \end{center}
    \end{theorem}

    Some remarks: 
    \begin{enumerate}
      \item Physically, if $x$ is interpreted as time and $f(b) - f(a)$ as the amount of displacement over the time $b-a$ of a particle moving along the line, this theorem says that the velocity $f^\prime (x)$ of the particle at some time $\zeta \in (a, b)$ is such that if the particle had moved with constant velocity $f^\prime (\zeta)$ over the whole time interval, it would have been displaced by the same amount $f(b) - f(a)$. We call $f^\prime (\zeta)$ the \textbf{average velocity} over the time interval $[a, b]$. 
      \item Note that the Mean Value Theorem is important in that it connects the increment of a function over a finite interval with the derivative of the function on that interval. Up to now, we have characterized only the local (infinitesimal) increment of a function in terms of the derivative or differential at a given point. MVT connects the increment of a function over a \textbf{finite} interval with the derivative of the function. 
    \end{enumerate}

    The MVT actually leads to multiple useful corollaries. 

    \begin{corollary}[Derivative of a Monotonic Function]
    If the derivative of a function is nonnegative (resp. positive) at every point of an open interval, then the function is nondecreasing (resp. increasing) on that interval. 
    \end{corollary}
    \begin{proof}
    If $x_1 < x_2$ are two points of the interval, then the MVT
    \[f(x_2) - f(x_1) = f^\prime (\zeta) (x_2 - x_1)\]
    shows that the sign of the left hand side must equal that of the right. 
    \end{proof}

    \begin{corollary}[Derivative of a Constant Function]
    A function that is continuous on a closed interval $[a,b]$ is constant on it if and only if its derivative equals $0$ at every point of the interval $[a,b]$ or the open interval $(a, b)$. 

    Therefore, if the derivatives $f_1^\prime (x)$ and $f_2^\prime (x)$ of two functions $f_1 (x)$ and $f_2 (x)$ are equal on some interval (that is, $f_1^\prime (x) = f_2^\prime (x)$ on the interval), then the difference
    \[(f_1 - f_2) (x) = f_1 (x) - f_2 (x)\]
    is constant. 
    \end{corollary}
    \begin{proof}
    Given constant function $f$, the MVT equation 
    \[0 = f(x_2) - f(x_1) = f^\prime (\zeta) (x_2 - x_1)\]
    implies that $f^\prime (\zeta) = 0$ for all $x_1, x_2 \in E$. It follows that by the arithmetic properties of the derivative, given two functions $f_1, f_2$ with the same derivative on an interval, the derivative of their difference $(f_1 - f_2)^\prime = 0$, and therefore must be constant on that interval. 
    \end{proof}

    The following proposition is a useful generalization of Lagrange's theorem. 
    \begin{theorem}[Cauchy's Finite-Increment Theorem]
    Let $x = x(t)$ and $y = y(t)$ be functions that are continuous on a closed interval $[\alpha, \beta]$ and differentiable on the open interval $(\alpha, \beta)$. Then, there exists a point $\tau \in [\alpha, \beta]$ such that
    \[x^\prime (\tau) \big( y(\beta) - y (\alpha)\big) = y^\prime (\tau) \big( x(\beta) - x(\alpha)\big)\]
    If in addition $x^\prime (t) \neq 0$ for each $t \in (\alpha, \beta)$, then $x(\alpha) \neq x(\beta)$ and we have the equality 
    \[\frac{y(\beta) - y(\alpha)}{x(\beta) - x(\alpha)} = \frac{y^\prime (\tau)}{x^\prime (\tau)}\]
    \end{theorem}

    \subsubsection{Taylor's Formula}
    From the following results one may deduce that the more derivatives of two functions coincide (including the derivative of the $0$th order) at a point, the better these functions approximate each other in a neighborhood of that point. Using Leibniz's rule, approximations up to a certain degree at a point can be expressed as a polynomial 
    \[P_n (x_0; x) = P_n (x_0) + \frac{P_n^\prime (x_0)}{1!} (x-x_0) + ... + \frac{P_n^{(n)} (x_0)}{n!} (x-x_0)^n\]
    where each coefficient of the polynomial 

    \begin{definition}[Taylor Polynomial]
      If a function $f:E \longrightarrow \mathbb{R}$ has derivatives of all orders $n \in \mathbb{N}$ at a point $x_0$, the unique series
      \[P_n (x_0; x) = f(x_0) + \frac{f^\prime (x_0)}{1!} (x-x_0) + ... + \frac{f^{(n)} (x_0)}{n!} (x-x_0)^n\]
      is the \textbf{Taylor polynomial of order $n$ of $f(x)$ at $x_0$}. We can see that the derivatives of $f$ and $P_n$ coincide up to order $n$. 
    \end{definition}

    \begin{definition}[Analytic Functions]
      We cannot assume that the Taylor series of an infinitely differentiable function converges to the function $f$ within a neighborhood $U(x_0)$, nor can we assume that it converges at all! These types of "nice" functions that have a Taylor approximation within the neighborhood of $x_0$ are called \textbf{analytic functions} and can be written in the form 
      \[f(x) =  f(x_0) + \frac{f^\prime (x_0)}{1!} (x-x_0) + ... + \frac{f^{(n)} (x_0)}{n!} (x-x_0)^n + r_n (x_0; x)\]
      where $r$ is called the \textbf{remainder term}. 
    \end{definition}

    \begin{example}[Infinitely Differentiable, Non-Analytic Function]
      A example of a non-analytic function is
      \[f(x) = \begin{cases}
      e^{-1/x^2} & \text{ if } x \neq 0 \\
      0 & \text{ if } x = 0
      \end{cases}\]
      which looks like
      \begin{center}
          \includegraphics[scale=0.25]{img/Infinitely_Differentiable_Non_Analytic_Function.PNG}
      \end{center}
      One can verify that the derivative $f^{(k)} (0) = 0$ for all $k$ and hence the Taylor series is identically equal to $0$, while $f(x) \neq 0$ if $x \neq 0$. 
    \end{example}

    The relationship between these different conditions is nicely summarized in the figure. 
    \begin{center}
    \begin{tikzpicture}
        \draw (-7.5,0) rectangle (7.5, 4);
        \draw[fill=lightgray] (-6.5, 0.5) rectangle (6.5, 3);
        \draw[fill=white] (-5.5, 1) rectangle (5.5, 2);
        \node[above] at (0, 1) {Taylor series converges to $f$ at $x_0 \iff f$ is analytic};
        \node[above] at (0, 2) {Taylor series converges at $x_0$};
        \node[above] at (0, 3) {$f$ infinitely differentiable at $x_0 \iff $ Taylor series of $f$ exists at $x_0$};
    \end{tikzpicture}
    \end{center}

    The following lemma proves why Taylor Polynomials are considered a "good" approximations to analytic functions. 

    \begin{lemma}[Infinitesimality of Functions with Vanishing Derivative up to Order $n$]
      Given a function $\varphi: E \longrightarrow \mathbb{R}$ defined on a closed interval $E$ with endpoint $x_0$, let its derivatives vanish up to order $n$ at $x_0$. That is
      \[\varphi(x_0) = \varphi^\prime (x_0) = \ldots = \varphi^{(n)} (x_0) = 0\]
      Then, $\varphi = o\big((x - x_0)^n\big)$ as $x \rightarrow x_0$. 
    \end{lemma}
    \begin{proof}
    We prove by induction. For $n = 1$, the definition of differentiability states that 
    \[\varphi(x) = \varphi^(x_0) + \varphi^\prime (x - x_0) + o(x - x_0) \text{ as } x \rightarrow x_0\]
    and so we have proved that 
    \[\varphi(x_0) = \varphi^\prime (x_0) = 0 \implies \varphi(x) = o(x - x_0) \text{ as } x \rightarrow x_0\]
    Now, suppose this assertion has been proved for order $n = k - 1 \geq 1$. That is, we have shown that 
    \[\varphi(x_0) = \ldots = \varphi^{(k-1)}(x_0) = 0 \implies \varphi= o\big((x - x_0)^{k-1}\big) \text{ as } x \rightarrow x_0\]
    Then we must show that this is valid for order $n = k \geq 2$. Assume that 
    \[\varphi(x_0) = \varphi^\prime (x_0) = \ldots = \varphi^{(k)} (x_0) = 0\]
    We can see that this is equivalent to
    \[(\varphi^\prime)^\prime (x_0) = (\varphi^\prime)^{(2)} (x_0) = \ldots = (\varphi^\prime)^{(k-1)} = 0\]
    and therefore by the induction assumption, we have
    \[\varphi^\prime = o\big( (x - x_0)^{k-1}\big) \text{ as } x \rightarrow x_0\]
    which means that we can put it in form 
    \[\varphi(x) = \alpha (x) (x - x_0)^{k-1} \text{ so that } \lim_{x \rightarrow x_0} \varphi(x) = \lim_{x \rightarrow x_0} \alpha(x) = 0 \]

    From the mean value theorem and substituting what we have above, we get 
    \begin{align*}
        \varphi(x) = \varphi(x) - \varphi(x_0) & = \varphi^\prime(\zeta) (x - x_0) \\
        & = \varphi (\zeta) (\zeta - x_0)^{k-1} (x - x_0)
    \end{align*}
    where $\zeta \in (x_0, x)$. However, this implies that $|\zeta - x_0| < |x - x_0|$, and thus, as $x \rightarrow x_0$, $\zeta \rightarrow x_0$, which then makes $\alpha(\zeta) \rightarrow 0$. Since
    \[|\varphi (x)| \leq |\alpha(\zeta)| |x - x_0|^{k-1} |x - x_0| = |\alpha(\zeta)| |x - x_0|^k\]
    This means that $\varphi(x)$ is bounded by function $|\alpha(\zeta)| |x - x_0|^k$, which is $o\big((x-x_0)^k\big)$, and so 
    \[\varphi = o\big( (x - x_0)^k \big) \text{ as } x \rightarrow x_0\]
    By induction, this works for all orders $n$. 
    \end{proof}

    \begin{theorem}[Peano's Form of the Remainder]
    Given analytic function $f: E \longrightarrow \mathbb{R}$, a point $x_0 \in E$, and its $n$th order Taylor polynomial $P_n (x_0; x)$ around $x_0$, $P_n$ is a "good" approximation of $f$ in the fact that its error term is $o\big((x - x_0)^n\big)$. That is, 
    \[f(x) = P_n (x_0; x) + o\big((x - x_0)^n \big) \text{ as } x \rightarrow x_0\]
    This equation where $r_n (x; x_0) = o\big((x - x_0)^n\big)$ is called the \textbf{Peano's form of the remainder}. 
    \end{theorem}
    \begin{proof}
    Since the Taylor polynomial $P_n (x_0; x)$ is constructed from the requirement that its derivatives up to order $n$ inclusive must coincide with the corresponding derivatives of $f$ at $x_0$, it follows that
    \[r_n (x_0; x_0) \equiv f^{(k)} (x_0) - P_n^{(k)} (x_0; x_0) = 0 \text{ for } k = 0, 1, \ldots, n\]
    Using the previous lemma, a this means that $r_n (x; x_0) = o\big((x - x_0)^n\big)$ as $x \rightarrow x_0$. 
    \end{proof}

    \begin{theorem}[Lagrange Form of the Remainder]
    If $f: E \longrightarrow \mathbb{R}$ has derivatives of order $n+1$ on the open interval with endpoints $x_0$ and $x$, then 
    \[f(x) = f(x_0) + \frac{f^\prime (x_0)}{1!} (x - x_0) + \ldots + \frac{f^{(n)}(x_0)}{n!} (x - x_0)^n + r_n (x; x_0)\]
    where 
    \[r_n (x; x_0) = \frac{f^{(n+1)} (\zeta)}{(n+1)!} (x - x_0)^{n+1}\]
    This form is called \textbf{Taylor's formula with the Lagrange form of the remainder}. Furthermore, this form says that if $f^{(n+1)} (x)$ is bounded in a neighborhood of $x_0$, it also implies the formula
    \[f(x) = f(x_0) + \frac{f^\prime (x_0)}{1!} (x - x_0) + \ldots + \frac{f^{(n)} (x_0)}{n!} (x - x_0)^n + O\big( (x - x_0)^{n+1} \big)\]
    Therefore, we can use this boundedness of $f^{(n+1)}$ to find the maximum error bound 
    \[|r_n (x; x_0)|\]
    of $P_n (x; x_0)$. 
    \end{theorem}
    \begin{proof}
    It is a direct result from the lemma. This is actually a generalization of the mean value theorem but for higher orders. 
    \end{proof}

    \begin{corollary}[Table of Asymptotic Formulas for Elementary Functions]
    We write the Maclaurin series (Taylor series around $x = 0$) for elementary functions. Note that these error terms are $O(x^{n+1})$ (bounded compared to $x^{n+1}$) and $o(x^n)$ (infinitesimal compared to $x^n$). 
    \begin{align*}
        e^x & = 1 + \frac{1}{1!} x + \frac{1}{2!} x^2 + \ldots + \frac{1}{n!} x^n + O(x^{n+1}) \\
        \cos{x} & = 1 - \frac{1}{2!} x^2 + \frac{1}{4!}x^4 - \ldots + \frac{(-1)^n}{(2n)!} x^{2n} + O(x^{2n+2}) \\
        \sin{x} & = x - \frac{1}{3!} x^3 + \frac{1}{5!}x^5 - \ldots + \frac{(-1)^n}{(2n+1)!} x^{2n+1} + O(x^{2n+3}) \\
        \cosh{x} & = 1 + \frac{1}{2!} x^2 + \frac{1}{4!} x^4 + \ldots + \frac{1}{(2n)!} x^{2n} + O(x^{2n+2}) \\
        \sinh{x} & = x + \frac{1}{3!} x^3 + \frac{1}{5!} x^5 + \ldots + \frac{1}{(2n+1)!} x^{2n+1} + O(x^{2n+3}) \\
        \ln{(1+x)} & = x - \frac{1}{2}x^2 + \frac{1}{3} x^3 - \ldots + \frac{(-1)^n}{n} x^n + O(x^{n+1}) \\
        (1 + x)^\alpha & = 1 + \frac{\alpha}{1!} x + \frac{\alpha(\alpha-1)}{2!} x^2 + \ldots + \frac{\alpha (\alpha-1) \ldots (\alpha - n + 1)}{n!} x^n + O(x^{n+1})
    \end{align*}
    \end{corollary}

  \subsection{The Study of Functions using Differential Calculus}

    \subsubsection{Conditions for Monotonicity of Functions}
    We can now connect the concepts of derivatives and monotonicity. 

    \begin{theorem}[Derivative $\implies$ Monotonicity]
    Given function $f: E \longrightarrow \mathbb{R}$ that is differentiable on an open interval $(a, b) = E$, 
    \begin{align*}
        f^\prime (x) > 0 & \implies f \text{ is increasing} \\
        f^\prime (x) \geq 0 & \iff f \text{ is nondecreasing} \\
        f^\prime (x) \equiv 0 & \iff f \text{ is constant} \\
        f^\prime (x) \leq 0 & \iff \text{ is nonincreasing} \\
        f^\prime (x) < 0 & \implies f \text{ is decreasing} 
    \end{align*}
    Note that if $f$ is strictly increasing (resp. decreasing), we cannot determine that $f^\prime(x) \geq 0$ (resp. $f^\prime (x) \leq 0$). For example, take the function $f(x) = x^3$, which is strictly increasing, but has derivative $f^\prime (0) = 0$ at $x = 0$. 
    \begin{center}
        \includegraphics[scale=0.25]{img/Monotonicity_Counterexample_x3.PNG}
    \end{center}
    It is clearly strictly increasing within a neighborhood $U(0)$, so we can see that
    \begin{align*}
        f \text{ is increasing} & \implies f^\prime (x) \geq 0 \\
        f \text{ is decreasing} & \implies f^\prime (x) \leq 0
    \end{align*}
    \end{theorem}


    \subsubsection{Conditions for Extrema of Functions}
    Similarly, we can connect the concepts of extrema and derivatives. 

    \begin{theorem}[First Derivative Test]
    Let function $f: E \longrightarrow \mathbb{R}$ be defined in a neighborhood $U(x_0)$ of point $x_0$, which is continuous at $x_0$ and differentiable in $\mathring{U}(x_0)$, a deleted neighborhood of $x_0$. (Note that this is broader hypothesis than just assuming that $f$ be differentiable at $x_0$.) Let
    \[\mathring{U}^- (x_0) \equiv \{x \in U(x_0) \;|\; x < x_0\}, \;\; \mathring{U}^+ (x_0) \equiv \{x \in U(x_0) \;|\; x > x_0\}\]
    That is, $\mathring{U}^- (x_0)$ is the left portion of $\mathring{U}(x_0)$ and $\mathring{U}^+ (x_0)$ is the right portion of $\mathring{U}(x_0)$. Then, 
    \begin{enumerate}
      \item $(x_0, f(x_0))$ is strict local minimum if $f^\prime(x) < 0$ in $\mathring{U}^- (x_0)$ and $f^\prime (x) > 0$ in $\mathring{U}^+ (x_0)$. 
      \begin{center}
          \includegraphics[scale=0.25]{img/Strict_Local_minimum.PNG}
      \end{center}
      \item $(x_0, f(x_0))$ is strict local maximum if $f^\prime(x) > 0$ in $\mathring{U}^- (x_0)$ and $f^\prime (x) < 0$ in $\mathring{U}^+ (x_0)$. 
      \begin{center}
          \includegraphics[scale=0.25]{img/Strict_Local_Maximum.PNG}
      \end{center}
      \item $(x_0, f(x_0))$ has no extremum at $x_0$ if $f^\prime (x) > 0$ in both $\mathring{U}^- (x_0), \mathring{U}^+ (x_0)$, or if $f^\prime(x)< 0$ in both $\mathring{U}^- (x_0), \mathring{U}^+ (x_0)$. 
      \begin{center}
          \includegraphics[scale=0.25]{img/No_Extremum.PNG}
      \end{center}
    \end{enumerate}
    \end{theorem}

    Note that if there is a discontinuity at a point $x_0$, then this theorem does not apply. For example, $(x_0, f(x_0))$ in the graph below is a local minimum, even though the derivatives to the left of $x_0$ are positive and those to the right of $x_0$ are negative (within neighborhood $U(x_0)$). Similarly, $(x_0, g(x_0))$ is a local maximum, even though the derivative to the left and to the right of $x_0$ are both positive. 
    \begin{center}
        \includegraphics[scale=0.3]{img/Theorem_not_apply_if_Discontinuity.PNG}
    \end{center}

    \begin{proposition}[2nd, $n$th Derivative Test]
    Let function $f: E \longrightarrow \mathbb{R}$ be defined on a neighborhood $U(x_0)$ of $x_0$ has derivatives of order up to $n$ inclusive at $x_0$. If its derivatives up to the $(n-1)$th order vanishes 
    \[f^\prime (x_0) = f^{\prime\prime} (x_0) ... = f^{(n-1)} (x_0) = 0\]
    but the $n$th derivative at $x_0$ does \textbf{not} vanish
    \[f^{(n)} (x_0) \neq 0\]
    then 
    \begin{enumerate}
      \item $n$ is odd $\implies$ there is no local extremum at $x_0$ 
      \item $n$ is even $\implies$ there is a local extremum at $x_0$
      \begin{enumerate}
        \item $f^{(n)} (x_0) > 0 \implies$ it is a strict local minimum
        \item $f^{(n)} (x_0) < 0 \implies$ it is a strict local maximum
      \end{enumerate}
    \end{enumerate}
    \end{proposition}

    \subsubsection{Important Algebraic Inequalities}

    We also introduce various inequalities that may be useful for producing future results. The following lemmas can be proved with elementary algebra. 

    \begin{lemma}[Young's Inequalities]
      If $a>0$ and $b>0$, and the numbers $p$ and $p$ are such that $p \neq 0, 1$ and $q \neq 0, 1$, and $\frac{1}{p} + \frac{1}{q} = 1$, then 
      \begin{align*}
          a^{\frac{1}{p}} b^{\frac{1}{q}} \leq \frac{1}{p} a + \frac{1}{q} b \text{  if } p > 1 \\
          a^{\frac{1}{p}} b^{\frac{1}{q}} \geq \frac{1}{p} a + \frac{1}{q} b \text{  if } p < 1
      \end{align*}
      and equality holds in both cases if and only if $a = b$. 
    \end{lemma}

    \begin{lemma}[Holder's Inequalities]
      Let $x_i \geq 0, y_i \geq 0$ for $i = 1, 2, ..., n$, and let $\frac{1}{p} + \frac{1}{q} = 1$. Then, 
      \begin{align*}
          &\sum_{i=1}^n x_i y_i \leq \bigg( \sum_{i=1} x_i^p \bigg)^{\frac{1}{p}} \, \bigg( \sum_{i=1} y_i^q \bigg)^{\frac{1}{q}} \text{  for } p > 1 \\
          &\sum_{i=1}^n x_i y_i \geq \bigg( \sum_{i=1} x_i^p \bigg)^{\frac{1}{p}} \, \bigg( \sum_{i=1} y_i^q \bigg)^{\frac{1}{q}} \text{  for } p < 1, p \neq 0
      \end{align*}
    \end{lemma}

    \begin{lemma}[Minkowski's Inequalities]
      Let $x_i \geq 0, y_i \geq 0$ for $i = 1, 2, ... ,n$. Then, 
      \begin{align*}
          \bigg( \sum_{i=1}^n (x_i + y_i)^p \bigg)^{\frac{1}{p}} & \leq \bigg( \sum_{i=1}^n x_i^p \bigg)^\frac{1}{p} + \bigg( \sum_{i=1}^n y_i^p \bigg)^{\frac{1}{p}} \text{  when } p > 1 \\
          \bigg( \sum_{i=1}^n (x_i + y_i)^p \bigg)^{\frac{1}{p}} & \geq \bigg( \sum_{i=1}^n x_i^p \bigg)^\frac{1}{p} + \bigg( \sum_{i=1}^n y_i^p \bigg)^{\frac{1}{p}} \text{  when } p < 1, p \neq 0
      \end{align*}
    \end{lemma}

    \subsubsection{Conditions for a Function to be Convex}
    \begin{definition}[Convex, Concave Functions]
      A function $f: (a, b) \longrightarrow \mathbb{R}$ defined on an open interval $(a, b) \subset \mathbb{R}$ is \textbf{convex} if the inequality
      \[f( \alpha_1 x_1 + \alpha_2 x_2) \leq \alpha_1 f(x_1) + \alpha_2 f(x_2)\]
      holds and \textbf{concave}, or \textbf{convex upward}, if the inequality 
      \[f( \alpha_1 x_1 + \alpha_2 x_2) \geq \alpha_1 f(x_1) + \alpha_2 f(x_2)\]
      holds for all pairs of points $x_1, x_2 \in (a, b)$ and any numbers $\alpha_1, \alpha_2 \geq 0$ such that $\alpha_1 + \alpha_2 = 1$. If this inequality is strict whenever $x_1 \neq x_2$ and $\alpha_1 \alpha_2 \neq 0$, the function is said to be \textbf{strictly convex} and \textbf{strictly concave}, respectively. 

      Visually, this just means that given any two points $a, b$, the graph of a convex function (left) in $(a, b)$ always lies underneath the secant line formed by the two points and the graph of a concave function (right) in $(a, b)$ lies over the secant line formed by the two points. 
      \begin{center}
          \includegraphics[scale=0.25]{img/Convex_Concave_Functions_Secant.PNG}
      \end{center}
      However, this is only a visual aid. In reality, it is actually not only the secant line formed by the two endpoints, but every pairs of points within that interval. For example, even though the secant line $l$ formed by the endpoints $a, b$ is above the whole graph in $(a, b)$, $f$ is not convex over $(a, b)$ since the secant line formed by points $\alpha, \beta$ do not lie completely underneath $f$. 
      \begin{center}
          \includegraphics[scale=0.25]{img/Multiple_Secant_Lines_Convex_Clarification.PNG}
      \end{center}
    \end{definition}

    The following is also another equivalent condition for a function to be convex over $(a, b)$. 

    \begin{proposition}
    A function $f: (a, b) \longrightarrow \mathbb{R}$ that is differentiable on the open interval $(a, b)$ is convex on $(a, b)$ if and only if its graph contains no points below any tangent drawn to it.
    \begin{center}
        \includegraphics[scale=0.25]{img/Convex_Function_Over_Tangent_Line.PNG}
    \end{center}
    \end{proposition}

    \begin{theorem}[2nd Derivatives of Convex Functions]
    Given a function $f: (a, b) \longrightarrow \mathbb{R}$ that is differentiable in its domain, 
    \begin{enumerate}
      \item $f$ is convex $\iff f^\prime$ is nondecreasing on $(a, b) \iff f^{\prime\prime} \geq 0$ on $(a, b)$ 
      \item $f$ is strictly convex $\iff f^\prime$ is increasing on $(a, b) \iff f^{\prime\prime} > 0$ on $(a, b)$ 
      \item $f$ is concave $\iff f^\prime$ is nonincreasing on $(a, b) \iff f^{\prime\prime} \leq 0$ on $(a, b)$ 
      \item $f$ is strictly concave $\iff f^\prime$ is decreasing on $(a, b) \iff f^{\prime\prime} < 0$ on $(a, b)$ 
    \end{enumerate}
    \end{theorem}

    \begin{definition}[Inflection Point]
      Let $f: E \longrightarrow \mathbb{R}$ be a function defined and differentiable on a neighborhood $U(x_0)$. If the function is convex downward (resp. upward) on the set $\mathring{U}^- (x_0) = \{x \in U(x_0) \;|\; x < x_0\}$ and convex upward (resp. downward) on $\mathring{U}^+ (x_0) = \{x \in U(x_0)\;|\; x > x_0\}$, then the point 
      \[\big( x_0, f(x_0) \big)\]
      is called a \textbf{inflection point of the graph}. 
      \begin{center}
          \includegraphics[scale=0.25]{img/Inflection_Point_Analysis.PNG}
      \end{center}
    \end{definition}

    \begin{proposition}[Jensen's Inequality]
    If $f: (a, b) \longrightarrow \mathbb{R}$ is a convex function, $x_1, ..., x_n$ are points of $(a, b)$, and $\alpha_1, ..., \alpha_n$ are nonnegative numbers such that $\alpha_1 + ... + \alpha_n = 1$, then 
    \[f(\alpha_1 x_1 + ... + \alpha_n x_n) \leq \alpha_1 f(x_1) + ... + \alpha_n f(x_n)\]
    \end{proposition}

    \subsubsection{L'Hopital's Rule}

    \begin{theorem}[L'Hopital's Rule]
    Let $c$ be an extended real number (i.e. $c \in \mathbb{R} \cup \{+\infty, -\infty\}$ and let $(a, b)$ be an open interval containing $c$ (for a two-sided limit) or an open interval with endpoint $c$ (for a one-sided limit, or a limit at infinity if $c$ is infinite). Assume that $f$ and $g$ are assumed to be differentiable on $(a, b) \setminus c$, and additionally $g^\prime (x) \neq 0$ on $(a, b) \setminus c$. If either 
    \[\lim_{x \rightarrow c} f(x) = \lim_{x \rightarrow c} g(x) = 0 \text{ or } \lim_{x \rightarrow c} |f(x)| = \lim_{x \rightarrow c} |g(x)| = \infty\]
    then 
    \[\lim_{x \rightarrow c} \frac{f(x)}{g(x)} = \lim_{x \rightarrow c} \frac{f^\prime (x)}{g^\prime (x)}\]
    L'Hopital's rule can be stated colloquially, but not quite accurately, as follows: \textbf{The limit of a ratio of functions equals the limit of the ratio of their derivatives if their derivatives exist.}
    \end{theorem}

    \begin{example}
      Let $f(x) = \sin{x}$ and $g(x) = -0.5x$. Then, the function 
      \[h(x) = \frac{f(x)}{g(x)} = \frac{\sin{x}}{-0.5x}\]
      is clearly undefined at $x = 0$. 
      \begin{center}
          \includegraphics[scale=0.25]{img/LHopital_Example_1.PNG}
      \end{center}
      However, we can solve the limit using L'Hopital's rule to get
      \[\lim_{x \rightarrow 0} \frac{\sin{x}}{-0.5x} = \lim_{x \rightarrow 0} \frac{\cos{x}}{-0.5} = -2\]
      Therefore, $h: \mathbb{R} \setminus 0 \longrightarrow \mathbb{R}$ can be completed to continuous function on all of $\mathbb{R}$ by defining the extension: 
      \[H(x) \equiv \begin{cases}
      h(x), & x \neq 0 \\
      -2, & x = 0
      \end{cases}\]
      \begin{center}
      \includegraphics[scale=0.25]{img/LHopital_Example_2.PNG}
      \end{center}
    \end{example}

  \subsection{Complex Analysis: An Introduction}

    Just as the equation $x^2 = 2$ has no solutions in the domain $\mathbb{Q}$ of rational numbers, the equation $x^2 = -1$ has no solutions in the domain $\mathbb{R}$ of real numbers. Just as we adjoin the symbol $\sqrt{2}$ as a solution of $x^2 = 2$ and connect it with rational numbers to get new numbers of the form 
    \[r_1 + r_2 \sqrt{2}, \;\;\; r_1, r_2 \in \mathbb{Q}\]
    we introduce the symbol $i$ as a solution of $x^2 = -1$ and attach this number to real numbers. 

    One feature of this enlargement of the field $\mathbb{R}$ of real numbers into the resulting field $\mathbb{C}$ of complex numbers, every algebraic equation with real or complex coefficients now has a solution. 

    \subsubsection[Algebraic Extension of Field R]{Algebraic Extension of Field $\mathbb{R}$}
    We introduce the number $i$, called the \textbf{imaginary unit}, such that $i^2 = -1$. We may multiply real numbers $y$ to $i$ to get $yi$, and we can add real numbers to such numbers, to get numbers of the form 
    \[x + yi, \;\;\; x, y \in \mathbb{R}\]
    We then define all objects of the form $x + iy$ as the \textbf{complex numbers}, with addition defined
    \[(x_1 + i y_1) + (x_2 + i y_2) \equiv (x_1 + x_2) + i (y_1 + y_2)\]
    and multiplication defined
    \[(x_1 + i y_1) \cdot (x_2 + i y_2) \equiv (x_1 x_2 - y_1 y_2) + i (x_1 y_2 + x_2 y_1)\]
    As expected, this makes $+$ and $\cdot$ commutative operations. Furthermore, two complex numbers $z = x_1 + i y_1$ and $w = x_2 + i y_2$ are equal if and only if $x_1 = x_2$ and $y_1 = y_2$. 

    One nontrivial property of field $\mathbb{C}$ is that every element $z \in \mathbb{C}$ has a multiplicative inverse $z^{-1}$. To find this, we must define the following. 

    \begin{definition}[Complex Conjugate]
      Given complex number $z = x + i y$, its \textbf{complex conjugate} is 
      \[\overline{z} = \overline{x + iy} = x - iy\]
      Note that 
      \[z \cdot \overline{z} = x^2 + y^2 \neq 0 \text{ iff } z \neq 0\]
    \end{definition}

    Thus, given $z$, 
    \[z^{-1} = \frac{1}{z \cdot \overline{z}} \cdot \overline{z} \iff (x + yi)^{-1} = \frac{x}{x^2 + y^2} - i \frac{y}{x^2 + y^2}\]

    \subsubsection[Geometric Interpretation of C]{Geometric Interpretation of $\mathbb{C}$}
    Once the algebraic operations $+$ and $\cdot$ has been introduced, the symbol $i$ is no longer needed. That is, we can define a new set $\mathbb{R}^2 = \mathbb{R} \times \mathbb{R}$ with the operations $+_\mathbb{R}, \cdot_\mathbb{R} : \mathbb{R}^2 \times \mathbb{R}^2 \longrightarrow \mathbb{R}^2$ defined
    \begin{align*}
        (x_1, y_1) +_\mathbb{R} (x_2, y_2) & \equiv (x_1 + x_2, y_1 + y_2) \\
        (x_1, y_1) \cdot_\mathbb{R} (x_2, y_2) & \equiv (x_1 x_2 - y_1 y_2, x_1 y_2 + x_2 y_1)
    \end{align*}
    We can check that this new set $(\mathbb{R}^2, +_\mathbb{R}, \cdot_{\mathbb{R}})$ is isomorphic to $(\mathbb{C}, +, \cdot)$ as fields, and therefore one can identify complex numbers with vectors $z = (x, y)$ of the plane $\mathbb{R}^2$, where $x = \text{Re}\,z$ is called the \textbf{real part} and $y = \text{Im}\,z$ is called the \textbf{imaginary part}. 

    \begin{definition}[Norm, Metric of $\mathbb{C}$]
      Moreover, the isomorphism
      \[\gamma: \mathbb{C} \longrightarrow \mathbb{R}^2, \;\; \gamma(x + yi) = (x, y)\]
      induces additional structures on $\mathbb{C}$, such as the norm and metric. 
      \begin{enumerate}
        \item The norm of $z = x + iy \in \mathbb{C}$ is defined as the norm of $\gamma(z) = (x, y) \in \mathbb{R}^2$. That is, 
        \[|z| = |x + yi| = |(x, y)| = \sqrt{x^2 + y^2}\]
        Or more simply, 
        \[|z| = z \cdot \overline{z}\]
        \item The metric of two complex numbers $z_1, z_2 \in \mathbb{C}$ is defined
        \[|z_1 - z_2| = |(x_1, y_1) - (x_2, y_2)| = |(x_1 - x_2, y_1 - y_2)| = \sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}\]
        Or more simply, 
        \[|z_1 - z_2| = (z_1 - z_2) \cdot \overline{(z_1 - z_2)}\]
      \end{enumerate}
    \end{definition}

    \begin{definition}[Polar Coordinates of $\mathbb{C}$]
      Given the basis transformation of polar coordinates $(r, \varphi) \mapsto p(r, \varphi) = (x, y)$ where 
      \[p\begin{pmatrix} r \\ \varphi \end{pmatrix} = \begin{pmatrix}
      r \cos{\varphi} \\ r \sin{\varphi} 
      \end{pmatrix} = \begin{pmatrix} x \\ y \end{pmatrix}\]
      the isomorphism $\mathbb{C} \simeq \mathbb{R}^2$ induces a similar polar transformation in $\mathbb{C}$
      \[\rho = \gamma^{-1} \circ p \circ \gamma: \mathbb{C}_{(r, \theta)} \longrightarrow \mathbb{C}_{(x, y)}, \;\;\rho(r + \theta i) = r \cos{\theta} + r \sin{\theta} i = x + y i\]
      as shown in the commutative diagram. 
      \[\begin{tikzcd}
          \mathbb{C}_{(r, \theta)} \arrow{d}{\gamma} \arrow{r}{\rho} & \mathbb{C}_{(x, y)} \arrow{d}{\gamma} \\
          \mathbb{R}^2_{(r, \theta)} \arrow{r}{p} & \mathbb{R}^2_{(x, y)}
        \end{tikzcd}\]
      Therefore, we can write 
      \[z = r ( \cos{\varphi} + i \sin{\varphi})\]
      where $r = |z|$ is called the \textbf{magnitude} of $z$, and $\varphi = \text{Arg}\,z$ is called the \textbf{argument} of $z$. 
    \end{definition}

    \begin{lemma}[Multiplication of Complex Numbers in Polar Form]
      It turns out that multiplication is a lot easier in polar coordinates than in rectangular ones: 
      \begin{align*}
          z_1 \cdot z_2 & = (r_1 \cos{\varphi_1} + i r_1 \sin{\varphi_1})(r_2 \cos{\varphi_2} + i r_2 \sin{\varphi_2}) \\
          & = \ldots \\
          & = r_1 r_2 \big(\cos{(\varphi_1 + \varphi_2)} + i \sin{(\varphi_1 + \varphi_2)}
      \end{align*}
      \begin{center}
          \includegraphics[scale=0.25]{img/Multiplication_Complex_Polar_Form.jpg}
      \end{center}
    \end{lemma}

    \begin{theorem}[De Moivre's Formula]
    By induction using the previous lemma, we get 
    \[z = r ( \cos{\varphi} + i \sin{\varphi}) \implies z^n = r^n (\cos{n\varphi} + i \sin{n \varphi})\]
    \end{theorem}

    \begin{corollary}[Roots of Unity]
    The $n$ complex solutions of the equation 
    \[z^n = a\]
    where $a = \rho (\cos{\psi} + i \sin{\psi})$ is 
    \[z_k = \sqrt[n]{\rho} \bigg( \cos\Big(\frac{\psi + 2\pi k}{n} \Big) + i \sin\Big(\frac{\psi + 2\pi k}{n}\Big) \bigg), \;\;\;\;\; k = 0, 1, 2, \ldots, n-1\]
    Moreover, if $a = 1$, then the $n$ complex solutions are called the \textbf{$n$th roots of unity}, defined
    \[z_k = \cos\Big(\frac{2\pi k}{n}\Big) + i \sin\Big(\frac{2\pi k}{n}\Big), \;\;\;\;\; k = 0, 1, 2, \ldots, n-1\]
    which shows that the $n$th roots of unity are at the vertices of a regular $n$-sided polygon inscribed in the unit circle, with one vertex at $1$, within the complex plane. The $5$th and $6$th roots of unity are shown below. 
    \begin{center}
        \includegraphics[scale=0.27]{img/5th_6th_Roots_of_Unity.PNG}
    \end{center}
    \end{corollary}

    Finally, we can visualize certain transformations in $\mathbb{C}$. For a fixed $b \in \mathbb{C}$, the sum $z + b$ cam be interpreted as the mapping of $\mathbb{C}$ onto itself given by the formula 
    \[z \mapsto z + b\]
    This mapping is a translation of the plane by the vector $b$. 
    \begin{center}
        \includegraphics[scale=0.25]{img/Translation_in_Complex_Plane.jpg}
    \end{center}
    Visualizing multiplication is a bit harder. Given a 
    \[a = |a| (\cos{\varphi} + i \sin{\varphi}) \neq 0\]
    the product $az$ can be interpreted as the mapping of $\mathbb{C}$ onto itself given by the formula
    \[z \mapsto az\]
    which is the composition of a dilation by a factor of $|a|$ and a rotation through the angle $\varphi \in \text{Arg}\,a$. 
    \begin{center}
        \includegraphics[scale=0.3]{img/Multiplication_in_Complex_Plane.jpg}
    \end{center}

    \subsubsection[Sequences and Series in C]{Sequences and Series in $\mathbb{C}$}
    Our previous construction of a metric within $\mathbb{C}$ enables to define the $\epsilon$-neighborhood of a number $z_0 \in \mathbb{C}$ as the set
    \[U_\epsilon (z_0) \equiv \{z \in \mathbb{C}\;|\; |z - z_0| < \epsilon\}\]
    which can be visualized as an open disk of radius $\epsilon$ in $\mathbb{R}^2$ centered at point $(x_0, y_0)$ if $z_0 = x_0 + i y_0$. 
    \begin{center}
        \includegraphics[scale=0.25]{img/Epsilon_Neighborhood_in_C.jpg}
    \end{center}

    \begin{definition}[Convergence of a Sequence in $\mathbb{C}$]
      A sequence $\{z_n\}$ of complex numbers \textbf{converges} to $z_0 \in \mathbb{C}$ if and only if 
      \[\lim_{n \rightarrow \infty} |z_n - z_0| = 0\]
      It is clear from the inequality
      \[\max\{|x_n - x_0|, |y_n - y_0|\} \leq |z_n - z_0| \leq |x_n - x_0| + |y_n - y_0|\]
      that a sequence of complex numbers converges if and only if the sequences of its real and imaginary parts of the terms of the sequence both converge. That is, 
      \[\{z_n\} \text{ converges} \iff \{\text{Re}\,z\} \text{ and } \{\text{Im}\,z\} \text{ converges}\]
    \end{definition}

    \begin{lemma}[Convergence of Cauchy Sequences over $\mathbb{C}$]
      A sequence of complex numbers $\{z_n\}$ is called a \textbf{Cauchy sequence} if for every $\epsilon>0$ there exists an index $N \in \mathbb{N}$ such that
      \[|z_n - z_m|<\epsilon \text{ for all } n, m > N\]
      It is also clear that 
      \[\{z_n\} \text{ is Cauchy} \iff \{\text{Re}\,z\} \text{ and } \{\text{Im}\,z\} \text{ is Cauchy}\]
      and using the Cauchy criterion for sequences of real numbers, we can easily see that a sequence of complex numbers converges if and only if it is a Cauchy sequence. 
    \end{lemma}

    \begin{lemma}[Convergence of Cauchy Series over $\mathbb{C}$]
      Interpreting the sum of a series of complex numbers
      \[z_1 + z_2 + \ldots + z_n + \ldots\]
      as the limit of the sequence its partial sums $\{s_n\}$, where $s_n = z_1 + \ldots z_n$ as $n \rightarrow \infty$, we can see that the series converges if and only if for every $\epsilon > 0$ there exists a $N \in \mathbb{N}$ such that 
      \[|z_m + \ldots + z_n| < \epsilon\]
      for any natural numbers $n \geq m > N$. 
    \end{lemma}

    \begin{definition}[Absolute Convergence of $\mathbb{C}$]
      A series $z_1 + \ldots + z_n + \ldots$ of complex numbers is \textbf{absolutely convergent} if the series
      \[|z_1| + |z_2| + \ldots + |z_n| + \ldots\]
      converges. Clearly, is a series converges absolutely, then it converges due to the inequality
      \[|z_m + \ldots + z_n| \leq |z_m| + \ldots + |z_n|\]
    \end{definition}

    \begin{example}
      The following complex series converges because they converges absolutely. That is, 
      \begin{align*}
          1 + \frac{1}{1!}|z| + \frac{1}{2!}|z^2| + \ldots \text{ converges } \forall \; \mathbb{C} & \implies 1 + \frac{1}{1!}z + \frac{1}{2!}z^2 + \ldots \text{ converges } \forall \; \mathbb{C} \\
          |z| + \frac{1}{3!}|z|^3 + \frac{1}{5!}|z|^5 + \ldots \text{ converges } \forall \; \mathbb{C}  & \implies z - \frac{1}{3!} z^3 + \frac{1}{5!}z^5 + \ldots \text{ converges } \forall \; \mathbb{C} \\
          1 + \frac{1}{2!}|z|^2 + \frac{1}{4!} |z|^4 + \ldots \text{ converges }  \forall \; \mathbb{C} & \implies 1 - \frac{1}{2!}z^2 + \frac{1}{4!} z^4 - \ldots \text{ converges }  \forall \; \mathbb{C} 
      \end{align*}
    \end{example}

    \begin{definition}[Complex Power Series]
      Series of the form 
      \[\sum_{n=0}^\infty c_n (z - z_0)^n = c_0 + c_1 (z - z_0) + \ldots + c_n (z - z_0) + \ldots\]
      are called \textbf{complex power series}, or \textbf{power series over $\mathbb{C}$}. 
    \end{definition}

    But a power series is quite useless unless we know the domain in which is converges (again, note that it is not always guaranteed to converge onto the function $f$ if its power series expansion does converge at all). To develop more sophisticated tests of convergence of a complex power series, we introduce the complex analogue of the root test for real power series. 

    \begin{theorem}[Cauchy-Hadamard Theorem]
    The complex power series 
    \[c_0 + c_1 (z - z_0) + \ldots + c_n (z - z_0) + \ldots\]
    converges inside the disk $|z - z_0| < R$ with center at $z_0$ and radius given by the formula
    \[R = \frac{1}{\varlimsup_{n \rightarrow \infty} \sqrt[n]{|c_n|}} = \frac{1}{\lim_{n \rightarrow \infty} \sup{\sqrt[n]{|c_n|}}}\]
    Where $\varlimsup$ denotes the superior limit. Furthermore, 
    \begin{enumerate}
      \item the power series diverges at any point exterior to the disk. 
      \item the power series converges absolutely at any point interior to the disk. 
      \item the power series is indeterminate at any point on the boundary of the disk. 
    \end{enumerate}
    Note that in the degenerate case when $R = 0$, the series converges only at the point $z = z_0$. 
    \end{theorem}

    \begin{corollary}[Abel's First Theorem on Power Series]
    If the power series 
    \[c_0 + c_1 (z - z_0) + \ldots + c_n (z - z_0) + \ldots\]
    converges at some value $z^*$, then it converges absolutely for any value of $z$ satisfying
    \[|z - z_0| < |z^* - z_0|\]
    The values of $z$ satisfying the inequality above can be intuitively visualized as the following region. 
    \begin{center}
        \includegraphics[scale=0.25]{img/Abels_First_Theorem.PNG}
    \end{center}
    \end{corollary}

    \begin{theorem}[Product of Absolutely Convergent Series]
    Let $a_1 + a_2 + \ldots$ and $b_1 + b_2 + \ldots$ be an absolutely convergent series such that
    \[\sum_{i=1}^\infty a_i = A \text{ and } \sum_{j=1}^\infty b_j = B\]
    Then, the Cauchy product of the two series 
    \[\bigg( \sum_{i=1}^\infty a_i \bigg) \cdot \bigg( \sum_{j=1}^\infty b_j \bigg) = \sum_{k=0}^\infty c_k = A B, \text{ where } c_k = \sum_{l=0}^k a_l b_{k-l}\]

    $a_1 b_1 + a_2 b_2 + \ldots$ is absolutely convergent and 
    \[\sum_{i = 1}^\infty a_i b_i = A B\]
    \end{theorem}
    \begin{proof}
    To be done. 
    \end{proof}

      \begin{example}[Convergence of the Cauchy Product of Absolutely Convergent Complex Series]
      The two series 
      \[\sum_{n = 0}^\infty \frac{1}{n!} a^n \text{ and } \sum_{m = 0}^\infty \frac{1}{m!} b^m\]
      converges absolutely. Therefore, we can see that their Cauchy product can be nicely represented by grouping together all monomials of the form $a^n b^m$ having the same total degree $m + n = k$. 
      \[\bigg( \sum_{n = 0}^\infty \frac{1}{n!} a^n \bigg) \cdot \bigg( \sum_{m = 0}^\infty \frac{1}{m!} b^m \bigg) = \sum_{k=0}^\infty \bigg(\sum_{n+m=k} \frac{1}{n!} a^n \frac{1}{m!} b^m \bigg)\]
      But we can simplify 
      \[\sum_{m + n = k} \frac{1}{n! m!} a^n b^m = \frac{1}{k!} \sum_{n=0}^k \frac{k!}{n! (k-n)!} a^n b^{k-n} = \frac{1}{k!} (a + b)^k\]
      and therefore we find that 
      \[\bigg( \sum_{n = 0}^\infty \frac{1}{n!} a^n \bigg) \cdot \bigg( \sum_{m = 0}^\infty \frac{1}{m!} b^m \bigg) = \sum_{k=0}^\infty \frac{1}{k!} (a + b)^k\]
    \end{example}

      \subsubsection{Euler's Formula}

      \begin{definition}[Complex Taylor Expansions of Transcendental Functions]
        Since we have determined absolute convergence, and therefore convergence, of all these series in all of $\mathbb{C}$, it is natural to extend the definitions of 
        \[\exp, \cos, \sin: \mathbb{R} \longrightarrow \mathbb{R}\]
        to the complex field 
        \[\exp, \cos, \sin: \mathbb{C} \longrightarrow \mathbb{C}\]
        by defining them as 
        \begin{align*}
            e^z & \equiv 1 + \frac{1}{1!}z + \frac{1}{2!} z^2 + \frac{1}{3!} z^3 + \ldots \\
            \cos{z} & \equiv 1 - \frac{1}{2!} z^2 + \frac{1}{4!} z^4 - \frac{1}{6!} z^6 + \ldots \\
            \sin{z} & \equiv z - \frac{1}{3!} z^3 + \frac{1}{5!} z^5 - \frac{1}{7!} z^7 + \ldots
        \end{align*}
        Notice that even in the complex field, $\cos{z}$ is an even function and $\sin{z}$ is an odd function. 
        \begin{align*}
            \cos(-z) & = \cos(z) \\
            \sin(-z) & = -\sin(z)
        \end{align*}
      \end{definition}

      In fact, the last example in the previous subsection just proves the following. 

      \begin{lemma}[Exponential Map as a Group Homomorphism]
        The exponential map $\exp: \mathbb{C} \longrightarrow \mathbb{C}\setminus \{0\}$ satisfies the following
        \[\exp(z_1 + z_2) = \exp(z_1) \cdot \exp (z_2)\]
        That is, $\exp$ is a group homomorphism from $(\mathbb{C}, +)$ to $(\mathbb{C} \setminus \{0\}, \cdot)$. 
      \end{lemma}

      \begin{definition}[Euler's Formula]
        By making the substitution $z = yi$ in the series expansion of $e^z$ (where $y$ is an arbitrary complex number), we get 
        \begin{align*}
            e^{iy} & = 1 + \frac{1}{1!} (iy) + \frac{1}{2!}(iy)^2 + \frac{1}{3!} (iy)^3 + \frac{1}{4!} (iy)^4 + \ldots \\
            & = \bigg(1 - \frac{1}{2} y^2 + \frac{1}{4!} y^4 - \ldots \bigg) + i \bigg(\frac{1}{1!} y - \frac{1}{3!} y^3 + \frac{1}{5!} y^5 - \ldots \bigg)
        \end{align*}
        which brings us the identity
        \[e^{iy} = \cos{y} + i \sin{y}\]
      \end{definition}

      Since $\cos$ is even and $\sin$ is odd, we can add the two identities
      \begin{align*}
          e^{iz} & = \cos{z} + i \sin{z} \\
          e^{-iz} & = \cos{z} - i \sin{z} 
      \end{align*}
      to get 
      \begin{align*}
          \cos{z} & = \frac{1}{2}\big( e^{iz} + e^{-iz} \big) \\
          \sin{z} & = \frac{1}{2i} \big( e^{iz} - e^{-iz} \big)
      \end{align*}
      This gives us a very elegant connection between these three transcendental functions. 

      \begin{definition}[Hyperbolic Functions]
        Likewise, the following series are convergent (since they are absolutely convergent) and therefore we can define the extension of $\cosh$ and $\sinh$ into the complex field as 
        \begin{align*}
            \cosh{z} & \equiv 1 + \frac{1}{2!} z^2 + \frac{1}{4!} z^4 + \frac{1}{6!} z^6 + \ldots \\
            \sinh{z} & \equiv z + \frac{1}{3!} z^3 + \frac{1}{5!} z^5 + \frac{1}{7!} z^7 + \ldots 
        \end{align*}
        The following identities immediately follow
        \begin{align*}
            \cosh{z} & = \frac{1}{2} \big( e^z + e^{-z} \big) \\
            \sinh{z} & = \frac{1}{2} \big( e^{z} - e^{-z}\big) 
        \end{align*}
      \end{definition}

      \begin{lemma}[Trigonometric, Hyperbolic Identities over $\mathbb{C}$]
        Common identities, which are exactly the same as their real analogues, are listed. 
        \begin{enumerate}
          \item $\cos^2{z} + \sin^2 {z} = 1$
          \item $\cosh^2{z} - \sinh^2{z} = 1$ 
          \item $e^{i(z_1 + z_2)} = (\cos{z_1} \cos{z_2} - \sin{z_1} \sin{z_2}) + i (\sin{z_1} \cos{z_2} + \cos{z_1} \sin{z_2})$
          \item $\cos{(z_1 + z_2)} = \cos{z_1} \cos{z_2} - \sin{z_1} \sin{z_2}$
          \item $\sin{(z_1 + z_2)} = \sin{z_1} \cos{z_2} + \cos{z_1} \sin{z_2}$
          \item $\cosh{z} = \cos{iz}$ 
          \item $\sinh{z} = -i \sin{iz}$
        \end{enumerate}
      \end{lemma}

      However, to obtain even such geometrically obvious facts as the equality
      \[\sin{\pi} = 0 \text{ or } \cos{z + 2\pi} = \cos{z}\]
      from the power series definitions of $\cos$ and $\sin$ is extremely difficult. What the properties actually do is present the remarkable unity of these seemingly different trigonometric and hyperbolic functions, which would have been impossible to detect without going into the domain of complex numbers. 

      If we just take the following identities
      \begin{align*}
          \cos{x} & = \cos{(x + 2 \pi)} \\
          \sin{x} & = \sin{(x + 2\pi)} \\
          \cos{0} & = 1 \\
          \sin{0} & = 0
      \end{align*}
      then we get the following identity. 

      \begin{theorem}[Euler's Identity]
      The following relation is true. 
      \[e^{i\pi} + 1 = 0\]
      which immediately implies 
      \[\exp(z + 2\pi i) = \exp{z}\]
      That is, the exponential function is a periodic function on $\mathbb{C}$ with the purely imaginary period $T = 2 \pi i$. 
      \end{theorem}

      \begin{corollary}[Trigonometric Notation of Complex Number]
      With Euler's formula and the periodic relation of $\exp{z}$, the trigonometric form of a complex number can be presented as
      \[z = r(\cos{\varphi} + i \sin{\varphi}) = r e^{i \varphi}\]
      We can rewrite DeMoivre's formula as
      \[z^n = r^n e^{n \varphi i}\]
      \end{corollary}

      \subsubsection{Visualizing Complex Functions}

        TBD

      \subsubsection{Continuity, Differentiability, Analyticity of Complex Functions}
      The definitions of continuity and differentiability are the same, just under a different field. 

      \begin{definition}[Limit of a Complex Function]
        The function $f: E \subset \mathbb{C} \longrightarrow \mathbb{C}$ tends to $A \in \mathbb{C}$ as $z \rightarrow a$, or that
        \[\lim_{z \rightarrow a} f(z) = A\]
        if for every $\epsilon > 0$ there exists a $\delta > 0$ such that
        \[0<|z - a|<\delta \implies |f(z) - A|<\epsilon\]
        Note that we set $0<|z - a|$ to ensure that $z \neq a$. 

        Therefore, in other words, for any arbitrarily small $\epsilon>0$, we can find a $\delta > 0$ such that the image of the deleted $\delta$-neighborhood of $a$, denoted $\mathring{U}_\delta (a)$), is completely within the $\epsilon$-neighborhood $U_\epsilon (A)$. 
        \begin{center}
            \includegraphics[scale=0.25]{img/Limit_of_Complex_Function.PNG}
        \end{center}
      \end{definition}

      \begin{definition}[Continuity of a Complex Function]
        A function $f: E \subset \mathbb{C} \longrightarrow \mathbb{C}$ is \textbf{continuous} at a point $z_0 \in E$ if for any neighborhood $U(f(z_0))$ there exists a neighborhood $U(z_0)$ such that its image is contained in $U(f(z_0))$. In short, 
        \[\lim_{z \longrightarrow z_0} f(z) = f(z_0)\]
        \begin{center}
          \includegraphics[scale=0.25]{img/Continuity_of_Complex_Function.PNG}
        \end{center}
      \end{definition}

      \begin{definition}[Differentiability of a Complex Function]
        The \textbf{derivative} of a function $f: E \subset \mathbb{C} \longrightarrow \mathbb{C}$ is defined
        \[f^\prime (z_0) = \lim_{z \rightarrow z_0} \frac{f(z) - f(z_0)}{z - z_0}\]
        if this limit exists. $f$ \textbf{differentiable} at $x_0$ means that a differential function 
        \[df(z_0): T_{z_0} \mathbb{C} \longrightarrow T_{f(z_0)} \mathbb{C}, \;\;\; h \mapsto df(z_0)(h)\]
        exists such that
        \[f(z) = f(z_0) + df(z_0)(h) + o(h)\]
        where $h = z - z_0$ is the increment of the argument. Just like the real case, it turns out that $df(z_0)(h) = f^\prime (z_0) h$, and 
        \[f(z) - f(z_0) = f^\prime(z_0) (z - z_0) + o(z - z_0)\]
        which elegantly weaves together the two concepts of differentiability and the derivative. 

        Visualizing this, we can see that for whatever function $f: \mathbb{C} \longrightarrow \mathbb{C}$ there is a linear function that transforms the entire space as such at $z_0$ (along with a given point $z_0 \in \mathbb{C}$), 
        \begin{center}
            \includegraphics[scale=0.25]{img/Differential_of_Complex_Valued_Function.PNG}
        \end{center}
        The differential $df(z_0)$ at the point $z_0$ is a linear mapping that "best" approximates $f$, with an error of $o(h) = o(z - z_0)$. 
      \end{definition}

      \begin{lemma}[Arithmetic Properties of Differentiation over $\mathbb{C}$]
        If functions $f, g: E \subset \mathbb{C} \longrightarrow \mathbb{C}$ are differentiable at a point $z \in E$, then 
        \begin{enumerate}
          \item their sum is differentiable at $z$, and 
          \[d(f + g)(z) = df(z) + dg(z) \iff (f + g)^\prime (z) = (f^\prime + g^\prime)(z)\]
          \item their product is differentiable at $z$, and 
          \[d(f \cdot g) (z) = g(z) df(z) + f(z) dg(z) \iff (f \cdot g)^\prime (z) = f^\prime (z) g(z) + f(z) \cdot g^\prime (z)\]
          \item their quotient is differentiable at $z$ if $g(z) \neq 0$, and 
          \[d \bigg( \frac{f}{g}\bigg) (z) = \frac{g(z) df(z) - f(z) dg(z)}{g^2 (z)} \iff \bigg(\frac{f}{g}\bigg)^\prime (z) = \frac{f^\prime (z) g(z) - f(z) g^\prime (z)}{g^2 (z)}\]
        \end{enumerate}
        Just like the real case, the operation of taking the derivative is a linear operator. 
      \end{lemma}

      \begin{lemma}[Chain Rule for Composite Functions over $\mathbb{C}$]
        Let there be functions $f: E_1 \subset \mathbb{C} \longrightarrow \mathbb{C}$ differentiable at point $z \in E_1$ and $g: E_2 \subset \mathbb{C} \longrightarrow \mathbb{C}$ differentiable at point $w = f(z) \in E_2$, with respective differentials 
        \begin{align*}
            df(z) & : T_z \mathbb{C} \longrightarrow T_w \mathbb{C} \\
            dg(w) & : T_w \mathbb{C} \longrightarrow T_{g(w)} \mathbb{C}
        \end{align*}
        Then, the composite function $g \circ f: E_1 \longrightarrow \mathbb{C}$ is differentiable at $z$, and $d(g \circ f)(z): T_z \mathbb{C} \longrightarrow T_{g \circ f(z)} \mathbb{C}$ is
        \[d(g \circ f) (z) = dg(w) \circ df(z) \iff (g \circ f)^\prime (z) = g^\prime \big(f(z)\big) \circ f^\prime (z)\]
      \end{lemma}

      \subsubsection{Power Series Representation of a Function}
      \begin{definition}[Holomorphic Function]
        If function $f: E \subset \mathbb{C} \longrightarrow \mathbb{C}$ is (complex) differentiable at a point $z_0 \in E$, then $f$ is said to be \textbf{holomorphic at $z_0$}. 
      \end{definition}

      We recall the diagram that summarizes the conditions of differetiability and analyticity of a function $f$ over the field $\mathbb{R}$. 
      \begin{center}
      \begin{tikzpicture}
          \draw (-7.5,0) rectangle (7.5, 4);
          \draw[fill=lightgray] (-6.5, 0.5) rectangle (6.5, 3);
          \draw[fill=white] (-5.5, 1) rectangle (5.5, 2);
          \node[above] at (0, 1) {Taylor series converges to $f$ at $x_0 \iff f$ is analytic};
          \node[above] at (0, 2) {Taylor series converges at $x_0$};
          \node[above] at (0, 3) {$f$ infinitely differentiable at $x_0 \iff $ Taylor series of $f$ exists at $x_0$};
      \end{tikzpicture}
      \end{center}
      In the theory of functions of a complex variable we actually have a remarkable theorem that does not have an analogue for functions over $\mathbb{R}$. 

      \begin{theorem}[Analyticity of Differentiable Functions over $\mathbb{C}$]
      If a function $f: E \subset \mathbb{C} \longrightarrow \mathbb{C}$ is differentiable in a neighborhood of a point $z_0 \in E$, then it is analytic at that point. In other words, 
      \[f \text{ is holomorphic at } z_0 \implies f \text{ is analytic at } z_0\]
      This means that the conditions in the diagram above all are equivalent! Visually, 
      \begin{center}
      \begin{tikzpicture}
          \draw (-6.5,1) rectangle (6.5, 5); 
          \node[above] at (0,4) {$f$ is differentiable at $z_0 \iff f$ is holomorphic at $z_0$};
          \node at (0,3.8) {$\Updownarrow$};
          \node at (0,2.8) {$\Updownarrow$};
          \node at (0,1.8) {$\Updownarrow$};
          \node[above] at (0, 1) {Taylor series converges to $f$ at $z_0 \iff f$ is analytic};
          \node[above] at (0, 2) {Taylor series converges at $z_0$};
          \node[above] at (0, 3) {$f$ infinitely differentiable at $z_0 \iff $ Taylor series of $f$ exists at $z_0$};
      \end{tikzpicture}
      \end{center}
      This is certainly an amazing fact, since it then follows from the theorem that if a function $f(z)$ has one derivative $f^\prime (z)$ in a neighborhood of a point, it also has derivatives of all orders in that neighborhood. 
      \end{theorem}

      \subsubsection[Algebraic Closedness of the Field C]{Algebraic Closedness of the Field $\mathbb{C}$}

      \begin{definition}[Algebraically Closed Field]
        A field $\mathbb{F}$ is \textbf{algebraically closed} if every nonconstant polynomial in $\mathbb{F}[x]$ (the polynomial ring with coefficients in $\mathbb{F}$) has a root in $\mathbb{F}$. 
      \end{definition}

      \begin{theorem}[Fundamental Theorem of Algebra]
      $\mathbb{C}$ is algebraically closed. That is, every polynomial 
      \[P(z) \equiv c_0 + c_1 z + c_2 z^2 + \ldots + c_n z^n\]
      of degree $n\geq 1$ with complex coefficients $c_i \in \mathbb{C}$ ($i = 0, 1, \ldots, n$) has a root in $\mathbb{C}$. This immediately implies that every polynomial $P(z)$ admits a representation (unique up to the order of the factors) in the form 
      \[P(z) = c_n (z - z_1) (z - z_2) \ldots (z - z_n)\]
      where $z_1, \ldots, z_n \in \mathbb{C}$ not necessarily all distinct. 
      \end{theorem}

      We can also prove the interesting property about zeroes of polynomials in $\mathbb{R}[x]$. 

      \begin{corollary}[Complex Conjugate Roots of Real Polynomials]
      Given a polynomial with real coefficients
      \[P(z) \equiv a_0 + a_1 z + a_2 z^2 + \ldots + a_n z^n\]
      $P$, as we know, does not always have real roots (e.g. $P(x) = x^2 + 1$). However, we state that
      \[\text{if } P(z_0) = 0, \text{ then } P(\overline{z}_0) = 0\]
      Therefore, every polynomial $P$ with real coefficients can be expanded as a product of linear and quadratic polynomial with real coefficients. 
      \end{corollary}
      \begin{proof}
      We can see from the properties of complex numbers that
      \begin{align*}
          \overline{(z_1 + z_2)} & = \overline{z_1} + \overline{z_2} \\
          \overline{(z_1 \cdot z_2)} & = \overline{(r_1 e^{i \varphi_1} \cdot r_2 e^{i \varphi_2})} \\
          & = \overline{r_1 r_2 e^{i(\varphi_1 + \varphi_2)}} = r_1 r_2 e^{-i(\varphi_1 + \varphi_2)} \\
          & = r_1 e^{-i\varphi_1} \cdot r_2 e^{-i \varphi_2} = \overline{z}_1 \cdot \overline{z}_2
      \end{align*}
      Thus, if $P(z_0) = 0$, then 
      \[0 = \overline{P(z_0)} = \overline{a_0 + \ldots + a_n z_0^n} = \overline{a}_0 + \ldots + \overline{a}_n \overline{z}_0^n  = a_0 + \ldots + a_n \overline{z}_0^n = P(\overline{z}_0)\]
      and thus $P(\overline{z}_0) = 0$. 
      \end{proof}

    \subsection{Primitives}

      \begin{definition}[Primitive]
        A function $F(x)$ is a \textbf{primitive} of a function $f(x)$ on an interval if $F$ is differentiable on the interval and satisfies the equation 
        \[F^\prime (x) = f(x)\]
        or equivalently, if their respective differentials satisfy
        \[d F(x) = f(x) \,dx\]
      \end{definition}

      \begin{lemma}
        If $F_1(x)$ and $F_2 (x)$ are two primitives of $f(x)$ on the same interval, then the difference $(F_1 - F_2)(x)$ is constant on that interval. 
      \end{lemma}

    \begin{example}
    Both 
    \[F_1(x) \equiv \arctan{x} \text{ and } F_2(x) \equiv \arccot{\frac{1}{x}}\]
    are primitives of $f(x) = \frac{1}{1 + x^2}$. Indeed, we can see by direct calculation that in the domain $\mathbb{R} \setminus 0$, 
    \[F_1 (x) - F_2 (x) = \arctan{x} - \arccot{\frac{1}{x}} = \begin{cases}
    0, & x > 0 \\
    -\pi, & x < 0
    \end{cases}\]
    which is supported by the lemma. 
    \end{example}

      Notice how given a function $f(x)$, the operation of finding its differential, denoted with $d$, gives us a new function of $h$, called the differential 
      \[df(x)(h)\]
      Similarly, the operation of finding a primitive of function $f(x)$, denoted with the symbol $\int$, gives us a new function. 

      \begin{definition}[Indefinite Integration]
        The operation of finding a primitive of a certain function $f(x)$ is called \textbf{indefinite integration}, and the mathematical notation 
        \[\int f(x) \,dx\]
        is called the \textbf{indefinite integral of $f(x)$} on a given interval ($f$ called the \textbf{integrand} and $f(x)\,dx$ called the \textbf{differential form}). 
        \begin{enumerate}
          \item It immediately follows from the lemma that if $F(x)$ is any particular primitive of $f(x)$ on the interval, then on that interval 
          \[\int f(x) \,dx = F(x) + C\]
          \item If $F^\prime (x) = f(x)$ (that is, $F$ is a primitive of $f$ on some interval), then we have
          \[d \int f(x)\,dx = d F(x) = F^\prime (x) \,dx \]
          \item It also follows that 
          \[\int d F(x) = \int F^\prime (x)\,dx = F(x) + C\]
        \end{enumerate}
      \end{definition}

      \begin{theorem}[Basic Methods of Indefinite Integration]
      The definition of the indefinite integral has three basic properties that can be used to solve indefinite integrals. 
      \begin{enumerate}
        \item Linearity of the indefinite integral.
        \[\int \big( \alpha u(x) + \beta v(x)\big) \, dx = \alpha \int u(x)\,dx + \beta \int v(x)\,dx + C\]
        \item Integration by parts. 
        \[\int (u v)^\prime \,dx = \int u^\prime (x) v(x) \,dx + \int u(x) v^\prime (x) \,dx + C\]
        \item Change of Variable, or $U$-substitution. Given that $F^\prime (x) = f(x)$ on an interval $I_x$ and $\varphi: I_t \longrightarrow I_x$ is a $C^1$ mapping of interval $I_t$ into $I_x$, then
        \[\int (f \circ \varphi) (t) \varphi^\prime (t) \,dt = (F \circ \varphi)(t) + C\]
      \end{enumerate}
      \end{theorem}

  \section{Integration}

    \subsection{Construction of the Riemann Integral}

      We shall first define the integral using the familiar notation of Riemann sums. 

      \begin{definition}[Partitions with Distinguished Points]
        A \textbf{partition} $P$ of a closed interval $[a, b]$, $a < b$, is a finite system of points $x_0, \ldots, x_n$ of the interval such that
        \[a = x_0 < x_1 < x_2 < \ldots < x_n = b\]
        The intervals $[x_{i-1}, x_i]$, $i = 1, 2, \ldots, n$, are called the \textbf{intervals} of the partition $P$. The largest of the lengths of the intervals of the partition $P$, denoted $\lambda(P)$, is called the \textbf{mesh} of the partition. 

        A \textbf{partition with distinguished points} $(P, \xi)$ on the closed interval $[a, b]$ is a partition $P$ of $[a,b]$ along with the set of $n$ points 
        \[\xi_1 \in [x_0, x_1], \xi_2 \in [x_1, x_2], \ldots, \xi_n \in [x_{n-1}, x_n]\]
        The $n$-tuple of $\xi_i$'s is denoted by the single letter $\xi$
        \[\xi = (\xi_1, \xi_2, \ldots, \xi_n)\]
      \end{definition}

      This naturally leads to the following construction. 

      \begin{definition}[Riemann Sums]
        If a function $f$ is defined on a closed interval $[a, b]$ and $(P, \xi)$ is a partition with distinguished points on this closed interval, the sum
        \[\sigma(f; P, \xi) \equiv \sum_{i=1}^n f(\xi_i)\, \Delta x_i, \text{ where } \Delta x_i = x_i - x_{i-1},\]
        is the \textbf{Riemann sum} of the function $f$ corresponding to the partition $(P, \xi)$ with distinguished points on $[a, b]$. 
        \begin{center}
            \includegraphics[scale=0.25]{img/Riemann_Sum_with_Partitions_Points.PNG}
        \end{center}
        Thus, when a function $f$ is fixed, the Riemann sum $\sigma (f; P, \xi)$ is a mapping that takes in a partition with distinguished points $p = (P, \xi)$ on the closed interval $[a, b]$ and outputs a number representing the total area of the Riemann sums. That is, for a fixed $f$ and some input $p = (P, \xi)$, we can define the function 
        \[\Phi: \mathcal{P} \longrightarrow \mathbb{R}, \;\;\; \Phi(p) \equiv \sigma(f; p) \equiv \sigma(f; (P, \xi))\]
        that takes in a partition with distinguished points on $[a,b]$ and outputs the corresponding Riemann sum for that fixed $f$. 
      \end{definition}

      \begin{definition}[Riemann Integral]
        The number $\int_a^b f(x)\,dx$ is the \textbf{Riemann integral} of the function $f$ on the closed interval $[a, b]$ if for every $\epsilon>0$ there exists a $\delta>0$ such that
        \[\Bigg| \int_a^b f(x)\,dx - \sum_{i=1}^n f(\xi_i) \Delta x_i \Bigg| < \epsilon\]
        for any partition $(P, \xi)$ with distinguished points on $[a, b]$ whose mesh $\lambda(P)$ is less than $\delta$. We can view this as a limit where $n \rightarrow \infty$, but there is a problem since we can increase the partition within different subsets of $[a,b]$, leading to multiple values of convergence. 
        \begin{center}
            \includegraphics[scale=0.28]{img/Riemann_Integral_Converging_onto_2_Numbers.PNG}
        \end{center}
        Rather, we can set the mesh $\lambda(P)$ to approach $0$, which would take care of the problems. We can visualize this by imagining the lengths of the rectangles converging "uniformly."
        \begin{center}
            \includegraphics[scale=0.28]{img/Riemann_Integral_Limit_Mesh_goes_to_0.PNG}
        \end{center}
        Therefore, we can culminate by defining the Riemann integral of $f(x)$ over $[a,b]$ as 
        \[\int_a^b f(x)\,dx \equiv \lim_{\lambda(P) \rightarrow 0} \sum_{i=1}^n f(\xi_i) \lambda x_i\]
      \end{definition}

      \subsubsection{Conditions for Integrability}

      \begin{definition}[Riemann Integrable Functions]
        A function $f$ is \textbf{Riemann integrable} on the closed interval $[a, b]$ if 
        \[\int_a^b f(x)\,dx \equiv \lim_{\lambda(P) \rightarrow 0} \sum_{i=1}^n f(\xi_i) \lambda x_i\]
        is defined, i.e. if the limit of the right-hand side of Riemann sums exists as $\lambda(P) \rightarrow 0$ (that is, the Riemann integral of $f$ is defined). 

        Furthermore, the set of Riemann-integrable functions on a closed interval $[a, b]$ is denoted $\mathcal{R}[a,b]$. 
      \end{definition}

      Remember that the Riemann integral, as complicated as the formula is, is still a limit of a function. That means that we can apply the Cauchy criterion to it to determine convergence. 

      \begin{lemma}[Cauchy Criterion on Existence of Riemann Integral]
        Given a function $f$, the integral of $f$ over $[a, b]$, defined
        \[\int_a^b f(x)\,dx \equiv \lim_{\lambda(P) \rightarrow 0} \sum_{i=1}^n f(\xi_i) \lambda x_i\]
        exists if and only if for every $\epsilon>0$, there exists a $\delta>0$ such that 
        \[\big| \sigma(f; P^\prime, \xi^\prime) - \sigma(f; P^{\prime\prime}, \xi^{\prime\prime} \big| < \epsilon\]
        or, what is the same, 
        \[\Bigg| \sum_{i=1}^{n^\prime} f(\xi_i^\prime) \Delta x_i^\prime - \sum_{i=1}^{n^{\prime\prime}} f^(\xi_i^{\prime\prime}) \Delta x_i^{\prime\prime} \Bigg| < \epsilon\]
        for any partitions $(P^\prime, \xi^\prime)$ and $(P^{\prime\prime}, \xi^{\prime\prime})$ with distinguished points on the interval $[a, b]$ with
        \[\lambda(P^\prime), \lambda(P^{\prime\prime}) < \delta\]
        In words, this means that for any $\epsilon>0$ that we choose, there always exists a $\delta>0$ such that \textbf{any} two Riemann sums with mesh size \textbf{both} smaller than $\delta$ will have an error difference of less than $\epsilon$. \begin{center}
            \includegraphics[scale=0.25]{img/Cauchy_Criterion_of_Riemann_Integral.jpg}
        \end{center}
      \end{lemma}

      \begin{theorem}[Necessary Condition for Integrability]
      A necessary condition for $f$ defined on a closed interval $[a, b]$ to be Riemann integrable on $[a, b]$ is that $f$ be bounded on $[a, b]$. That is, 
      \[f \in \mathcal{R}[a, b] \implies f \text{ is bounded on } [a, b]\]
      We can clearly see the necessity of $f$ being bounded by looking at the contrapositive of the following statement. 
      \end{theorem}

      \begin{theorem}[Refinement]
      Given a partition $P$ on interval $[a, b]$, recall that we have points $x_0, \ldots, x_n$ such that
      \[a = x_0 < x_1 < \ldots < x_n = b\]
      Here we introduce new notation: 
      \begin{enumerate}
        \item $\Delta_i$ denotes the interval $[x_{i-1}, x_i]$
        \item $\Delta x_i$ denotes the difference $x_i - x_{i-1}$, i.e. the length of $\Delta_i$
      \end{enumerate}
      If a partition $\Tilde{P}$ of the closed interval $[a, b]$ is obtained from the partition $P$ by the addition of new points to $P$, we call $\Tilde{P}$ a \textbf{refinement} of $P$. 

      When a refinement $\Tilde{P}$ of a partition $P$ is constructed, some (perhaps all) of the closed intervals $\Delta_i = [x_{i-1}, x_i]$ of the partition $P$ themselves undergo partitioning. 
      \[x_{i-1} = x_{i0} < x_{i1} < \ldots < x_{in_i} = x_i\]
      In that connection, it will be useful to label to points of $\Tilde{P}$ by double indices, where in the notation $x_{ij}$ the first index $i$ means that 
      \[x_{ij} \in \Delta_i = [x_{i-1}, x_i]\]
      and the second index $j$ is the ordinal number of the point on the closed interval $\Delta_i = [x_{i-1}, x_i]$. Therefore, it is natural to set the notations
      \begin{enumerate}
        \item $\Delta_{ij} = [x_{i j-1}, x_{ij}]$
        \item $\Delta x_{ij} = x_{ij} - x_{ij-1}$
      \end{enumerate}
      This means that 
      \[\Delta x_i = \Delta x_{i1} + \Delta x_{i2} + \ldots + \Delta x_{in_i}\]
      which can be visualized below
      \begin{center}
        \includegraphics[scale=0.25]{img/Refinement_Definition_Analysis.PNG}
      \end{center}
      \end{theorem}

    \begin{example}[Union of Partitions as a Refinement]
    For some interval $[a, b]$, given partitions $P^\prime$ ($a = x_0 < \ldots < x_n = b$) and $P^{\prime\prime}$ ($a = y_0 < \ldots < y_n = b$), the union of the two partitions $\Tilde{P} = P^\prime \cup P^{\prime\prime}$ is a refinement of both $P^\prime$ and $P^{\prime\prime}$. 
    \begin{center}
        \includegraphics[scale=0.25]{img/Refinement_as_Union_of_Partitions.PNG}
    \end{center}
    \end{example}

      Recall that $\omega(f; E)$ denotes the oscillation of the function $f$ on the set $E$; that is, 
      \[\omega(f; E) \equiv \sup_{x^\prime, x^{\prime\prime} \in E} \big| f(x^\prime) - f(x^{\prime\prime})\big|\]
      In particular, $\omega(f; \Delta_i)$ is the oscillation of $f$ on the closed interval $\Delta_i$. 

      \begin{theorem}[Sufficient Condition for Integrability]
      Let $f$ be a bounded on a closed interval $[a, b]$ such that for every $\epsilon > 0$ there exists a number $\delta>0$ such that
      \[\sum_{i=1}^n \omega(f; \Delta_i) \Delta x_i < \epsilon\]
      for any partition $P$ of $[a, b]$ with mesh $\lambda(P) < \delta$. This is equivalent to saying that
      \[\lim_{\lambda(P) \rightarrow 0} \sum_{i = 1}^n \omega (f; \Delta_i) \, \Delta x_i = 0\]
      Then, $f$ is integrable. We can visualize
      \[\sum_{i=1}^n \omega(f; \Delta_i) \Delta x_i\]
      as the following sum of rectangles below. 
      \begin{center}
          \includegraphics[scale=0.25]{img/Sufficient_Condition_for_Integrability.PNG}
      \end{center}
      What the theorem states, visually, is that as we make all the rectangles smaller and smaller (by putting a limit on the mesh $\lambda(P)<\delta$), we can make the sum of all these rectangles also arbitrarily small. 
      \end{theorem}

      \begin{corollary}[Integrability of Continuous Functions]
      Every continuous function on a closed interval is integrable on that closed interval. That is, 
      \[f \in C[a, b] \implies f \in \mathcal{R}[a, b]\]
      \end{corollary}

      We can actually make a stronger claim. 

      \begin{corollary}[Integrability of Discontinuous Functions]
      If a bounded function $f$ on a closed interval $[a, b]$ is continuous everywhere except at a finite set of points, then $f \in \mathcal{R}[a, b]$. 
      \end{corollary}

      \begin{corollary}[Integrability of Monotonic Functions]
      A bounded monotonic function on a closed interval is integrable on that interval. 
      \begin{center}
          \includegraphics[scale=0.25]{img/Integrability_of_Monotonic_Function.PNG}
      \end{center}
      \end{corollary}

      \begin{definition}[Upper, Lower Riemann Sums]
        Let $f: [a, b] \longrightarrow \mathbb{R}$ be a real-valued function that is defined and bounded on the closed interval $[a, b]$, and let $P$ be a partition of $[a, b]$, and let $\Delta_i$ ($i = 1, 2, \ldots, n$) be the intervals of the partition $P$. Let 
        \begin{align*}
            m_i &= \inf_{x \in \Delta_i} f(x) \\
            M_i &= \sup_{x \in \Delta_i} f(x)
        \end{align*}
        be the infimum and supremum of $f$ over $\Delta x_i$. Then, the sums
        \begin{align*}
            s(f; P) & \equiv \sum_{i = 1}^n m_i \, \Delta x_i \\
            S(f; P) & \equiv \sum_{i=1}^n M_i \, \Delta x_i
        \end{align*}
        are respectively called the \textbf{lower} and \textbf{upper Riemann sums} of the function $f$ on the interval $[a, b]$ corresponding to the partition $P$ of that interval. 

        Given an arbitrary partition $(P, \xi)$ with distinguished points on $[a, b]$, it is clear that
        \[s(f; P) = \inf_{\xi} \sigma(f; P, \xi) \leq \sigma(f; P, \xi) \leq \sup_{\xi} \sigma(f; P, \xi) = S(f; P)\]
      \end{definition}

      \begin{theorem}
      A bounded real-valued function $f: [a, b] \longrightarrow \mathbb{R}$ is Riemann integrable on $[a, b]$ if and only if the following limits exist and are equal to each other. 
      \[\underline{I} \equiv \lim_{\lambda(P) \rightarrow 0} s(f; P) = \lim_{\lambda(P) \rightarrow 0} S(f; P) \equiv \overline{I}\]
      When the relation is true, then the integral is this common value. 
      \[\int_a^b f(x) \,dx = \underline{I} = \overline{I}\]
      \end{theorem}

      Note that this condition of the upper and lower Riemann sums converging to the same value and the condition that 
      \[\lim_{\lambda(P) \rightarrow 0} \sum_{i = 1}^n \omega (f; \Delta_i) \, \Delta x_i = 0\]
      are the same. For we can see that the rectangles visualized from the equation above are the exact same rectangles formed by $S(f; P) - s(f; P)$! 
      \begin{center}
          \includegraphics[scale=0.3]{img/Equivalent_Conditions_for_Integrability.PNG}
      \end{center}

      \subsubsection{The Vector Space of Riemann Integrable Functions}

      \begin{theorem}[The Vector Space of Integrable Functions]
      The set of Riemann integrable functions $\mathcal{R}[a, b]$ over closed interval $[a, b]$ is a vector space. That is, given $f, g \in \mathcal{R}[a, b]$ and $\alpha \in \mathbb{R}$, then
      \begin{enumerate}
        \item $(f + g) \in \mathcal{R}[a, b]$ 
        \item $(\alpha f) \in \mathcal{R}[a, b]$
      \end{enumerate}
      Furthermore, 
      \begin{enumerate}
        \item $|f| \in \mathcal{R}[a, b]$
        \item The restriction of $f$ in any $[c, d] \subset [a, b]$, denoted $f \big|_{[c,d]}$, is in $\mathcal{R}[c,d]$
        \item $(f \cdot g) \in \mathcal{R}[a, b]$
      \end{enumerate}
      \end{theorem}
      \begin{proof}

      \end{proof}

      \subsubsection{Lebesgue's Criterion for Riemann Integrability}
      We give Lebesgue's version of an intrinsic description of a Riemann integrable function. 

      \begin{definition}[Measure]
        A set $E \subset \mathbb{R}$ has \textbf{(Lebesgue) measure zero} if for every number $\epsilon > 0$ there exists a covering of the set $E$ be an at most countable system $\{I_k\}$ of intervals, the sum of whose lengths 
        \[\sum_{k=1}^\infty |I_k| \leq \epsilon\]
        This means that the above series summing up the lengths of the intervals is an absolutely convergent series. 
      \end{definition}

      \begin{lemma}
        We can deduce measures of basic sets. 
        \begin{enumerate}
          \item A finite number of points are sets of measure zero. 
          \item The union of a finite or countable number of sets of measure zero is a set of measure zero. \item A subset of a set of measure zero is itself a set of measure zero. 
          \item A closed interval $[a, b]$ with $a<b$ is not a set of measure zero. 
        \end{enumerate}
      \end{lemma}

      \begin{definition}
        If a property holds at all points of a set $X$ except possible the points of a set of measure zero, we say that this property holds \textbf{almost everywhere on $X$} or \textbf{at almost every point of $X$}. 
      \end{definition}

      Now, we can state Lebesgue's criterion for integrability, which nicely summarizes what we have so far. 

      \begin{theorem}[Lebesgue's Criterion for Integrability]
      A function defined on a closed interval is Riemann integrable on that interval if and only if it is bounded and continuous at almost every point. 
      \end{theorem}

    \begin{example}[Non-Integrability of the Dirichlet Function]
    The Dirichlet function
    \[\mathcal{D}(x) \equiv \begin{cases}
    1, & \text{ for } x \in \mathbb{Q} \\
    0, & \text{ for } x \in \mathbb{R} \setminus \mathbb{Q}
    \end{cases}\]
    on the interval $[0,1]$ is not integrable on that interval. We state two different reasons why. 
    \begin{enumerate}
      \item For any partition $P$ of $[0,1]$ we can find in each interval $\Delta_i$ both a rational point $\xi^\prime_i$ and an irrational point $\xi_i^{\prime\prime}$. Then, we can see that the lower and upper Riemann sums do not necessarily converge to each other since
      \[\sigma(f; P, \xi^\prime) = \sum_{i=1}^n 1 \cdot \Delta x_i = 1 \text{ while } \sigma(f;P, \xi^{\prime\prime}) = \sum_{i=1}^n 0 \cdot \Delta x_i = 0\]
      as $\lambda(P) \rightarrow 0$. 
      \item From the point of view of the Lebesgue criterion the nonintegrability of the Dirichlet function is obvious since $\mathcal{D}(x)$ is discontinuous at every point of $[0, 1]$, which is not a set of measure zero. 
    \end{enumerate}
    \end{example}

      Notice that by the Lebesgue criterion, integrability is a weaker condition than continuity. That is, 
      \[f \text{ continuous } \implies f \text{ Riemann integrable}\]
      but not necessarily the other way around. It turns out that this has consequences when determining the composition of functions. 

      \begin{proposition}[Integrable + Continuous Composition]
      Let $f: I_1 = [a, b] \longrightarrow\mathbb{R}$ be a function that is integrable on $[a, b]$, with Im$\,f = [c, d] = I_2$. Define a continuous (remember, continuity is stronger than integrability) function $g: [c, d] \longrightarrow \mathbb{R}$. Then the composition
      \[g \circ f: [a, b] \longrightarrow \mathbb{R}\]
      is clearly defined and continuous at all the points of $[a, b]$ where $f$ is continuous. But since $f$ is integrable, the union of all the discontinuities in $[a, b]$ must have measure zero, and so it follows that since $[a, b]$ is the same  
      \[g \circ f \in \mathcal{R}[a, b]\]
      Therefore, we can found out that 
      \[f \text{ integrable and } g \text{ continuous} \implies g \circ f \text{ integrable}\]
      as visualized in the commutative diagram below. 
      \[
        \begin{tikzcd}
          I_1 \arrow[r, "f"] \arrow[rr, bend left, "g \circ f"] & I_2 \arrow[r, "g"] & \mathbb{R}
        \end{tikzcd}
      \]
      However, contrary to intuition, 
      \[f \text{ integrable and } g \text{ integrable} \centernot\implies g \circ f \text{ integrable}\]
      \end{proposition}

      We present a counterexample. 
    \begin{example}
    Consider the functions
    \[|sgn|(x) \equiv \begin{cases}
    1 & x \neq 0 \\
    0 & x = 0
    \end{cases}\]
    and the Riemann function 
    \[\mathcal{R}(x) \equiv \begin{cases}
    \frac{1}{n} & x = \frac{m}{n} \in \mathbb{Q}, \gcd(m, n) = 1 \\
    0 & x \in \mathbb{R} \setminus \mathbb{Q}
    \end{cases}\]
    We can see that $\mathcal{R}$ is continuous at all irrational points and discontinuous at all rational points except $0$, meaning that it is integrable ($\mathbb{Q}$ has measure zero). Then, the composition of these two functions is precisely the Dirichlet function
    \[\mathcal{D}(x) = |sgn| \circ \mathcal{R}\]
    which is not integrable. 
    \end{example}

    \subsection{Basic Properties of the Integral}

      One of the most basic properties of the integral is that it is a linear map. 
      \begin{lemma}[Linearity of the Integral]
        Given closed interval $[a, b] \subset \mathbb{R}$, the Riemann integration function 
        \[\int_a^b: \mathcal{R}[a, b] \longrightarrow \mathbb{R}\]
        is a linear functional living within the dual space $\mathbb{R}^* [a, b]$. That is, given $f, g \in \mathcal{R}[a, b]$, a linear combination of them $\alpha f + \beta g$ is also integrable on $[a,b]$, and 
        \[\int_a^b (\alpha f + \beta g)(x)\,dx = \alpha \int_a^b f(x)\,dx + \beta \int_a^b g(x)\,dx\]
      \end{lemma}
      \begin{proof}
      It is clear from basic algebraic transformation that the Riemann sums for the integral expressions on both sides are equal. 
      \[\sum_{i=1}^n (\alpha f + \beta g) (\xi_i) \Delta x_i = \alpha \sum_{i=1}^n f(\xi_i) \Delta x_i + \beta \sum_{i=1}^n g(\xi_i) \Delta x_i\]
      Taking the limit as $\lambda(P) \rightarrow 0$ on both sides leads to the respective Riemann integrals. 
      \end{proof}


      The next property of the Riemann integral is its additive property \textbf{on the interval of integration}. Note that the value of the integral 
      \[\int_a^b f(x) \,dx \equiv \lim_{\lambda(P) \rightarrow 0} \sigma(f; P, \xi)\]
      depends on both the integrand and the closed interval over which the integral is taken. 

      \begin{lemma}[Properties of the Interval of Integration]
        If $a < b < c$ and $f \in \mathcal{R}[a, c]$, then $f \big|_{[a,b]} \in \mathcal{R}[a, b]$, $f \big|_{[b,c]} \in \mathcal{R}[b, c]$, and the following equality holds 
        \[\int_a^c f(x)\,dx = \int_a^b f(x)\, dx + \int_b^c f(x)\,dx\]
        From these we set
        \[\int_a^b f(x)\,dx \equiv - \int_b^a f(x)\,dx\]
        and 
        \[\int_a^a f(x)\,dx \equiv 0\]
      \end{lemma}

      \begin{theorem}[Symmetry of the Riemann Integral]
      Let $a, b, c \in \mathbb{R}$ and let $f$ be integrable over the largest closed interval having two of these points as endpoints. Then, the restriction of $f$ to each of the other closed intervals is also integrable over those intervals and the following equality holds. 
      \[\int_a^b f(x)\,dx + \int_b^c f(x)\,dx + \int_c^a f(x)\,dx = 0\]
      This property can be abstractified to those of additive interval functions, which will be shown soon. 
      \end{theorem}

      We finally end with an important property of the integral which, as seen later, allows us to define inner products on function spaces. 
      \begin{theorem}
      If $a \leq b$ and $f \in \mathcal{R}[a, b]$, then $|f| \in \mathcal{R}[a, b]$, and 
      \[\Bigg| \int_a^b f(x)\,dx \Bigg| \leq \int_a^b |f|(x)\,dx\]
      \end{theorem}

      \subsubsection{Mean Value Theorem of the Integral}

      \begin{lemma}[Monotonicity of the Integral]
        If $a \leq b, f_1, f_2 \in \mathcal{R}[a, b]$, and $f_1 (x) \leq f_2 (x)$ for every $x \in [a, b]$, then
        \[\int_a^b f_1 (x)\,dx \leq \int_a^b f_2 (x)\,dx\]
        \begin{center}
            \includegraphics[scale=0.27]{img/Monotonicity_of_Integral.PNG}
        \end{center}
        This immediately implies that given constants $m, M$ such that $m \leq f(x) \leq M$ at each $x \in [a, b]$, we have
        \[m \cdot (b - a) \leq \int_a^b f(x)\,dx \leq M \cdot (b-a)\]
        This is very easily visualized below. 
        \begin{center}
            \includegraphics[scale=0.27]{img/Monotonicity_of_Intergral_2.PNG}
        \end{center}
        In particular, if $0 \leq f(x)$ on $[a, b]$, then
        \[0 \leq \int_a^b f(x)\,dx\]
      \end{lemma}

      \begin{theorem}[Mean Value Theorem of the Integral]
      Given $f \in \mathcal{R}[a, b]$, with 
      \[m = \inf_{x \in [a, b]} f(x) \text{ and } M = \sup_{x \in [a, b]} f(x)\]
      then there exists a number $\mu \in [m, M]$ such that
      \[\int_a^b f(x)\,dx = \mu \cdot (b - a)\]
      Furthermore, if $f \in C[a, b]$ (that is, continuous on $[a, b]$), it immediately follows by the intermediate value theorem that there exists a point $\xi \in [a, b]$ such that
      \[\int_a^b f(x)\,dx = f(\xi) (b - a)\]
      \begin{center}
          \includegraphics[scale=0.27]{img/Mean_Plus_Intermediate_Value_Theorem_Integral.PNG}
      \end{center}
      \end{theorem}

      Due to the length of the proof, we ask the reader to take it for granted the following theorem. 

      \begin{theorem}[Bonnet's Formula]
      If $f, g \in \mathcal{R}[a, b]$ and $g$ is a monotonic function on $[a, b]$, then there exists a point $\xi \in [a, b]$ such that
      \[\int_a^b (f \cdot g) (x)\,dx = g(a) \int_a^\xi f(x)\,dx + g(b) \int_\xi^b f(x)\,dx\]
      \end{theorem}

    \subsection{Connections between Integrals, Primitives, Derivatives}

      \begin{definition}[Integral with Variable Upper Limit]
        Let $f \in \mathcal{R}[a, b]$, and let us choose an $x \in [a, b]$ in order to construct the function
        \[F(x) \equiv \int_a^x f(t)\,dt\]
        which is called an \textbf{integral with variable upper limit}. Note that since $[a, x] \subset [a, b]$, it follows that $f \big|_{[a,x]} \in \mathcal{R}[a, x]$ and therefore the function $x \mapsto F(x)$ is unambiguously defined for $x \in [a, b]$. 
        \begin{center}
            \includegraphics[scale=0.27]{img/Integral_with_Variable_Upper_Limit.PNG}
        \end{center}
        Furthermore, $F(x)$ is continuous on $[a, b]$. Since $f$ is integrable on $[a, b]$, it is bounded by a constant $C$ such that
        \[|f(t)| \leq C \text{ on } [a, b]\]
        It follows from the additive properties of the integral and boundedness theorem that 
        \[|F(x + h) - F(x)| \leq C|h|\]
        if $x, x + h \in [a, b]$, as visualized. This means that for any $\delta$-neighborhood of $F(x)$, we can find an arbitrary small $h$ such that the $C|h|$-neighborhood of $F(x)$ is completely contained in the $\delta$-neighborhood. But by the inequality above, this means that there exists an $\epsilon = h$-neighborhood of $x$ such that its entire image is contained within the $C|h|$-neighborhood, which itself is contained within the $\delta$-neighborhood. This shows that $F$ is continuous. 
      \end{definition}

      \begin{theorem}[First Fundamental Theorem of Calculus]
      Let $f \in \mathcal{R}[a, b]$ be continuous at point $x \in [a, b]$ (resp. continuous on closed interval $[a, b]$). Let $F$ be the function, defined for all $x \in [a, b]$ by 
      \[F(x) \equiv \int_a^x f(t)\,dt\]
      Then, $f$ is continuous and differentiable at $x$ (resp. uniformly continuous on $[a, b]$ and differentiable on $(a, b)$), 
      \[F^\prime (x) = f(x)\]
      at $x$ (resp. for all $x \in [a, b]$). This is an amazing fact, because visually, it tells us that the rate at which the integral $F$ is increasing at $x$ (represented by the increasing area under the curve of $f$) is equal to the value of $f$ at the point $x$ itself! 
      \begin{center}
          \includegraphics[scale=0.25]{img/First_Fundamental_Theorem_Analysis.jpg}
      \end{center}
      \end{theorem}
      \begin{proof}
      Let $x, x + h \in [a, b]$, and let us estimate the difference $F(x+h) - F(x)$. It follows from the continuity of $f$ at $x$ that $f(t) = f(x) + \Delta(t)$, where $\Delta(t) \rightarrow 0$ as $t \rightarrow x$. If point $x$ is held fixed, the function 
      \[\Delta(t) = f(t) - f(x)\]
      is integrable on $[a, b]$, being the difference of the integrable function $t \mapsto f(t)$ and the constant $f(x)$. Let us denote
      \[M(h) \equiv \sup_{t \in [x, x+h]} |\Delta(t)|\]
      which means that $M(h)$ is the largest difference between $f(x)$ and $f(t)$ in the interval $[x, x+h]$. 
      \begin{center}
          \includegraphics[scale=0.25]{img/Proof_First_Fundamental_Theorem_Analysis.jpg}
      \end{center}
      Clearly $M(h) \rightarrow 0$ as $h \rightarrow 0$. We can now find
      \begin{align*}
          F(x + h) - F(x) & = \int_a^{x+h} f(t)\,dt - \int_a^x f(t)\,dt \\
          & = \int_x^{x+h} f(t)\,dt \\
          & = \int_x^{x+h} \big( f(x) + \Delta(t)\big)\,dt \\
          & = \int_x^{x+h} f(x)\,dt + \int_x^{x+h} \Delta(t)\,dt \\
          & = f(x) h + \alpha(h) h
      \end{align*}
      where we have set 
      \[\int_x^{x+h} \Delta(t)\,dt = \alpha(h) h\]
      where $\alpha$ is infinitesimal as $h \rightarrow 0$, since 
      \[\Bigg| \int_x^{x+h} \Delta(t)\,dt \Bigg| \leq \Bigg| \int_x^{x+h} |\Delta(t)|\,dt \Bigg| \leq \Bigg| \int_x^{x+h} M(h)\,dt \Bigg| = M(h) |h| = \alpha(h)|h|\]
      Therefore, we have shown that if the function $f$ is continuous at a point $x \in [a, b]$, then for displacements $h$ from $x$ such that $x +h \in [a, b]$, the following equality holds.
      \[F(x + h) - F(x) = f(x) h + \alpha(h) h\]
      where $\alpha(h) \rightarrow 0$ as $h \rightarrow 0$, and by definition, this means that $F(x)$ is differentaible on $[a, b]$ at the point $x \in [a, b]$ and that $F^\prime(x) = f(x)$. 
      \end{proof}

      \begin{corollary}
      Every bounded function $f: [a, b] \longrightarrow \mathbb{R}$ on the closed interval $[a, b]$ and has only a finite number of points of discontinuity has a primitive, and every primitive of $f$ on $[a, b]$ has the form 
      \[\mathcal{F}(x) \equiv \int_a^x f(t)\,dt + c\]
      where $c$ is a constant. 
      \end{corollary}

      \begin{theorem}[Second Fundamental Theorem of Calculus]
      Let $f$ be a real-valued function on a closed interval $[a, b]$ with $\mathcal{F}$ any primitive of $f$ on $[a, b]$. If $f$ is Riemann-integrable (i.e. $f$ bounded with finite points of Lebesgue measure zero) on $[a, b]$, then 
      \[\int_a^b f(x)\,dx  = \mathcal{F} \big|_a^b \equiv \mathcal{F}(b) - \mathcal{F}(a)\]
      \begin{center}
          \includegraphics[scale=0.25]{img/Second_Fundamental_Theorem_Analysis.PNG}
      \end{center}
      \end{theorem}
      \begin{proof}
      We already know that a bounded function on a closed interval having a finite number of discontinuities is integrable, and by the corollary, we are guaranteed an existence of a primitive $\mathcal{F}(x)$ of the function $f$ on $[a, b]$ with the form 
      \[\mathcal{F} (x) \equiv \int_a^x f(t)\,dt + c\]
      Setting $x = a$, we find that $c = \mathcal{F}(a)$, and so 
      \[\mathcal{F}(x) \equiv \int_a^x f(t)\,dt + \mathcal{F}(a)\]
      Evaluating $\mathcal{F}$ at $x = b$ gives
      \[\int_a^b f(t)\,dt = \mathcal{F}(b) - \mathcal{F}(a)\]
      \end{proof}

      \subsubsection{Integration by Parts and Taylor's Formula}
      \begin{theorem}[Definite Integration by Parts]
      If the functions $u(x)$ and $v(x)$ are continuously differentiable on a closed interval with endpoints $a$ and $b$, then
      \[\int_a^b (u \cdot v^\prime)(x)\,dx = (u \cdot v)\big|^b_a - \int_a^b (v \cdot u^\prime)(x)\,dx\]
      which is customarily written in the form as
      \[\int_a^b u\,dv = u \cdot v \big|_a^b - \int_a^b v\,du\]
      \end{theorem}
      \begin{proof}
      By the product rule of differentiation, we have
      \[(u \cdot v)^\prime (x) = (u^\prime \cdot v)(x) + (u \cdot v^\prime) (x)\]
      where by hypothesis, $u^\prime \cdot v, u \cdot v^\prime$ are continuous and hence integrable on $[a, b]$. Using the linearity of the integral and the 2nd fundamental theorem of calculus, we get
      \[(u \cdot v) (x) \big|^b_a = \int_a^b (u^\prime \cdot v)(x)\,dx + \int_a^b (u \cdot v^\prime) (x)\,dx\]
      \end{proof}

      \begin{theorem}[Integral Form of the Remainder]
      If $f: E \longrightarrow \mathbb{R}$ has continuous derivatives up to order $n$ on the closed interval $[a, x]$, then Taylor's formula holds
      \[f(x) = f(a) + \frac{f^\prime (a)}{1!} (x - a) + \ldots + \frac{f^{(n-1)}(a)}{(n-1)!} (x - a)^{n-1} + r_{n-1}(a; x)\]
      where 
      \[r_{n-1} (a;x) = \frac{1}{(n-1)!} \int_a^x f^{(n)} (t) (x - t)^{n-1} \,dt\]
      This form is called \textbf{Taylor's formula with the integral form of the remainder}. 
      \end{theorem}
      \begin{proof}
      Using the 2nd fundamental theorem and the definite integration by parts formula, we can carry out the following chain of transformations, assuming continuity and differentiability when needed. 
      \begin{align*}
          f(x) - f(a) & = \int_a^x f^\prime (t) \,dt \\
          & = - \int_a^x f^\prime(t) (x - t)^\prime \,dt \\
          & = -f^\prime (t) (x - t)\big|_a^x + \int_a^x f^{\prime\prime} (t) (x - t) \,dt \\
          & = f^\prime (a) (x - a) - \frac{1}{2} \int_a^x f^{\prime\prime} (t) \big( (x - t)^2\big)^\prime \,dt \\
          & = f^\prime (x - a) - \frac{1}{2} f^{\prime\prime} (t) (x - t)^2 \big|_a^x + \frac{1}{2} \int_a^x f^{\prime\prime\prime} (t) (x - t)^2\,dt \\
          & = f^\prime(a) (x - a) + \frac{1}{2} f^{\prime\prime} (a) (x - a)^2 - \frac{1}{2 \cdot 3} \int_a^x f^{\prime\prime\prime} (t) \big((x - t)^3\big)^\prime\,dt \\
          & = \ldots \\
          & = f^\prime (a) (x - a) + \ldots + \frac{1}{(n-1)!} f^{(n-1)} (a)(x - a)^{n-1} + r_{n-1}(a;x)
      \end{align*}
      where $r_{n-1}(a;x)$ is given by the integral formula mentioned. 
      \end{proof}

      \subsubsection{Change of Variables in Integration}
      We now show and prove the method what we call "u-substitution" for definite integration. 

      \begin{theorem}[Change of Variable]
      If $\varphi: [\alpha, \beta] \longrightarrow [a, b]$ is a continuously differentiable mapping such that $\varphi(\alpha) = a$ and $\varphi(\beta) = b$, then for any continuous function $f(x)$ on $[a, b]$ the function $f\big(\varphi(t)\big) \varphi^\prime (t)$ is continuous on the closed interval $[\alpha, \beta]$ and 
      \[\int_a^b f(x)\,dx = \int_\alpha^\beta f\big(\varphi(t)\big) \varphi^\prime(t)\,dt\]
      \begin{center}
          \includegraphics[scale=0.25]{img/Change_of_Variable_Analysis_Integral.jpg}
      \end{center}
      \end{theorem}
      \begin{proof}
      We prove a slightly weaker form of the theorem with the additional hypothesis that $\varphi$ is strictly monotonic. 
      \end{proof}

      \subsubsection{Additive Interval Functions and the Integral}
      In this section we take a step back and construct the integral in a more abstract sense, using the concepts of an additive interval function. 

      \begin{definition}[Additive Interval Function]
        An \textbf{additive (oriented) interval function} is a function 
        \[(\alpha, \beta) \mapsto I(\alpha, \beta) \in \mathbb{R}\]
        that assigns a number $I(\alpha, \beta)$ to each ordered pair of points $(\alpha, \beta)$ of a fixed closed interval $[a, b]$ in such a way that the following equality holds for any triple of points $\alpha, \beta, \gamma \in [a, b]$. 
        \[I(\alpha, \gamma) = I(\alpha, \beta) + I(\beta, \gamma)\]
        Notice that the integral holds this property, shown in the theorem on the symmetric property of the integral. It follows that all additive interval functions are anticommutative: 
        \[I(\alpha, \beta) + I(\beta, \alpha) = 0\]
        which immediately results in
        \[I(\alpha, \alpha) = 0\]
      \end{definition}

      \begin{lemma}[Generating Functions of Additive Interval Functions]
        For any function $x \mapsto \mathcal{F}(x)$ that maps points on the interval $[a, b]$ to $\mathbb{R}$, we set
        \[\mathcal{F}(x) \equiv I(a, x)\]
        and by additivity we have
        \[I(\alpha, \beta) = I(\alpha, \beta) - I(a, \alpha) = \mathcal{F}(\beta) - \mathcal{F}(\alpha)\]
        and thus, every additive oriented interval function has the form 
        \[I(\alpha, \beta) = \mathcal{F}(\beta) - \mathcal{F}(\alpha)\]
        By constructing $I$ in this manner, we say that \textbf{the function $\mathcal{F}$ generates the additive function $I$}. 
      \end{lemma}

    \begin{example}
    If $f \in \mathcal{R}[a, b]$, the function $\mathcal{F} = \int_a^x f(t)\,dt$ generates the additive function
    \[I(\alpha, \beta) = \mathcal{F}(\beta) - \mathcal{F}(\alpha) = \int_a^\beta f(t)\,dt - \int_a^\alpha f(t)\,dt = \int_\alpha^\beta f(t)\,dt\]
    \end{example}

      We conclude by stating a sufficient condition for an additive interval function to be generated by an integral. 
      \begin{theorem}
      Suppose the additive function $I(\alpha, \beta)$ defined for points $\alpha, \beta \in [a, b]$ has the property that, for some known function $f \in \mathcal{R}[a, b]$, 
      \[\inf_{x \in [\alpha, \beta]} f(x) (\beta - \alpha) \leq I(\alpha, \beta) \leq \sup_{x \in [\alpha, \beta]} f(x) (\beta - \alpha)\]
      holds for any closed interval $[\alpha, \beta] \subset [a, b]$ ($\alpha \leq \beta$). Then, the additive function $I$ must be the definite integral
      \[I(a, b) = \int_a^b f(x)\,dx\]
      \end{theorem}

      This theorem is extremely useful. It says that if we have any abstract additive interval function $I(\alpha, \beta)$ that satisfies the properties above, then it \textbf{must} be generated by an integral with variable upper limit, meaning that (by the previous example) $I$ itself must be a definite integral! 

      \subsubsection{Arc Length}
      When modeling systems in physics, one of the most fundamental tools we use are path functions that models the movement of a particle in $\mathbb{R}^3$. 

      \begin{definition}[Path]
        A \textbf{path} in $\mathbb{R}^3$ is a continuous mapping $r: [a, b] \subset \mathbb{R} \longrightarrow \mathbb{R}^3$ defined
        \[t \mapsto \big(x(t), y(t), z(t)\big)\]
        of an interval of the real line into $\mathbb{R}^3$ defined by the (continuous) scalar functions $x, y, z$. The endpoints 
        \[A = \big(x(a), y(a), z(a)\big) \text{ and } B = \big(x(b), y(b), z(b)\big)\]
        in $\mathbb{R}^3$ are called the \textbf{initial point} and \textbf{terminal point} of the path. Furthermore, a path is \textbf{closed} if its initial and terminal points coincide. 
      \end{definition}

      \begin{definition}[Support]
        If $\Gamma: I \longrightarrow \mathbb{R}^3$ is a path, the image $\Gamma(I) \subset \mathbb{R}^3$ is called the \textbf{support} of the path. 
      \end{definition}

      \begin{definition}[Simple Paths]
        A path $\Gamma: I \longrightarrow \mathbb{R}^3$ that is injective is called a \textbf{simple path}, or a \textbf{paramaterized curve}, and its support is called a \textbf{curve} in $\mathbb{R}^3$. 

        A closed path $\Gamma: [a, b] \longrightarrow \mathbb{R}^3$ is called a \textbf{simple closed path/curve} if the path $\Gamma: [a, b) \longrightarrow \mathbb{R}^3$ is simple. 
      \end{definition}

      \begin{definition}[Smooth Paths]
        A path $\Gamma: [a, b] \longrightarrow \mathbb{R}^3$ is $C^k$ smooth if the functions $x(t), y(t), z(t)$ are $C^k$ smooth. $\Gamma$ is \textbf{piecewise smooth} if the closed interval $[a, b]$ can be partitioned into a finite number of closed intervals on each of which the corresponding restriction of $\Gamma$ is smooth. 
      \end{definition}

      Now, we are ready to construct the length of a smooth path $\Gamma: [a, b] \longrightarrow \mathbb{R}^3$. Our initial ideas about the length $l[a, b]$ of the path traversed during the time interval $\alpha \leq t \leq \beta$ are as follows: 
      \begin{enumerate}
        \item If $\alpha < \beta < \gamma$, then $l$ is an additive interval function.
        \[l[\alpha, \gamma] = l[\alpha, \beta] + l[\beta, \gamma]\]
        \item If $v(t) = \big( x^\prime (t), y^\prime (t), z^\prime (t)\big)$ is the velocity of the point at time $t$, then 
        \[\int_{x \in [\alpha, \beta]} |v(t)| (\beta - \alpha) \leq l[\alpha, \beta] \leq \sup_{x \in [\alpha, \beta]} |v(t)| (\beta - \alpha)\]
      \end{enumerate}
      Thus, if the functions $x, y, z$ are continuously differentiable on $[a, b]$, this is sufficient condition (by the theorem in the previous subsection) that the additive function $l$ is an integral.

      \begin{definition}[Arc Length Integral]
        The length of a smooth path $\Gamma: [a, b] \longrightarrow \mathbb{R}^3$ is defined by 
        \[l[a, b] \equiv \int_a^b |\Gamma^\prime (t)|\,dt \equiv \int_a^b \sqrt{x^{\prime 2} (t) + y^{\prime 2} (t) + z^{\prime 2} (t)}\, dt\]
        We can visualize this by partitioning the interval $[a, b]$ into the intervals $\Delta_i$, each with point $\xi_i \in \Delta_i$. This would partition the path to $\Gamma(\Delta_i)$, each with points $\Gamma(\xi_i)$, and at each point $\Gamma(\xi_i)$, we can imagine the velocity vector of the curve. By taking the magnitude of this vector $\Gamma^\prime (\xi_i)$, we multiply it by the length of the interval $\Delta x_i$ to get one rectangle, creating an approximation for one partition of the path. 
        \begin{center}
            \includegraphics[scale=0.25]{img/Arc_Length_Integral.PNG}
        \end{center}
        An immediate result of this formula is the formula for the length of a graph of a function $f: [a, b] \longrightarrow \mathbb{R}$ in $\mathbb{R}^2$, by looking at the paramaterization $t \mapsto \Gamma(t) = \big(t, f(t)\big)$. 
        \[l[a,b] \equiv \int_a^b \sqrt{1 + (f^\prime (t))^2}\,dt\]
      \end{definition}

      The question on the effect of paramaterization on the integral now arises. 

      \begin{definition}[Admissible Change of Parameter]
        The path $\Tilde{\Gamma}: [\alpha, \beta] \longrightarrow \mathbb{R}^3$ is obtained from $\Gamma: [a, b] \longrightarrow \mathbb{R}^3$ by an \textbf{admissible change of parameter} if there exists a smooth mapping 
        \[T: [\alpha, \beta] \longrightarrow [a, b]\]
        such that $T(\alpha) = a, T(\beta) = b$, $T^\prime (\tau) > 0$ (that is, the reparamaterization $T$ is monotonic) on $[\alpha, \beta]$, and 
        \[\Tilde{\Gamma} = \Gamma \circ T\]
        The series of mappings can be represented with the following commutative diagram, where $I_{\alpha, \beta} = [\alpha, \beta] \subset \mathbb{R}$ and $I_{a, b} = [a, b] \subset \mathbb{R}$. 
        \[
          \begin{tikzcd}
            I_{\alpha, \beta} \arrow{r}{T} \arrow{rd}{\Tilde{\Gamma}}& I_{a, b} \arrow{d}{\Gamma}\\
             & \mathbb{R}^3
          \end{tikzcd}
        \]
        or with the more detailed visual below (Note that the points are labeled $0, 1, 2, 3, 4, 5$ do not represent numerical values, but rather the order in which the points are paramaterized. We can see from this ordering that $T$ is monotonic.)
        \begin{center}
            \includegraphics[scale=0.25]{img/Admissible_Change_of_Parameter.jpg}
        \end{center}
      \end{definition}

      \begin{theorem}[Invariance of Arclength Integral under Admissible Change of Parameters]
      If a smooth path $\Tilde{\Gamma}: [\alpha, \beta] \longrightarrow \mathbb{R}^3$ is obtained from a smooth path $\Gamma: [a, b] \longrightarrow \mathbb{R}^3$ by an admissible change of parameter, then the lengths of the two paths are equal. That is, a
      \[\int_a^b |\Gamma^\prime (t) |\,dt = \int_\alpha^\beta |\Tilde{\Gamma}^\prime (t)|\,dt \equiv \int_\alpha^\beta |(\Gamma \circ T)^\prime (t)|\,dt\]
      \end{theorem}

    \subsection{Improper Integrals}

      Due to some limitations of the Riemann integral, we cannot integrate over "singularities" where either the interval or the function is unbounded. We develop the tools of improper integration to deal with this problem; there are two types of improper integrals. 

      \begin{definition}[Improper Integral of Unbounded Interval]
        Suppose the function $x \mapsto f(x)$ is defined on the interval $[a, +\infty)$ and is integrable on every closed interval $[a, b]$ contained in that interval. Then, we call the following term
        \[\int_a^{+\infty} f(x)\,dx \equiv \lim_{b \rightarrow + \infty} \int_a^b f(x)\,dx\]
        the \textbf{improper Riemann integral of $f$ over the interval $[a, +\infty)$} and 
        \[\int_{-\infty}^b f(x)\,dx \equiv \lim_{a \rightarrow -\infty} \int_a^b f(x)\,dx \]
        the \textbf{improper Riemann integral of $f$ over the interval $(-\infty, b]$}.If the limit exists, then we say that the integral \textbf{converges} and \textbf{diverges} otherwise. 
      \end{definition}

      \begin{definition}[Improper Integral of Unbounded Function]
        Suppose the function $x \mapsto f(x)$ is defined on the interval $[a, B)$ and integrable on any closed interval $[a, b] \subset [a, B)$. Then, we call the following term
        \[\int_a^B f(x)\,dx \equiv \lim_{b \rightarrow B^-} \int_a^b f(x)\,dx\]
        the \textbf{improper Riemann integral of $f$ over interval $[a, B)$} and
        \[\int_A^b f(x)\,dx \equiv \lim_{a \rightarrow A^+} \int_a^b f(x)\,dx\]
        the \textbf{improper Riemann integral of $f$ over interval $(A,b]$}.
      \end{definition}

      For cohesiveness, we can combine these two definitions of improper integrals into the following one. 

      \begin{definition}[Improper Integrals]
        Let $[a, \omega)$ be a finite or infinite interval and $x \mapsto f(x)$ a function defined on that interval and integrable over every closed interval $[a, b] \subset [a, \omega)$. Then, by definition
        \[\int_a^\omega f(x)\,dx \equiv \lim_{b \rightarrow \omega} \int_a^b f(x)\,dx\]
        if this limit exists as $b \rightarrow \omega, b \in [a, \omega)$. Similarly, given the finite or infinite interval $(\omega, b]$ with $f$ integrable over every closed interval $[a, b] \subset (\omega, b]$, we have
        \[\int_\omega^b f(x)\,dx \equiv \lim_{a \rightarrow \omega} \int_a^b f(x)\,dx\]
        Note that if $\omega \in \mathbb{R}$ and $f \in \mathcal{R}[a, \omega]$, the improper integral is equivalent to the regular Riemann integral. 
        \[\int_a^\omega f(x) = \lim_{b\rightarrow \omega} \int_a^b f(x)\,dx\]
      \end{definition}

      \begin{lemma}[Properties of the Improper Integral]
        Suppose $f, g$ are functions defined on interval $[a, \omega)$ (without loss of generality, we let $\omega$ be the upper limit of integration) and integrable on every closed interval $[a, b] \subset [a, \omega)$. Suppose the improper integrals 
        \[\int_a^\omega f(x)\,dx \text{ and } \int_a^\omega g(x)\,dx\]
        are well-defined. 
        \begin{enumerate}
          \item For any $\lambda_1, \lambda_2 \in \mathbb{R}$ the function $(\lambda_1 f + \lambda_2 g)(x)$ is integrable in the improper sense on $[a, \omega)$ and
          \[\int_a^\omega (\lambda_1 f + \lambda_2 g)(x)\,dx = \lambda_1 \int_a^\omega f(x)\,dx + \lambda_2 \int_a^\omega g(x)\,dx\]
          \item For any $c \in [a, \omega)$, 
          \[\int_a^\omega f(x)\,dx = \int_a^c f(x)\,dx + \int_c^\omega f(x)\,dx\]
          \item If $\varphi: [\alpha, \gamma) \longrightarrow [a, \omega)$ is a smooth strictly monotonic mapping with $\varphi(\alpha) = a$ and $\varphi(\beta) \rightarrow \omega$ as $\beta \rightarrow \gamma^-$, then the improper integral of the function $t \mapsto (f \circ \varphi)(t) \varphi^\prime (t)$ over $[\alpha, \gamma)$ exists and 
          \[\int_a^\omega f(x)\,dx = \int_\alpha^\gamma (f \circ \varphi)(t) \varphi^\prime (t)\,dt\]
        \end{enumerate}
      \end{lemma}

      \subsubsection{Convergence of an Improper Integral}

        Note that by definition, an improper integral 
        \[\int_a^\omega f(x)\,dx \equiv \lim_{b \rightarrow \omega} \int_a^b f(x) \,dx\]
        is a limit of the function 
        \[\mathcal{F}(b) \equiv \int_a^b f(x)\,dx\]
        as $b \rightarrow \omega$. This means that we can use the Cauchy criterion to determine the convergence of this limit, and hence, existence of this improper integral. 

        \begin{theorem}[Cauchy Criterion for Convergence of an Improper Integral]
        If the function $x \mapsto f(x)$ is defined on the interval $[a, \omega)$ and integrable on every closed interval $[a, b] \subset [a, \omega)$, then the integral 
        \[\int_a^\omega f(x)\,dx\]
        converges if and only if for every $\epsilon > 0$ there exists $B \in [a, \omega)$ such that the relation
        \[\Bigg| \int_{b_1}^{b_2} f(x)\,dx \bigg| < \epsilon\]
        holds for any $b_1, b_2 \in [a, \omega)$ satisfying $B < b_1$ and $B < b_2$. 
        \end{theorem}
        \begin{proof}
        We have
        \[\int_{b_1}^{b_2} f(x)\,dx = \int_a^{b_2} f(x)\,dx - \int_a^{b_1} f(x)\,dx = \mathcal{F}(b_2) - \mathcal{F}(b_1)\]
        and therefore the condition is simply the Cauchy criterion for the existence of a limit for the function $\mathcal{F}(b)$ as $b \rightarrow \omega$. 
        \end{proof}

        \begin{definition}[Absolute Convergence of an Improper Integral]
          The improper integral 
          \[\int_a^\omega f(x)\,dx\]
          \textbf{converges absolutely} if the integral
          \[\int_a^\omega |f|(x)\,dx\]
          converges. Clearly, the inequality
          \[\Bigg| \int_{b_1}^{b_2} f(x)\,dx \Bigg| \leq \Bigg| \int_{b_1}^{b_2} |f|(x)\,dx \Bigg|\]
          implies that if an improper integral converges absolutely, then it converges. 
        \end{definition}

        This study of absolute convergence reduces to the study of convergence of integrals of nonnegative functions. The following lemma is useful in determining convergence of such functions. 

        \begin{lemma}
          Let there be a function $f$ defined on interval $[a, \omega)$ that is also integrable over every closed interval $[a, b] \subset [a, \omega)$. If $f(x) \geq 0$ on $[a, \omega)$, then the improper integral 
          \[\int_a^\omega f(x)\,dx\]
          exists if and only if the function 
          \[\mathcal{F}(b) \equiv \int_a^b f(x)\,dx\]
          is bounded on $[a, \omega)$. 
        \end{lemma}
        \begin{proof}
        It is clear that 
        \[\int_a^\omega f(x)\,dx = \lim_{b \rightarrow \omega} \mathcal{F}(b)\]
        If $f(x)\geq 0$, then the function $\mathcal{F}(b)$ is nondecreasing on $[a, \omega)$ and therefore has a limit as $b \rightarrow \omega$ only if it is bounded (since every monotonically increasing sequence that is bounded always converges). 
        \end{proof}

        This leads to the familiar integral test for convergence of a series. 

        \begin{theorem}[Integral Test for Convergence of a Series]
        If the function $x \mapsto f(x)$ is defined on the interval $[1, +\infty)$, nonnegative, nonincreasing, and integrable on each closed interval $[1, b] \subset [1, +\infty)$, then the series 
        \[\sum_{n=1}^\infty f(n) = f(1) + f(2) + \ldots\]
        and the integral 
        \[\int_a^{+\infty} f(x)\,dx\]
        either both converge or both diverge. 
        \end{theorem}

        We can use the comparison test analogue to determine convergence of improper integrals. 

        \begin{theorem}[Comparison Test for Convergence of Improper Integrals]
        Suppose the functions $f(x), g(x)$ are defined on the interval $[a, \omega)$ and integrable on any closed interval $[a, b] \subset [a, \omega)$. If 
        \[0 \leq f(x) \leq g(x)\]
        on $[a, \omega)$, then 
        \[\int_a^\omega g(x)\,dx \text{ converges} \implies \int_a^\omega f(x)\,dx \text{ converges}\]
        and the inequality 
        \[\int_a^\omega f(x)\,dx \leq \int_a^\omega g(x)\,dx\]
        holds. Also, 
        \[\int_a^\omega f(x)\,dx \text{ diverges} \implies \int_a^\omega g(x)\,dx \text{ diverges}\]
        \end{theorem}
      
      \subsubsection{Improper Integrals with Multiple Singularities}

        \begin{definition}[Improper Integral with Both Limits as Singularities]
          Given singularities $\omega_1, \omega_2$, the improper integral is defined
          \[\int_{\omega_1}^{\omega_2} f(x)\,dx \equiv \int_{\omega_1}^c f(x)\,dx + \int_c^{\omega_2} f(x)\,dx\]
          where $c$ is an arbitrary point in $(\omega_1, \omega_2)$. 
        \end{definition}

      \begin{example}[Gaussian Integral]
      The integral 
      \[\int_{-\infty}^{+\infty} e^{-x^2}\,dx = \sqrt{\pi}\]
      \end{example}

\end{document}

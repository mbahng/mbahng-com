\documentclass{article}
% preamble
  \usepackage[letterpaper, top=1in, bottom=1in, left=1in, right=1in]{geometry}
  \usepackage[utf8]{inputenc}
  \usepackage[english]{babel}
  \usepackage{tikz-cd, lipsum, bm, dcolumn}
  \usetikzlibrary{arrows}
  \usepackage{amsmath, amssymb, amsthm, mathrsfs, mathtools, centernot, hyperref, fancyhdr, lastpage}
  \usepackage{extarrows, esvect, esint, pgfplots}
  \pgfplotsset{compat=1.18}

  \setlength{\parindent}{0pt} % set no indent
  \hfuzz=5.0pt % ignore overfull hbox badness warnings below this limit

  \DeclareMathOperator{\Tr}{Tr}
  \DeclareMathOperator{\Sym}{Sym}
  \DeclareMathOperator{\Span}{span}
  \DeclareMathOperator{\std}{std}
  \DeclareMathOperator{\Cov}{Cov}
  \DeclareMathOperator{\Var}{Var}
  \DeclareMathOperator{\Corr}{Corr}


  \theoremstyle{definition}
  \newtheorem{theorem}{Theorem}[section]
  \newtheorem{proposition}[theorem]{Proposition}
  \newtheorem{lemma}[theorem]{Lemma}
  \newtheorem{example}{Example}[section]
  \newtheorem{corollary}{Corollary}[theorem]
  \theoremstyle{remark}
  \newtheorem*{remark}{Remark}
  \theoremstyle{definition}
  \newtheorem{definition}{Definition}[section]
  \renewcommand{\qed}{\hfill$\blacksquare$}
  \renewcommand{\footrulewidth}{0.4pt}% default is 0pt

  \renewcommand{\thispagestyle}[1]{}

\begin{document}
\pagestyle{fancy}

\lhead{Stochastic Processes}
\chead{Muchang Bahng}
\rhead{\date{Spring 2023}}
\cfoot{\thepage / \pageref{LastPage}}

\title{Stochastic Processes}
\author{Muchang Bahng}
\date{Spring 2023}

\maketitle
\tableofcontents 
\pagebreak 

\section{Introduction}

  Ordinary differential equations model deterministic systems that can be solved exactly through integration. For example, consider the population model determined by a linear DEQ 

    \[\frac{d N} {dt} = \alpha(t) N (t)\]

  where $N$ is the population size and $\alpha$ is a growth rate. Then, we can solve with analysis by integrating the following with a change of basis

  \begin{align*}
    \int \frac{1}{N(t)} \frac{dN}{dt} \,dt = \int \alpha(t) \,dt \iff \int \frac{1}{N} \, dN = \int \alpha(t) \,dt \\
    & \iff N(t) = C \exp \bigg( \int \alpha (t) \,dt \bigg) 
  \end{align*}
  
  This classical exponential growth model is not only continuous, but \textit{smooth}, and it is this smoothness that allows us to do calculus on it. But more realistic models will have noise, which can be modeled by a random variable. Let $\alpha = r + \eta$, where $r$ is the deterministic term and $\eta$ is the random term. Then, integrating gives us 

    \[\frac{dN}{dt} = \big( r(t) + \eta(t) \big) N(t) \iff \int \frac{1}{N} \frac{dN}{dt} \,dt = \int r(t) \,dt + \int \eta(t) \,dt\]
    
  The first integral can be evaluated, but classical calculus does not allow us to integrate the random part. This is where stochastic calculus is needed. Now recall from probability that a random variable over a probability space $(\Omega, \mathcal{F}, \mathbb{P})$ is simply a $\mathcal{F}$-measurable function $X$. As some warm up exercises, let us prove a few examples. 

  \begin{example}[Class 1] 
    
  \end{example}

  \begin{example}[Class 2]
    
  \end{example}

  \begin{definition}[Stochastic Process]
    A \textbf{stochastic process} is a collection of random variables indexed by time $\{X_t\}_{t \in T}$ with their respective measures $\rho_t$. 

    \begin{enumerate}
      \item If $T$ is countable (usually integers), then it is called a \textbf{discrete-time} stochastic process. 
      \item If $T$ is continuous, then it is called a \textbf{continuous-time} stochastic process. 
    \end{enumerate}

    It is also good to think of it as a probability distribution over a space of paths. 
  \end{definition}

  We first start off with Markov processes. We can divide them into four kinds, depending on whether we are using discrete or continuous time, and whether we are using discrete or continuous state space. Since process over continuous state space is a natural generalization of those in a discrete one, we only distinguish between the times. When talking about continuous time, there are additional operators we must introduce, such as generators. Before we go any further, I would like to mention that these set of notes will write down the transition matrices of Markov chains as left-stochastic matrices, as they are usually written in convention. Therefore, a transition matrix would look like 

    \[\mathbb{P} = \begin{pmatrix} P(1, 1) & \ldots & P(d, 1) \\ \vdots & \ddots & \vdots \\ P(1, d) & \ldots & P(d, d) \end{pmatrix} \]

  where $P(i, j)$ represents the probability of transition from state $i$ to state $j$. Therefore, the rows must sum to $1$. I use this notation because it is consistent with when we are working with Markov processes over general measurable state spaces. Note that we will denote in math font general objects and operators ($X_t, \rho_t, P_s, \pi$) and their realization as vectors and matrices in bold font ($\boldsymbol{\rho_t}, \mathbf{P_s}, \boldsymbol{\pi}$). 

  \subsection{Transitioning from Discrete to Continuous State Space}

    Let us remind ourselves of the definitions involving Markov chains over a discrete state space. Let $X_t$ be the state at time $t$. The discrete distribution of $X_t$ can be represented as a column vector $\boldsymbol{\rho_t}$, where $\boldsymbol{\rho_t} (i) = \mathbb{P}(X_t = i)$, and we can calculate the distribution of $X_{t + s}$ as 

      \[\boldsymbol{\rho_{t + s}}^T = \boldsymbol{\rho_{t}}^T \boldsymbol{P_s} \]

    where $\boldsymbol{P_s}$ is a stochastic matrix. Note that representing a discrete measure on discrete $S = \{1, \ldots, d\}$ with a vector really just a notational convenience for computations. We must properly distinguish the three: 

    \begin{enumerate}
      \item the actual state $X_t$ 
      \item the probability distribution $\rho_t$, which is a measure 
      \item the PMF vector $\boldsymbol{\rho_t}$, which is just a convenient representation of $\rho_t$ in the way that 
      \[\boldsymbol{\rho_t} (i) = \rho_t (\{i\}) = \mathbb{P}(X_t = i)\]
      That is, the $i$th element is just the measure on the singleton set $\{i\} \in \mathcal{S} = 2^S$. 
    \end{enumerate}

    The PMF vector $\boldsymbol{\rho_t}$ is really just a way to describe $X_t$ and its distribution, which is redundant. Furthermore, when we try to describe states $X_t$ in general measure spaces $(S, \mathcal{S})$, we cannot think of it as a vector anymore. This is not a problem in even countable spaces since we can just assign $\boldsymbol{\rho_t} (i) = \mathbb{P}(X_t = i)$ in a finite space, but for uncountably infinite spaces we cannot do this. Therefore, we must have some measurable \textbf{function} $f: S \rightarrow \mathbb{R}$ that extracts this kind information from $X_{t}$. Therefore, we must really work with the following: 

    \begin{enumerate}
      \item the actual state $X_t: (\Omega, \mathcal{F}, \mathbb{P}) \longrightarrow (S, \mathcal{S})$ 
      \item the probability distribution $\rho_t$ of the state $X_t$ 
      \item a collection of $\mathcal{S}$-measurable functions $f: S \longrightarrow \mathbb{R}$ that describes the state 
    \end{enumerate}

    At this point, we are not sure what $f$ is since it seems quite arbitrary. But if we fix some $A \in \mathcal{S}$ and take $f = 1_A$, then $1_A (X_t)$ encodes the information of whether $X_t$ is in $A$ or not. This is quite nice, since now we can think of the PMF vector $\boldsymbol{\rho_t}$ as having components defined by the functions 

      \[\boldsymbol{\rho_t} (i) = 1_{\{i\}} (X_t) = \mathbb{P}(X_t = i)\]

    The following theorem formalizes this concept. 

    \begin{theorem}
      Two random variables $X, Y : (\Omega, \mathcal{F}, \mathbb{P}) \rightarrow (S, \mathcal{S})$ have the same distribution if 

        \[\mathbb{E}[f(X)] = \mathbb{E}[f(Y)]\]

      for all $\mathcal{F}$-measurable $f: S \rightarrow \mathbb{R}$, which can be seen by setting $f = 1_A$ for any $A \in \mathcal{F}$. 

      \begin{align*}
        \mathbb{E}[1_A (X)] = \mathbb{E}[1_A (Y)] & \implies \mathbb{P}(X \in A) = \mathbb{P}(Y \in A) \\
        & \implies \mathbb{P}_X (A) = \mathbb{P}_Y (A)
      \end{align*}
      and so the measure that $X$ and $Y$ pushes forward to $(S, \mathcal{S})$ is precisely the same. This does not mean that they are the same random variable. 
    \end{theorem}

    Let's talk more about $f$ in the discrete case setting. We know that the discrete distributions are represented by a column vector. It is true that every measurable function can be written as a linear combination of simple (indicator) functions, and so in a discrete space $S = \{1, \ldots, d\}$, we can write every $f$ as 

      \[f = \sum_{i \in S} f_i 1_{\{i\}} \]

    which outputs $f_i$ if its input is $i$. We can interpret it as a column vector $\mathbf{f} = (f_1, \ldots, f_d)^T$ . We can see that 

      \[\boldsymbol{\rho_t}^T \mathbf{f} = \begin{pmatrix} \boldsymbol{\rho_t} (1) & \ldots & \boldsymbol{\rho_t} (d) \end{pmatrix} \begin{pmatrix} f_1 \\ \vdots \\ f_d \end{pmatrix} = \mathbb{E}[f(X_t)]\]

    and if $\mathbf{f}$ is any standard unit vector, say $(1, 0, 0)$ with $d = 3$, then

      \[\boldsymbol{\rho_t}^T \mathbf{f}  = \begin{pmatrix} \boldsymbol{\rho_t} (1) & \boldsymbol{\rho_t} (2) & \boldsymbol{\rho_t} (3) \end{pmatrix} \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix}= \mathbb{E}[1_{\{1\}} (X_t)] = \mathbb{P}(X_t = 1)\]

    Therefore, every time we compute $\mathbb{E}[f(X_t)]$, we can think of it in the discrete case as dotting $\boldsymbol{\rho_t}$ with a function vector $\mathbf{f}$ to extract whatever we want from the vector $X_t$. And as we will find out later, the linearity of the stochastic matrix $\mathbf{P_s}$ is analogous to the linearity of the Markov semigroup $P_s$.  

    Therefore, our Markov process is really just some stochastic process $\{X_t\}_{t \geq 0}$ over some measurable space $(S, \mathcal{S})$ with the property that 

      \[\mathbb{P}(X_{t + s} \in A \mid \{X_r \in B_r\}_{r \leq t}) = \mathbb{P}( X_{t + s} \in A \mid X_t \in B_t)\]

    where $A \in \mathcal{S}$, and this captures the discrete case by setting $A = \{j\} \in 2^S$ which gives 

      \[\mathbb{P}( X_{t + s} = j \mid \{X_r = i_r\}_{r \leq t}) = \mathbb{P}( X_{t + s} = j \mid X_t = i_t)\]

    This basically says that the probability that $X_{t + s}$ lying in $A$ is only dependent on its present state $X_t \in B_t$, not the history $\{X_r \in B_r\}_{r \leq t}$. In fact, by using the identity $\mathbb{E}[1_A] = \mathbb{P}(A)$ and setting $f = 1_A$, we can capture this effect for \textit{all} measurable $f: (S, \mathcal{S}) \rightarrow (\mathbb{R}, \mathcal{R})$. Thus, the Markov property now looks like  

      \[\mathbb{E}[f (X_{t + s}) \mid \{X_r \in B_r\}_{r \leq t}] = \mathbb{E}[ f(X_{t + s}) \mid X_t \in B_t]\]

    We don't need to fix the $X_r$'s into sets $B_r$'s and so we can write 

      \[\mathbb{E}[f (X_{t + s}) \mid \{X_r\}_{r \leq t}] = \mathbb{E}[ f(X_{t + s}) \mid X_t]\]


    Now let's talk about this Markov property. It is true that $\sigma$-algebra $\sigma(\{X_r\}_{r \leq t})$ is bigger than $\sigma(X_t)$; the Markov property does not imply that they are the same size. Rather, we should interpret this as the extra information introduced by the bigger $\sigma(\{X_r\}_{r \leq})$ is irrelevant. This is analogous to trying to approximate a function with a pointlessly large $\sigma$-algebra. For example, given a piecewise function $X$ defined on the unit interval $\Omega = [0, 1]$, let $\mathcal{G}$ be the $\sigma$-algebra generated by $[0, 0.5), [0.5, 1]$ and $\mathcal{H}$ be that generated by $[0, 0.25), [0.25, 0.5), [0.5, 0.75), [0.75, 1]$. 

    \begin{center}
      \includegraphics[scale=0.3]{img/pointless_approximation.jpg}
    \end{center}

    Then, we can see that 

      \[\mathbb{E}[X \mid \mathcal{G}] = \mathbb{E}[X \mid \mathcal{H}]\]

    That is, the two random variables are exactly equal, even though $\mathcal{H}$ has more information than $\mathcal{G}$. Note that this is not the law of iterated expectations. This rule does not say that $\mathbb{E}[\mathbb{E}[X \mid \mathcal{G}]] = \mathbb{E}[ \mathbb{E}[X \mid \mathcal{H}]]$; this law is true regardless. Rather, this property is a special property of the function $X$, and therefore the Markov property is a special property of the stochastic process $\{X_t\}_{t \geq 0}$. 

\section{Discrete-Time Markov Processes}

  \begin{definition}[DTMP]
    Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space and $(S, \mathcal{S})$ a measurable space. Then, a homogeneous \textbf{discrete-time Markov process} is a stochastic process $\{X_n\}_{n \in \mathbb{N}}$ which takes values in $S$ (i.e. $X_n: \Omega \rightarrow S$) satisfying the \textbf{Markov property}: for every bounded measurable $f$ and $n \geq 1$, 

      \[\mathbb{E}[f(X_{n + m}) \mid \{X_r\}_{r=0}^n] = \mathbb{E}[f (X_{n + m}) \mid X_n] = (P_m f) (X_n)\]

    Since this is true for all $n$, this process is \textbf{time-homogeneous}. Note that both sides are random variables, and it says that the best estimate of $f(X_{n+m})$ as a function of $\{X_r\}_{r=0}^n$ can be simply expressed as as a function of the current $X_n$. Notice also that we have given a specific label $P_m f$ to the conditional expectation on the right hand side. 
  \end{definition}

  Since every $X_n$ has distribution $\rho_n$, we can describe the entire distribution of $X_n$ by "extracting" our desired information $f$ with 

    \[\mathbb{E}[f(X_n)] = \int_S f \, \rho_n\]

  Now, if we wanted to extract information $f$ from $X_{n + m}$, we may not know its distribution $\rho_{n + m}$, but the Markov property allows us to condition $X_n$ (which we know the distribution of) by integrating over the measure $\rho_n$, which we do know: 

    \[\mathbb{E}[f(X_{n + m}] = \mathbb{E}[ \mathbb{E}[f(X_{n + m}) \mid X_n]] = \mathbb{E}[(P_m f) (X_n)] = \int_S P_m f \, \rho_n\]

  So, $P_m$ is an operator that allows us to compute anything about the distribution of $X_{n + m}$ from the measure of $X_n$. That is, $\rho_{n + m} (f) = \rho_n (P_m f)$. 

    \[\mathbb{E}[f(X_{n + m})] = \int_S f \, \rho_{n + m} = \int_S P_m f \, \rho_n = \mathbb{E}[ (P_m f) (X_n)]\] 

  for all measurable $f$. Let us now show how $P_1 = P$ realizes as a matrix in the discrete state space case. 

  \begin{example}[Transition Operator as a Matrix in Discrete Space]
    Given $S = \{1, \ldots, d\}$, let us construct a column vector $\boldsymbol{\rho_n}$ representing the distribution of $X_n$. Then, 

    \begin{align*}
      \boldsymbol{\rho_{n+1}} (j) & = \mathbb{P}(X_{n + 1} = j) \\
      & =  \mathbb{E}[1_{\{j\}} (X_{n+1})] \\
      & = \mathbb{E} [ \mathbb{E}[1_{\{j\}} (X_{n+1}) \mid X_n] ] && = \mathbb{E}[(P 1_{\{j\}}) (X_n)] \\
      & = \int_S \mathbb{E}[1_{\{j\}} (X_{n+1}) \mid X_n] \, d\rho_n && = \int_S P 1_{\{j\}} (X_n) \, d\rho_n \\
      & = \sum_{i \in S} \mathbb{P}[ X_{n+1} = j \mid X_n = i] \, \mathbb{P}(X_n = i) && = \sum_{i \in S} P 1_{\{j\}} (i) \, \mathbb{P}(X_n = i) 
    \end{align*}

    which can be summarized as 

    \[\boldsymbol{\rho_{n + 1}} (j) = \sum_{i = 1}^d P 1_{\{j\}} (i) \boldsymbol{\rho_n} (i) = \sum_{i = 1}^d \mathbb{P}(X_{n+1} = j \mid X_n = i) \, \boldsymbol{\rho_n} (i)\]

    We can compactly organize the probabilities of these internode travel inside a $d \times d$ right stochastic \textbf{transition matrix}

      \[\mathbf{P_t} = \begin{pmatrix} P 1_{\{1\}} (1) & \ldots & P 1_{\{1\}} (d) \\ \vdots & \ddots & \vdots \\ P 1_{\{d\}} (1) & \ldots & P 1_{\{d\}} (d) \end{pmatrix} = \begin{pmatrix} \mathbb{P}(X_{n+1} = 1 \mid X_n = 1) & \ldots & \mathbb{P}(X_{n+1} = d \mid X_n = 1) \\ \vdots & \ddots & \vdots \\ \mathbb{P}(X_{n+1} = 1 \mid X_n = d) & \ldots & \mathbb{P}(X_{n+1} = d \mid X_n = d) \end{pmatrix} \]

    and compactly write the above equation as 

      \[\boldsymbol{\rho_{n + 1}}^T = \boldsymbol{\rho_{n}}^T \mathbf{P_t}\]

    It immediately follows from computation that $P_m$ is realized as $\mathbf{P}^m$, the $m$th power of matrix $\mathbf{P}$, which can also be shown by the Chapman-Kolmogorov equation below. 
  \end{example}

  Therefore, this linear operator $P_m$ can be seen as analogous to the probability transition matrix $\mathbf{P_m}$ of a Markov chain. We know that since they are matrices, from first glance we would guess that $P_m$ is linear. This is indeed trivial by linearity of conditional expectation. 

  \begin{lemma}
    $P_m$ is a linear operator. That is, for $\alpha, \beta \in \mathbb{R}$, and bounded measurable functions $f, g$, 

      \[P_m (\alpha f + \beta g) = \alpha P_m f + \beta P_m g\] 
  \end{lemma}
  \begin{proof}
    By linearity of conditional expectation, 

    \begin{align*}
      (P_m (\alpha f + \beta g))(X_n) & = \mathbb{E}[(\alpha f + \beta g)(X_{n+m}) \mid X_n] \\
      & = \mathbb{E}[(\alpha f) (X_{n + m}) \mid X_n] + \mathbb{E}[(\beta g) (X_{n+m}) \mid X_n] \\
      & = \alpha (P f) (X_n) + \beta (P g) (X_n)
    \end{align*}
  \end{proof}

  We can now interpret linearity and the Markov property in the discrete space. 

  \begin{example}[Markov Property in Discrete Space]
    If we wanted to extract information from $X_n$ with function $f$ (i.e. compute $\mathbb{E}[f(X_n)]$), we can calculate 

      \[\mathbb{E}[f(X_n)] = \boldsymbol{\rho_n}^T \mathbf{f} =  \begin{pmatrix} \boldsymbol{\rho_n} (1) & \ldots & \boldsymbol{\rho_n} (d) \end{pmatrix} \begin{pmatrix} f_1 \\ \vdots \\ f_d \end{pmatrix}\] 

    Now, say that $m$ units of time later, we want to extract information $f$ from $X_{n + m}$ by computing 

      \[\mathbb{E}[f(X_{n + m})] = \boldsymbol{\rho_{n + m}}^T \mathbf{f} = \begin{pmatrix} \boldsymbol{\rho_{n+m}} (1) & \ldots &  \boldsymbol{\rho_{n+m}} (d) \end{pmatrix} \begin{pmatrix} f_1 \\ \vdots \\ f_d \end{pmatrix}\]

    The problem is that we don't know what the distribution of $X_{n + m}$ is (i.e. don't know $\boldsymbol{\rho_{n + m}} (i)$), so we get its expectation by conditioning it on $X_n$, which realizes as taking the expectation of a \textit{different} function $P_m f$ with respect to $\rho_n$. 

      \[\mathbb{E}[f(X_{n + m})] = \mathbb{E}[ \mathbb{E}[ f(X_{n + m}) \mid X_n]] = \mathbb{E}[(P_m f)(X_n)] = \begin{pmatrix} \boldsymbol{\rho_n} (1) & \ldots & \boldsymbol{\rho_n} (d) \end{pmatrix} \begin{pmatrix} (P_m f)_1 \\ \vdots \\(P_m f)_d \end{pmatrix}\]

    It turns out that this transformation $\mathbf{f} \mapsto \mathbf{P_m} \mathbf{f}$ (from row vector to row vector) is linear, and so we can interpret $\mathbf{P_m}$ as $\mathbf{f}$ that has been left-multiplied by some transformation matrix $\mathbf{P_m}$. 

      \[\begin{pmatrix} \boldsymbol{\rho_n} (1) & \ldots & \boldsymbol{\rho_n} (d) \end{pmatrix} \begin{pmatrix} (P_m f)_1 \\ \vdots \\(P_m f)_d \end{pmatrix} = \begin{pmatrix} \boldsymbol{\rho_n} (1) & \ldots & \boldsymbol{\rho_n} (d) \end{pmatrix} \underbrace{\begin{pmatrix} && \\ & \mathbf{P_m} & \\ && \end{pmatrix} \begin{pmatrix} f_1 \\ \vdots \\ f_d \end{pmatrix}}_{\mathbf{P_m f}}\]

    It turns out that this $\mathbf{P_m}$ acts linearly on $\mathbf{f}$ through left multiplication, but we can also right-multiply $\boldsymbol{\rho_n}$ by $\mathbf{P_m}$ to get the new distribution of $X_{n + m}$! 

      \[\begin{pmatrix} \boldsymbol{\rho_n} (1) & \ldots & \boldsymbol{\rho_n} (d) \end{pmatrix} \begin{pmatrix} (P_m f)_1 \\ \vdots \\(P_m f)_d \end{pmatrix} = \underbrace{\begin{pmatrix} \boldsymbol{\rho_n} (1) & \ldots & \boldsymbol{\rho_n} (d) \end{pmatrix} \begin{pmatrix} && \\ & \mathbf{P_m} & \\ && \end{pmatrix}}_{\boldsymbol{\rho_n}^T \mathbf{P_m} = \boldsymbol{\rho_{n+m}}^T} \begin{pmatrix} f_1 \\ \vdots \\ f_d \end{pmatrix}\]

    Therefore, it turns out that the linearity of $\mathbf{P_m}$ on $\mathbf{f}$ implies linearity of it on the vector $\boldsymbol{\rho_n}$. 
  \end{example}

  Now focusing on $f = 1_A$, we can define the following. 

  \begin{definition}[Transition Probability]
    Let us have Markov process $(X_n)$ with operator $P_m$. The function $p_m: S \times \mathcal{S} \rightarrow \mathbb{R}$ defined 

      \[p_m(x, A) \coloneqq P_m 1_A (x) = \mathbb{E}[ 1_A (X_{n+m}) \mid X_n = x] = \mathbb{P}(X_{n + m} \in A \mid X_n = x)\]

    is the \textbf{transition probability}, or \textbf{transition kernel}, of this chain. Note that 

    \begin{enumerate}
      \item For each $x \in S$, $A \mapsto p_m(x, A)$ is a probability measure on $(S, \mathcal{S})$. This means that if we are in some place $x$ at time $n$, then the probability that we will land in some subset $A \in \mathcal{S}$ of $S$ at time $n+m$ is $p_m(x, A)$. 
      \item For each $A \in \mathcal{S}$, $P_m 1_A = p_m (\cdot, A)$ is a measurable function. 
    \end{enumerate}



      \[p(x, A) = \int_A p(x, y) \,dy\]
  \end{definition}

  Note that by the law of total probability, we must have 

    \[\int_S dp(x) = 1 \text{ and } \int_S dp^{(m)} (x) = 1\]

  Given that we have an initial distribution $X_0 \sim \mu_0$, we can see that the distribution $X_1 \sim \mu_1$ is defined as 

  \begin{align*}
    \mathbb{P}(X_1 \in A_1) & = \int_{A_0} \mathbb{P}(X_1 \in A_1 \mid X_0 = x) \, \mathbb{P}(X_0 = x) \,dx \\
    & = \int_{A_0} p(x_0, A_1) \, \mu_0 (d x_0) 
  \end{align*}

  Note that in the matrix realization of the example above, it looks like $P_m$ acts on the distribution $\rho_n$ to get a new distribution $\rho_{n + m}$, but this is not strictly the case since $P_m$ is an operator on $f$. However, for the sake of intuitiveness, we can interpret $P_m$ in two ways: 

  \begin{enumerate}
    \item It operates on the measure $\rho_n$ by pushing it forward in time to get $\rho_{n + m}$. This operator is defined as 

      \[\rho_n \mapsto \rho_{n + m}(\cdot) = p_m (X_n, \cdot)\]

    which corresponds to the matrix multiplication $\boldsymbol{\rho_n}^T \mapsto \boldsymbol{\rho_{n + m}}^T = \boldsymbol{\rho_{n}}^T \mathbf{P_m}$

    \item It operates on the function $f$ (at $X_{n + m}$) by pulling it back to $P_m f$ that operates on $X_n$. This operation $f \mapsto P_m f$ corresponds to the matrix multiplication $\mathbf{f} \mapsto \mathbf{P_m} \mathbf{f}$. 
  \end{enumerate}
  Either way, we can think of the order of operations as either $(\boldsymbol{\rho_n}^T \mathbf{P_m}) \mathbf{f}$ or $\boldsymbol{\rho_n}^T (\mathbf{P_m} \mathbf{f})$. 

  Just like stochastic transition matrices, we can also deduce a semigroup property of the collection $(P_m)_{m \in \mathbb{N}}$. 

  \begin{lemma}[Chapman-Kolmogorov Equation]
    Given the operator $P$, we have 

      \[P_{m + k} = P_m P_k\]

    which indicates 

      \[p_{m + k} (x, A) = \int_S p_k (x, y) \, p_m (y, A) \,dy\]
  \end{lemma}
  \begin{proof}
    We can compute 
    \begin{align*}
      P_{m + k} f (X_n) & = \mathbb{E}[ f (X_{n + m + k}) \mid X_{n}] \\
      & = \mathbb{E}[ \mathbb{E}[ f(X_{n + m + k}) \mid X_{n + m}, X_n] \mid X_n] \\
      & = \mathbb{E}[ \mathbb{E}[ f(X_{n + m + k}) \mid X_{n + m}] \mid X_n] \\ 
      & = \mathbb{E}[ P f_k (X_{n + m}) \mid X_n] \\ 
      & = P_m P_k f (X_n)
    \end{align*}
  \end{proof}

  \begin{example}[Chapman-Kolmogorov in Discrete Space]
    By conditioning on intermediate nodes, we can compute that 

      \[\mathbf{P_{m + k}} (i, j) = \sum_{s \in S} \mathbf{P_m} (i, s)\, \mathbf{P_k} (s, j) \implies \mathbf{P_{m + k}} = \mathbf{P_m} \mathbf{P_k}\]

    which can be seen by setting $x = i$ and $A = \{j\} \in 2^S$ in the transition probability above. 

      \[\mathbf{P_{m + k}} (i, j) = p_{m + k} (i, \{j\}) = \int_S p_m (i, \{s\})\, p_k (s, \{j\}) \,ds = \sum_{s \in S} p_m (i, \{s\})\, p_k (s, \{j\}) = \sum_{s=1}^d \mathbf{P_m} (i, s) \mathbf{P_k} (s, j)\]

    and summing this for each entry gives $\mathbf{P_{m + k}} = \mathbf{P_m} \mathbf{P_k}$. By setting $k = 1$, an immediate consequence of this is that the $m$ step transition probability $\mathbb{P}(X_{n + m} = j \mid X_n = i)$ is simply $\mathbf{P}^m (i, j)$, the $k$th power of the transition matrix $\mathbf{P}$. 
  \end{example}

  We give one more property. 

  \begin{lemma}[Conservativeness]
    $\{P_m\}$ satisfies 

      \[P_m 1 = 1\]

    for all $m \geq 0$, where $1 = 1_S$ is the constant function of $1$.
  \end{lemma}
  \begin{proof}
    This is trivial since it is just the law of total probability. That is, $1_S (X_n) = 1$, and 

      \[(P_m 1_S) (X_n) = \mathbb{E}[ 1_S (X_{n + m}) \mid X_n]\]

    and note that $\sigma(X_n)$ is a finer $\sigma$-algebra than that generated by $1_S (X_{n + m})$, meaning that the right hand side is equal to $1_S (X_{n + m})$ itself, which equals $1$. 
  \end{proof}

  In discrete spaces, this property realizes into the fact that the transition matrix is stochastic, since the constantly $1$ function $f = \sum_{i \in S} 1_{\{i\}}$ realizes into the $(1, \ldots, 1)$ vector, and 

    \[\begin{pmatrix} && \\ & \mathbf{P_m} & \\ && \end{pmatrix} \begin{pmatrix} 1 \\ \vdots \\ 1 \end{pmatrix} = \begin{pmatrix} 1 \\ \vdots \\ 1 \end{pmatrix}\]

  if and only if $\mathbf{P_m}$ is stochastic. But this is quite redundant for discrete spaces since the fact that $\mathbf{P_m}$ acts on the indicator functions as $P_s 1_{\{j\}} (i) = \mathbb{P}(X_{t + s} = j \mid X_t = i)$ already implies that it should be stochastic (by law of total probability). 

  We provide with a variety of examples. 

  \begin{example}[Random Walks]
    A \textit{random walk} on the integers $\mathcal{S} = \mathbb{Z}$ where a point has equal probability of moving right or left can be modeled with the probability transition matrix. 

      \[\mathbf{P}(i, j) = \mathbb{P}(X_{n+1} = j \, | \, X_n = i) = \begin{cases}
      \frac{1}{2} & j = i + 1 \\
      \frac{1}{2} & j = i - 1\\
      0 & otherwise
      \end{cases}\]

    This can be generalized to multiple dimensional random walks on graphs with probability function 

      \[\mathbf{P}(i, j) = \frac{1}{\text{deg}(i)}\]

    where $\mathrm{deg}(i)$ is the number of adjacent nodes to node $i$. In this way, the point hops randomly from node to node, and if the graph is connected, then the walker can visit any vertex in the graph. 
  \end{example}

  \begin{example}[Discrete Moran Model]
    Consider a population of size $N$. Each individual is one of two types (say, red or blue). At each time step, the system evolves in the following way: First, one of the individuals is chosen uniformly at random to be eliminated from the population; and another individual is chosen uniformly at random to produce one offspring identical to itself. These two choices are made independently. So, if a red individual is chosen to reproduce, and a blue one is chosen for elimination, then the total number of red particles increases by one and the number of blue particles decreases by one. If a red is chosen for reproduction and a red is chosen for elimination, then there is no net change in the number of reds and blues. Let $X_n$ be the number of red individuals at time $n$. The transition matrix for this chain is

      \[\mathbf{P}(j, i) = \begin{cases}
      \frac{i}{N} \bigg(\frac{N-i}{N} \bigg) & j=i-1, i \neq 0 \\
      \bigg(\frac{N-i}{N} \bigg) \frac{i}{N} & j=i+1, i \neq N \\
      1 - 2 \bigg(\frac{N-i}{N} \bigg) \frac{i}{N} & j = i \\
      0 & \text{otherwise}
      \end{cases}\]

    Note that the states $X_n = 0$ and $X_n = N$ are absorbing states, which represents a phenomenon called \textit{fixation}. 
  \end{example}

  \subsection{Classification of States}

    \subsubsection{Stopping Time and Strong Markov Property}

      \begin{definition}[Stopping Time]
        Given a stochastic process $\{X_n\}$, a nonnegative integer random variable $T$ is called a stopping time if for all integers $k \geq 0$, $T \leq k$ depends only on $X_0, \ldots, X_k$. 
      \end{definition}

      \begin{example}[Coin Toss]
        Let $\{X_n\}$ be a stochastic process with $X_n - X_{n - 1}$ be iid standard Gaussians, with $X_0 = 0$. Then, 
        \begin{enumerate}
          \item Let $T = \min\{n \geq 1 \mid X_n > 10\}$ be the first time that we surpass $10$. This is a stopping time since 

            \[\mathbb{P}(T = k) = \mathbb{P}(X_0 \leq 10, X_1 \leq 10, \ldots, X_{k-1} \leq 10, X_{k} > 10)\]
          
          \item Let $T = \min\{n \geq 1 \mid X_{n+1} - X_n < 0\}$ be the time of the first peak. This is not a stopping time because you can't determine whether we have peaked at time $k$ by looking at the $X_n$'s up to $k$. You need information on $X_{n + 1}$. 
          
          \item Let $T = \min\{n \geq 1 \mid X_{n} - X_{n-1} < 0\}$ be the first time we have gone down from a peak. This is a stopping time since 

            \[\mathbb{P}(T = k) = \mathbb{P}(X_0 < X_1 < X_2 < \ldots < X_{k-1} > X_k)\]
        \end{enumerate}
      \end{example}

      \begin{definition}[Time of Return]
        Given a stochastic process, let the stopping time 

          \[T_A \coloneqq \min \{ n \geq 1 \mid X_n \in A\}\]

        be the random variable defined as the \textbf{time of first return to $A$} (being there at time $t = 0$ doesn't count). Let 
        Let $T^1_A = T_A$ and for $k \geq 2$, 

          \[T_A^k \coloneqq \min \{ n > T^{k-1}_A \mid X_n \in A\}\]

        be the \textbf{stopping time of the $k$th return to $A$}. 
      \end{definition}

      Since stopping at time $k$ depends only on the values $X_0, \ldots, X_k$, and in a Markov chain the distribution of the future only depends on the past through the current state, it should not be hard to believe that the Markov property holds at stopping times. 

      \begin{theorem}[Strong Markov Property]
        Suppose $T$ is a stopping time. Then, for natural $k \geq 1$, 
          \[\mathbb{P}(X_{T + k} = j \mid X_T = i, \ldots, X_0 = i) = \mathbb{P}(X_k = j \mid X_0 = i) \]
      \end{theorem}

    \subsubsection{Irreducibility}

      \begin{definition}[Closed Set, Absorbing State]
        A set $A \subset S$ is \textbf{closed} if it is impossible to get out. 

          \[\mathbb{P}(X_{n + 1} \in A \mid X_n \in A) = 1\]

        If $A = \{i\}$ is a singleton set in some discrete state space, then $i$ is said to be an \textbf{absorbing state}. 

          \[\mathbb{P}(X_{n+1} \neq i \; | \; X_n = i) = 0\]
      \end{definition}

      \begin{definition}[Recurrence, Transience]
        A state $x \in S$ is called \textbf{recurrent} if 

          \[\rho_{xx} = \mathbb{P}(T_x < \infty \mid X_0 \in A) = 1\]

        i.e. if the chain returns to $x$ infinitely many times. $x$ is said to be \textbf{transient} if $\rho_{xx} < 1$, and so eventually the Markov chain does not find its way back to $x$ ever again. 
      \end{definition}

      \begin{definition}[Communication]
        We say that $x \in S$ communicates with $y \in S$, denoted $x \rightarrow y$, if 

          \[\rho_{xy} \coloneqq \mathbb{P}(T_y < \infty \mid X_0 = y) > 0\]

        That is, there is a positive probability that we will jump from $x$ to $y$ in a finite amount of steps. We can also see this as there existing an $m > 0$ such that $\mathbb{P}(X_m = y \mid X_0 = x) p^m (x, y) > 0$. 
      \end{definition}

      \begin{lemma}
        The following hold. 

        \begin{enumerate}
          \item If $x \rightarrow y$ and $y \rightarrow z$, then $x \rightarrow z$. 
          \item If $\rho_{xy} > 0$ but $\rho_{yx} = 0$, then $x$ is transient.  
          \item If $x$ is recurrent and $\rho_{xy} > 0$, then $\rho_{yx} = 1$. 
        \end{enumerate}
      \end{lemma}

      \begin{definition}[Irreducible Set]
        A set $B \subset S$ is called \textbf{irreducible} if for all $i, j \in B$, $i$ communicates with $j$. 
      \end{definition}

      \begin{theorem}
        If $C$ is a finite closed and irreducible set, then all states in $C$ are recurrent. 
      \end{theorem}

      \begin{theorem}[Decomposition]
        If the state space $S$ is finite, then $S$ can be written as a disjoint union 

          \[T \cup R_1 \cup \ldots \cup R_k\]

        where $T$ is a set of transient states and $R_i$ are closed irreducible sets of recurrent states. 
      \end{theorem}

      \begin{lemma}
        If $x$ is recurrent and $x \rightarrow y$, then $y$ is recurrent. 
      \end{lemma}

      \begin{lemma}
        In a finite closed set there has to be at least one recurrent state. 
      \end{lemma}

    \subsubsection{Periodicity}

      \begin{definition}[Period]
        For any state $x \in \mathcal{S}$, the \textbf{period} of $x$ is defined to be

          \[d(x) \equiv \gcd \{n \geq 1 \; | \; P^{(n)} (x, x) > 0\}\]
      \end{definition}

      \begin{lemma}
        If $p(x, x) > 0$ (not $\rho_{xx} > 0$!), then $x$ has period $1$. 
      \end{lemma}

      \begin{theorem}
        If two states $x$ and $y$ communicate, then they must have the same period

          \[d(x) = d(y)\]

        It naturally follows that if $B \subset S$ is irreducible, then all states must have the same period. 
      \end{theorem}

      \begin{definition}
        If an irreducible chain has period $1$, the chain is said to be \textbf{aperiodic}. Otherwise, the chain is \textit{periodic} with period $d > 1$. 
      \end{definition}

  \subsection{Stationary Measures}

    Recall that a discrete time Markov process $(X_n)_{n \in \mathbb{N}}$ evolves, and this evolution can be described by the sequence of measures $(\rho_n)_{n \geq 0}$ for each $X_n$. If we would like to measure $X_{n + m}$ with function $f$, we can calculate $\mathbb{E}[f(X_{n + m})] = \mathbb{E}_{\rho_{n + m}} [f]$, but we don't know $\rho_{n + m}$. Fortunately, we can "pull back" the $f$ to compute the equivalent 

      \[\mathbb{E}_{\rho_{n + m}} [f] = \mathbb{E}[f(X_{n + m})] = \mathbb{E}[\mathbb{E}[ f(X_{n + m}) \mid X_n]] = \mathbb{E}[P_m f (X_n)] = \mathbb{E}_{\rho_{n}} [ P_m f] \]

    which essentially measures $X_{n + m}$ with $f$ by measuring $X_n$ with $P_m f$. Now, we want to construct a stationary measure $\mu$ that captures the fact that if a certain state $X_n \sim \rho_n = \mu$, then the measure of future $X_{n + m} \sim \rho_{n + m} = \mu$ also. If $\mu$ is stationary, then both $\rho_{n + m} = \rho_n = \mu$, and this is equivalent to

      \[\mathbb{E}_\mu [f] = \mathbb{E}_\mu [P_m f]\]

    for all measurable $f$ and $m \geq 0$. This will be the definition that we will work with. To help with the interpretation, we can restrict the case to $f = 1_A$ to get $\mathbb{P}(X_n \in A) = \mathbb{P}(X_{n + m} \in A)$ for all $A \in \mathcal{S}$, which means that the probability of $X_{n + m}$ realizing in $A$ is equal to the probability of $X_n$ realizing in $A$. In summary, stationary measures describe the equilibrium or steady-state behavior of the Markov process.  

    \begin{definition}[Stationary Measure]
      A probability measure $\mu$ is called \textbf{stationary} or \textbf{invariant} if 

        \[\mathbb{E}_\mu[f] = \mathbb{E}_\mu [P_m f] \text{, conventionally written as } \mu(f) = \mu(P_m f)\]

      for all $m \geq 0$ and bounded measurable $f$. This is a property of the \textit{measure}. 
    \end{definition}

    To give a pictorial interpretation, imagine an initial distribution $X_0 \sim \rho_0$ as some amount of sand placed on the state space $S$ (either as a continuous mass or mounds on discrete nodes). After one step, the distribution will evolve to $X_1 \sim \rho_1$, where a different mound of sand will form on $S$. If $\rho_0 = \mu$, then the flow of sand between the nodes will balance each other out, and we still have the same amount of sand $\rho_1 = \mu$ after each step. The discrete case is simpler, since we can just imagine there being $\boldsymbol{\pi} (i)$ of sand at node $i$, and $\mathbf{P} (i, j)$ of its proportion of sand flowing from node $i$ to $j$ at each step. Therefore, all the sand flowing out of $i$, which is $\sum_{j=1}^d P(i, j) \boldsymbol{\pi}(i) = 1$, balances out with the flow of sand into $i$, which is $\sum_{j=1}^d P(j, i) \boldsymbol{\pi}(j)$. 

      \[1 = \sum_{i=1}^d P(i, j) \boldsymbol{\pi}(i) = \sum_{j=1}^d P(j, i) \boldsymbol{\pi}(j)\]

    and doing this for all $i$ realizes into the matrix equation $\boldsymbol{\pi} = \boldsymbol{\pi} \mathbf{P}$. 

    \begin{example}[Stationary Distribution in Discrete Space]
      Given discrete state space $S = \{1, \ldots, d\}$, our stationary measure $\mu$ can be represented by the all familiar vector 

        \[\boldsymbol{\pi} = \begin{pmatrix} \boldsymbol{\pi} (1) & \ldots & \boldsymbol{\pi} (d) \end{pmatrix} = \begin{pmatrix} \mu(\{1\}) & \ldots & \mu(\{d\}) \end{pmatrix}\] 

      Given the PMF vectors $\boldsymbol{\rho_n} = \boldsymbol{\pi}$ and $\boldsymbol{\rho_{n + m}} = \boldsymbol{\pi}$ and some measurable function $\mathbf{f} = (f_1, \ldots, f_d)^T$, the stationary distribution property says that 

        \[\mathbb{E}[f(X_{n + s})] = \mathbb{E}[(P_m f)(X_n)] \iff \boldsymbol{\pi} \mathbf{f} = \boldsymbol{\pi} \mathbf{P_m} \mathbf{f}\]

      which means that $\mathbf{P_m} \mathbf{f}$ will act on $\boldsymbol{\pi}$ the same way that $\mathbf{f}$ does (though $\mathbf{P_m} \mathbf{f} \neq \mathbf{f}$). We can also interpret $\boldsymbol{\pi}$ as the eigenvector of $\mathbf{P}$ with eigenvalue $1$, so that it is invariant. 
    \end{example}

    \begin{example}[Two Node System]
      Let us have a two node system with nodes labeled $L$ and $R$. That is, $\mathcal{S} = \{L, R\}$. Consider a chain on this state space with transition probability matrix. 

        \[\mathbf{P} = \begin{pmatrix}
        1-a & a \\ b & 1-b 
        \end{pmatrix}\]

      which can be visualized in the following diagram below.

      \begin{center}
        \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,
                          thick,main node/.style={circle,draw}]
          \node[main node] (R) {R};
          \node[main node] (L) [left of=R] {L};
          \path[every node/.style={font=\sffamily\small}]
          (L) edge [loop left] node {1-a} (L)
              edge [bend left] node {a} (R)
          (R) edge [loop right] node {1-b} (R)
              edge [bend left] node {b} (L);
        \end{tikzpicture}
      \end{center}

      Then, the stationary distribution is 

        \[\boldsymbol{\pi} = \Big( \frac{b}{a+b}, \frac{a}{a+b} \Big)\]

      Notice that if $a = b = 0$, then this definition is ill-defined, and any probability distribution is invariant since $P = I_2$, the identity matrix. 
    \end{example}

    This is also stationary since with certain conditions, the limiting behavior of the chain converges to $\pi$, but we will prove that later. 

    \begin{definition}[Doubly Stochastic Chains]
      A transition matrix $\mathbf{P}$ is said to be \textbf{doubly stochastic} if its columns also sum to $1$. 
    \end{definition}

    \begin{theorem}
      Given a Markov chain with state space $S = \{1, \ldots, d\}$, its transition probability matrix $\mathbf{P}$ is doubly stochastic if and only if its stationary distribution is the uniform distribution 

        \[\boldsymbol{\pi} = \bigg( \frac{1}{d}, \frac{1}{d}, \ldots, \frac{1}{d} \bigg)\]
    \end{theorem}
    \begin{proof}
      We prove the only if part. Let $\pi(i) = 1/N$ for all $i = 1, \ldots, N$. Then, for $j = 1, \ldots, N$, 

        \[(\boldsymbol{\pi} \mathbf{P}) (i) = \sum_{j=1}^N \pi(j) \mathbf{P}(j, i) = \frac{1}{N} \sum_{j=1}^N \mathbf{P}(j, i) = \frac{1}{N} = \pi(i) \]

      The if part is very similar. 
    \end{proof}


    \subsubsection{Uniqueness}

      TBD
      TBD

    \subsubsection{Reversed Markov Process}

      From now, given the state space $(S, \mathcal{S})$ we can put a measure $\mu$ on it to get a measure space $(S, \mathcal{S}, \mu)$. The Banach space of all $\mu$-measurable functions $f: (S, \mathcal{S}, \mu) \rightarrow (\mathbb{R}, \mathcal{R})$ (i.e. for every Borel $B \in \mathcal{R}$, $f^{-1}(B) \in \mathcal{S}$) will be denoted $L^p (\mu)$, equipped with the norm 
      \[||f||_{L^p(\mu)} \coloneqq \mathbb{E}_\mu [f^p]^{1/p} = \bigg( \int_S |f|^p \,d\mu \bigg)^{1/p}\]
      If $p = 2$, then we can define the inner product 
      \[\langle f, g \rangle_\mu \coloneqq \mathbb{E}_\mu [f g] = \int_S f g \, d\mu\]

      \begin{lemma}[Contraction of Stationary Measure]
      Let $\mu$ be a stationary measure. Then, 
      \[||P_t f||_{L^p(\mu)} \geq ||f||_{L^p (\mu)} = \mathbb{E}_\mu [f^p]^{1/p}\]
      \end{lemma}

      Now, we can construct reversed Markov processes. 

      \begin{definition}[Reversed Markov Process]
      Let $\{X_n\}_{n=0}^N$ be a discrete time Markov process with transition operator $P = P_1$ (and semigroup $(P_m = P^m)$) and stationary distribution $\mu$. Then, fix $N$ and let $Y_n = X_{N - n}$. Then, $Y_n$ is a discrete time Markov process with the \textbf{dual transition operator} $P^*$, the adjoint of $P$ satisfying 
      \[\langle f, P g \rangle_\mu = \langle P^* f, g \rangle_\mu\]
      for all bounded measurable $f, g \in L^2 (\mu)$. 
      \end{definition}

      Though we have given the reversed Markov process as a definition above, we can prove that this satisfies the Markov property. 

      \begin{proof}

      \end{proof}

      We can see how this definition realizes in a discrete space. 

      \begin{example}
      Given $S = \{1, \ldots, d\}$ and function vectors $\mathbf{f}, \mathbf{g}$, 
      \[\langle f, g \rangle_\mu = \int_S f g d\mu = \sum_{i=1}^d f_i g_i \pi(i)\]
      and by definition of the adjoint, we must have 
      \begin{align*}
          \langle f, P g \rangle_\mu = \sum_{i=1}^d f_i (\mathbf{P} \mathbf{g})_i \pi(i) & = \sum_{i=1}^d f_i \bigg( \sum_{j=1}^d \mathbf{P}(i, j) g_j \bigg) \pi(i) \\
          & = \sum_{i=1}^d g_i \bigg( \sum_{j=1}^d \mathbf{P}^* (i, j) f_j \bigg) \pi(i) = \sum_{i=1}^d (\mathbf{P}^* \mathbf{f})_i \, g_i \, \pi(i) = \langle P^* f, g \rangle_\mu 
      \end{align*}
      A bit of computation will show us that 
      \[\mathbf{P}^*(i, j) = \frac{\mathbf{P}(j, i) \pi(j)}{\pi(i)}\]
      and we can indeed check that 
      \begin{align*}
          \langle P^* f, g \rangle_\mu  & = \sum_{i=1}^d g_i \bigg( \sum_{j=1}^d \mathbf{P}^* (i, j) f_j \bigg) \pi(i) \\
          & = \sum_{i=1}^d g_i \bigg( \sum_{j=1}^d f_j \frac{\mathbf{P}(j, i) \pi(j)}{\pi(i)} \bigg) \pi(i) \\
          & = \sum_{j=1}^d \sum_{i=1}^d g_i \, f_j \mathbf{P}(j, i) \, \pi(j) \\
          & = \sum_{j=1}^d f_j \bigg( \sum_{i=1}^d g_i \mathbf{P}(j, i) \bigg) \pi(j) \\
          & = \sum_{j=1}^d f_j (\mathbf{P} \mathbf{g})_j \pi(j) = \langle f, P g \rangle_\mu
      \end{align*}
      Note that $\mathbf{P}^*$ also satisfies $\mathbf{P}^* (i, j) \geq 0$ and by definition of the stationary distribution $\pi$, 
      \[\sum_{j=1}^d \mathbf{P}^* (i, j) = \sum_{j=1}^d \frac{\mathbf{P}(j, i) \pi(j)}{\pi(i)} = \frac{1}{\pi(i)} \sum_{j=1}^d \mathbf{P}(j, i) \pi(j) = \frac{\pi(i)}{\pi(i)} = 1 \]
      \end{example}

      Note that the transition probability is computed using Bayes rule 
      \begin{align*}
          \mathbf{P^*}(i, j) & = \mathbb{P}(Y_{m + 1} = j \mid Y_m = i) \\
          & = \frac{\mathbb{P}(Y_m = i \mid Y_{m+1} = j) \mathbb{P}(Y_{m+1} = j)}{\mathbb{P}(Y_m = i)} \\
          & = \frac{\mathbb{P}(X_{n-m} = i \mid X_{n-m-1} = j) \mathbb{P}(X_{n-m-1} = j)}{\mathbb{P}(X_{n-m} = i)} \\
          & = \frac{\mathbf{P}(j, i) \pi(j)}{\pi(i)}
      \end{align*}
      and $\{Y_m\}$ also satisfies the Markov property. 
      \begin{align*}
          \mathbb{P}&(Y_{m+1} = j \mid Y_m = i, Y_{m-1} = i_{m-1}, \ldots, Y_0 = i_0) \\
          & = \frac{\mathbb{P}(Y_0 = i_0, \ldots, Y_{m-1} = i_{m-1}, Y_{m} = i, Y_{m+1} = j)}{\mathbb{P}(Y_0 = i_0, \ldots, Y_{m-1} = i_{m-1}, Y_{m} = i)} \\
          & = \frac{\mathbb{P}(X_n = i_0, \ldots, X_{m-n+1} = i_{m-1}, X_{n-m} = i, X_{n-m-1} = j)}{\mathbb{P}(X_n = i_0, \ldots, X_{m-n+1} = i_{m-1}, X_{n-m} = i)} \\
          & = \frac{\mathbb{P}(X_n = i_0, . , X_{m-n+1} = i_{m-1} \mid X_{n-m} = i, X_{n-m-1} = j) \mathbb{P}(X_{n-m} = i \mid X_{n-m-1} = j) \mathbb{P}(X_{n-m-1} = j)}{\mathbb{P}(X_n = i_0, \ldots, X_{m-n+1} = i_{m-1} \mid X_{n-m} = i) \mathbb{P}(X_{n-m} = i)} \\
          & = \frac{\mathbb{P}(X_n = i_0, \ldots, X_{m-n+1} = i_{m-1} \mid X_{n-m} = i) p(j, i) \pi(j)}{\mathbb{P}(X_n = i_0, \ldots, X_{m-n+1} = i_{m-1} \mid X_{n-m} = i) p(i)} \\
          & = \frac{p(j, i) \pi(j)}{p(i)}
      \end{align*}
      Thus, $\{Y_m\}$ is a Markov chain with the indicated transition probability. 

  \subsection{Reversibility (Detailed Balance)}

    Note that reversibility of a Markov process and a reversed Markov process are two entirely different things. There is always a reveresed Markov process, but the fact that it is reversible is a much stronger condition. 

    \begin{definition}[Reversibility]
      The Markov semigroup $\{P_m\}$ with stationary measure $\mu$ is called \textbf{reversible} (or in the physics literature, is said to satisfy \textbf{detailed balance}) if $P_m$ is self-adjoint for every $f, g, \in L^2 (\mu)$. That is, 

        \[\langle f, P_m g \rangle_\mu = \langle P_m f, g \rangle_\mu\]

      By the properties of the adjoint and the Chapman-Kolmogorov equation, we only need to check if $P$ is adjoint. 
    \end{definition}

    Note that if the Markov property is reversible, then assuming $X_0 \sim \mu$, then 

    \begin{align*}
      \langle P_m f, g \rangle_\mu & = \langle f, P_m g \rangle_\mu = \mathbb{E}[ f(X_n) \, \mathbb{E}[g(X_{n + m}) \mid X_n]] \\
      & = \mathbb{E}[ f(X_n) \, g(X_{n + m})] = \mathbb{E}[ \mathbb{E}[ f(X_n) \mid X_{n + m})] \, g(X_{n + m}] 
    \end{align*}

    for every $f, g \in L^2 (\mu)$. So that in particular, 

      \[P_m f (x) = \mathbb{E}[f(X_{n + m} \mid X_n = x] = \mathbb{E}[f(X_n) \mid X_{n + m} = x]\]

    \begin{example}[Detailed Balance in Finite State Space]
      We know that if $P$ is self adjoint, then its transition probability matrix will satisfy 

        \[\mathbf{P}(i, j) = \frac{\mathbf{P}(j, i) \, \pi(j)}{\pi(i)} \implies \mathbf{P}(j, i) \, \pi(j) = \mathbf{P}(i, j) \, \pi(i)\]

      which is the familiar detailed balance condition that we are used to. To see that this is a stronger condition than $\mathbf{P} \boldsymbol{\pi} = \boldsymbol{\pi}$, we sum over $j$ on each side to get 

        \[\sum_j \mathbf{P}(i, j) \, \pi(i) = \pi(i) \, \sum_j \mathbf{P}(i, j) = \pi(j)\]

      Remember that we could interpret $\pi(i)$ as the amount of water at $x$, and we send $\mathbf{P}(j, i) \pi(i)$ water from node $i$ to $j$ in one step. The detailed balance condition tells us that the amount of sand going from $i$ to $j$ in one step is exactly balanced by the amount going back from $j$ to $i$. In contrast, the condition $\boldsymbol{\pi} \mathbf{P} = \boldsymbol{\pi}$ says that after all the transfers are made, the amount of water that ends up at each node is the same as the amount there. 
    \end{example}

    Many chains do not have stationary distributions that satisfy the detailed balance condition. 

    \begin{example}
      Consider the chain with 

        \[\mathbf{P} = \begin{pmatrix} .5 & .5 & 0 \\ .3 & .1 & .6 \\ .2 & .4 & .4 \end{pmatrix}\]

      There is no stationary distribution with detailed balance since $\pi(1) \pi(1, 3) = 0$ but $\mathbf{P}(1, 3) > 0$ so we must have $\pi(3) = 0$. But this would imply that $\pi(3) \mathbf{P}(3, i) = \pi(i) \mathbf{P}(i, 3)$ for all $i$ so we conclude all $\pi(i) = 0$, which doesn't make sense. In fact, the stationary distribution is $(1/3, 1/3, 1/3)$ since $\mathbf{P}$ is doubly stochastic. 
    \end{example}

    \subsubsection{Metropolis-Hastings Algorithm}

      A huge application of Markov chains are in monte carlo algorithms, specifically the Metropolis-Hastings. We begin with a Markov chain with transition probability $q(x, y)$ that is the proposed jump distribution. A move is accepted with probability 

        \[r(x, y) = \min\bigg\{ \frac{\pi(y) q(y, x)}{\pi(x) q(x, y)}, 1 \bigg\}\]

      so the transition probability becomes 

        \[p(x, y) = q(x, y) r(x, y)\]

      Why do we do this? Multiplying by $r$ guarantees that $\pi$ now satisfies detailed balance under $p$. Without loss of generality, we can assume $\pi(y) q(y, x) > \pi(x) q(x, y)$, and so we have 

      \begin{align*}
        \pi(x) p(x, y) & = \pi(x) q(x, y) \,1 \\
        \pi(y) p(y, x) & = \pi(y) q(y, x) \frac{\pi(x) q(x, y)}{\pi(y) q(y, x)} = \pi(x) q(x, y)
      \end{align*}

      which satisfies detailed balance. 

    \subsubsection{Kolmogorov Cycle Condition}

      Let us take a motivating example. 

      \begin{example}
        Consider the chain with transition probability 

          \[p = \begin{pmatrix} 1 - (a + d) & a & d \\ e & 1 - (b + e) & b \\ c & f & 1 - (c + f) \end{pmatrix}\]

        and suppose that all entries are positive. To satisfy detailed balance, we must have $\pi(x) p(x, y) = \pi(y) p(y, x)$ for all $x, y$. So we must have

          \[e \pi(2) = a \pi(1) \;\;\;\;\; f \pi(3) = b \pi(2) \;\;\;\;\; d \pi(1) = c \pi(3)\]

        Multiplying the three equations gives $abc = def$, or in other words, 

          \[\frac{p(1, 2) \, p(2, 3) \, p(3, 1)}{p(2, 1)\, p(3, 2) \, p(1, 3)} = \frac{abc}{def} = 1\]
      \end{example}

      \begin{definition}[Kolmogorov Cycle Condition]
        Given a finite irreducible Markov chain with state space $S$. We say that the \textbf{cycle condition} is satisfied if given a cycle of states $x_0, x_1, \ldots, x_n = x_0$ with $p(x_{i-1}, x_i) > 0$ for $1 \leq i \leq n$, we have 

          \[\prod_{i=1}^n p(x_{i-1}, x_i) = \prod_{i=1}^n p(x_i, x_{i-1})\]
      \end{definition}

      \begin{theorem}
        Given a Markov chain $S$ with transition probability $p$, there exists a stationary distribution $\pi$ that satisfies detailed balance if and only if the cycle condition holds. 
      \end{theorem}

  \subsection{Ergodicity}

    Now, we want to talk about "well-behaved" Markov processes that have a limiting distribution that is the stationary measure, i.e. the process will eventually end up in its steady state $\rho_n \rightarrow \mu$ as $n \rightarrow +\infty$ even if it is not started there. That is, given some fixed initial condition $X_0 = x$, is it true that 

      \[\mathbb{E}[f(X_n) \mid X_0 = x] \rightarrow \mathbb{E}_\mu [f] \text{ as } n \rightarrow \infty\]

    \begin{definition}[Ergodicity]
      The Markov semigroup $(P_n)$ is called \textbf{ergodic} if 

        \[P_n f \rightarrow \mu(f) = \mathbb{E}_\mu [f]\]

      as $n \rightarrow +\infty$ for every $f \in L^2 (\mu)$ (i.e. converges to the constant function $\mu f = \mu(f)$). That is, if we would like to measure $X_n \sim \rho_n$ with $f$, then far enough in time this measurement converges to measuring $X \sim \mu$ with $f$. Since this applies to all $f$ (think $f = 1_A$), we can determine that $\rho_n \rightarrow \mu$ as $n \rightarrow +\infty$. 
    \end{definition}

    The following theorem determines whether a chain is ergodic, but note that we don't know anything about the \textit{rate of convergence} to the stationary measure. 

    \begin{theorem}
      If Markov process $\{X_n\}$ with stationary measure $\mu$ and semigroup $(P_n)$ is irreducible, then $(P_n)$ is ergodic. 
    \end{theorem}

    \begin{theorem}
      Suppose $|S| < \infty$. If the chain is irreducible and all states positive recurrnent, then there always exists a unique stationary distribution $\pi$. If the chain is also aperiodic, then for any initial distribution $\nu$, 

        \[\lim_{k \rightarrow \infty} \nu P^k = \pi \]

      Hence

        \[\lim_{k \rightarrow \infty} P^{(k)}(x, y) = \pi(y)\]

      for all $x, y \in S$. Furthermore, for any measurable function $f: S \longrightarrow \mathbb{R}$, the limit 

        \[\lim_{N \rightarrow \infty} \frac{1}{N} \sum_{n=1}^N f(X_n) = \sum_{x \in S} f(x)\, \pi(x) = \mathbb{E} \big( f(x) \big)\]

      holds with probability $1$. In particular, the limit does not depend on the initial distribution. 
    \end{theorem}
    \begin{proof}
      The Frobenius Extension to Perron's theorem (Linear Algebra, Theorem 7.31) combined with its applications to stochastic matrices (Linear Algebra, Theorem 7.30) proves this statement. 
    \end{proof}

    The next result describes the limiting fraction of time we spend in each state. 

    \begin{theorem}[Asymptotic Frequency]
      Suppose we have a finite Markov chain with $p$ irreducible and all states recurrent. Then, let 

        \[N_n (y) = \sum_{i=1}^n 1_{X_i = y}\]

      be the number of visits to $y$ up to time $n$. Then, 

        \[\frac{N_n (y)}{n} \rightarrow \frac{1}{\mathbb{E}_y [T_y]}\]

      If the chain is aperiodic, then we also have 

        \[\pi(y) = \frac{1}{\mathbb{E}_y [T_y]}\]
    \end{theorem}

    \begin{theorem}
      Suppose that a chain is irreducible and there exists stationary distribution $\pi$. Then, 

        \[\frac{1}{n} \sum_{m=1}^n p^m (x, y) \rightarrow \pi(y)\]

      Thus while the sequence $p^m (x, y)$ will not converge in the periodic case, the average of the first $n$ values will. 
    \end{theorem}

\section{Poisson Processes}

  \subsection{Exponential Distribution}

    Let us do some review. The \textbf{exponential distirbution} of rate $\lambda$ is a random variable $T \sim \mathrm{Exponential}(\lambda)$ with CDF 

      \[F_T (t) = \mathbb{P}(T \leq t) = 1 - e^{-\lambda t}\]

    and the PDF 

      \[f_T (t) = \begin{cases} \lambda e^{-\lambda t} & t \geq 0 \\ 0 & t < 0 \end{cases}\]

    We have 

      \[\mathbb{E}[T] = \frac{1}{\lambda}, \;\; \mathrm{Var}(T) = \frac{1}{\lambda^2}\]

    \begin{lemma}[Memoryless Property]
      The Exp$(\lambda)$ distribution has the property that for all $t, s \geq 0$, 

        \[\mathbb{P}(W > t + s \; | \; W > t) = \mathbb{P}(W > s)\]

      which is called the \textit{memoryless property}. We can interpret this in the following way. Let $W$ be the time you have to wait for the first arrival. Given that you already waited $t$ units of time, the probability that you have the wait $s$ additional units of time is just the probability that you wait at least $s$ from the beginning. That is, knowing that $t$ units of time have elapsed does not affect the distribution of the remaining waiting time. 
    \end{lemma}

    \begin{theorem}
      Let $W$ be a continuously distributed random variable. Then $W \sim$ Exp$(\lambda)$ for some $\lambda > 0$ if and only if $W$ satisfies the memoryless property. 
    \end{theorem}

    \begin{theorem}
      Let $T_i \sim \mathrm{Exponential}(\lambda_i)$ for $i = 1, \ldots n$. Then, 

        \[\min\{T_1, \ldots, T_n\} \sim \mathrm{Exponential}(\lambda_1 + \ldots + \lambda_n)\]

      and the random variable $I$ which takes the index of $\min\{T_1, \ldots, T_n\}$ has the PMF 

        \[\mathbb{P}(I = i) = \frac{\lambda_i}{\lambda_1 + \ldots + \lambda_n}\]
    \end{theorem}

  \subsection{Defining the Poisson Process}

    We first describe a limiting behavior of binomial random variables. 

    \begin{theorem}[Poisson Limit Theorem]
      Let $X_n \sim \mathrm{Bernoulli}(n, p_n)$, where $\{p_n\}_{n \in \mathbb{N}}$ is a sequence of reals in $[0, 1]$ such that 

        \[\lim_{n \rightarrow \infty} n p_n = \lambda\]

      Letting $Y \sim \mathrm{Poisson}(\lambda)$

        \[X_n \xrightarrow{D} Y\]

      That is, the CDFs, and since this is a discrete distribution, the PMFs, converge. 
    \end{theorem}
    \begin{proof}
      We will show that $\lim_{n \rightarrow \infty} \mathbb{P}(X_n = k) = \mathbb{P}(Y = k)$, which shows that the CDFs converge and therefore convergence in distribution. 
      \begin{align*}
        \lim_{n \rightarrow \infty} \mathbb{P}(X_n = k) & = \lim_{n \rightarrow \infty} \binom{n}{k} p_n^k (1 - p_n)^k \\
        & = \lim_{n \rightarrow \infty} \frac{n (n - 1) \ldots (n - k + 1)}{k!} \bigg(\frac{\lambda}{n}\bigg)^k \bigg( 1 - \frac{\lambda}{n} \bigg)^{n - k} \\
        & = \lim_{n \rightarrow \infty} \frac{n^k + O(n^{k-1})}{k!} \frac{\lambda^k}{n^k} \bigg( 1 - \frac{\lambda}{n} \bigg)^{n - k} \\
        & = \lim_{n \rightarrow \infty} \frac{\lambda^k}{k!} \bigg( 1 - \frac{\lambda}{n} \bigg)^{n - k} \\
        & = \frac{\lambda^k}{k!} \lim_{n \rightarrow \infty} \bigg( 1 - \frac{\lambda}{n} \bigg)^n \; \lim_{n \rightarrow \infty} \bigg( 1 - \frac{\lambda}{n} \bigg)^{-k} \\
        & = \frac{\lambda^k}{k!} \, e^{\lambda} \, 1 = \frac{\lambda^k e^\lambda}{k!}
      \end{align*}
    \end{proof}

    Note that this is different from CLT because in CLT, we just assume that the $p_n$'s are constant and take the limiting behavior of $X_n \sim \mathrm{Bernoulli}(n, p)$ as $n \rightarrow \infty$. 

    This result justifies the following model. A Poisson Arrival Process with rate $\lambda > 0$ on the interval $[0, \infty)$ is a model for the occurrence of some events which may have at any time. We can interpret the process as a collection of random points in $[0, \infty)$ which are the times at which the arrivals occur. Suppose that we would like to model the arrival of events that happen completely at random at a rate $\lambda$ per unit time. At time $t = 0$, we have no arrivals yet, so $N(0) = 0$. Let us fix some $T$, and now divide $[0, T)$ into $n$ tiny subintervals of length $\delta$. 

    \begin{center}
      %\includegraphics[scale=0.3]{}
    \end{center}

    Assume that in each time slot, we assign a $X_k \sim \mathrm{Bernoulli}(\lambda \delta)$ random variable that determines whether there was an arrival within the interval $((k-1)\delta, k\delta]$. So with probability $\lambda\delta$, there will be an arrival within it, and as the time interval gets smaller, this probability also gets smaller too. Since every $n$ subinterval is $\mathrm{Bernoulli}(\lambda \delta)$, the number of arrivals in the interval $[0, T)$, defined as the random variable $N_n (T)$, is 

      \[N_n (T) \sim \mathrm{Binomial}(n, \lambda \delta) = \mathrm{Binomial}\big(n, \frac{\lambda T}{n}\big)\]

    As we increase the $n$ (equivalently, decrease $\delta$), we divide $[0, T)$ into smaller and smaller subintervals, resulting in finer and finer $N_n(T)$ Binomial distributions. Since $n p_n = n \frac{\lambda T}{n} = \lambda T$ is finite, we can invoke the Poisson limit theorem and say 

      \[N_n (T) \xrightarrow{D} \mathrm{Poisson}(\lambda T)\]

    Note that the starting point $0$ does not matter, and this works for any interval of length $T$. Therefore, we can model the arrival times on any interval of length $T$ as a $\mathrm{Poisson}(\lambda T)$ random variable. 

    \begin{definition}[Poisson Process]
      Let $\lambda > 0$ be fixed, representing the rate of arrival in some unit time. The stochastic counting process $\{N(t)\}_{t \geq 0}$, where $N(t)$ represents the number of arrivals by time $t$, is called a \textbf{Poisson process} with rate $\lambda$ if 
      \begin{enumerate}
        \item $N(0) = 0$ 

        \item The number of arrivals in any interval of length $s > 0$ is $N(t + s) - N(t) \sim \mathrm{Poisson}(\lambda s)$ 

        \item $N(Tt)$ has independent increments, i.e. if $t_0 < t_1 < \ldots, < t_n$, then 

          \[N(t_1) - N(t_0), \ldots, N(t_n) - N(t_{n-1})\]

        are independent. 
      \end{enumerate}
    \end{definition}

  \subsection{Constructing the Poisson Process}

    Now we have modeled this process using random variables $N(t)$ that counts the number of arrivals up to time $t$. Now, we can interpret it using random variables that represent the \textit{time} in which they arrive. 

    \begin{definition}
      Set $T_0 = 0$. The arrival times are random variables $0 < T_1 < T_2 < T_3 < \ldots$ such that the inter-arrival waiting times

        \[\tau_k = T_k - T_{k-1}, \;\;\; k \geq 0\]

      have the property that $\{W_k\}_{k=1}^\infty$ are independent $\mathrm{Exp}(\lambda)$ random variables. Define 

        \[N(s) \coloneqq \max\{k \mid T_k \leq s \}\]
    \end{definition}

    Now we prove that this process is equivalent to the Poisson process defined before. 

    \begin{theorem}[Equivalent Interpretations]
      Let $\{T_n\}$ be defined as above and $N(s) \coloneqq \max\{k \mid T_k \leq s \}$. Then, 

      \begin{enumerate}
        \item $N(0) = 0$
        \item $N(s) \sim \mathrm{Poisson}(\lambda s)$ 
        \item $N(t + s) - N(t) \sim \mathrm{Poisson}(\lambda s)$ independent of $N(r)$ for $0 \leq r \leq s$. 
        \item $N(t)$ has independent incremements. 
      \end{enumerate}

      $N(s) \coloneqq \max\{k \mid T_k \leq s \}$ is a Poisson distribution with mean $\lambda s$. 
    \end{theorem}

\section{Continuous-Time Markov Processes}

  As the name suggests, in a continuous time Markov process $X_t$, the time parameter is continuous ($t \geq 0$). As before, the system jumps randomly between states in $S$, but now the jumps may occur at any time and they occur randomly. This implies that there are \textit{two} sources of randomness:

  \begin{enumerate}
    \item \textit{where} the system jumps, which is determined by the transition probabilities, and 
    \item \textit{when} the system jumps, which is called the holding time
  \end{enumerate}

  \begin{definition}[CTMP]
    Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space and $(S, \mathcal{S})$ a measurable space. Then, a homogeneous \textbf{continuous-time Markov chain} is a stochastic process $\{X_t\}_{t \geq 0}$ taking values in $S$ (i.e. $X_t: \Omega \rightarrow S$) satisfying the \textbf{Markov property}: for every bounded measurable $f$ and and $t, s \geq 0$, 

      \[\mathbb{E}[ f(X_{t + s}) \mid \{X_r\}_{r \leq t} ] = \mathbb{E}[ f(X_{t + s}) \mid X_t ] = (P_s f)(X_t)\]

    This again says that the probability of $X_{t + s}$ does not depend on the history $\{X_r = i_r\}_{r \leq t}$, but on the current value of $X_t$. 
  \end{definition}

  Just like the discrete-time case, to describe random variable $X_{t + s}$ with function $f$, we can pull back the function to compute 

    \[\mathbb{E}[f(X_{t + s})] = \mathbb{E}[ \mathbb{E}[ f(X_{t + s}) \mid X_t]] = \mathbb{E}[ (P_s f) (X_t)] = \int_S P_s f \, d\rho_t\]

  which integrates a new function $P_s f$ over the measure $\rho_t$. 

  \begin{example}[Transition Operator as a Matrix in Discrete Space]
    Let us have a discrete space $S = \{1, \ldots, d\}$ with indicators $1_{\{i\}}$ for $i = 1, \ldots, d$. Let $x_t$ represent the column vector of the PMF of $X_t$. From the same work as shown for discrete time Markov processes, we can let $f = 1_{\{j\}}$ and compute the probability of $X_{t + s}$ landing in each point $j \in S$, since that is what we're interested in for discrete probability distributions. 

    \begin{align*}
      \boldsymbol{\rho_{t + s}} (j) & = \mathbb{P}( X_{t + s} = j) \\ 
      & = \mathbb{E}[1_{\{j\}} (X_{t + s})] \\
      & = \mathbb{E} [ \mathbb{E}[ 1_{\{j\}} (X_{t + s}) \mid X_t] ] && = \mathbb{E}[P_s 1_{\{j\}} (X_t)] \\
      & = \int_S \mathbb{E}[ 1_{\{j\}} (X_{t + s}) \mid X_t] d\rho_t && = \int_S P_s 1_{\{j\}} (X_t) \,d \rho_t \\
      & = \sum_{i \in S} \mathbb{P}[ X_{t + s} = j \mid X_t = i] \, \mathbb{P} ( X_t = i) && = \sum_{i \in S} P_s 1_{\{j\}} (i) \mathbb{P}(X_t = i) 
    \end{align*}

    which can be summarized as 

      \[\boldsymbol{\rho_{t + s}} (j) = \sum_{i = 1}^d P_s 1_{\{j\}} (i) \boldsymbol{\rho_t} (i) = \sum_{i = 1}^d \mathbb{P}(X_{t + s} = j \mid X_t = i) \, \boldsymbol{\rho_t} (i)\]

    We can compactly organize the probabilities of these internode travel inside a $d \times d$ right stochastic \textbf{transition matrix}

      \[\mathbf{P_s} = \begin{pmatrix} P_s 1_{\{1\}} (1) & \ldots & P_s 1_{\{1\}} (d) \\ \vdots & \ddots & \vdots \\ P_s 1_{\{d\}} (1) & \ldots & P_s 1_{\{d\}} (d) \end{pmatrix} = \begin{pmatrix} \mathbb{P}(X_{t + s} = 1 \mid X_t = 1) & \ldots & \mathbb{P}(X_{t + s} = d \mid X_t = 1) \\ \vdots & \ddots & \vdots \\ \mathbb{P}(X_{t + s} = 1 \mid X_t = d) & \ldots & \mathbb{P}(X_{t + s} = d \mid X_t = d) \end{pmatrix} \]

    and compactly write the above equation as 

      \[\boldsymbol{\rho_{t + s}}^T = \boldsymbol{\rho_{t}}^T \mathbf{P_s}\]
  \end{example}

  \begin{lemma}
    $P_t$ is linear. That is, for $t, s \geq 1$, $\alpha, \beta \in \mathbb{R}$, and bounded measurable functions $f, g$, 

      \[P_t (\alpha f + \beta g) = \alpha P_t f + \beta P_t g\] 
  \end{lemma}
  \begin{proof}
    By linearity of conditional expectation, 
    \begin{align*}
      (P_s (\alpha f + \beta g))(X_t) & = \mathbb{E}[(\alpha f + \beta g)(X_{t + s}) \mid X_t] \\
      & = \mathbb{E}[(\alpha f) (X_{t + s}) \mid X_t] + \mathbb{E}[(\beta g) (X_{t + s}) \mid X_t] \\
      & = \alpha (P_s f) (X_t) + \beta (P_s g) (X_t)
    \end{align*}
  \end{proof}

  We can now interpret linearity and the Markov property in the discrete space. 

  \begin{example}[Markov Property in Discrete Space]
    If we wanted to extract information from $X_t$ with function $f$ (i.e. compute $\mathbb{E}[f(X_t)]$), we can calculate 

      \[\mathbb{E}[f(X_t)] = \boldsymbol{\rho_t}^T \mathbf{f} =  \begin{pmatrix} \boldsymbol{\rho_t} (1) & \ldots & \boldsymbol{\rho_t} (d) \end{pmatrix} \begin{pmatrix} f_1 \\ \vdots \\ f_d \end{pmatrix}\] 

    Now, say that $s$ units of time later, we want to extract information $f$ from $X_{t + s}$ by computing 

      \[\mathbb{E}[f(X_{t + s})] = \boldsymbol{\rho_{t + s}}^T \mathbf{f} = \begin{pmatrix} \boldsymbol{\rho_{t + s}} (1) & \ldots &  \boldsymbol{\rho_{t + s}} (d) \end{pmatrix} \begin{pmatrix} f_1 \\ \vdots \\ f_d \end{pmatrix}\]

    The problem is that we don't know what the distribution of $X_{t + s}$ is (i.e. don't know $\boldsymbol{\rho_{t + s}} (i)$), so we get its expectation by conditioning it on $X_t$, which realizes as taking the expectation of a \textit{different} function $P_s f$ with respect to $\rho_t$. 

      \[\mathbb{E}[f(X_{t + s})] = \mathbb{E}[ \mathbb{E}[ f(X_{t + s}) \mid X_t]] = \mathbb{E}[(P_s f)(X_t)] = \begin{pmatrix} \boldsymbol{\rho_t} (1) & \ldots & \boldsymbol{\rho_t} (d) \end{pmatrix} \begin{pmatrix} (P_s f)_1 \\ \vdots \\(P_s f)_d \end{pmatrix}\]

    It turns out that this transformation $\mathbf{f} \mapsto \mathbf{P_s} \mathbf{f}$ (from row vector to row vector) is linear, and so we can interpret $\mathbf{P_s}$ as $\mathbf{f}$ that has been left-multiplied by some transformation matrix $\mathbf{P_s}$. 

      \[\begin{pmatrix} \boldsymbol{\rho_t} (1) & \ldots & \boldsymbol{\rho_t} (d) \end{pmatrix} \begin{pmatrix} (P_s f)_1 \\ \vdots \\(P_s f)_d \end{pmatrix} = \begin{pmatrix} \boldsymbol{\rho_t} (1) & \ldots & \boldsymbol{\rho_t} (d) \end{pmatrix} \underbrace{\begin{pmatrix} && \\ & \mathbf{P_s} & \\ && \end{pmatrix} \begin{pmatrix} f_1 \\ \vdots \\ f_d \end{pmatrix}}_{\mathbf{P_s f}}\]

    It turns out that this $\mathbf{P_s}$ acts linearly on $\mathbf{f}$ through left multiplication, but we can also right-multiply $\boldsymbol{\rho_t}$ by $\mathbf{P_s}$ to get the new distribution of $X_{t + s}$! 

      \[\begin{pmatrix} \boldsymbol{\rho_t} (1) & \ldots & \boldsymbol{\rho_t} (d) \end{pmatrix} \begin{pmatrix} (P_s f)_1 \\ \vdots \\(P_s f)_d \end{pmatrix} = \underbrace{\begin{pmatrix} \boldsymbol{\rho_t} (1) & \ldots & \boldsymbol{\rho_t} (d) \end{pmatrix} \begin{pmatrix} && \\ & \mathbf{P_s} & \\ && \end{pmatrix}}_{\boldsymbol{\rho_t}^T \mathbf{P_s} = \boldsymbol{\rho_{t+s}}^T} \begin{pmatrix} f_1 \\ \vdots \\ f_d \end{pmatrix}\]

    Therefore, it turns out that the linearity of $\mathbf{P_s}$ on $\mathbf{f}$ implies linearity of it on the vector $\boldsymbol{\rho_t}$. 
  \end{example}

  Now focusing on $f = 1_A$, we can define the following. 

  \begin{definition}[Transition Probability]
  Let us have Markov process $(X_t)$ with operator $P_s$. The function $p_s: S \times \mathcal{S} \rightarrow \mathbb{R}$ defined 

    \[p_s(x, A) \coloneqq P_s 1_A (x) = \mathbb{E}[ 1_A (X_{t+s}) \mid X_t = x] = \mathbb{P}(X_{t + s} \in A \mid X_t = x)\]

  is the \textbf{transition probability}, or \textbf{transition kernel}, of this chain. Note that 

  \begin{enumerate}
    \item For each $x \in S$, $A \mapsto p_s(x, A)$ is a probability measure on $(S, \mathcal{S})$. This means that if we are in some place $x$ at time $t$, then the probability that we will land in some subset $A \in \mathcal{S}$ of $S$ at time $t + s$ is $p_s(x, A)$. 
    \item For each $A \in \mathcal{S}$, $P_s 1_A = p_s (\cdot, A)$ is a measurable function. 
  \end{enumerate}
  The \textbf{transition kernel density} is simply the pdf of the measure $p_s(x, \cdot)$. 

    \[p_s(x, A) = \int_A p_s (x, y) \,dy\]
  \end{definition}

  Note that in the matrix realization of the example above, it looks like $P_s$ acts on the distribution $\rho_t$ to get a new distribution $\rho_{t + s}$, but this is not strictly the case since $P_s$ is an operator on $f$. However, for the sake of intuitiveness, we can interpret $P_s$ in two ways: 

  \begin{enumerate}
    \item It operates on the measure $\rho_t$ by pushing it forward in time to get $\rho_{t + s}$. This operator is defined as 

      \[\rho_t \mapsto \rho_{t + s}(\cdot) = p_s (X_t, \cdot)\]

    which corresponds to the matrix multiplication $\boldsymbol{\rho_t}^T \mapsto \boldsymbol{\rho_{t + s}}^T = \boldsymbol{\rho_{t}}^T \mathbf{P_s}$

    \item It operates on the function $f$ (at $X_{t + s}$) by pulling it back to $P_s f$ that operates on $X_t$. This operation $f \mapsto P_s f$ corresponds to the matrix multiplication $\mathbf{f} \mapsto \mathbf{P_s} \mathbf{f}$. 

  \end{enumerate}

  Either way, we can think of the order of operations as either $(\boldsymbol{\rho_t}^T \mathbf{P_s}) \mathbf{f}$ or $\boldsymbol{\rho_t}^T (\mathbf{P_s} \mathbf{f})$. 

  Just like stochastic transition matrices, we can also deduce a semigroup property of the collection $(P_s)_{s \geq 0}$. 

  \begin{lemma}[Chapman-Kolmogorov]
  $\{P_t\}$ satisfies 
  \[P_{t + s} f = P_t P_s f\]
  for all $t, s, \geq 1$, with $P_0 = I$, the identity. 
  \end{lemma}
  \begin{proof}
  We can easily see that $(P_0 f) (X_t) = \mathbb{E}[f(X_t) \mid X_t] = f(X_t)$, and 
  \begin{align*}
      (P_{t + s} f) (X_n) & = \mathbb{E}[ f(X_{n + t + s}) \mid X_n] \\
      & = \mathbb{E} [ \mathbb{E}[ f(X_{n + t + s} \mid X_{n + t}) ] \mid X_n] \\
      & = \mathbb{E}[(P_s f) (X_{n + t}) \mid X_n] \\
      & = (P_t (P_s f))(X_n) \\
      & = (P_t P_s f) (X_n)
  \end{align*}
  \end{proof}

  We give one final condition. 

  \begin{lemma}[Conservativeness]
  $\{P_t\}$ satisfies 
  \[P_t 1 = 1\]
  for all $t \geq 0$, where $1 = 1_S$ is the constant function of $1$. 
  \end{lemma}
  \begin{proof}
  This is trivial since it is just the law of total probability. That is, $1_S (X_t) = 1$, and 
  \[(P_s 1_S) (X_t) = \mathbb{E}[ 1_S (X_{t + s}) \mid X_t]\]
  and note that $\sigma(X_t)$ is a finer $\sigma$-algebra than that generated by $1_S (X_{t + s})$, meaning that the right hand side is equal to $1_S (X_{t + s})$ itself, which equals $1$. 
  \end{proof}

  \begin{example}
  Given the transition matrix 
  \[\mathbf{P_s} = \begin{pmatrix} P_s 1_{\{1\}} (1) & \ldots & P_s 1_{\{1\}} (d) \\ \vdots & \ddots & \vdots \\ P_s 1_{\{d\}} (1) & \ldots & P_s 1_{\{d\}} (d) \end{pmatrix}\]
  note that by linearity of $P_s$ and the fact that $\{j\}$ forms a partition of $S$, we have a
  \[\sum_{j \in S} (P_s 1_{\{j\}}) (i) = \bigg[ P_s \bigg( \sum_{j \in S} 1_{\{j\}} \bigg) \bigg] \big( i \big) = (P_s 1_S) (i) = 1_S (i) = 1\]
  which means that the columns must sum to $1$. 
  \end{example}

  \begin{example}[Markov Chain with Continuous Jumps]
  Let $N(t), t \geq 0$ be a Poisson process with rate $\lambda$ and let $Y_n$ be a discrete time Markov chain with transition probability $u(i, j)$. Then, $X_t = Y_{N(t)}$ is a continuous time Markov chain that takes one jump according to $u(i, j)$ at each arrival time $N(t)$. 
  \end{example}


  \subsection{Generator}

    In the discrete time case, we had $P_t = (p_1)^t$ for $t \in \mathbb{N}$, and from the Chapman-Kolmogorov equation, knowing $p_1$ allows us to compute $p_t$ for all $t \in \mathbb{N}$. Likewise, if we know the transition probability for some $t < t_0$ for any $t_0 > 0$, we know it for all $t$. This observation suggests that the transition probabilities $p_t$ can be determined from their derivatives at $0$. 

    We now define the analogous operator to the transition rate matrix in continuous-time chains with a finite state space. This is a natural extension, since we are just taking the right-derivative of $P_t$ at $t = 0$. 

    \begin{definition}[Generator]
    The generator $\mathscr{L}$ is defined as 
    \[\mathscr{L} f \coloneqq \lim_{t \downarrow 0} \frac{P_t f - f}{t}\]
    for every $f \in L^2 (\mu)$ for which the above limit exists in $L^2 (\mu)$. Intuitively, $\mathscr{L} f$ represents the instantaneous rate of change of the measurement $f$. The set of $f$ for which $\mathscr{L}f$ is defined is called the domain $\mathrm{Dom}(\mathscr{L})$ of the generator, and $\mathscr{L}$ defines a linear operator from $\mathrm{Dom}(\mathscr{L}) \subset L^2 (\mu)$ to $L^2 (\mu)$. 
    \end{definition}

    We have defined the generator $\mathscr{L}$ from the Markov semigroup $\{P_t\}_{t \geq 0}$. Now, let's try to define the semigroup in terms of the generator $\mathscr{L}$. Given that we have some map $\mathscr{L})$, can we define some semigroup $\{P_t\}$ satisfying the definition? We know that by the semigroup property, we can split $P_{t + h}$ into $P_t P_h$ and $P_h P_t$, from which we get the \textbf{Kolmogorov backward equation} and the \textbf{forward equation}, respectively. 
    \begin{align*}
        \frac{d}{dt} P_t & = \lim_{h \downarrow 0} \frac{P_{t + h} - P_t}{h} = \lim_{h \downarrow 0} \frac{P_t ( P_h - I)}{h} = P_t \bigg( \lim_{h \downarrow 0} \frac{P_h - I}{h} \bigg) = P_t \mathscr{L} \\
        \frac{d}{dt} P_t & = \lim_{h \downarrow 0} \frac{P_{t + h} - P_t}{h} = \lim_{h \downarrow 0} \frac{( P_h - I) P_t}{h} = \bigg( \lim_{h \downarrow 0} \frac{P_h - I}{h} \bigg) P_t = \mathscr{L} P_t
    \end{align*}
    From which we see that the generator $\mathscr{L}$ is commutes with the semigroup 
    \[\mathscr{L} P_t = P_t \mathscr{L}\]
    and solving this differential equation gives 
    \[P_t = e^{t \mathscr{L}}\]

    Let's observe how this generator acts on the indicator functions $f = 1_A$. Note that $P_s 1_A (i) = \mathbb{P}(X_{t + s} \in A \mid X_t = i)$. 
    \[(\mathscr{L} 1_A) (i) = \bigg( \lim_{h \downarrow 0} \frac{P_h 1_A - 1_A}{h} \bigg) (i) = \lim_{h \downarrow 0} \frac{ P_h 1_A (i) - 1_A (i)}{h}\]
    and so $(\mathscr{L} 1_A)(i)$ represents the infinitesimal rate of change of the probability that $X_t$ will be in $A$ given that it is at $1$. 

    Now, how does the generator realize into the finite state space? 

    \begin{example}[Transition Rate Matrix]
      We know that the semigroup operator $P_t$ is equivalent to the transition matrix 

        \[\mathbf{P_t} = \begin{pmatrix} P_t (1, 1) & \ldots & P_t (1, d) \\ \vdots & \ddots & \vdots \\ P_t (d, 1) & \ldots & P_t (d, d) \end{pmatrix}\]

      Let's say that we have the function $f = \sum_{i \in S} c_i 1_{\{i\}}$, which realizes as the function vector $\mathbf{f}$, and we have generator $\mathscr{L}$. We know that $P_t f$ realizes as the matrix multiplication $\mathbf{P_t} \mathbf{f}$, and so we can define the \textbf{transition rate matrix} $\mathbf{Q}$ satisfying the equation 

        \[\mathbf{Q} \mathbf{f}= \lim_{h \rightarrow 0} \frac{\mathbf{P_h} \mathbf{f} - \mathbf{f}}{h} \implies \mathbf{Q} = \lim_{h \rightarrow 0} \frac{\mathbf{P_h} - \mathbf{I}}{h}\]

      This derivatives has entries 

        \[Q(i, j) = \frac{d}{dt} \bigg|_{t = 0} \mathbf{P_t} (i, j) = \lim_{h \rightarrow 0} \frac{\mathbf{P_h} (i, j) - \mathbf{P_0} (i, j)}{h} = \begin{dcases} \lim_{h \rightarrow 0} \frac{P_h (i, j)}{h} & \text{ if } i \neq j \\ \lim_{h \rightarrow 0} \frac{P_h (i, i) - 1}{h} & \text{ if } i = j \end{dcases}\]

      representing the flow of probability from $i \mapsto j$. Note that by the law of total probability, 

        \[\sum_j \mathbf{P_t} (i, j) = 1 \implies \frac{d}{dt} \bigg|_{t = 0} \sum_{j} \mathbf{P_t} (i, j) =  \sum_j \frac{d}{dt} \bigg|_{t = 0} \mathbf{P_t} (i, j) = \sum_{j} \mathbf{Q} (i, j) = 0\]

      So the diagonal entries is simply $\mathbf{Q}(i, i) = - \sum_{j \neq i} Q(i, j)$. This realization $\mathbf{Q}$ is consistent with the way $\mathscr{L}$ operates. Given $f = \sum_i f_i 1_{\{i\}}$, and not worrying about whether we evaluate a limit of functions or the limit of evaluations, we can get 

      \begin{align*}
        (\mathscr{L} f) (i) & = \bigg[ \mathscr{L} \bigg( \sum_{j=1}^d f_j 1_{\{j\}} \bigg) \bigg] (i) = \bigg( \sum_{j=1}^d f_j \mathscr{L} 1_{\{j\}} \bigg) (i) = \sum_{j=1}^d f_j ( \mathscr{L} 1_{\{j\}}) (i) \\ 
        & = \sum_{j=1}^d f_j \bigg( \lim_{h \downarrow 0} \frac{P_h 1_{\{ j\}} (i) - 1_{\{j\}} (i)}{h} \bigg) = \sum_{j=1}^d f_j \bigg( \lim_{h \downarrow 0} \frac{\mathbf{P_h} (i, j) - \mathbf{P_0} (i, j)}{h} \bigg) \\
        & = \sum_{j=1}^d \mathbf{Q}(i, j) f_j = (\mathbf{Q} \mathbf{f})_i 
      \end{align*}

      and therefore, setting $f = 1_{\{j\}}$, we get 

        \[\mathscr{L} 1_{\{j\}} (i) = Q(j, i)\]
    \end{example}

    \begin{example}
      Given a two-state Markov chain, $\{0, 1\}$, with some $\lambda \geq 0$. Then, we can model our transition probability matrix as 

        \[P_s  = \begin{pmatrix} \frac{1}{2} + \frac{1}{2} e^{-2\lambda t} &  \frac{1}{2} - \frac{1}{2} e^{-2\lambda t} \\  \frac{1}{2} - \frac{1}{2} e^{-2\lambda t} &  \frac{1}{2} + \frac{1}{2} e^{-2\lambda t} \end{pmatrix} \]

      Its generator matrix is 

        \[Q = \begin{pmatrix} -\lambda & \lambda \\ \lambda & -\lambda \end{pmatrix}\]
    \end{example}

  \subsection{Classification of States}

    \subsubsection{Holding Times and Jumping Times}

      Now, we would like to find how long a chain stays at a state $x \in S$. 

      \begin{definition}[Holding Time]
        Let $\{X_t\}_{t \geq 0}$ be a continuous time Markov chain, and define $T_x$ to be the \textbf{holding time} at $x$. 
          \[X_t = x, \;\;\; T_x = \inf\{s \geq t, X_s \neq x\} \]
      \end{definition}

      We can characterize the distribution of $T_x$, but first we define the following. 

      \begin{definition}[Memoryless Property]
        A random variable $X$ has the \textbf{memoryless property} if it satisfies for all $t, s \geq 0$ 

          \[\mathbb{P}(X > s + t \mid X > t) = \mathbb{P}(X > s)\]

        which is just abuse of notation for the following: We know that $(t, \infty)$, $(s, \infty)$, and $(s + t, \infty)$ are all in $\mathcal{R}$ and so they are events. So it really translates to the probability of an outcome landing in $(s + t, \infty)$ given that it lands in $(t, \infty)$ is equal the probability of it landing in $(s, \infty)$. 

          \[\mathbb{P}_X \big( (s + t, \infty) \mid (t, \infty) \big) = \frac{\mathbb{P}_X \big( (s + t, \infty) \cap (t, \infty) \big)}{\mathbb{P}_X \big( (t, \infty) \big)} = \frac{\mathbb{P}_X \big( (s + t, \infty) \big)}{\mathbb{P}_X \big( (t, \infty) \big)} = \mathbb{P}_X \big( (s, \infty) \big)\]
      \end{definition}

      The exponential random variable is memoryless because the LHS just reduces to 

        \[\frac{\mathbb{P}_X \big( (s + t, \infty) \big)}{\mathbb{P}_X \big( (t, \infty) \big)} = \frac{1 - F_X (s + t)}{1 - F_X (t)} = \frac{e^{-\lambda(s + t)}}{e^{-\lambda t}} = e^{-\lambda s} = 1 - F_X (s) = \mathbb{P}_X \big( (s, \infty) \big) \]

      \begin{theorem}
        The only continuous random variable having the memoryless property is the exponential random variable. 
      \end{theorem}

      \begin{theorem}
        $T_x$ has the memoryless property. 
      \end{theorem}
      \begin{proof}
        We can show that 

        \begin{align*}
          \mathbb{P}(T_x > t + s \mid T_x > t) & = \mathbb{P}(X_u = x, \, u \in [t, t + s] \mid X_u = x, \, u \in [0, t]) \\
          & = \mathbb{P}(X_u = x, \, u \in [t, t + s] \mid X_t = x) \\
          & = \mathbb{P}(T_x > s) 
        \end{align*}
      \end{proof}

      Therefore, we know that $T_x$ must have the exponential distribution, and for each $x$, we have $T_x \sim \mathrm{Exp}(\lambda_x)$. 

    \subsubsection{Irreducibility}

      \begin{definition}[Irreducibility]
        The Markov chain $X_t$ is \textbf{irreducible} if for any two states $i, j \in S$, it is possible to get from $i$ to $j$ in a finite number of steps. To be precise, there is a sequence of states $k_0 = i, k_1, \ldots, k_n = j$ s.t. 

          \[Q(k_{m-1}, k_m) > 0\]
      \end{definition}

      \begin{lemma}
        If $X_t$ is irreducible and $t > 0$, then $P_t (i, j) > 0$ for all $i, j \in S$. 
      \end{lemma}

  \subsection{Stationary Measures}

    Recall that the Markov process $(X_t)_{t \geq 0}$ evolves, and this evolution can be described by the sequence of measures $(\rho_t)_{t \geq 0}$ for each $X_t$. If we would like to measure $X_{t + s}$ with function $f$, we can calculate $\mathbb{E}[f(X_{t + s})] = \mathbb{E}_{\rho_{t + s}} [f]$, but we don't know $\rho_{t + s}$. Fortunately, we can "pull back" the $f$ to compute the equivalent 

      \[\mathbb{E}_{\rho_{t + s}} [f] = \mathbb{E}[f(X_{t + s})] = \mathbb{E}[\mathbb{E}[ f(X_{t + s}) \mid X_t]] = \mathbb{E}[P_s f (X_t)] = \mathbb{E}_{\rho_{t}} [ P_s f] \]

    which essentially measures $X_{t + s}$ with $f$ by measuring $X_t$ with $P_s f$. Now, we want to construct a stationary measure that captures the fact that if a certain state $X_t \sim \rho_t = \mu$ follows a stationary measure, then the measure of future $X_{t + s} \sim \rho_{t + s} = \mu$ also. If $\mu$ is stationary, then both $\rho_{t + s} = \rho_t = \mu$, and this is equivalent to

      \[\mathbb{E}_\mu [f] = \mathbb{E}_\mu [P_s f]\]

    for all measure $f$ and $s \geq 0$. This will be the definition that we will work with. To help with the interpretation, we can restrict the case to $f = 1_A$ to get $\mathbb{P}(X_t \in A) = \mathbb{P}(X_{t + s} \in A)$ for all $A \in \mathcal{S}$, which means that the probability of $X_{t + s}$ realizing in $A$ is equal to the probability of $X_t$ realizing in $A$. In summary, stationary measures describe the equilibrium or steady-state behavior of the Markov process.  

    \begin{definition}[Stationary Measure]
      A probability measure $\mu$ is called \textbf{stationary} or \textbf{invariant} if 

        \[\mathbb{E}_\mu[f] = \mathbb{E}_\mu [P_t f] \text{, conventionally written as } \mu(f) = \mu(P_t f)\]

      for all $t \geq 0$ and bounded measurable $f$. This is a property of the \textit{measure}. We can describe the way it operates on the measure as if $\rho_t = \mu$, then 

        \[\rho_{t + s} (\cdot) = p_s (X_t, \cdot) = \rho_t\]
    \end{definition}

    To give a pictorial interpretation, imagine an initial distribution $X_0 \sim \rho_0$ as some amount of sand placed on the state space $S$ (either as a continuous mass or mounds on discrete nodes). As time flows continuously, the distribution will evolve to $X_t \sim \rho_t$, where a different mound of sand will form on $S$. If $\rho_0 = \mu$, then the flow of sand between the nodes will balance each other out, and we still have the same amount of sand $\rho_t = \mu$ after each step. The discrete case is simpler, since we can just imagine there being $\boldsymbol{\pi} (i)$ of sand at node $i$, and $\mathbf{P_t} (i, j)$ of its proportion of sand flowing from node $i$ to $j$ after time $t$. Therefore, all the sand flowing out of $i$, which is $\sum_{j=1}^d \mathbf{P_t} (i, j) \boldsymbol{\pi}(i) = 1$, balances out with the flow of sand into $i$, which is $\sum_{j=1}^d P(j, i) \boldsymbol{\pi}(j)$. 

      \[1 = \sum_{i=1}^d P(i, j) \boldsymbol{\pi}(i) = \sum_{j=1}^d P(j, i) \boldsymbol{\pi}(j)\]

    and doing this for all $i$ realizes into the matrix equation $\boldsymbol{\pi} = \boldsymbol{\pi} \mathbf{P_t}$. 


    \begin{example}[Stationary Distribution in Discrete Space]
      Given discrete state space $S = \{1, \ldots, d\}$, our stationary measure $\mu$ can be represented by the all familiar row vector 

        \[\boldsymbol{\pi} = \begin{pmatrix} \boldsymbol{\pi} (1) & \ldots & \boldsymbol{\pi} (d) \end{pmatrix} = \begin{pmatrix} \mu(\{1\}) & \ldots & \mu(\{d\}) \end{pmatrix}\] 

      Given the PMF vectors $\boldsymbol{\rho_t} = \boldsymbol{\pi}$ and $\boldsymbol{\rho_{t + s}} = \boldsymbol{\pi}$ and some measurable function $\mathbf{f} = (f_1, \ldots, f_d)$, the stationary distribution property says that 

        \[\mathbb{E}[f(X_{n + m})] = \mathbb{E}[(P_ms f)(X_n)] \iff \boldsymbol{\pi} \mathbf{f} = \boldsymbol{\pi} \mathbf{P_m} \mathbf{f}\]

      which means that $\mathbf{P_s} \mathbf{f}$ will act on $\boldsymbol{\pi}$ the same way that $\mathbf{f}$ does (though $\mathbf{P_s} \mathbf{f} \neq \mathbf{f}$). We can also interpret $\boldsymbol{\pi}$ as the eigenvector of $\mathbf{P_s}$ with eigenvalue $1$ since $\rho_{t + s} (\cdot) = p_s (X_t, \cdot) = \rho_t(\cdot)$. 
    \end{example}

    \begin{theorem}
      If $\mu$ is a stationary measure of a continuous-time Markov process with generator $\mathscr{L}$, then 

        \[\mu(\mathscr{L} f) = 0\]

      for every $f \in L^2 (\mu)$. 
    \end{theorem}
    \begin{proof}
    Not worrying about interchanging limits and integrals, we have 

    \begin{align*}
      \mu(\mathscr{L} f) = \mathbb{E}_\mu [\mathscr{L} f] & = \int_S \lim_{t \downarrow 0} \frac{P_t f - P_0 f}{t} \,d\mu \\ 
      & = \lim_{t \downarrow 0} \int_S \frac{P_t f - P_0 f}{t} \,d\mu \\
      & = \lim_{t \downarrow 0} \frac{1}{t} \big( \mathbb{E}_\mu [P_t f] - \mathbb{E}_\mu [f] \big) = \lim_{t \downarrow 0} \frac{1}{t} \cdot 0 = 0 
    \end{align*}
    \end{proof}

    For a finite state space, this theorem reduces to the following. 

    \begin{corollary}
      $\pi$ is a stationary distribution of a continuous time Markov chain if and only if 

        \[\boldsymbol{\pi} \mathbf{Q} = \mathbf{0}\]
    \end{corollary}
    \begin{proof}
      To prove the if, we have 

        \[\pi Q = 0 \implies \pi P_t= \pi e^{t Q} = \pi \bigg(I + t Q + \frac{t^2 Q^2}{2!} + \ldots \bigg) = \pi + 0 + \ldots = \pi\]
      To prove the only if, we have 

        \[\pi P_t = \pi \implies 0 = \frac{d}{dt} \pi P_t = \pi \frac{d}{dt} P_t = \pi Q P_t \implies \pi Q = 0\]
    \end{proof}

    \begin{theorem}
      If a continuous-time Markov chain $X_t$ is irreducible and has a stationary distribution $\pi$, then 

        \[\lim_{t \rightarrow \infty} P_t (i, j) = \pi(j)\]
    \end{theorem}

    \subsubsection{Uniqueness} 

      TBD
      TBD

    \subsubsection{Reversed Markov Process}

      From now, given the state space $(S, \mathcal{S})$ we can put a measure $\mu$ on it to get a measure space $(S, \mathcal{S}, \mu)$. The Banach space of all $\mu$-measurable functions $f: (S, \mathcal{S}, \mu) \rightarrow (\mathbb{R}, \mathcal{R})$ (i.e. for every Borel $B \in \mathcal{R}$, $f^{-1}(B) \in \mathcal{S}$) will be denoted $L^p (\mu)$, equipped with the norm 

        \[||f||_{L^p(\mu)} \coloneqq \mathbb{E}_\mu [f^p]^{1/p} = \bigg( \int_S |f|^p \,d\mu \bigg)^{1/p}\]

      If $p = 2$, then we can define the inner product 

        \[\langle f, g \rangle_\mu \coloneqq \mathbb{E}_\mu [f g] = \int_S f g \, d\mu\]

      \begin{lemma}[Contraction of Stationary Measure]
        Let $\mu$ be a stationary measure. Then, 

          \[||P_t f||_{L^p(\mu)} \geq ||f||_{L^p (\mu)} = \mathbb{E}_\mu [f^p]^{1/p}\]
      \end{lemma}

      Now, we can construct reversed Markov processes. 

      \begin{definition}[Reversed Markov Process]
        Let $\{X_t\}_{0 \leq t \leq T}$ be a continuous time Markov process with semigroup $(P_t)_{t \geq 0}$ and stationary distribution $\mu$. Then, fix $T$ and let $Y_t = X_{T - t}$. Then, $Y_t$ is a discrete time Markov process with the \textbf{dual transition operator} $P_t^*$, the adjoint of $P_t$ satisfying 

          \[\langle f, P_t g \rangle_\mu = \langle P_t^* f, g \rangle_\mu\]

        for all bounded measurable $f, g \in L^2 (\mu)$. 
      \end{definition}

      Though we have given the reversed Markov process as a definition above, we can prove that this satisfies the Markov property. 

      \begin{proof}

      \end{proof}

      We can see how this definition realizes in a discrete space. 

      \begin{example}
      Given $S = \{1, \ldots, d\}$ and function vectors $\mathbf{f}, \mathbf{g}$, 

        \[\langle f, g \rangle_\mu = \int_S f g d\mu = \sum_{i=1}^d f_i g_i \pi(i)\]

      and by definition of the adjoint, we must have 

      \begin{align*}
        \langle f, P_t g \rangle_\mu = \sum_{i=1}^d f_i (\mathbf{P_t} \mathbf{g})_i \pi(i) & = \sum_{i=1}^d f_i \bigg( \sum_{j=1}^d \mathbf{P_t}(i, j) g_j \bigg) \pi(i) \\
        & = \sum_{i=1}^d g_i \bigg( \sum_{j=1}^d \mathbf{P_t^*} (i, j) f_j \bigg) \pi(i) = \sum_{i=1}^d (\mathbf{P_t^*} \mathbf{f})_i \, g_i \, \pi(i) = \langle P_t^* f, g \rangle_\mu 
      \end{align*}

      A bit of computation will show us that 

        \[\mathbf{P_t^*}(i, j) = \frac{\mathbf{P_t}(j, i) \pi(j)}{\pi(i)}\]

      and we can indeed check that 

      \begin{align*}
        \langle P_t^* f, g \rangle_\mu  & = \sum_{i=1}^d g_i \bigg( \sum_{j=1}^d \mathbf{P_t^*} (i, j) f_j \bigg) \pi(i) \\
        & = \sum_{i=1}^d g_i \bigg( \sum_{j=1}^d f_j \frac{\mathbf{P_t}(j, i) \pi(j)}{\pi(i)} \bigg) \pi(i) \\
        & = \sum_{j=1}^d \sum_{i=1}^d g_i \, f_j \mathbf{P_t}(j, i) \, \pi(j) \\
        & = \sum_{j=1}^d f_j \bigg( \sum_{i=1}^d g_i \mathbf{P_t}(j, i) \bigg) \pi(j) \\
        & = \sum_{j=1}^d f_j (\mathbf{P_t} \mathbf{g})_j \pi(j) = \langle f, P_t g \rangle_\mu
      \end{align*}

      Note that $\mathbf{P_t^*}$ also satisfies $\mathbf{P_t^*} (i, j) \geq 0$ and by definition of the stationary distribution $\pi$, 

        \[\sum_{j=1}^d \mathbf{P_t^*} (i, j) = \sum_{j=1}^d \frac{\mathbf{P_t}(j, i) \pi(j)}{\pi(i)} = \frac{1}{\pi(i)} \sum_{j=1}^d \mathbf{P_t}(j, i) \pi(j) = \frac{\pi(i)}{\pi(i)} = 1 \]

      Note that the transition probability is computed using Bayes rule 

      \begin{align*}
        \mathbf{P_s^*}(i, j) & = \mathbb{P}(Y_{t + s} = j \mid Y_t = i) \\
        & = \frac{\mathbb{P}(Y_t = i \mid Y_{t + s} = j) \mathbb{P}(Y_{t + s} = j)}{\mathbb{P}(Y_t = i)} \\
        & = \frac{\mathbb{P}(X_{T - t} = i \mid X_{T - t - s} = j) \mathbb{P}(X_{T - t - s} = j)}{\mathbb{P}(X_{T - t} = i)} \\
        & = \frac{\mathbf{P}_s (j, i) \pi(j)}{\pi(i)}
      \end{align*}
      \end{example}

  \subsection{Reversibility (Detailed Balance)}

    Note that reversibility of a Markov process and a reversed Markov process are two entirely different things. There is always a reveresed Markov process, but the fact that it is reversible is a much stronger condition. 

    \begin{definition}[Reversibility]
      The Markov semigroup $\{P_s\}$ with stationary measure $\mu$ is called \textbf{reversible} (or in the physics literature, said to satify \textbf{detailed balance}) if $P_s$ is self-adjoint for every $f, g, \in L^2 (\mu)$. That is, 

        \[\langle f, P_s g \rangle_\mu = \langle P_s f, g \rangle_\mu\]

      Since $P_s = e^{s \mathscr{L}}$, this condition is equivalent to $\mathscr{L}$ being self-adjoint.
    \end{definition}

    Note that if the Markov property is reversible, then assuming $X_0 \sim \mu$, then 

    \begin{align*}
      \langle P_s f, g \rangle_\mu & = \langle f, P_s g \rangle_\mu = \mathbb{E}[ f(X_t) \, \mathbb{E}[g(X_{t + s}) \mid X_t]] \\
      & = \mathbb{E}[ f(X_t) \, g(X_{t + s})] = \mathbb{E}[ \mathbb{E}[ f(X_t) \mid X_{t + s})] \, g(X_{t + s}] 
    \end{align*}

    for every $f, g \in L^2 (\mu)$. So that in particular, 

      \[P_s f (x) = \mathbb{E}[f(X_{t + s} \mid X_t = x] = \mathbb{E}[f(X_t) \mid X_{t + s} = x]\]

    which means that the reversed process follows the same law as the forwrad process. 

    \begin{example}[Detailed Balance in Finite State Space]
      We know that if $P_s$ is self adjoint, then its transition probability matrix will satisfy 

        \[\mathbf{P_s}(i, j) = \frac{\mathbf{P_s}(j, i) \, \pi(j)}{\pi(i)} \implies \mathbf{P_s}(j, i) \, \pi(j) = \mathbf{P_s}(i, j) \, \pi(i)\]

      which is the familiar detailed balance condition that we are used to. To see that this is a stronger condition than $\boldsymbol{\pi} \mathbf{P_t} = \boldsymbol{\pi}$, we sum over $j$ on each side to get 

        \[\sum_j \mathbf{P_s}(i, j) \, \pi(i) = \pi(i) \, \sum_j \mathbf{P_s}(i, j) = \pi(j)\]

      Remember that we could interpret $\pi(i)$ as the amount of water at $x$, and we send $\mathbf{P_s}(j, i) \pi(i)$ water from node $i$ to $j$ in one step. The detailed balance condition tells us that the amount of sand going from $i$ to $j$ in one step is exactly balanced by the amount going back from $j$ to $i$. In contrast, the condition $\boldsymbol{\pi} \mathbf{P_s} = \boldsymbol{\pi}$ says that after all the transfers are made, the amount of water that ends up at each node is the same as the amount there. 
    \end{example}

  \subsection{Ergodicity}

    Now, given a Markov semigroup $P_t$ with generator $\mathscr{L}$ and stationary measure $\mu$, we know that $X_0 \sim \mu$ implies $X_t \sim \mu$ for all times $t$. It is natural to ask whether the Markov process will eventually end up in its steady state even if it is not started there, but rather at some fixed initial condition. That is, given $X_0 = x$, is it true that 

      \[\mathbb{E}[f(X_t) \mid X_0 = x] \rightarrow \mu f = \mathbb{E}_\mu [f] \text{ as } t \rightarrow \infty\]

    If this is the case, the Markov process is said to be ergodic. 

    \begin{definition}[Ergodicity]
      The Markov semigroup $(P_t)$ is called \textbf{ergodic} if 

        \[P_t f \rightarrow \mu f = \mathbb{E}_\mu [f]\]

      as $t \rightarrow +\infty$ for every $f \in L^2 (\mu)$ (i.e. converges to the constant function $\mu f = \mu(f)$). That is, if we would like to measure $X_t \sim \rho_t$ with $f$, then far enough in time this measurement converges to measuring $X \sim \mu$ with $f$. Since this applies to all $f$ (think $f = 1_A$), we can determine that $\rho_t \rightarrow \mu$ as $t \rightarrow +\infty$. 
    \end{definition}

    The following theorem determines whether a chain is ergodic, but note that we don't know anything about the \textit{rate of convergence} to the stationary measure. 

    \begin{theorem}
      If Markov process $\{X_t\}$ with stationary measure $\mu$ and semigroup $(P_t)$ is irreducible, then $(P_t)$ is ergodic. 
    \end{theorem}

\section{Martingales}

  Let us first start with the discrete-time martingale for simplicity. In introductory courses, a martingale might be defined as a stochastic process satisfying 

    \[X_n = \mathbb{E}[X_{n+1} \mid X_0, \ldots, X_n]\]

  for all $n$, which models a "fair game." They also may construct the random variables $\{X_n\}$ first and then define the filtration as the sequence of $\sigma$-algebras $\sigma(X_1, \ldots, X_n)$. In here, we will construct the filtration $\{\mathcal{F}_n\}$ first and then define the random variables to be adapted to the filtration if $X_n$ is $\mathcal{F}_n$-measurable for each $n \in \mathbb{N}$. 

  \begin{definition}[Discrete-Time Martingale]
    Given a probability space $(\Omega, \mathcal{F}, \mathbb{P})$, let $\mathbb{F} = \{\mathcal{F}_n\}_{n \in \mathbb{N}}$ be a filtration (an increasing sequence of $\sigma$-algebras). A sequence $\{X_n\}$ is said to be \textbf{adapted} to $\{\mathcal{F}_n\}$ if $X_n$ is $\mathcal{F}_n$-measurable for all $n$. If the stochastic process $\{X_n\}_{n \in \mathbb{N}}$ is a sequence with 

    \begin{enumerate}
      \item $\mathbb{E}[X_n] < \infty$ for all $n$, 
      \item $X_n$ is adapted to $\mathcal{F}_n$, 
      \item $\mathbb{E}[X_{n+1} \mid X_1, \ldots, X_n] = \mathbb{E}[X_{n+1} \mid \mathcal{F}_n] = X_n$ for all $n$, 
    \end{enumerate}

    then $\{X_n\}$ is a \textbf{martingale}. If $\mathbb{E}[X_{n+1} \mid \mathcal{F}_n] \leq X_n$ or $\mathbb{E}[X_{n+1} \mid \mathcal{F}_n] \geq X_n$, the $\{X_n\}$ is said to be a \textbf{supermartingale} or \textbf{submartingale}, respectively. 
  \end{definition}

  A martingale just represents a sequence of random variables that get finer and finer as the $\sigma$-algebra increases. While they do get finer and finer, they do not change the "average" of the function. For example, consider the filtration generated by finer subsets of the unit interval $\Omega = (0, 1]$. We have 

  \begin{enumerate}
    \item $\mathcal{F}_0 = \{\emptyset, \Omega\}$ 
    \item $\mathcal{F}_1 = \sigma( (0, 0.5], (0.5, 1])$ 
    \item $\mathcal{F}_2 = \sigma( (0, 0.25], (0.25, 0.5], (0.5, 0.75], (0.75, 1] )$
  \end{enumerate}

  Then, we would have 

  \begin{center}
    \includegraphics[scale=0.3]{img/martingale_increments.jpg}
  \end{center}
  
  A supermartingale (and submartingale) just means that as we make the function finer and finer, its mean goes down (or up). 


  Martingales are used to model lots of random walk events. In the following three examples, let $\xi_1, \xi_2, \ldots$ be iid, and let $S_n = S_0 + \xi_1 + \ldots + \xi_n$., where $S_0$ is a constant. Let $\mathcal{F}_n = \sigma(\xi_1, \ldots, \xi_n)$ for $n \geq 1$ and let $\mathcal{F}_0 = \{\emptyset, \Omega\}$. 

  \begin{example}[Linear Martingale]
    Let $\mu = \mathbb{E}[\xi_i] = 0$. Then, $\{S_n\}$ is a martingale with respect to $\mathcal{F}_n$. We show the three requirements: 
    \begin{enumerate}
      \item $\mathbb{E}[S_n] = \mathbb{E}[S_0] + \mathbb{E}[\xi_1] + \ldots + \mathbb{E}[\xi_n] = S_0 < \infty$. 
      
      \item By definition, we know that $\xi_i$ is $\sigma(\xi)$-measurable for all $i \in [n]$, so $\xi_i$ is $\mathcal{F}_n = \sigma(\xi_1, \ldots, \xi_n)$-measurable. Since the set of $\mathcal{F}_n$-measurable functions has a vector space structure, $S_n$ is also $\mathcal{F}_n$-measurable. 
      
      \item We can simply solve 

        \[\mathbb{E}[S_{n+1} \mid \mathcal{F}_n] = \mathbb{E}[S_n \mid \mathcal{F}_n] + \mathbb{E}[\xi_{n+1} \mid \mathcal{F}_n] = X_n + \mathbb{E}[\xi_{n+1}] = X_n\]

      where the first equality follows from linearity. For the second equality, note that $S_n$ is $\mathcal{F}_n$-measurable from above, and so the best $\mathcal{F}_n$-measurable approximation of $X$ is $X$ itself (i.e. we have complete information). We know that $\xi_{n+1}$ is independent of the $\xi_i$'s, and so by definition their $\sigma$-algebras are independent. This implies that $\sigma(\xi_{n+1})$ and $\mathcal{F}_n = \sigma(\xi_1, \ldots, \xi_n)$ are independent, and so due to irrelevant information, $\mathbb{E}[\xi_{n+1} \mid \mathcal{F}_n] = \mathbb{E}[\xi_{n+1}]$. 
    \end{enumerate}
    If $\mu \leq 0$ or $\mu \geq 0$, then the computation above shows that $\mathbb{E}[S_{n+1} \mid \mathcal{F}_n] \leq 0$ or $\mathbb{E}[S_{n+1} \mid \mathcal{F}_n] \geq 0$, making it a supermartingale or submartingale, respectively. 
  \end{example}

  Given a supermartingale or submartingale, we can change it to be a martingale. 

  \begin{example}
  Given that $\mu = \mathbb{E}[\xi_i] \neq 0$, then $\{S_n - n\mu\}$is a martingale with respect to $\mathcal{F}_n$. We can see this because

  \begin{align*}
    \mathbb{E}[S_{n+1} - (n + 1)\mu \mid \mathcal{F}_n] & = \mathbb{E}[S_n - n \mu \mid \mathcal{F}_n] + \mathbb{E}[\xi_{n+1} - \mu \mid \mathcal{F}_n] \\
    & = S_n - n \mu + \mathbb{E}[\xi_{n+1}] - \mu \\
    & = S_n - n
  \end{align*}
  \end{example}

  \begin{example}[Quadratic Martingale]
  Say $\mu = \mathbb{E}[\xi_i] = 0$ and $\sigma^2 = \mathrm{Var}(\xi_i) < \infty$. Then, $\{S_n^2 - n \sigma^2\}$ is a martingale. 

  \begin{align*}
    \mathbb{E}[S_{n+1}^2 - (n + 1) \sigma^2 \mid \mathcal{F}_n] & = \mathbb{E}[ (S_n + \xi_{n+1})^2 - (n _ 1) \sigma^2 \mid \mathcal{F}_n] \\
    & = \mathbb{E}[S^2 - n \sigma^2 \mid \mathcal{F}_n ] + \mathbb{E}[ 2 S_n \xi_{n+1} + \xi_{n+1}^2 - \sigma^2 \mid \mathcal{F}_n] \\
    & = \mathbb{E}[S^2 - n \sigma^2 \mid \mathcal{F}_n ] + 2 \mathbb{E}[ S_n \xi_{n+1} \mid \mathcal{F}_n ] + \mathbb{E}[\xi_{n+1}^2] - \sigma^2  \\
    & = \mathbb{E}[S^2 - n \sigma^2 \mid \mathcal{F}_n ]
  \end{align*}

  where we have used the fact that due to independence of $\xi_{n+1}$ with $\mathcal{F}_n$, we have $\mathbb{E}[S_n \xi_{n+1} \mid \mathcal{F}_n] = \mathbb{E}[S_n \mathbb{E}[ \xi_{n+1} \mid \mathcal{F}_n]] = \mathbb{E}[S_n \cdot 0] = 0$. 
  \end{example}

  This following result shows that martingales with bounded increments either converge or oscillate between $+\infty$ and $-\infty$. 

  \begin{theorem}
    Let $\{X_n\}_{n \in \mathbb{N}}$ be a martingale with $|X_{n+1} - X_n| \leq M < \infty$. Let 

    \begin{align*}
      C & = \{\lim_{n \rightarrow \infty} X_n \text{ exists and is finite}\} \\
      D & = \{\lim_{n \rightarrow \infty} \sup X_n = +\infty \text{ and } \lim_{n \rightarrow \infty} \inf X_n = -\infty\}
    \end{align*}

    Then $\mathbb{P}(C \cup D) = 1$. 
  \end{theorem}

\section{Concentration Inequalities}

  An informal statement of concentration of measure is the following: \textit{If $X_1, \ldots, X_n$ are independent random variables, then the random variable $f(X_1, \ldots, X_n)$ is "close" to its mean $\mathbb{E}[f(X_1, \ldots, X_n)]$ provided that the function $f(x_1, \ldots, x_n)$ is not too "sensitive" to any of the coordinates $x_i$.} Intuitively, say that we have a bunch of independent random variables $X_i$ and sample from them, to get some values $x_i$. Calculating $f(x_1, \ldots, x_n)$, we have sampled from $f(X_1, \ldots, X_n)$. Since $f$ depends smoothly w.r.t. its arguments, to drastically change $f$, we must drastically change all the arguments. This is not likely, since all the $X_i$'s are independent. 

  Most of our intuition about probability in low-dimensional spaces breaks down in high-dimensional ones (on the order of perhaps $10$ or $20$). We start off with two geometric examples in high-dimensional space. 

  \begin{example}[Uniform Measure on Sphere]
  Let $\mu_n$ be the uniform probability distribution on the $n$-sphere $\mathbf{S}^{n} \subset \mathbb{R}^{n+1}$. That is, let us consider any measurable set $A \subset \mathbb{S}^{n}$ such that $\mu_n (A) \geq 1/2$. Then, if we let $d(x, A)$ be the geodesic distance between $x \in \mathbb{S}^n$ and $A$ , we define the expanded set 
  \[A_t = \{x \in \mathbb{S}^n \mid d(x, A) < t\}\]
  and it turns out that 
  \[\mu_n (A_t) \geq 1 - e^{- (n -1) t^2 / 2}\]
  which states that given \textit{any} length $t > 0$, no matter how small, $A_t$ almost covers the whole space. Then, for large enough $n$, $\mu_n$ is highly concentrated around the equator. 
  \end{example}

  Note that the bounds decay \textit{exponentially} (or of greater order). 

  \begin{example}[Uniform Measure on Cube]

  \end{example}

  \begin{example}[High Dimensional Gaussian]
  Given iid $X_1, \ldots, X_n \sim \mathcal{N}(0, \sigma^2)$, then let $\mathbf{X}$ be the random $n$-vector of these random variables. Then, the random variable 
  \[||\mathbf{X}|| = \sqrt{X_1^2 + \ldots, X_n^2}\]
  has a distribution that is very concentrated around the expectation 
  \[\mathbb{E}[||\mathbf{X}||] = \sqrt{\frac{n}{3}}\]
  \end{example}

  Naturally, this concentration phenomenon extends to random variables. 

  \begin{example}
  Let us have iid random variables $X_i$ with $\mathbb{P}(X_i = 1) = 1/2$ and $\mathbb{P}(X_i = -1) = 1/2$. Then, let's define $S_n = \sum_{i=1}^n X_i$. The strong law of large numbers tell us that 
  \[\frac{S_n}{n} \xrightarrow{a.s.} 0\]
  while the central limit theorem tells us that 
  \[\frac{S_n}{\sqrt{n}} \xrightarrow{d} \mathcal{N}(0, 1)\]
  since $\mathbb{E}[X_i] = 0$ and $\mathrm{Var}[X_i] = 1$. The CLT result shows us that the fluctuations (variance) of $S_n$ of are order $n$. However, note that $|S_n|$ can take values as large as $n$, so the maximum value of $S_n / n$ is of order $1$. If we measure $S_n$ using this scale, then $\frac{S_n}{n}$ is essentially $0$. The actual bound looks like 
  \[\mathbb{P} \bigg( \frac{|S_n|}{n} \geq r \bigg) \leq 2 e^{-n r^2 / 2}\]
  \end{example}

  \begin{lemma}[Markov's Inequality]
  Given any random variable $X$, we have 
  \[\mathbb{P}(X \geq \alpha) \leq \frac{\mathbb{E}[X]}{\alpha}\]
  \end{lemma}

  \begin{lemma}[Chebyshev's Inequality]
  Given $X$ with finite variance and expectation, we have 
  \[\mathbb{P}(|X - \mathbb{E}[X]| \geq \alpha) \leq \frac{\Var[X]}{\alpha^2}\]
  \end{lemma}

  An inequality that we will use often in proofs is Jensen's inequality. 

  \begin{lemma}[Jensen's Inequality]
  Given a convex function $g: \mathbb{R} \rightarrow \mathbb{R}$ and random variable $X$, we have 
  \[g(\mathbb{E}[X]) \leq \mathbb{E}[g(X)]\]
  \end{lemma}
  \begin{proof}
  We will assume that $f$ is differentiable for simplicity and let $\mathbb{E}[X] = \mu$. Define the linear function centered at $\mu$ to be $l(x) \coloneqq f(\mu) + f^\prime (\mu) (x - \mu)$. Then, we know that $f(x) \geq l(x)$ for all $x$, so 
  \begin{align*}
      \mathbb{E}[f(X)] & \geq \mathbb{E}[ l(X)] \\ 
      & = \mathbb{E}[f(\mu) + f^\prime (\mu) \, (X - \mu)] \\
      & = \mathbb{E}[f(\mu)] + f^\prime (\mu) ( \mathbb{E}[X] - \mu) \\
      & = \mathbb{E}[f(\mu)] \\
      & = f(\mathbb{E}[X])
  \end{align*}
  \end{proof}

  \begin{definition}[Lipschitz Continuity]
  A function $f: (X, d_X) \longrightarrow (Y, d_Y)$ is \textbf{Lipschitz continuous}, with Lipschitz constant $A$, if it satisfies 
  \[d_Y \big( f(\mathbf{x}), f(\mathbf{y})\big) \leq A \, d_X (\mathbf{x}, \mathbf{y})\]
  for all $\mathbf{x}, \mathbf{y} \in X$. 
  \end{definition}

  \subsection{Talagrand's Gaussian Inequality}

  \begin{lemma}[Gaussian Integration by Parts Formula]
  For Gaussian random variables $x, x_1, \ldots, x_n$ and a function $F$ of moderate growth at infinity, we have 
  \[\mathbb{E}\big[ x \, F(x_1, \ldots, x_n) \big] = \sum_{i=1}^n \mathbb{E}[x \, x_i] \; \mathbb{E}\bigg[ \frac{\partial F}{\partial x_i} (x_1, \ldots, x_n) \bigg]\]
  \end{lemma}

  \begin{theorem}[Talagrand's Gaussian Inequality]
  Consider a Lipschitz function $F: \mathbb{R}^N \longrightarrow \mathbb{R}$ (with Lipschitz constant $A$). Let $x_1, \ldots, x_N \sim \mathcal{N}(0, 1)$ be iid, and let $\mathbf{x} = (x_1, \ldots, x_N)$. Then, for each $t > 0$, we have 
  \[\mathbb{P} \big( | F(\mathbf{x}) - \mathbb{E} F(\mathbf{x}) | \geq t \big) \leq 2 \exp \bigg(- \frac{t^2}{4A^2} \bigg)\]
  \end{theorem}
  \begin{proof}
  For this proof, we assume that $F$ is not only Lipschitz, but $C^2$. This is the case in most applications of this theorem, and if it is not the case, then we can regularize $F$ by convolving with a smooth function to solve the problem. We begin with a parameter $s$ and consider the function $G: \mathbb{R}^{2N} \longrightarrow \mathbb{R}$ defined 
  \[G(z_1, \ldots, z_{2N}) = \exp \Big( s \big[ F ( z_1, \ldots, z_N) - F(z_{N+1}, \ldots, z_{2N}) \big] \Big)\]
  For clarity, we will denote variables of $F$ with $x_i$ and variables of $G$ with $z_i$. Let $u_1, \ldots, u_{2N} \sim \mathcal{N}(0, 1)$ be iid, and let $v_1, \ldots, v_n \sim \mathcal{N}(0, 1)$ be iid, with $v_{N+1}, \ldots, v_{2N}$ copies of the first $N$. For shorthand, we can denote the collection as $\mathbf{u}$ and $\mathbf{v}$. Then, we have 
  \[\mathbb{E} [u_i u_j] - \mathbb{E}[ v_i v_j] = 0\]
  except when $j = i + M$ or $i = j + M$, in which case we have 
  \[\mathbb{E} [u_i u_j] - \mathbb{E}[ v_i v_j] = 0 - 1 = -1\]
  since $v_i v_j = X^2$, where $X \sim \mathcal{N}(0, 1) = \chi^2_1$, a Chi-Squared distribution with 1 degree of freedom. We consider the transformed random variable
  \[\mathbf{f}(t) \coloneqq \sqrt{t} \, \mathbf{u} + \sqrt{1 - t} \, \mathbf{v} \sim \mathcal{N}(0, 1) \text{ for all } t\]
  that is essentially some smooth path from $\mathbf{f}(0) = \mathbf{u}$ and $\mathbf{f}(1) = \mathbf{v}$. Note that given some $t \in [0, 1]$, $\mathbf{f}(t)$ is some random vector, $G ( \mathbf{f}(t))$ is some random variable, and $\mathbb{E}[ G(\mathbf{f}(t))]$ is some number. We can define the function $\phi: [0, 1] \longrightarrow \mathbb{R}$ as 
  \begin{align*}
      \phi(t) = \mathbb{E} [G (\mathbf{f}(t))] & = \int_\mathbb{R} x \; p_{G(f(t))} (x) \,dx \\
      & = \int_{\mathbb{R}^{2N}} G(y) \; p_{f(t)} (y) \,dy 
  \end{align*}
  where $p_X$ is the PDF of the distribution $X$. Take the derivative with respect to $t$ to get the first line, and we can simplify using Gaussian integration by parts 
  \begin{align*}
      \phi^\prime (t) & \mathbb{E}\bigg[ \sum_{i=1}^{2N} \frac{d}{dt} f_i (t) \; \frac{\partial G}{\partial z_i} \big( \mathbf{f}(t)\big) \bigg] \\
      & = \sum_{i=1}^{2N} \mathbb{E} \bigg[ \frac{d}{dt} f_i (t) \, \frac{\partial G}{\partial z_i} \big( \mathbf{f}(t)\big)\bigg] \\
      & = \sum_{i=1}^{2N} \sum_{j=1}^{2N} \mathbb{E} \bigg[ \Big( \frac{\partial}{\partial t} f_i (t) \Big) \, f_i (t) \bigg] \; \mathbb{E} \bigg[ \frac{\partial^2 G}{\partial z_i \partial z_{j}} \mathbf{f} (t) \bigg] 
  \end{align*}
  But we can simplify 
  \begin{align*}
      \mathbb{E} \bigg[ \Big( \frac{\partial}{\partial t} f_i (t) \Big) \, f_i (t) \bigg] & = \mathbb{E} \bigg[ \Big( \frac{1}{2 \sqrt{t}} u_i - \frac{1}{2 \sqrt{1 - t}} v_i \Big) \big( \sqrt{t} u_j - \sqrt{1 - t} \, v_j \big) \bigg] \\
      & = \frac{1}{2} \big(\mathbb{E}[ u_i u_j] - \mathbb{E}[v_i v_j] \big) = \begin{cases} -1 & \text{ if } j = i + M , i = j + M \\
      0 & \text{ else} \end{cases} 
  \end{align*}
  So, we can simplify the above to
  \[\phi^\prime (t) = - \mathbb{E} \bigg[  \sum_{i=1}^N \frac{\partial^2 G}{\partial z_i \, \partial z_{i + M}} \big( \mathbf{f}(t)\big) \bigg]\]
  and computing the second derivative using the chain rule gives 
  \begin{align*}
      \frac{\partial G}{\partial z_i} (\mathbf{z}) & = \frac{\partial G}{\partial F} \frac{\partial F}{\partial x_i} (z_1, \ldots, z_N) \\
      & = s \; G(\mathbf{z}) \, \frac{\partial F}{\partial x_i} (z_1, \ldots, z_N) \\
      \frac{\partial^2 G}{\partial z_i \partial z_{i + N}} (\mathbf{z}) & = - s^2 \, G(\mathbf{z}) \, \frac{\partial F}{\partial x_i} (z_1, \ldots, z_N) \, \frac{\partial F}{\partial x_i} (z_{N+1}, \ldots, z_{2N}) 
  \end{align*}
  for all $\mathbf{z}$. So we have for all $t \in [0, 1]$, 
  \begin{align*}
      \phi^\prime (t) & = s^2 \, \mathbb{E} \bigg[ \sum_{i=1}^N G(\mathbf{f}(t)) \, \frac{\partial F}{\partial x_i} \big( f_1 (t), \ldots, f_N (t) \big) \, \frac{\partial F}{\partial x_i} \big( f_{N+1} (t), \ldots, f_{2N} (t) \big) \bigg] \\ 
      & \leq s^2 \mathbb{E} \bigg[ G(\mathbf{f}(t)) \sum_{i=1}^N \frac{\partial F}{\partial x_i} \big( f_1 (t), \ldots, f_N (t) \big) \, \frac{\partial F}{\partial x_i} \big( f_{N+1} (t), \ldots, f_{2N} (t) \big) \bigg] \\ 
      & \leq s^2 \mathbb{E} \big[ G(\mathbf{f}(t) \big) \, A^2 \big] \\
      & \leq s^2 A^2 \mathbb{E}[G(\mathbf{f}(t))] = s^2 A^2 \phi(t)
  \end{align*}
  Solving the inequality for $\phi$ gives 
  \begin{align*}
      \phi^\prime (t) / \phi(t) \leq s^2 A^2 & \implies \int \phi^\prime (t) / \phi(t) \,dt \leq \int s^2 A^2 \,dt \\
      & \implies \log{\phi(t)} \leq s^2 A^2 t + C \\
      & \implies \phi(t) \leq e^{s^2 A^2 t} \leq e^{s^2 A^2}
  \end{align*}
  Recalling that $\mathbf{f}(1) = \mathbf{u}$, we have 
  \[\mathbb{E}[\exp\{ s ( F(u_1, \ldots, u_N) - F(u_{N+1}, \ldots, u_{2N})) \}] \leq e^{s^2 A^2}\]
  and by independence of the $u_i$'s, the LHS equals $\mathbb{E}[e^{s F(u_1, \ldots, u_N)}]\, \mathbb{E}[e^{-s F(u_{N+1}, \ldots, u_{2N})}]$ and by Jensen's inequality, we have $\mathbb{E}[e^{-s F(u_{N+1}, \ldots, u_{2N})}] \geq e^{-s \mathbb{E}[F(u_{N+1}, \ldots, u_{2N})]}$. We can derive as follows: 
  \begin{align*}
      e^{s^2 A^2} & \geq \mathbb{E}[e^{s F(u_1, \ldots, u_N)}]\, \mathbb{E}[e^{-s F(u_{N+1}, \ldots, u_{2N})}] \\
      & \geq \mathbb{E}[e^{s F(u_1, \ldots, u_N)}]\, e^{-s \mathbb{E}[F(u_{N+1}, \ldots, u_{2N})]} \\
      & = \mathbb{E}[e^{s F(u_1, \ldots, u_N)}]\, \mathbb{E}[e^{-s \mathbb{E}[F(u_{N+1}, \ldots, u_{2N})]}] \\
      & = \mathbb{E}[e^{s F(u_1, \ldots, u_N) -s \mathbb{E}[F(u_{N+1}, \ldots, u_{2N})]}] \\
      & = \mathbb{E}[\exp \big( s F(u_1, \ldots, u_N) -s \mathbb{E}[F(u_{N+1}, \ldots, u_{2N})] \big) ]
  \end{align*}
  and by Markov's inequality, we get for a random vector of standard Gaussian random variables $\mathbf{x}$
  \begin{align*}
      \mathbb{P} \big( F(\mathbf{x}) - \mathbb{E}[F(\mathbf{x})] \geq t) & = \mathbb{P} \big( e^{s( F(\mathbf{x}) - \mathbb{E}[F(\mathbf{x})]} \geq e^{st} \big) \\
      & \leq \frac{\mathbb{E}[e^{s( F(\mathbf{x}) - \mathbb{E}[F(\mathbf{x})]}]}{e^{st}} \\
      & \leq e^{s^2 A^2 - st} \\
      & = e^{- t^2 / 4A^2} \text{ when } s = t / 2A^2
  \end{align*}
  \end{proof}

\section{Variance Bounds and Poincare Inequalities}

  Let us first describe this concentration phenomenon by investigating bounds on the variance 
  \[\mathrm{Var}[f(x_1, \ldots, x_n)] \coloneqq \mathbb{E}\big[ \big( f(x_1, \ldots, x_n) - \mathbb{E}[f(x_1, \ldots, x_n)] \big)^2 \big] \]
  We can first bound 
  \[\Var[f(X_1, \ldots, X_n)] = \mathbb{E}\big[ \big( f(X_1, \ldots, X_n)\big)^2 \big] - \mathbb{E}\big[ f(X_1, \ldots, X_n) \big]^2 \leq \mathbb{E}\big[ \big( f(X_1, \ldots, X_n)\big)^2 \big]\]
  and since adding a constant term to $f$ doesn't affect the variance, we can utilize this to get our first variance bound. 

  \begin{lemma}
  Let $\mathbf{X}$ be a random variable or vector. Then, 
  \[\mathrm{Var}[f(\mathbf{X})] \leq \mathbb{E} \big[ \big( f(\mathbf{X}) - \inf f \big)^2 \big] \text{ and } \mathrm{Var}[f(\mathbf{X})] \leq \mathbb{E} \big[ \big(\sup f - f(\mathbf{X}) \big)^2 \big]\]
  and 
  \[\mathrm{Var}[ f(\mathbf{X})] \leq \frac{1}{4} ( \sup f - \inf f)^2\]
  \end{lemma}
  \begin{proof} 
  Since $\mathrm{Var}[\mathbf{X}] = \mathbb{E}[\mathbf{X}^2] - \mathbb{E}[\mathbf{X}]^2$ from above, we have 
  \[\mathrm{Var}[ f(\mathbf{X})] = \mathrm{Var}[f(\mathbf{X}) - a] = \mathbb{E}[(f(\mathbf{X}) - a)^2] - \mathbb{E}[f(\mathbf{X}) - a]^2 \leq \mathbb{E}[(f(\mathbf{X}) - a)^2]\]
  By letting $a = \inf f$, we get the first inequality. By letting $a = (\sup f + \inf f) /2$ be the "middle" of $f$, we have $|f(\mathbf{X}) - a| \leq (\sup f - \inf f)/2 \implies [f(\mathbf{X}) - a]^2 \leq (\sup f - \inf f)^2/4$, and so 
  \[\Var[ f(\mathbf{X})] \leq \mathbb{E}[(f(\mathbf{X}) - a)^2] \leq \frac{1}{4} (\sup f - \inf f)^2\]
  which gives our third inequality. We can also see that 
  \[\mathrm{Var}[ f(\mathbf{X})] = \Var[ -f (\mathbf{X})] = \Var[ b - f(\mathbf{X})] \leq \mathbb{E}[ (b - f(\mathbf{X}))^2]\]
  to get our second. 
  \end{proof} 

  This allows us to bound the random vector $f(\mathbf{X})$ if $f$ itself is bounded, no matter what $\mathbf{X}$ is. But this generally turns out to be a very conservative bound, which is unsurprising since we assume so little about $\mathbf{X}$. For example, if we let $X_1, \ldots, X_n$ be iid random variables taking values in $[-1, 1]$, and let $f(x_1, \ldots, x_n) = \frac{1}{n} \sum_{i=1}^n x_i$. Then, $f$ takes values in $[-1, 1]$, and by the previous lemma, we have
  \[\mathrm{Var}[f(X_1, \ldots, X_n)] \leq \frac{1}{4} (1 - (-1))^2 = 1\]
  which looks good, until we see that we can derive a better bound from direct computation (which becomes much better as $n$ increases). 
  \[\mathrm{Var}[f(X_1, \ldots, X_n)] = \frac{1}{n^2} \sum_{i=1}^n \mathrm{Var}[X_i] = \frac{1}{n}\]
  However, this computation assumes independence of $X_i$'s, which the previous lemma doesn't. This is the reason we're able to get a better bound, since if we took $n$ copies of the same $X$, we would have 
  \[\mathrm{Var}[f(X_1, \ldots, X_n)] = \mathrm{Var}[n X / n] = \mathrm{Var}[X] = 1\]
  Therefore, we will capitalize on the independence of these random variables in high dimensions to obtain better bounds. Now in the next result, we shall show that the variance of a high dimensional $f(X_1, \ldots, X_n)$ can be bounded by the variances of each random variable. Those quantities, like the variance, that behave well in high dimensions is said to \textit{tensorize}. 

  Consider independent random variables $X_1, \ldots, X_n$ and a function $f: \mathbb{R}^n \longrightarrow \mathbb{R}$. If we fix values $x_1, \ldots, x_n$, then we can define for all $k = 1, \ldots, n$ the function $g_{k}(x_1, \ldots, x_{k-1}, x_{k+1}, \ldots, x_n): \mathbb{R} \rightarrow \mathbb{R}$ as 
  \[g_{k}(x_1, \ldots, x_{k-1}, x_{k+1}, \ldots, x_n)(z) = f(x_1, \ldots, x_{k-1}, z, x_{k+1}, \ldots, x_n)\]
  where 
  \[(g_{k}(x_1, \ldots, x_{k-1}, x_{k+1}, \ldots, x_n))^\prime (z) = \frac{\partial}{\partial x_k} f(x_1, \ldots, x_{k-1}, z, x_{k+1}, \ldots, x_n)\]
  and $g_k (x_1, \ldots, x_{k-1}, x_{k+1}, \ldots, x_n) (X_k)$ is a random variable of $X_k$. Then, we can define 
  \begin{align*}
      \Var_k f(x_1, \ldots, x_n) & = \Var_{X_k} [ f(x_1, \ldots, x_{k-1}, X_k, x_{k+1} \ldots, x_n)] \\ 
      & = \mathbb{E}_{X_k} \big[ \big( f (x_1, \ldots, x_{k-1}, X_k, x_{k+1}, \ldots, x_n) - \mathbb{E}_{X_k} [f (x_1, \ldots, x_{k-1}, X_k, x_{k+1}, \ldots, x_n)] \big)^2 \big] \\
      & = \Var[g_{k} (x_1, \ldots, x_{k-1}, x_{k+1}, \ldots, x_n) (X_k)] \\
      & = \Var_{X_k} [g(x_1, \ldots, x_{k-1}, x_{k+1}, \ldots, x_n)] 
  \end{align*}
  which takes the variance of $f$ with respect to $X_k$, keeping all other variables fixed. However, this value will change for different $x_1, \ldots, x_n$'s, and so we can loosen the restriction that they are fixed. We can take 
  \[g_k (X_1, \ldots, X_{k-1}, X_{k+1}, \ldots, X_n) (z) = f(X_1, \ldots, X_{k-1}, z, X_{k+1}, \ldots, X_n)\]
  where $g_k (X_1, \ldots, X_{k-1}, X_{k+1}, \ldots, X_n) (X_k)$ is a random variable of $X_1, \ldots, X_n$. Now if we calculate its partial variance, we get 
  \begin{align*}
      \Var_k f(X_1, \ldots, X_n) & = \Var_{X_k} [f(X_1, \ldots, X_k, \ldots, X_n)]\\
      & = \Var [g_k (X_1, \ldots, X_{k-1}, X_{k+1}, \ldots, X_n) (X_k)] \\
      & = \Var_{X_k} [g_k (X_1, \ldots, X_{k-1}, X_{k+1}, \ldots, X_n)]
  \end{align*}
  which is now a random variable of all $X_i$'s, $i \neq k$, that outputs the variance of $f$ with respect to $X_k$. \textbf{But is it true that }
  \[\mathbb{E}_{X_k} [ f(X_1, \ldots, X_n)] = \mathbb{E}[ f(X_1, \ldots, X_n) \mid X_1, \ldots, X_{k-1}, X_{k+1}, \ldots, X_n] ?\]


  Now, we can show a very useful property of variance: that the variance of some arbitrary function can be bounded by the expected sum of the partial variances. 

  \begin{theorem}[Tensorization of Variance]
  That is, $\Var_i f(\mathbf{x})$ is the variance of $f(X_1, \ldots, X_n)$ w.r.t. the variable $X_i$ only, the remaining variables kept fixed. Then, we have 
  \[\Var[f(X_1, \ldots, X_n)] \leq \mathbb{E} \bigg[ \sum_{i=1}^n \Var_i f(X_1, \ldots, X_n) \bigg] \]
  \end{theorem}
  \begin{proof}
  We try to mimic the fact that the variance of the sum of independent random variables is the sum of the variances. At first sight, the general function $f(x_1, \ldots, x_n)$ need not look anything like a sum, but we can expand it as a telescoping sum of random variables. We will prove this using the \textit{martingale method}, which constructs this random variable $f(X_1, \ldots, X_n)$ as a sum of finer and finer increments starting from the "coarse" constant function $\mathbb{E}[f(X_1, \ldots, X_n)]$. We define the random variable 
  \[\Delta_k \coloneqq \mathbf{E}[ f(X_1, \ldots, X_n) \mid X_1, \ldots X_k] - \mathbb{E}[ f(X_1, \ldots, X_n) \mid X_1, \ldots, X_{k-1}]\]
  Then, we can express 
  \[f( X_1, \ldots, X_n) - \mathbf{E}[ f(X_1, \ldots, X_n)] = \sum_{k=1}^n \Delta_k\]
  Note that $\mathbb{E}[\Delta_k \mid X_1, \ldots, X_{k-1}] = 0$ (i.e. $\Delta_k$'s are martingale increments). In particular, even though the $\Delta_k$'s are not independent, if we have $l < k$, then 
  \begin{align*}
      \mathbb{E}[ \Delta_k \Delta_l] & = \mathbb{E}[ \mathbb{E}[\Delta_k \Delta_l \mid X_1, \ldots, X_{k-1}]] \\
      & = \mathbb{E}[ \mathbb{E}[\Delta_k \mid X_1, \ldots X_{k-1} ] \, \mathbb{E}[\Delta_l \mid X_1, \ldots X_{k-1} ]] \\
      & = \mathbb{E}[ \mathbb{E}[\Delta_k \mid X_1, \ldots X_{k-1} ] \, \Delta_l] \\
      & = \mathbb{E}[0 \cdot \Delta_l] = 0
  \end{align*}
  and so, the variance can be expanded into terms that vanish. 
  \begin{align*}
      \Var[ f(X_1, \ldots, X_n)] & = \mathbb{E} \big[ \big( f( X_1, \ldots, X_n) - \mathbf{E}[ f(X_1, \ldots, X_n)] \big)^2\big] \\
      & = \mathbb{E} \bigg[ \bigg( \sum_{k=1}^n \Delta_k \bigg)^2 \bigg] = \sum_{k=1}^n \mathbb{E}[ \Delta_k^2]
  \end{align*}
  Now it remains to show that $\mathbb{E}[\Delta_k^2] \leq \mathbb{E}[\Var_k f(X_1, \ldots, X_n)]$ for every $k$. Let us define 
  \[\Tilde{\Delta}_k = f(X_1, \ldots, X_n) - \mathbb{E}[f(X_1, \ldots, X_n) \mid X_1, \ldots, X_{k-1}, X_{k+1}, \ldots, X_n]\]
  to be the approximation of $f(X_1, \ldots, X_n)$ "one step" before the final increment. Then, we have 
  \[\Delta_k = \mathbb{E}[\Tilde{\Delta}_k \mid X_1, \ldots, X_k]\]
  and as $X_k$ and $X_1, \ldots, X_{k-1}, X_{k+1}, \ldots, X_n$ are independent, we have 
  \[\Var_k f(X_1, \ldots, X_n) = \mathbb{E}[\Tilde{\Delta}_k^2 \mid X_1, \ldots, X_{k-1}, X_{k+1}, \ldots, X_n] \]
  and therefore using Jensen's inequality we can prove 
  \[\mathbb{E}[\Delta_k^2] = \mathbb{E}[\mathbb{E}[ \Tilde{\Delta}_k \mid X_1, \ldots, X_k]^2 ] \leq \mathbb{E}[\Tilde{\Delta}_k^2] = \mathbb{E}[\Var_k f(X_1, \ldots, X_n)]\]
  \end{proof}

  What we want to eventually do is prove an inequality of the form where for any function $h: \mathbb{R} \rightarrow \mathbb{R}$ and some $X \sim \mu$, 
  \[\Var_\mu[h] = \Var [h(X)] \leq ||\mathcal{L}(h)||^2_{L^2 (\mu)}\]
  where $\mathcal{L}$ is an operator on $h$. This will allow us to bound 
  \[\Var [g_k (x_1, \ldots, x_{k-1}, x_{k+1}, \ldots, x_n)(X_k)] \leq ||\mathcal{L}(g_k (x_1, \ldots, x_{k-1}, x_{k+1}, \ldots, x_n))||^2\]
  for all $x_1, \ldots, x_n$, simply by taking $h = g(x_1, \ldots, x_{k-1}, x_{k+1}, \ldots, x_n)$. Since this works for all $x_1, \ldots, x_n$, we can claim that this inequality holds for all $X_1 (\omega), \ldots, X_n (\omega)$ for all $\omega \in \Omega$. That is, we can loosen the fixed values into random variables. 
  \begin{align*}
      \Var_{k} f(X_1, \ldots, X_n) & = \Var[g_k (X_1, \ldots, X_{k-1}, X_{k+1}, \ldots, X_n)(X_k)] \\
      & \leq || \mathcal{L}(g_k (X_1, \ldots, X_{k-1}, X_{k+1}, \ldots, X_n))||^2_{L^2 (\mu)} 
  \end{align*}
  Note that all terms are random variables of $X_1, \ldots, X_n$, and so the same inequality holds for their expectations over the entire joint measure. 
  \[\mathbb{E}[ \Var_{k} f(X_1, \ldots, X_n) ] \leq \mathbb{E} \big[ || \mathcal{L}(g_k (X_1, \ldots, X_{k-1}, X_{k+1}, \ldots, X_n))||^2_{L^2 (\mu)} \big] \]
  and so by tensorization (i.e. summing them up), we get 
  \[\Var[f(X_1, \ldots, X_n)] \leq \sum_{i=1}^n \mathbb{E} \big[ \Var_i f(X_1, \ldots, X_n) \big] \leq \sum_{i=1}^n \mathbb{E} \big[ || \mathcal{L}(g_k (X_1, \ldots, X_{k-1}, X_{k+1}, \ldots, X_n))||^2_{L^2 (\mu)} \big] \]

  Furthermore, this bound is sharp when $f$ is linear. Let us demonstrate this by letting $f(x_1, \ldots, x_n) = a_1 x_1 + \ldots + a_n x_n$. On the left hand side, we have 
  \[\Var[ f(X_1, \ldots, X_n)] = \Var\bigg[ \sum_{i=1}^n a_i X_i \bigg] = \sum_{i=1}^n a_i^2 \Var[X_i] \]
  and on the right hand side, each component divides up to 
  \begin{align*}
      \Var_i f(x_1, \ldots, x_n ) & = \Var[ f(x_1, \ldots, X_i, \ldots, x_n)] \\
      & = \Var[ a_1 x_1 + \ldots + a_i X_i + \ldots a_n x_n] \\
      & = \Var[a_i X_i] \\
      & = a_i^2 \Var[X_i]
  \end{align*}
  \textbf{Then?} Note that since $f$ is linear, the values of all $x_j, j \neq i$ have no effect on the variance of $X_i$, and so $\Var_i f(X_1, \ldots, X_n)$, which is originally a random variable of $X_1, \ldots, X_{i-1}, X_{i+1}, \ldots, X_n$, is really just the constant (random variable) $a_i^2 \Var[X_i]$. This is because no matter what values $X_1, \ldots, X_{i-1}, X_{i+1}, \ldots, X_n$ are realized, these values will only contribute to a translation of the random variable $f(X_1, \ldots, X_n)$, and hence will not affect the variance w.r.t. $X_i$. So, the right hand side also becomes 
  \[\mathbb{E} \bigg[ \sum_{i=1}^n \Var_i f(X_1, \ldots, X_n) \bigg] = \mathbb{E} \bigg[ \sum_{i=1}^n a_i^2 \Var[X_i] \bigg] = \sum_{i=1}^n a_i^2 \Var[X_i]\]
  which is the same as the LHS. 

  We can view the tensorization of the variance in itself as an expression of the concentration phenomenon. $\Var_i f (\mathbf{x})$ quantifies the sensitivity of the function $f(\mathbf{x})$ of the coordinate $x_i$ in a distribution-dependent manner. If this sensitivity w.r.t. each coordinate ($\mathbb{E}[ \Var_i f(X_1, \ldots, X_n)]$) is small, then $f(X_1, \ldots, X_n)$ is close to its mean. However, it might not be so straightforward to compute $\Var_i f$, since it depends on both the function $f$ and on the distribution of $X_i$. So, we can try combining this with a suitable bound on the component-wise variance. 

  Let us define the quantities: 
  \[D_i f (\mathbf{x}) \coloneqq \sup_z f(x_1, \ldots, x_{i-1}, z, x_{i+1}, \ldots, x_n) - \inf_z f(x_1, \ldots, x_{i-1}, z, x_{i+1}, \ldots, x_n)\]
  and 
  \[D_i^- f(\mathbf{x}) \coloneqq f(x_1, \ldots, x_n) - \inf_z f(x_1, \ldots, x_{i-1}, z, x_{i+1}, \ldots, x_n)\]
  which quantifies the sensitivity of the function $f$ to the coordinate $x_i$ in a distribution-independent manner. Now we can introduce the following bounds. 

  \begin{corollary}
  We have 
  \[\Var[ f(X_1, \ldots, X_n)] \leq \frac{1}{4} \mathbb{E} \bigg[ \sum_{i=1}^n \big( D_i f(X_1, \ldots, X_n) \big)^2 \bigg] \]
  \end{corollary}
  \begin{proof}
  We start off with 
  \begin{align*}
      \Var_i f (X_1, \ldots, X_n) & = \Var[ f(X_1, \ldots, X_i, \ldots, X_n)] \\
      & \leq \frac{1}{4} \big( D_i f (X_1, \ldots, X_n)\big)^2 
  \end{align*}
  Since these a random variables follow this inequality (for all $\omega \in \Omega$), we can attach an expectation on them to get 
  \[\mathbb{E}[\Var_i f (X_1, \ldots, X_n)] \leq \mathbb{E} \bigg[ \frac{1}{4} \big( D_i f (X_1, \ldots, X_n)\big)^2\bigg] \]
  and substituting in the previous theorem gives 
  \begin{align*}
      \Var[f(X_1, \ldots, X_n)] & \leq \mathbb{E} \bigg[ \sum_{i=1}^n \Var_i f(X_1, \ldots, X_n) \bigg] \\
      & = \sum_{i=1}^n \mathbb{E}\big[ \Var_i f(X_1, \ldots, X_n) \big] \\
      & \leq \sum_{i=1}^n \mathbb{E} \bigg[ \frac{1}{4} \big( D_i f (X_1, \ldots, X_n)\big)^2\bigg] \\
      & = \frac{1}{4} \mathbb{E} \bigg[ \sum_{i=1}^n \big( D_i f(X_1, \ldots, X_n) \big)^2 \bigg] 
  \end{align*}
  \end{proof}

  \begin{example}[Random Matrices]

  \end{example}



  \begin{exercise}[Banach-Valued Sums]
  Let $X_1, X_2, \ldots, X_N$ be independent random variables with values in a Banach space $(B, ||\cdot ||_B)$. Suppose these random variables are bounded in the sense that $||X_i||_B \leq C$ a.s. for every $i$. Show that 
  \[\Var\bigg( \bigg| \bigg| \frac{1}{n} \sum_{k=1}^n X_k \bigg| \bigg|_B \bigg) \leq \frac{C^2}{n}\]
  This is a simple vector-valued variant of the elementary fact that the variance of $\frac{1}{n} \sum_{k=1}^n X_k$ for real-valued random variables $X_k$ is of order $\frac{1}{n}$. 
  \end{exercise}
  \begin{solution}
  We can tensorize the variance to get 
  \begin{align*}
      \Var_k \bigg| \bigg| \frac{1}{n} \sum_{k=1}^n X_k \bigg| \bigg|_B & = \Var \bigg| \bigg| \frac{1}{n} X_k \bigg| \bigg|_B = \frac{1}{n^2} \Var ||X_k||_B \\
      & \leq \frac{1}{n^2} \bigg( \frac{1}{4} (C - (-C))^2 \bigg) = \frac{C^2}{n^2} 
  \end{align*}
  and so letting $f(X_1, \ldots, X_n) = \big| \big| \frac{1}{n} \sum_{k=1}^n X_k \big| \big|_B$, we get 
  \begin{align*}
      \Var [f(X_1, \ldots X_n)] & \leq \sum_{k=1}^n \mathbb{E}[ \Var_k f (X_1, \ldots, X_n)] \\
      & \leq \sum_{k=1}^n \frac{C^2}{n^2} = \frac{C^2}{n} 
  \end{align*}
  \end{solution}

  \begin{exercise}[Rademacher Processes]
  Let $\epsilon_1, \ldots, \epsilon_n$ be independent symmetric Bernoulli random variables $\mathbb{P}(\epsilon_i = \pm 1) = \frac{1}{2}$ (also called Rademacher variables), let $T \subset \mathbb{R}^n$. The following identity is completely trivial: 
  \[\sup_{t \in T} \Var \bigg[ \sum_{k=1}^n \epsilon_k t_k \bigg] = \sup_{t \in T} \sum_{k=1}^n t_k^2\]
  Prove the following nontrivial fact: 
  \[\Var \bigg[ \sup_{t \in T} \sum_{k=1}^n \epsilon_k t_k \bigg] \leq 4 \sup_{t \in T} \sum_{k=1}^n t_k^2\]
  \end{exercise}
  \begin{solution}
  Let us consider a fixed $\boldsymbol{\epsilon} = (\epsilon_1, \ldots, \epsilon_n)$ and index $i \in [n]$. Then, consider the random variable formed by taking the value $f(\epsilon_1, \ldots, \epsilon_n)$ and loosening $\epsilon_i$ to be an random variable. That is, 
  \begin{align*}
      \mathbb{P} \Big[ f(\epsilon_1, \ldots, \epsilon_n) = \sup_{t \in T} \{\epsilon_1 t_1 + \ldots + 1 t_i + \ldots + \epsilon_n t_n\} \Big] = \frac{1}{2} \\
      \mathbb{P} \Big[ f(\epsilon_1, \ldots, \epsilon_n) = \sup_{t \in T} \{\epsilon_1 t_1 + \ldots - 1 t_i + \ldots + \epsilon_n t_n\} \Big] = \frac{1}{2} 
  \end{align*}
  Then, we compute 
  \[D_i^- f (\epsilon_1, \ldots, \epsilon_n) = \inf_{\epsilon_i \in \{-1, 1\}} \sup_{t \in T} \sum_{k=1}^n \epsilon_k t_k\]
  and we can estimate 
  \begin{align*}
      D_i^- f(\boldsymbol{\epsilon}) & = f(\epsilon_1, \ldots, \epsilon_n) - D_i f (\epsilon_1, \ldots, \epsilon_n) \\
      & = \sup_{t \in T} \sum_{k=1}^n \epsilon_k t_k - \inf_{\epsilon_i \in \{-1, 1\}} \sup_{t \in T} \sum_{k=1}^n \epsilon_k t_k \\
      & \leq \sup_{t \in T} 2 |t_i| 
  \end{align*}
  We can finally bound 
  \begin{align*}
      \Var[ f(\epsilon_1, \ldots, \epsilon_n)] & \leq \mathbb{E} \bigg[ \sum_{i=1}^n \big( D_i^- f(\boldsymbol{\epsilon})\big)^2 \bigg] \\
      & \leq 4 \mathbb{E} \bigg[ \sum_{i=1}^n \sup_{t \in T} t_i^2 \bigg] \\
      & = 4 \sup_{t \in T} \sum_{i=1}^n t_i^2 
  \end{align*}
  \end{solution}

  \begin{exercise}[Bin Packing]
  This is a classical application of bounded difference inequalities. Let $X_1, \ldots, X_n$ i.i.d. random variables with values in $[0, 1]$. Each $X_i$ represents the size of a package to be shipped. The shipping containers are bins of size $1$ (so each bin can hold a set packages whose sizes sum to at most $1$). Let $B_n = f(X_1, \ldots, X_n)$ be the minimal number of bins needed to store the packages. Note that computing $B_n$ is a hard combinatorial optimization problem, but we can bound its mean and variance by easy arguments. 
  \begin{enumerate}
      \item Show that $\Var[B_n] \leq n/4$
      \item Show that $\mathbb{E}[B_n] \geq n \mathbb{E}[X_1]$
  \end{enumerate}
  Thus the fluctuations $\sim \sqrt{n}$ of $B_n$ are much smaller than its magnitude $\sim n$. 
  \end{exercise}
  \begin{solution}
  Listed. 
  \begin{enumerate}
      \item Given fixed sizes $X_1, \ldots, X_n$ and some $i \in [n]$, we can see that a property of $f$ is that 
      \[f(X_1, \ldots, X_{i-1}, 0, X_{i+1}, \ldots, X_n) + 1 = f(X_1, \ldots, X_{i-1}, 1, X_{i+1}, \ldots, X_n)\]
      since for an extra package with size $1$, you would for sure need one more bin. So the maximum difference of $f$ based on the $x_i$ value is the constant random variable 
      \begin{align*}
          D_i f(X_1, \ldots, X_n) & = \sup_{z \in [0, 1]} f(X_1, \ldots, z, \ldots, X_n) - \inf_{z \in [0, 1]} f(X_1, \ldots, z, \ldots, X_n)\\
          & = f(X_1, \ldots, 1, \ldots, X_n) - f(X_1, \ldots, 0, \ldots, X_n) = 1
      \end{align*}
      and so by the bounded difference inequalities, 
      \begin{align*}
          \Var[B_n] = \Var[f(X_1, \ldots, X_n)] & \leq \frac{1}{4} \mathbb{E} \bigg[ \sum_{i=1}^n \big( D_i f(X_1, \ldots, X_n) \big)^2 \bigg] \\
          & = \frac{1}{4} \sum_{i=1}^n \mathbb{E} \big[ \big( D_i f(X_1, \ldots, X_n) \big)^2 \big] \\
          & \leq \frac{n}{4} 
      \end{align*}
      \item Given the sizes $X_1, \ldots, X_n$, $B_n$ must satisfy 
      \[B_n = f(X_1, \ldots, X_n) \geq X_1 + \ldots + X_n\] 
      since the total volume of bins $B_n$ must exceed the total volume $X_1 + \ldots + X_n$ of packages. So, 
      \[\mathbb{E}[B_n] \geq \mathbb{E}\bigg[ \sum_{k=1}^n X_k \bigg] = n \mathbb{E}[X_1]\]
  \end{enumerate}
  \end{solution}

  \begin{exercise}[Order Statistics and Spacings]
  Let $X_1, \ldots, X_n$ be independent random variables, and denote by $X_{(1)} \geq \ldots \geq X_{(n)}$ their decreasing rearrangement ($X_{(1)} = \max_i X_i$, $X_{(n)} = \min_i X_i$, etc.). Show that 
  \[\Var[X_{(k)}] \leq k \, \mathbb{E}[(X_{(k)} - X_{(k+1)})^2] \text{ for } 1 \leq k \leq n/2\]
  and that 
  \[\Var[X_{(k)}] \leq (n - k + 1)\, \mathbb{E}[(X_{(k-1)} - X_{(k)})^2] \text{ for } n/2 < k \leq n\]
  \end{exercise}

  \begin{exercise}[Convex Poincare Inequality]
  Let $X_1, \ldots, X_n$ be independent random variables taking values in $[a, b]$. The bounded difference inequalities estimate the variance $\Var[f(X_1, \ldots, X_n)]$ in terms of \textit{discrete} derivatives $D_i f$ or $D_i^- f$ of the function $f$. The goal of this problem is to show that if the function $f$ is convex, then one can obtain a similar bound in terms of the ordinary notion of derivative $\nabla_i f(x) = \partial f(x)/\partial x_i$ in $\mathbb{R}^n$. 
  \begin{enumerate}
      \item Show that if $g: \mathbb{R} \longrightarrow \mathbb{R}$ is convex, then 
      \[g(y) - g(x) \geq g^\prime (x)\, (y - x) \text{ for all } x, y \in \mathbb{R}\]
      
      \item Show using part (a) and the bounded difference inequalities that if $f: \mathbb{R}^n \rightarrow \mathbb{R}$ is convex, then 
      \[\Var[f(X_1, \ldots, X_n)] \geq (b - a)^2 \mathbb{E}[ ||\nabla f (X_1, \ldots, X_n)||^2]\]
      
      \item Conclude that if $f$ is convex and $L$-Lipschitz, i.e. $|f(x) - f(y)| \leq L ||x - y||$ for all $x, y \in [a, b]^n$, then $\Var[f(X_1, \ldots, X_n)] \geq L^2 (b - a)^2$. 
  \end{enumerate}
  \end{exercise}
  \begin{solution}
  Listed. 
  \begin{enumerate}
      \item Assuming $g$ is differentiable, let us choose any $x, y \in \mathbb{R}$ and define some $z = \lambda x + (1 - \lambda)y$ in between. Then, pictorially, we would like to formally show that 
      \[\frac{f(z) - f(x)}{z - x} \leq \frac{f(y) - f(x)}{y - x}\]
      and take the limit as $z \rightarrow x$ to get $f^\prime(x)$ on the LHS. By definition, we have 
      \[f(z) = f\big( \lambda x + (1 - \lambda) y\big) \leq \lambda f(x) + (1 - \lambda) f(y)\]
      Subtracting $f(x)$ and then dividing by $1 - \lambda > 0$ on both sides gives 
      \[\frac{f(z) - f(x)}{1 - \lambda} \leq f(y) - f(x)\] 
      Note that $z - x = \lambda x + (1 - \lambda y) - x = (1 - \lambda)(y - x)$. So, dividing by $y - x > 0$ on both sides gives 
      \[\frac{f(z) - f(x)}{z - x} \leq \frac{f(y) - f(x)}{y - x}\]
      and taking the limit on the LHS gives 
      \[f^\prime (x) = \lim_{z \rightarrow x} \frac{f(z) - f(x)}{z - x} \leq \frac{f(y) - f(x)}{y - x}\]
      Since $y - x > 0$, we can multiply both on the same side to get 
      \[f(y) - f(x) \geq f^\prime (x) \, (y - x)\]
      If $y < x$, then the proof is the same, and the inequality sign ends up getting switched around twice, leading to the same conclusion. 
      
      \item Note that from the above result, we can multiply both sides by $-1$ to get that $g(x) - g(y) \leq g^\prime (x) (x - y)$ for all $x, y \in \mathbb{R}$, and then swap the two variables to get $g(y) - g(x) \leq g^\prime (y) (y - x)$. Let us consider fixed $x_1, \ldots, x_n$ and some $i \in [n]$. Given $f: \mathbb{R}^n \rightarrow \mathbb{R}$, we define $f_i (\mathbf{x}): \mathbb{R} \rightarrow \mathbb{R}$ by unfixing the $i$th variable. Then, given some $\alpha, \beta \in [a, b]$, 
      \[f_i (\mathbf{x}) (\beta) - f_i (\mathbf{x}) (\alpha) \leq g^\prime (\beta) (\beta - \alpha)\]
      or equivalently, 
      \[f(x_1, \ldots, \beta, \ldots, x_n) - f(x_1, \ldots, \alpha, \ldots, x_n) \leq \frac{\partial f}{\partial x_i} (x_1, \ldots, \beta, \ldots, x_n) \; (\beta - \alpha)\]
      Now let $z^\ast \in [a, b]$ be the value s.t. 
      \[z^\ast = \arg \min_{z \in [a, b]} f(x_1, \ldots, z, \ldots, x_n) \]
      Then, 
      \[D_i^- f(\mathbf{x}) = f(x_1, \ldots, x_i, \ldots x_n) - f(x_1, \ldots, z^\ast, \ldots, x_n) \leq \frac{\partial f}{\partial x_i} (x_1, \ldots, x_i, \ldots, x_n) \; (x_i - z^\ast)\]
      and so 
      \[\big( D_i^- f(\mathbf{X}) \big)^2 \leq \nabla_i f (\mathbf{x})^2 \, (x_i - z^\ast)^2 \leq \nabla_i f (\mathbf{x})^2 \, (b - a)^2\]
      which gives from the bounded difference inequality 
      \begin{align*}
          \Var[f(X_1, \ldots, X_n)] & \leq \mathbb{E} \bigg[ \sum_{i=1}^n \big( D_i^- f(X_1, \ldots, X_n) \big)^2 \bigg] \\
          & \leq \mathbb{E} \bigg[ \sum_{i=1}^n \nabla_i f (\mathbf{x})^2 \, (b - a)^2 \bigg] \\
          & = (b - a)^2 \mathbb{E} \big[ \big| \big| \nabla f(\mathbf{X})\big| \big|^2 \big]
      \end{align*}
      
      \item If $f$ is $L$-lipschitz, then $||\nabla f(\mathbf{X})|| \leq L$, and so  
      \[\Var[f(X_1, \ldots, X_n)] \leq (b - a)^2 L^2\]
  \end{enumerate}
  \end{solution}

  \subsection{Markov Semigroups}

  \begin{definition}[Markov Process]
  Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space and $(S, \mathcal{S})$ be a measurable space. A homogeneous Markov process $\{X_t\}_{t \geq 0}$ is a stochastic process that satisfies the \textbf{Markov property}: for every bounded measurable function $f$ and $s, t \geq 0$, there exists a bounded measurable function $P_s f$ satisfying 
  \[\mathbb{E}[f (X_{t + s}) \mid \{X_r\}_{r \leq t}] = (P_s f) (X_t) = \mathbb{E}[ f(X_{t + s}) \mid X_t]\]
  \end{definition}

  \begin{definition}[Stationary Measure]
  A probability measure $\mu$ is called \textbf{stationary} or \textbf{invariant} if 
  \[\mathbb{E}_\mu[f] = \mathbb{E}_\mu [P_t f] \text{ i.e. } \int_S f \,d \mu = \int_S P_t f d\mu\]
  for all $t \geq 0$ and bounded measurable $f$. By abusing notation, this is conventionally written 
  \[\mu(f) = \mu(P_t f)\]
  \end{definition}

  To interpret this notion, suppose that $X_0 \sim \mu$. Then, 
  \[\mathbb{E}[f(X_t)] = \mathbb{E}[\mathbb{E}[f(X_t) \mid X_0]] = \mathbb{E}[P_t f (X_0)] = \mathbb{E}_\mu [P_t f]\]
  and if $\mu$ is stationary, then we have $\mathbb{E}[f(X_t)] = \mathbb{E}_\mu [f]$. If $f = 1_A$ for some measurable $A \subset S$, then $\mathbb{E}[1_A (X_t)] = \mathbb{P}(X_t \in A)$, and 
  \[\mathbb{P}(X_t \in A) = \mathbb{E}_\mu [1_A] = \int_S 1_A \,d\mu = \int_A d\mu = \mu(A) = \mathbb{P}(X_0 \in A)\]
  which means that the probability that for all $A \in \mathcal{S}$ and all $t \geq 0$, the probability of $X_t$ realizing in $A$ is equivalent to the initial probability of $X_0$ realizing in $A$. This means that the process remains distributed according to the stationary measure $X_t \sim \mu$ for every time $t$. In summary, stationary measures describe the equilibrium or steady-state behavior of the Markov process.  

  From now, given the state space $(S, \mathcal{S})$ we can put a measure $\mu$ on it to get a measure space $(S, \mathcal{S}, \mu)$. The Banach space of all $\mu$-measurable functions $f: (S, \mathcal{S}, \mu) \rightarrow (\mathbb{R}, \mathcal{R})$ (i.e. for every Borel $B \in \mathcal{R}$, $f^{-1}(B) \in \mathcal{S}$) will be denoted $L^p (\mu)$, equipped with the norm 
  \[||f||_{L^p(\mu)} \coloneqq \mathbb{E}_\mu [f^p]^{1/p} = \bigg( \int_S |f|^p \,d\mu \bigg)^{1/p}\]
  If $p = 2$, then we can define the inner product 
  \[\langle f, g \rangle_\mu \coloneqq \mathbb{E}_\mu [f g] = \int_S f g \, d\mu\]

  \begin{lemma}
  Let $\mu$ be a stationary measure. Then, the following hold for all $p \geq 1$, $t, s \geq 1$, $\alpha, \beta \in \mathbb{R}$, and bounded measurable functions $f, g$. 
  \begin{enumerate}
      \item Contraction: 
      \[||P_t f||_{L^p(\mu)} \leq ||f||_{L^p (\mu)} = \mathbb{E}_\mu [f^p]^{1/p}\]
      
      \item Linearity: 
      \[P_t (\alpha f + \beta g) = \alpha P_t f + \beta P_t g\] 
      
      \item Semigroup Property: 
      \[P_{t + s} f = P_t P_s f\]
      
      \item Conservativeness: 
      \[P_t 1 = 1\]
  \end{enumerate}
  \end{lemma}

  \begin{lemma}
  Let $\mu$ be a stationary measure. Then, $t \mapsto \Var_\mu [P_t f]$ is a decreasing function of time for every function $f \in L^2 (\mu)$. 
  \end{lemma}
  \begin{proof}
  Note that 
  \begin{align*}
      \Var_\mu [P_t f] & = ||P_t f - \mu f||^2_{L^2(\mu) } =  ||P_t (f - \mu f)||^2_{L^2 (\mu)} = ||P_{t - s} P_s (f - \mu f)||^2_{L^2 (\mu)} \\
      & \leq ||P_s (f - \mu f)||^2_{L^2 (\mu)} = ||P_s f - \mu f||^2_{L^2 (\mu)} = \Var_\mu (P_s f)
  \end{align*}
  \end{proof}

  We now define the analogous operator to the transition rate matrix in discrete time chains with a finite state space. 

  \begin{definition}[Generator]
  The generator $\mathscr{L}$ is defined as 
  \[\mathscr{L} f \coloneqq \lim_{t \downarrow 0} \frac{P_t f - f}{t}\]
  for every $f \in L^2 (\mu)$ for which the above limit exists in $L^2 (\mu)$. The set of $f$ for which $\mathscr{L}f$ is defined is called the domain $\mathrm{Dom}(\mathscr{L})$ of the generator, and $\mathscr{L}$ defines a linear operator from $\mathrm{Dom}(\mathscr{L}) \subset L^2 (\mu)$ to $L^2 (\mu)$. 
  \end{definition}

  We have defined the generator $\mathscr{L}$ from the Markov semigroup $\{P_t\}_{t \geq 0}$. Now, let's try to define the semigroup in terms of the generator $\mathscr{L}$. Given that we have some map $\mathscr{L})$, can we define some semigroup $\{P_t\}$ satisfying the definition? To do this, we must solve the differential equation: 
  \[\frac{d}{dt} P_t = \lim_{\delta \downarrow 0} \frac{P_{t + \delta} - P_t}{\delta} = \lim_{\delta \downarrow 0} \frac{P_t P_\delta - P_t}{\delta} = P_t \lim_{\delta \downarrow 0} \frac{P_\delta - I}{\delta} = P_t \mathscr{L}\]
  For function $P_t$ to satisfy this differential equation, we have the solution 
  \[P_t = e^{t \mathscr{L}}\]
  which also implies that $\mathscr{L}$ and $P_t$ must commute. 

  \begin{definition}[Reversibility]
  The Markov semigroup $\{P_t\}_{t \geq 0}$ with stationary measure $\mu$ is called \textbf{reversible} if 
  \[\langle f, P_t g \rangle_\mu = \langle P_t f, g \rangle_\mu\]
  for every $f, g \in L^2 (\mu)$. Equivalently, we can say that $P_t$ is self-adjoint on $L^2 (\mu)$, or since $P_t = e^{t \mathscr{L}}$, we have $\mathscr{L}$ is self-adjoint. 
  \end{definition}

  \begin{definition}[Ergodicity]
  The Markov semigroup $\{P_t\}_{t \geq 0}$ with stationary measure $\mu$ if called \textbf{ergodic} if 
  \[P_t f \rightarrow \mu f\]
  in $L^2 (\mu)$ as $t \rightarrow +\infty$ for every $f \in L^2 (\mu)$. Note that $\mu f = \mu(f)$ is the constant function in $L^2 (\mu)$. 
  \end{definition}

  \begin{exercise}[Elementary Identities]
  Let $P_t$ be a Markov semigroup with generator $\mathscr{L}$ and stationary measure $\mu$. Prove the following elementary facts. 
  \begin{enumerate}
      \item Show that $\mu( \mathscr{L} f) = 0$ for every $f \in L^2 (\mu)$ 
      \item If $\phi : \mathbb{R} \rightarrow \mathbb{R}$ is convex, then $P_t \phi (f) \geq \phi (P_t f)$ when $f, \phi(f) \in L^2(\mu)$ 
      \item If $\phi : \mathbb{R} \rightarrow \mathbb{R}$ is convex, then $\mathscr{L} \phi(f) \geq \phi^\prime (f) \mathscr{L} f$ when $f, \phi(f) \in L^2 (\mu)$ 
      \item Let $f \in L^2 (\mu)$. Show that the following process is a martingale. 
      \[M_t^f \coloneqq f(X_t) - \int_0^t \mathscr{L} f(X_s) \,ds\]
  \end{enumerate}
  \end{exercise}
  \begin{solution}
  Listed. 
  \begin{enumerate}
      \item This is simply a property of the generator. Not worrying about interchanging limits and integrals, we have 
      \begin{align*}
          \mu(\mathscr{L} f) = \mathbb{E}_\mu [\mathscr{L} f] & = \int_S \lim_{t \downarrow 0} \frac{P_t f - P_0 f}{t} \,d\mu \\ 
          & = \lim_{t \downarrow 0} \int_S \frac{P_t f - P_0 f}{t} \,d\mu \\
          & = \lim_{t \downarrow 0} \frac{1}{t} \big( \mathbb{E}_\mu [P_t f] - \mathbb{E}_\mu [f] \big) = \lim_{t \downarrow 0} \frac{1}{t} \cdot 0 = 0 
      \end{align*}
      
      \item By Jensen's inequality, 
      \begin{align*}
          P_s \phi(f) & = \mathbb{E} [ \phi(f) (X_{t + s}) \mid X_t] \\
          & \geq \phi \bigg( \mathbb{E}[f(X_{t + s} \mid X_t] \big) = \phi(P_s f)
      \end{align*}

  \end{enumerate}
  \end{solution}


  \subsection{Poincare Inequalities}

  Recall that a Poincare inequality for $\mu$ is, informally, of the form 
  \[\mathrm{variance}(f) \leq \mathbb{E}_\mu[ ||\mathrm{gradient}(f)||^2 ]\]
  At first sight, such an inequality has nothing to do with Markov processes. However, the validity of a Poincare inequality for $\mu$ turns out to be related to the rate of convergence of an ergodic Markov process for which $\mu$ is the stationary distribution. That is, a measure $\mu$ satisfies a Poincare inequality for a certain notion of gradient if and only if an ergodic Markov semigroup associated to this gradient converges exponentially fast to $\mu$. 

  \begin{definition}[Dirichlet Form]
  Given a Markov process with generator $\mathscr{L}$ and stationary measure $\mu$, the corresponding Dirichlet form is defined as 
  \[\mathcal{E}(f, g) \coloneqq - \langle f, \mathscr{L} g \rangle_\mu\]
  \end{definition}

  \begin{theorem}[Poincare Inequality]
  Let $P_t$ be a reversible ergodic Markov semigroup with stationary measure $\mu$. The following are equivalent given $c \geq 0$. 
  \begin{enumerate}
      \item $\mathrm{Var}_\mu (f) \leq c \mathcal{E}(f, f)$ for all $f$ (Poincare Inequality) 
      \item $||P_t f - \mu f||_{L^2 (\mu)} \leq e^{-t /c} ||f - \mu f||_{L^2 (\mu)}$
      \item $\mathcal{E}(P_t f, P_t f) \leq e^{-2t /c} \mathcal{E}(f, f)$ for all $f, t$
      \item For every $f$ there exists $\kappa (f)$ s.t. $||P_t f - \mu f||_{L^2 (\mu)} \leq \kappa(f) e^{-t/c}$
      \item For every $f$ there exists $\kappa (f)$ s.t. $\mathcal{E}(P_t f, P_t f) \leq \kappa(f) e^{-2t/c}$ 
  \end{enumerate}
  \end{theorem}

  We should view properties 2 through 5 as different notions of exponential convergence of the Markov semigroup $P_t$ to the stationary measure $\mu$. Properties 2 and 4 directly measure the rate of convergence of $P_t f$ to $\mu f$ in $L^2 (\mu)$, while properties 3 and 5 measure the rate of convergence of the "gradient" (now depicted as $\mathcal{E}$) of $P_t f$ to $0$. 

  \subsubsection{The Gaussian Poincare Inequality}

  \begin{definition}[Ornstein-Uhlenbeck Process]
  Given standard Brownian motion $(W_t)_{t \geq 0}$, the \textbf{Ornstein-Uhlenbeck process} is defined as 
  \[X_t = e^{-t} X_0 + e^{-t} W_{e^{2t} - 1}\]
  \end{definition}

  \begin{lemma}[Gaussian Integration by Parts]
  If $\xi \sim \mathcal{N}(0, 1)$, then 
  \[\mathbb{E}[ \xi f(\xi)] = \mathbb{E}[f^\prime (\xi)]\]
  \end{lemma}
  \begin{proof}
  Assuming that $f$ is smooth with compact support, we have by integration by parts 
  \begin{align*}
      \mathbb{E}[f^\prime (\xi)] & = \int_{-\infty}^\infty f^\prime(x) \frac{e^{-x^2 / 2}}{\sqrt{2\pi}} \,dx \\ 
      & = \frac{e^{-x^2 / 2}}{\sqrt{2\pi}} \, f(x) \bigg|_{-\infty}^\infty - \int_{-\infty}^\infty f(x) \frac{d}{dx} \bigg(\frac{e^{-x^2 / 2}}{\sqrt{2\pi}}\bigg) \,dx \\
      & = - \int_{-\infty}^\infty -x f(x) \frac{e^{-x^2 / 2}}{\sqrt{2\pi}} \,dx \\
      & = \int_{-\infty}^\infty \big( x f(x)\big) \frac{e^{-x^2 / 2}}{\sqrt{2\pi}}\,dx = \mathbb{E}[\xi f(\xi)]
  \end{align*}
  \end{proof}

  \begin{theorem}
  The Ornstein-Uhlenbeck Process $(X_t)_{t \geq 0}$ 
  \begin{enumerate}
      \item is a Markov process with semigroup 
      \[P_t f(x) = \mathbb{E} \big[ f(e^{-t} x + \sqrt{1 - e^{-2t}} \xi) \big] \text{ with } \xi \sim \mathcal{N}(0, 1)\]
      \item admits $\mu = \mathcal{N}(0, 1)$ as its stationary measure
      \item is ergodic
      \item has generator and Dirichlet form given by 
      \[\mathscr{L} f(x) = -x f^{\prime} (x) + f^{\prime\prime} (x), \;\;\;\;\; \mathcal{E}(f, g) = \langle f^\prime , g^\prime \rangle_\mu\]
      \item is reversible
  \end{enumerate}
  \end{theorem}
  \begin{proof}
  Let $s \geq t$. 
  \begin{enumerate}
      \item By definition of $X_t$, we have $X_t = e^{-t} X_0 + e^{-t} W_{e^{2t - 1}}$ and 
      \[X_s = e^{-s} X_0 + e^{-s} W_{e^{2s} - 1} \implies X_0 = (X_s - e^{-s} W_{e^{2s} - 1} ) e^{s}\]
      Substituting in the equation for $X_s$ gives 
      \begin{align*}
          X_t & = e^{-(t - s)} X_s + e^{-t} (W_{e^{2t} - 1} - W_{e^{2s} - 1}) \\
          & = e^{-(t - s)} X_s + \sqrt{1 - e^{-2 (t - s)}} \xi
      \end{align*}
      where $\xi = (W_{e^{2t} - 1} - W_{e^{2s} - 1}) / \sqrt{e^{2t} - e^{2s}} \sim N(0, 1)$ is independent of $\{X_r\}_{r \leq s}$. Therefore, we can write 
      \[\mathbb{E}[ f(X_t) \mid \{X_r\}_{r \leq s}] = P_{t - s} f (X_s) = \mathbb{E}\big[f \big( e^{-(t - s)} X_s + \sqrt{1 - e^{-2 (t - s)}} \xi \big) \big]\]
      which proves the Markov property and gives the semigroup. 
      
      \item We can clearly see that if $X_t \sim N(0, 1)$, then $X_{t + s} = e^{-s} X_t + \sqrt{1 - e^{-2s}}\xi$ is a sum of Gaussians, one with variance $e^{-2s}$ and the other with variance $1 - e^{-2s}$, and so their sum has variance $1$. 
      
      \item We will take for granted that this is ergodic. 
      
      \item To compute the generator, we use the chain rule (and not worry about whether we take the derivative within the expectation integral) and then use Gaussian integration by parts to get 
      \begin{align*}
          \frac{d}{dt} P_t f(x) & = \mathbb{E} \bigg[ f^\prime (e^{-t} x + \sqrt{1 - e^{-2t}} \xi) \bigg( \frac{e^{-2t}}{\sqrt{1 - e^{-2t}}} \xi - e^{-t} x \bigg) \bigg] \\
          & = \mathbb{E} \big[ e^{-t} x f^\prime (e^{-t} x + \sqrt{1 - e^{-2t}} \xi) + e^{-2t} f^{\prime\prime} (e^{-t} x + \sqrt{1 - e^{-2t}} \xi ) \big]
      \end{align*}
      and therefore have 
      \[\frac{d}{dt} P_t f (x) = \bigg( -x \frac{d}{dx} + \frac{d^2}{dx^2} \bigg) P_t f (x)\]
      The Dirichlet form can be simplified using the Gaussian integration by parts as 
      \begin{align*}
          \mathcal{E} (f, g) & = - \langle f, \mathscr{L} g \rangle_\mu \\
          & = \mathbb{E}[ f(\xi) \big( x g^\prime (\xi) - g^{\prime\prime} (\xi) \big)] \\
          & = \mathbb{E}[\xi f(\xi) g^\prime(\xi)] - \mathbb{E}[f(\xi) g^{\prime\prime} (\xi)] \\
          & = \mathbb{E}[f^\prime (\xi) g^\prime (\xi) + f(\xi) g^{\prime\prime} (\xi)] - \mathbb{E}[f(\xi) g^{\prime\prime} (\xi)] \\
          & = \mathbb{E}[f^\prime (\xi) g^\prime (\xi) ]
      \end{align*} 
      
      \item Since $\mathcal{E}(f, g) = \mathbb{E}[f^\prime(\xi) g^\prime (\xi)]$, it is symmetric and so $\mathscr{L}$ is self-adjoint. 
  \end{enumerate}

  \end{proof}

  From the previous theorem part 4, we can see that 
  \[\mathcal{E}(f, f) = \langle f^\prime, f^\prime \rangle_\mu = ||f^\prime||_{L^2(\mu)}^2 = \mathbb{E}_\mu[ f^{\prime 2} ]\]
  which means that the Dirichlet form of an Ornstein-Uhlenbeck process is precisely the expected square gradient of function $f$! Therefore, with the Poincare inequality, we can bound the variance of $f$ with the Dirichlet form, which is the expected square gradient of $f$. 

  \begin{theorem}
  Let $\mu = \mathcal{N}(0, 1)$. Then, 
  \[\mathrm{Var}_\mu [f] \leq ||f^\prime||_{L^2(\mu)}^2\]
  \end{theorem}
  \begin{proof}
  We have from the properties of the Ornstein-Uhlenbeck process that
  \begin{align*}
      \frac{d}{dx} P_t f(x) & = \frac{d}{dx} \mathbb{E}[ f(e^{-t} x + \sqrt{1 - e^{-2t}} \xi)] \\
      & = \mathbb{E} \bigg[ \frac{d}{dx} f(e^{-t} x + \sqrt{1 - e^{-2t}} \xi)] \\
      & = \mathbb{E}[f^\prime (e^{-t} x + \sqrt{1 - e^{-2t}} \xi) \; e^{-t}] \\
      & = e^{-t} \mathbb{E}[f^\prime (e^{-t} x + \sqrt{1 - e^{-2t}} \xi)] \\
      & = e^{-t} P_t f^\prime (x) 
  \end{align*}
  Thus
  \[\mathcal{E}(P_t f, P_t f) = ||(P_t f)^\prime||_{L^2 (\mu)}^2 = e^{-2t} || P_t f^\prime ||^2_{L^2(\mu)} \leq e^{-2t} ||f^\prime||^2_{L^2(\mu)} = e^{-2t} \mathcal{E}(f, f) \]
  where the inequality follows from contraction. 
  \end{proof}

  By tensorization, we can prove the following. 

  \begin{corollary}[Gaussian Poincare Inequality]
  Let $X_1, \ldots, X_n \sim N(0, 1)$ be iid. Then, 
  \[\Var[ f(X_1, \ldots, X_n)] \leq \mathbb{E}[ || \nabla f (X_1, \ldots, X_n)||^2 ]\]
  \end{corollary}
  \begin{proof}
  Computation. 
  \begin{align*}
      \mathrm{Var}[f(X_1, \ldots, X_n)] & \leq \mathbb{E} \bigg[ \sum_{i=1}^n \mathrm{Var}_i f(X_1, \ldots, X_n) \bigg] \\
      & \leq \mathbb{E} \bigg[ \sum_{i=1}^n \bigg| \bigg| \frac{d}{dx_i} f(X_1, \ldots, X_n)\bigg|\bigg|^2 \bigg] \\
      & = \mathbb{E}[ ||\nabla f (X_1, \ldots, X_n) ||^2 ]
  \end{align*}
  \end{proof}

  So what have we done so far? If we have some distribution $\mu$ and want to prove an inequality that bounds $\Var_\mu [f]$, then we should choose some (reversible ergodic) Markov process that has a stationary distribution $\mu$. We can identify its semigroup, generator, and ultimately its Dirichlet form $\mathcal{E}(f, g)$, which will allow us to invoke the Poincare inequality to bound 
  \[\Var_\mu [f] \leq c \mathcal{E}(f, f)\]
  and since $\mu = N(0, 1)$, we have shown above using both the properties of the generator of the Ornstein-Uhlenbeck process and Gaussian integration by parts that this Dirichlet form is precisely the norm of $f^\prime$. This is clear since the Dirichlet form $\langle f, \mathscr{L} g\rangle_\mu$ only depends on $\mathscr{L}$ and $\mu$. However, the Dirichlet form does not have to be this form. 
  \begin{enumerate}
      \item If $\mu$ is some other distribution, we would not be able to reduce $\mathcal{E}(f, f)$ to the norm of its derivative, and so it make take on a different form. 
      \item If we choose a different Markov process, even with the same stationary measure $\mu = N(0, 1)$, the generator may be different and so will the Dirichlet form. 
  \end{enumerate}

  \begin{exercise}[Carre du Champ]
  We have interpreted the Dirichlet form $\mathcal{E}(f, f)$ as a general notion of expected square gradient that arises in the study of Poincare inequalities. There is an analogous quantity $\Gamma(f, f)$ that plays the role of square gradient in this setting (without the expectation). In good probabilistic tradition, it is universally known by its French name carre du champ (literally, square of the field). The carre du champ is defined as
  \[\Gamma(f, g) \coloneqq \frac{1}{2} \big[ \mathscr{L}(f g) - f \mathscr{L} g - g \mathscr{L} f \big] \]
  in terms of the generator $\mathscr{L}$ of a Markov process with stationary measure $\mu$. 
  \begin{enumerate}
      \item Show that $\mathcal{E}(f, f) = \int \Gamma(f, f) \, d\mu$ and that $\mathcal{E}(f, g) = \int \Gamma(f, g) \,d\mu$ if the Markov process is in addition reversible. 
      \item Show that $\Gamma(f, f) \geq 0$ so it can indeed by interpreted as a square. 
      \item Prove the Cauchy-Schwartz inequality $\Gamma(f, g)^2 \leq \Gamma(f, f) \, \Gamma(g, g)$ 
      \item Compute the carre du champ of the Ornstein-Uhlenbeck process and confirm that it should indeed be interpreted as the appropriate notion of "square gradient." 
  \end{enumerate}
  \end{exercise}
  \begin{solution}
  Listed. 
  \begin{enumerate}
      \item By stationarity, we have 
      \[\mu ( \mathscr{L} f) = \int_S \mathscr{L} f \, d\mu = 0\]
      for all $f \in L^2 (\mu)$, which reduces the first term below to $0$. So, we can reduce the carre du champ to 
      \begin{align*}
          \int_S \Gamma(f, f) \, d\mu & = \frac{1}{2} \bigg( \int_S \mathscr{L} (f^2) \, d\mu - 2 \int_S f \mathscr{L} f \, d\mu \bigg) \\
          & = - \int_S f \mathscr{L} f \, d\mu = - \langle f, \mathscr{L} f \rangle_\mu = \mathcal{E}(f, f)
      \end{align*}
      Furthermore, assuming that $P_t$ is reversible, we have 
      \[\mathcal{E}(f, g) = - \langle f, \mathscr{L} g \rangle_\mu = -\langle \mathscr{L} f, g \rangle_\mu = - \langle g, \mathscr{L} f \rangle_\mu = \mathcal{E}(g, f)\]
      and so 
      \begin{align*}
          \int \Gamma (f, g) \, d\mu & = \frac{1}{2} \bigg( \int \mathscr{L}(f g) \, d\mu - \int f \mathscr{L} g \, d\mu - \int g \mathscr{L} f \, d\mu \bigg) \\
          & = \frac{1}{2} \big( - \langle f, \mathscr{L} g \rangle_\mu - \langle g, \mathscr{L} f\rangle_\mu \big) \\
          & = - \langle f, \mathscr{L} g \rangle_\mu = \mathcal{E}(f, g)
      \end{align*}
      
      \item Since $\Gamma(f, f) = \frac{1}{2} \big( \mathscr{L} (f^2) - 2 f \mathscr{L}f \big)$, the problem now reduces to proving that $\mathscr{L} (f^2) \geq 2 f \mathscr{L}f$. By Jensen's inequality, we have $P_t (f^2) \geq (P_t f)^2$, and so 
      \begin{align*}
          \mathscr{L}(f^2) & = \lim_{t \downarrow 0} \frac{P_t (f^2) - f^2}{t} \geq \lim_{t \downarrow 0} \frac{(P_t f)^2 - f^2}{t} \\
          & = \frac{d}{dt} (P_t f)^2 \bigg|_{t = 0} = \bigg( 2 (P_t f) \cdot \frac{d}{dt} (P_t f) \bigg)\bigg|_{t = 0} = 2 f \mathscr{L} f
      \end{align*}
      
      \item We know that $\Gamma(f + t g, f + tg) \geq 0$ from above, and so if we expand out, we get
      \begin{align*}
          \Gamma(f + t g, f + tg) & = \frac{1}{2} \Big[ \mathscr{L} \big( (f + t g)^2 \big) - 2 (f + t g) \mathscr{L}(f + t g) \Big] \\
          & = \Gamma(g, g) t^2 + 2 \Gamma (f, g) t + \Gamma(f, f) \geq 0 
      \end{align*}
      for all $t$. Since this quadratic is nonnegative, its discriminant must be $\leq 0$, and so 
      \[\Delta = \big( 2 \Gamma (f, g) \big)^2 - 2 \Gamma(g, g) \Gamma(f, f) \leq 0 \implies \Gamma(f, g)^2 \leq \Gamma(f, f) \Gamma(g, g)\]
      
      \item The generator of the Ornstein-Uhlenbeck process is $\mathscr{L}f(x) = -x f^\prime(x) + f^{\prime\prime} (x)$. Therefore, 
      \begin{align*}
          \Gamma (f, g) (x) & = \frac{1}{2} \big[ \mathscr{L} (f g)(x) - f(x) \mathscr{L} g(x) - g(x) \mathscr{L} f (x) \big] \\
          & = \frac{1}{2} \Big[ \big( -x (f g)^\prime (x) + (f g)^{\prime\prime} (x) \big) - f(x) \big( -x g^\prime(x) + g^{\prime\prime} (x) \big) - g(x) \big( -x f^\prime(x) + f^{\prime\prime} (x) \big) \Big]
      \end{align*}
      which simplifies down to $f^\prime(x) g^\prime(x)$, and so $\Gamma(f, f) = [ f^\prime(x)]^2$ can be interpreted as the square gradient of $f$. 
  \end{enumerate}
  \end{solution}

  \subsection{Variance Identities and Exponential Ergodicity}

  Now, let us develop some intuition on the connection between Markov semigroups, $\Var_\mu [f]$ and the Dirichlet form $\mathcal{E}(f, f)$. 

  \begin{lemma}
  The following identity holds. 
  \[\frac{d}{dt} \Var_\mu [P_t f] = -2 \mathcal{E} (P_t f, P_t f)\]
  \end{lemma}
  \begin{proof}
  By stationarity, $\mu (P_t f) = \mu(f)$, and so 
  \begin{align*}
      \frac{d}{dt} \Var_\mu [P_t f] & = \frac{d}{dt} \big\{ \mu((P_t f)^2) - \mu(P_t f)^2\big\} \\
      & = \frac{d}{dt} \big\{ \mu((P_t f)^2) - \mu( f)^2\big\} = \frac{d}{dt} \mu((P_t f)^2) \\
      & = \frac{d}{dt} \int_S (P_t f)^2 \, d\mu = \int_S \frac{d}{dt} (P_t f)^2 \,d\mu = 2 \int_S (P_t f) \, \frac{d}{dt} P_t f \, d\mu \\
      & = 2 \mathbb{E}_\mu [P_t f, \mathscr{L} (P_t f) ] = 2 \langle P_t f, \mathscr{L} P_t f \rangle_\mu = -2 \mathcal{E}(P_t f, P_t f) 
  \end{align*}
  \end{proof}

  \begin{theorem}
  $\mathcal{E}(f, f) \geq 0$ for every $f$. 
  \end{theorem}
  \begin{proof}
  We know that $t \mapsto \Var_\mu [P_t f]$ is a decreasing function of $t$ (by contraction of $P_t$), so 
  \[\frac{d}{dt} \Var_\mu [P_t f] = - 2 \mathcal{E}(P_t f, P_t f) \leq 0\]
  \end{proof}

  \begin{theorem}
  Suppose that the Markov semigroup is ergodic. Then, we have for every $f$ 
  \[\Var_\mu [f] = 2 \int_0^\infty \mathcal{E}(P_t f, P_t f) \, dt\]
  \end{theorem}

\section{Subgaussian Concentration and log-Sobolev Inequalities}

  \subsection{Subgaussian Variables and Chernoff Bounds}

  We should first consider how one might go about proving that a random variable satisfies a Gaussian tail bound. Most tail bounds in probability theory are proved using some form of Markov's inequality. 
  \begin{lemma}[Markov's Inequality]
  Given a nonnegative random variable $X$, we have 
  \[\mathbb{P}(X > \alpha) \leq \frac{\mathbb{E}[X]}{\alpha}\]
  which means that the probability that $X > \alpha$ goes down at least as fast as $1/\alpha$. 
  \end{lemma}

  Markov's inequality is very conservative but very general, too. If we make further assumptions about the random variable $X$, we can often make stronger bounds. Chebyshev's inequality assumes a (possibly negative) random variable with finite variance and states that the probability will go down as $1/x^2$. 

  \begin{theorem}[Chebyshev Inequality]
  Given (possibly negative) random variable $X$, if $\mathbb{E}[X] = \mu < +\infty$ and $\Var(X) = \sigma^2 < +\infty$, then for all $\alpha > 0$, 
  \[\mathbb{P} \big( |X - \mu| > k \sigma \big) \leq \frac{1}{k^2} \iff \mathbb{P}(|X - \mu| > \alpha) \leq \frac{\mathrm{Var}[X]}{\alpha^2}\]
  That is, the probability that $X$ takes a value further than $k$ standard deviations away from $\mu$ goes down by $1/k^2$. Therefore, if $\sigma$ is small, then this bound will be small since there is more concentration in the mean. 
  \end{theorem}
  \begin{proof}
  We apply Markov's inequality to the non-negative random variable $|X - \mu|$. 
  \[\mathbb{P}(|X - \mu| > \alpha) = \mathbb{P}(|X - \mu|^2 > \alpha^2) \leq \frac{\mathbb{E}(|X - \mu|^2)}{\alpha^2} = \frac{\mathrm{Var}[X]}{\alpha^2}\]
  since the numerator on the RHS is the definition of variance. 
  \end{proof}

  Using higher powers, we can obtain better and better bounds, but not exponential ones. To obtain these Gaussian tail bounds, we must use more sophisticated methods. 

  \begin{lemma}[Chernoff Bound]
  Define the log-moment generating function $\psi$ of a random variable $X$ and its Legendre dual $\psi^*$ as 
  \[\psi_X (\lambda) \coloneqq \log \mathbb{E}[ e^{\lambda (X - \mathbb{E}[X])} ] = \mathbb{E}[ e^{\lambda X} ] - \lambda \mathbb{E}[X] \;\;\;\;\; \psi_X^* (t) = \sup_{\lambda \geq 0} \{ \lambda t - \psi_X(\lambda)\}\]
  Then, the following is known as the \textbf{Chernoff bound}. 
  \[\mathbb{P}[X - \mathbb{E}[X] \geq t] \leq e^{-\psi_X^* (t)}\]
  for all $t \geq 0$. We can lower bound it too with 
  \[\mathbb{P}[X - \mathbb{E}[X] \leq -t] \leq e^{-\psi_X^* (t)}\]
  and union bounding them gives 
  \[\mathbb{P}(|X - \mathbb{E}[X] | \geq t ] \leq 2e^{- \psi_X^* (t)}\]
  \end{lemma}
  \begin{proof}
  We take some $\lambda \geq 0$ and given that the map $x \mapsto e^{\lambda x}$ is nondecreasing, we can exponentiate and then use Markov's inequality: 
  \[\mathbb{P}[X - \mathbb{E}[X] \geq t ] = \mathbb{P}[ e^{\lambda(X - \mathbb{E}[X])} \geq e^{\lambda t}] \leq e^{-\lambda t} \mathbb{E}[e^{\lambda (X - \mathbb{E}[X])}] = e^{- (\lambda t - \psi_X (\lambda))} \leq e^{-\psi_X^* (t)}\]
  as the left hand does not depend on the choice of $\lambda$, we have the additional flexibility of tuning $\lambda$ to get potentially better bounds. We can also use Chernoff bound on the random variable $-X$ to bound \begin{align*}
      \mathbb{P}(X - \mathbb{E}[X] \leq -t) & = \mathbb{P}(-X - \mathbb{E}[-X] \geq t) \\
      & = \mathbb{P}(e^{\lambda(-X + \mathbb{E}[X])} \geq e^{\lambda t} ] \\
      & \leq e^{-\lambda t} \mathbb{E}[ e^{\lambda (-X + \mathbb{E}[X])}] \\
      & = e^{-(\lambda t - \psi_{-X}(\lambda))} \leq e^{-\psi_{-X}^* (t)} 
  \end{align*}
  There seems to be a minor problem in the fact that $-\psi^*_X$ and $-\psi^*_{-X}$ are different, and so provide different bounds for the upper and lower tail. But note that $\psi_X (\lambda) = \psi_{-X}(-\lambda)$, and so their maximum will coincide and $\psi_X^* (t) = \psi_{-X}^* (t)$, allowing us to get the union bound. 
  \[\mathbb{P}(|X - \mathbb{E}[X] | \geq t ] \leq 2e^{- \psi^* (t)}\]
  \end{proof}

  To observe how the Chernoff bound can give rise to Gaussian tail bounds, let us first consider the case of an actual Gaussian random variable. 

  \begin{example}
  Let $X \sim N(\mu, \sigma^2)$. Then, $\mathbb{E}[ e^{\lambda (X - \mathbb{E}[X])} ] = e^{\lambda^2 \sigma^2 / 2}$, so 
  \[\psi(\lambda) = \frac{\lambda^2 \sigma^2}{2}, \;\;\;\;\; \psi^* (t) = \sup_{\lambda \geq 0} \big\{ \lambda t - \frac{\lambda^2 \sigma^2}{2} \big\} = \frac{t^2}{2 \sigma^2}\]
  and by the Chernoff bound, we have $\mathbb{P}(X - \mathbb{E}[X] \geq t ] \leq e^{-t^2 / 2\sigma^2}$. 
  \end{example}

  Note that in order to get the tail bound, the fact that $X$ is Gaussian was not actually important. It would suffice to assume that the log-MGF is bouded from above by a Gaussian. 

  \begin{definition}[Subgaussian Random Variables]
  A random variable is called $\sigma^2$-\textbf{subgaussian} if its log-MGF satisfies 
  \[\psi(\lambda) \leq \frac{\lambda^2 \sigma^2}{2}\]
  for all $\lambda \in \mathbb{R}$. The constant $\sigma^2$ is called the \textbf{variance proxy}. 
  \end{definition}

  Remember that if $\psi(\lambda)$ is the log-MGF of a random variable $X$, then $\psi(-\lambda)$ is the log-MGF of the random variable $-X$. For a $\sigma^2$-subgaussian random variable $X$, we can therefore apply the Chernoff bound to both the upper and lower tails and union bound to obtain 
  \[\mathbb{P}(|X - \mathbb{E}[X]| \geq t ) \leq 2 e^{-t/2\sigma^2}\]

  We have only worked with Gaussians, which are trivially subgaussian. A nontrivial results is that every bounded random variable is subgaussian. 

  \begin{lemma}[Hoeffding's Lemma]
  Let $a \leq X \leq b$ a.s. for some $a, b \in \mathbb{R}$. Then, 
  \[\mathbb{E}[e^{\lambda(X - \mathbb{E}[X])}] \leq \exp \bigg( \frac{\lambda^2 (b - a)^2}{8} \bigg)\]
  That is, $X$ is $(b-a)^2 /4$-subgaussian. 
  \end{lemma}
  \begin{proof}
  We assume without loss of generality that $\mathbb{E}[X] = 0$. Then, we have $\psi(\lambda) = \log \mathbb{E}[ e^{\lambda X}]$, and we can compute 
  \[\psi^\prime (\lambda) = \frac{\mathbb{E}[X e^{\lambda X}]}{\mathbb{E}[e^{\lambda X}]}, \;\;\;\;\; \psi^{\prime\prime} (\lambda) = \frac{\mathbb{E}[X^2 e^{\lambda X}]}{\mathbb{E}[e^{\lambda X}]} - \bigg( \frac{\mathbb{E}[X e^{\lambda X}]}{\mathbb{E}[e^{\lambda X}]} \bigg)^2\]
  and thus 
  \[\psi^{\prime\prime} (\lambda) = \int_\Omega X^2 \, \frac{e^{\lambda X}}{\mathbb{E}[e^{\lambda X}]} \,d\mathbb{P} - \bigg( \int_\Omega X \, \frac{e^{\lambda X}}{\mathbb{E}[e^{\lambda X}]} \,d\mathbb{P} \bigg)^2 \] 
  can be interpreted as the variance of the random variable $X$ under the twisted probability measure $d\mathbb{Q} = \frac{e^{\lambda X}}{\mathbb{E}[e^{\lambda X}]} \,d\mathbb{P}$. But $a \leq X \leq b$, so we can bound the variance by its infimum and suprememum $\psi^{\prime\prime} (\lambda) = \Var_\mathbb{Q} [X] \leq (b-a)^2 / 4$, and the fundamental theorem of calculus yields 
  \[\psi(\lambda) = \int_0^\lambda \int_0^\mu \psi^{\prime\prime} (\rho) \, d\rho \, d\mu \leq \frac{\lambda^2 (b - a)^2}{8}\]
  using $\psi(0) = 0$ and $\psi^\prime (0)$. 
  \end{proof}

  \begin{exercise}[Subgaussian Variables]
  There are several different notions of random variables with a Gaussian tail that are all essentialy equivalent up to constants. The aim of this problem is to obtain some insight into these notions. 
  \begin{enumerate}
      \item Show that if $X$ is $\sigma^2$-subgaussian, then $\Var[X] \leq \sigma^2$. 
      \item Show that for any increasing and differentiable function $\Phi$, 
      \[\mathbb{E}[ \Phi(|X|)] = \Phi(0) + \int_0^\infty \Phi^\prime (t) \, \mathbb{P}(|X| \geq t) \, dt\]
  \end{enumerate}
  In the following, we will assume for simplicity that $\mathbb{E}[X] = 0$. We now prove that the following three properties are equivalent for suitable constants $\sigma, b, c$: (1) $X$ is $\sigma^2$-subgaussian; (2) $\mathbb{P}(|X| \geq t) \leq 2 e^{-b t^2}$; and (3) $\mathbb{E}[e^{c X^2}] \leq 2$. 
  \begin{enumerate}[resume]
      \item Show that if $X$ is $\sigma^2$-subgaussian , then $\mathbb{P}(|X| \geq t) \leq 2 e^{-t^2 / 2 \sigma^2}$ 
      \item Show that if $\mathbb{P}(|X| \geq t) \leq 2 e^{- t^2 / 2 \sigma^2}$, then $\mathbb{E}[e^{X^2 / 6 \sigma^2} ] \leq 2$. 
      \item Show that if $\mathbb{E}[e^{X^2 / 6 \sigma^2}] \leq 2$, then $X$ is $18 \sigma^2$-subgaussian. 
  \end{enumerate}
  In addition, the subgaussian property of $X$ is equivalent to the fact that the moments of $X$ scale as is the case for the Gaussian distribution. 
  \begin{enumerate}[resume]
      \item Show that if $X$ is $\sigma^2$-subgaussian, then $\mathbb{E}[X^{2q}] \leq (4 \sigma^2)^q q!$ for all $q \in \mathbb{N}$. 
      \item Show that if $\mathbb{E}[X^{2q}] \leq (4 \sigma^2)^q q!$ for all $q \in \mathbb{N}$, then $\mathbb{E}[e^{X^2 / 8 \sigma^2}] \leq 2$. 
  \end{enumerate}
  \end{exercise}

  \begin{solution}
  Listed. 
  \begin{enumerate}
      \item We can expand out 
      \begin{align*}
          \mathbb{E}[e^{\lambda (X - \mathbb{E} X}] & = \mathbb{E} \bigg[ 1 + \lambda( X - \mathbb{E}X) + \frac{\lambda^2}{2} (X - \mathbb{E} X)^2 + \ldots \bigg] \\
          & = 1 + \frac{\lambda^2}{2} \Var[X] + o(\lambda^2) \\
          & \leq e^{\lambda^2 \sigma^2 / 2} = 1 + \frac{\lambda^2 \sigma^2}{2} + o (\lambda^2)
      \end{align*}
      which is true for all $\lambda$. Setting $\lambda = 0$, we get $\Var[X] \leq \sigma^2$. 
      
      \item Unfinished. 
      
      \item Since $X$ is $\sigma^2$ subgaussian, its log-MGF satisfies $\psi(\lambda) = \log \mathbb{E}[e^{\lambda X}] \leq \frac{\lambda^2 \sigma^2}{2} \implies - \psi(\lambda) \geq - \frac{\lambda^2 \sigma^2}{2}$. Then, its Legendre dual is 
      \[\psi^\ast (t) = \sup_{\lambda \geq 0} \{ \lambda t - \psi(\lambda)\} \geq \sup_{\lambda \geq 0} \{ \lambda t - \frac{\lambda^2 \sigma^2}{2} \} = \frac{t^2}{2 \sigma^2}\]
      where we optimize the quadratic w.r.t. $\lambda$. Therefore, $-\psi^* (t) \leq - \frac{t^2}{2\sigma^2} \implies \mathbb{P}(X \geq t) \leq e^{ - \psi^* (t)} \leq e^{- t^2/ 2 \sigma^2}$. 
      
      
      \item By using the identity above with $\Phi(t) = e^{t^2 / 6 \sigma^2}$, we have 
      \begin{align*}
          \mathbb{E}[e^{X^2 / 6 \sigma^2}] & = \mathbb{E}[e^{|X|^2 / 6 \sigma^2}] \\
          & = e^{0^2 / 6 \sigma^2} + \int_0^\infty e^{t^2 / 6 \sigma^2} \, \frac{t}{3 \sigma^2} \mathbb{P}(|X| \geq t) \, dt \\
          & \leq 1 + \frac{1}{3t^2} \int_0^\infty t e^{t^2 / 6 \sigma^2} \, 2 e^{-t^2 / 2\sigma^2} \, dt\\
          & = 1 + \frac{2}{3 \sigma^2} \int_0^\infty t e^{-\frac{1}{3} \frac{t^2}{\sigma^2}} \, dt \\
          & = 1 - \frac{1}{\sigma^2} \int_0^\infty \Big( - \frac{2}{3 \sigma} t \Big) \, e^{- \frac{t^2}{3 \sigma^2}} \, dt \\
          & = 1 - e^{- \frac{t^2}{3 \sigma^2}} \bigg|_0^\infty \\
          & = 1 - (0 - 1) = 2 
      \end{align*}
      
      \item Unfinished. 
      
      \item We know $X^{2q} = |X|^{2q}$ for all $q \in \mathbb{N}$. By setting $\Phi(t) = t^{2q}$ from the identity above, we can get 
      \[\mathbb{E}[|X|^{2q}] = 0^{2q} + \int_0^\infty (2q) t^{2q - 1} \mathbb{P}(|X| \geq t) \,dt\]
      and from (3), we get the first line, where we can just keep doing integration by parts: 
      \begin{align*}
          \mathbb{E}[|X|^{2q}] & \leq \int_0^\infty (2q) t^{2q - 1} e^{- t^2 / 2 \sigma^2} \,dt \\
          & = 2 (4q \sigma^2) \int_0^\infty (2q - 2) t^{2q - 3} e^{-t^2 / 2 \sigma^2} \, dt \\
          & = 2 (4q \sigma^2) (4 (q - 1) \sigma^2) \int_0^\infty (2q - 4) t^{2q - 5} e^{-t^2 / 2\sigma^2} \,dt \\
          & = \ldots \\
          & = 2 (4q \sigma^2) \ldots (4 \cdot 2\sigma^2) \int_0^\infty 2t e^{-t^2 / 2 \sigma^2} \,dt \\
          & = \prod_{k=1}^q (4 k \sigma^2) = (4 \sigma^2)^q q! 
      \end{align*}
      
      \item We can expand and from the inequality above, we get 
      \begin{align*}
          \mathbb{E}[e^{X^2 / 8 \sigma^2}] = \mathbb{E} \bigg[ 1 + \frac{X^2}{8 \sigma^2} + \frac{1}{2} \bigg( \frac{X^2}{8 \sigma^2}\bigg)^2 + \ldots \bigg] \\
          & = 1 + \sum_{q = 1}^\infty \frac{1}{(8 \sigma^2)^q q!} \mathbb{E}[X^{2q}] \\
          & \leq 1 + \sum_{q = 1}^\infty \frac{1}{(8 \sigma^2)^q q!} (4 \sigma^2)^q q! \\
          & = 1 + \sum_{q=1}^\infty \frac{1}{2^q} = 2 
      \end{align*}
  \end{enumerate}


  \end{solution}

  \begin{exercise}[Tightness of Hoeffding's Lemma]
  Show that the bound on Hoeffding's lemma is the best possible by consider $\mathbb{P}(X = a) = \mathbb{P}(X = b) = \frac{1}{2}$. 
  \end{exercise}
  \begin{solution}
  From computing the expectation 
  \[\mathbb{E}[ e^{\lambda (X - \mathbb{E} X)}] = e^{\lambda (a - \frac{a + b}{2})} \mathbb{P}(X = a) + e^{\lambda (b - \frac{a + b}{2})} \mathbb{P}(X = b) = \frac{1}{2} e^{ \lambda \frac{a - b}{2}} + \frac{1}{2} e^{\lambda \frac{b - a}{2}}\]
  we know that this is always less than $\lambda^2 (b - a)^2/ 8$ for all $\lambda$. But setting $\lambda = 0$ satisfies equality. 
  \end{solution}

  \subsection{The Martingale Method}

  In this section, we will use the martingale method to derive useful results. Recall that in order to derive some property (like tensorization of variance) of $f(X_1, \ldots, X_n) - \mathbb{E}[f(X_1, \ldots, X_n)]$, we can expand it as a telescoping sum of martingale differences 
  \[f(X_1, \ldots, X_n) - \mathbb{E}[f(X_1, \ldots, X_n)] = \sum_{k=1}^n \Delta_k\]
  where 
  \[\Delta_k = \mathbb{E}[f(X_1, \ldots, X_n) \mid X_1, \ldots, X_k] - \mathbb{E}[f(X_1, \ldots, X_n) \mid X_1, \ldots, X_{k-1}]\]
  and then deriving bounds on each difference. Note that these are martingale differences because given the filtration $\mathbb{F} = \{\mathcal{F}_k = \sigma(X_1, \ldots, X_k)\}$, the stochastic process
  \[Y_k = \sum_{i=1}^k \Delta_i = \mathbb{E}[f(X_1, \ldots, X_n) \mid X_1, \ldots, X_k] - \mathbb{E}[f(X_1, \ldots, X_n)]\]
  is a martingale. 

  \begin{lemma}[Azuma]
  Let $\mathbb{F} = \{\mathcal{F}_k\}_{k \leq n}$ be any filtration, and $\Delta_1, \ldots, \Delta_n$ be random variables that satisfy the following properties for $k = 1, \ldots, n$. 
  \begin{enumerate}
      \item Martingale Difference Property: $\Delta_k$ is $\mathcal{F}_k$-measurable and $\mathbb{E}[\Delta_k \mid \mathcal{F}_{k-1}] = 0$ 
      \item Conditional Subgaussian Property: $\mathbb{E}[e^{\lambda \Delta_k} \mid \mathcal{F}_{k-1}] \leq e^{\lambda^2 \sigma^2_k / 2}$ a.s. 
  \end{enumerate}
  Then, the sum $\sum_{k=1}^n \Delta_k$ is subgaussian with variance proxy $\sum_{k=1}^n \sigma_k^2$. 
  \end{lemma}
  \begin{proof}
  For any $1 \leq k \leq n$, we can compute 
  \[\mathbb{E}[ e^{\lambda \sum_{i=1}^k \Delta_i} ] = \mathbb{E}[e^{\lambda \sum_{i=1}^{k-1} \Delta_i} \mathbb{E}[e^{\lambda \Delta_k} \mid \mathcal{F}_{k-1}]] \leq e^{\lambda^2 \sigma_k^2 / 2} \, \mathbb{E}[e^{\lambda \sum_{i=1}^{k-1} \Delta_i}]\]
  and by induction, this proof is finished. Note that $\mathbb{E}[e^{\lambda \Delta_k} \mid \mathcal{F}_{k-1}] \leq e^{\lambda^2 \sigma^2_k / 2}$ can only hold if $\mathbb{E}[\Delta_k \mid \mathcal{F}_{k-1}] = 0$. 
  \end{proof}

  What this lemma basically says is that if we decompose a random variable into martingale differences, and each martingale difference is conditionally subgaussian, then their sum is also subgaussian. Now, if we just assume that each of these martingale differences are bounded, then we can use Hoeffding's lemma on each of them to make them subgaussian, and then use Azuma's lemma to show that their sum is subgaussian. This is exactly what we do here. 

  \begin{theorem}[Azuma-Hoeffding Inequality]
  Let $\mathbb{F} = \{ \mathcal{F}_k \}_{k \leq n}$ be any filtration, and let $\Delta_k, A_k, B_k$ satisfy the following properties for $k = 1, \ldots, n$. 
  \begin{enumerate}
      \item Martingale Difference Property: $\Delta_k$ is $\mathcal{F}_k$-measurable and $\mathbb{E}[\Delta_k \mid \mathcal{F}_{k-1}] = 0$ 
      \item Predictable bounds: $A_k, B_k$ are $\mathcal{F}_{k-1}$-measurable and $A_k \leq \Delta_k \leq B_k$ a.s. 
  \end{enumerate}
  Then, $\sum_{k=1}^n \Delta_k$ is subgaussian with variance proxy $\frac{1}{4} \sum_{k=1}^n ||B_k - A_k||^2_\infty$. In particular, we obtain for every $t \geq 0$ the tail bound 
  \[\mathbb{P} \bigg( \sum_{k=1}^n \Delta_k \geq t \bigg) \leq \exp \bigg( - \frac{2t^2}{\sum_{k=1}^n ||B_k - A_k||_\infty^2} \bigg)\]
  \end{theorem}

  The Azuma-Hoeffding's inequality is often applied in the following setting. Let $X_1, \ldots, X_n$ be independent random variables s.t. $a \leq X_i \leq b$ for all $i$ (we can interpret $a$ and $b$ as simply constant random variables). Then, let $\Delta_k = (X_k - \mathbb{E}[X_k])/n$ be martingale differences, which we can show that $\Delta_k$ is clearly $\mathcal{F}_k$-measurable and that by independence of $X_i$'s,  $\mathbb{E}[\Delta_k \mid \mathcal{F}_{k-1}] = \mathbb{E}[\Delta_k] = 0$. Therefore, we can show that its sum satisfies
  \[\mathbb{P} \bigg( \frac{1}{n} \sum_{k=1}^n \{X_k - \mathbb{E}[X_k]\} \geq t \bigg) \leq e^{-2n t^2 / (b - a)^2}\]
  which is consistent with the central limit theorem. 

  Now we can return to the case of functions $f(X_1, \ldots, X_n)$ of independent random variables. Recall that the discrete derivative is defined 
  \[D_k f(x) = \sup_z f(x_1, \ldots, x_{k-1}, z, x_{k+1}, \ldots, x_n) - \inf_z f(x_1, \ldots, x_{k-1}, z, x_{k+1}, \ldots, x_n)\]

  \begin{theorem}[McDiarmid]
  For $X_1, \ldots, X_n$ independent, $f(X_1, \ldots, X_n)$ is subgaussian with variance proxy $\frac{1}{4} \sum_{k=1}^n ||D_k f||^2$. That is, 
  \[\mathbb{P}\big[ f(X_1, \ldots, X_n) - \mathbb{E}[f(X_1, \ldots, X_n)] \geq t \big] \leq \exp \bigg( -\frac{2 t^2}{\sum_{k=1}^n ||D_k f||^2_\infty} \bigg)\]
  \end{theorem}
  \begin{proof}
  We use the martingale method again to write 
  \[f(X_1, \ldots, X_n) - \mathbb{E}[f(X_1, \ldots, X_n)] = \sum_{k=1}^n \Delta_k\]
  where 
  \[\Delta_k = \mathbb{E}[f(X_1, \ldots, X_n) \mid X_1, \ldots, X_k] - \mathbb{E}[f(X_1, \ldots, X_n) \mid X_1, \ldots, X_{k-1}]\]
  What we want to do is set some upper and lower bound on $\mathbb{E}[f(X_1, \ldots, X_n) \mid X_1, \ldots, X_k]$, which will set bounds on $\Delta_k$. We can do this by bounding $f$ by the infimum and supremum w.r.t. each element, getting 
  \begin{align*}
      &\mathbb{E}[ \inf_z f(X_1, \ldots, X_{k-1}, z, X_{k+1}, \ldots, X_n) \mid X_1, \ldots, X_k] \\
      &\;\;\;\;\;\leq \mathbb{E}[f (X_1, \ldots, X_n) \mid X_1, \ldots, X_k] \\
      &\;\;\;\;\;\;\;\;\;\; \leq \mathbb{E}[ \sup_z f(X_1, \ldots, X_{k-1}, z, X_{k+1}, \ldots, X_n) \mid X_1, \ldots, X_k]
  \end{align*}
  but by independence of $X_k$'s, we have 
  \[\mathbb{E}[ \inf_z f(X_1, \ldots, z, \ldots, X_n) \mid X_1, \ldots, X_k] = \mathbb{E}[ \inf_z f(X_1, \ldots, X_{k-1}, z, X_{k+1}, \ldots, X_n) \mid X_1, \ldots, X_{k-1}]\]
  So, setting 
  \begin{align*}
      A_k & = \mathbb{E}[ \inf_z f(X_1, \ldots, X_{k-1}, z, X_{k+1}, \ldots, X_n) - f(X_1, \ldots, X_n) \mid X_1, \ldots, X_{k-1}] \\
      B_k & = \mathbb{E}[ \sup_z f(X_1, \ldots, X_{k-1}, z, X_{k+1}, \ldots, X_n) - f(X_1, \ldots, X_n) \mid X_1, \ldots, X_{k-1}]
  \end{align*}
  we have $A_k \leq \Delta_k \leq B_k$ for all $k$, and by Azuma-Hoeffding's inequality along with the fact that $||B_k - A_k|| \leq ||D_k f||_\infty$, we get 
  \[\mathbb{P}[ f(X_1, \ldots, X_n) - \mathbb{E}[ f(X_1, \ldots, X_n)] \geq t] \leq \exp \bigg(- \frac{2t^2}{\sum_{k=1}^n ||B_k - A_k||^2_\infty} \bigg) \leq \exp \bigg(- \frac{2t^2}{\sum_{k=1}^n ||D_k f||^2_\infty} \bigg)\]
  \end{proof}

  We should treat McDiarmid's inequality as a subgaussian form of the bounded difference inequality 
  \[\Var[ f(X_1, \ldots, X_n)] \leq \frac{1}{4} \mathbb{E} \bigg[ \sum_{k=1}^n \big(D_k f (X_1, \ldots, X_n)\big)^2 \bigg]\]
  The bounded difference inequality says that the variance is controlled by the expectation of the square gradient of the function $f$. In contrast, McDiarmid's inequality asserts the stronger subgaussian inequality, but under the stronger condition that the variance proxy is controlled by a uniform upper bound on the square gradient rather than its expectation. This will be a recurring theme: 
  \begin{enumerate}
      \item the expectation of the square gradient controls the variance 
      \item a uniform bound on the square gradient controls the subgaussian property
  \end{enumerate}
  Note that McDiarmid's theorem is not satisfactory. The appropriate notion of a square gradient in both inequalities is the random variable $\sum_{k=1}^n |D_k f|^2$. To control the variance, we want to take its expectation $\mathbb{E} [\sum_{k=1}^n |D_k f|^2]$, and to control the upper bound of the square gradient, we simply want to take its supremum $|| \sum_{k=1}^n |D_k f|^2||_\infty$. However, McDiarmid's inequality only yields control in terms of the larger quantity $\sum_{k=1}^n || D_k f||^2_\infty$ (by triangle inequality), which gets worse in higher dimensions. Rather than taking the supremum of square gradient, we just take the supremum of each (squared) component and add them up, which may be much greater than the actual upper bound. Therefore, the martingale method is far too crude to capture this idea, and we will need new techniques for more refined bounds. 

  \begin{exercise}[Bin Packing]
  For the Bin packing problem previoulsly, show that the variance bound $\Var[B_n] \leq n/4$ can be strengthened to a Gaussian tail bound 
  \[\mathbb{P}(|B_n - \mathbb{E} B_n| \geq t) \leq 2e^{-2t^2/n}\]
  \end{exercise}
  \begin{solution}
  We can see that 
  \[D_k f(X_1, \ldots, X_n) = f(X_1, \ldots, X_{k-1}, 1, X_{k+1}, \ldots, X_n) - f(X_1, \ldots, X_{k-1}, 1, X_{k+1}, \ldots, X_n) = 1\]
  and by McDiarmid's inequality, we are done. 
  \end{solution}

  \begin{exercise}[Rademacher Processes]

  \end{exercise}

  \begin{exercise}[Sums in Hilbert Space]
  Let $X_1, \ldots, X_n$ be independent random variables with zero mean that map to a Hilbert space, and suppose that $||X_k|| \leq C$ a.s. for every $k$. 
  \begin{enumerate}
      \item Show that for all $t \geq 0$, 
      \[\mathbb{P} \bigg[ \bigg| \bigg| \frac{1}{n} \sum_{k=1}^n X_k \bigg| \bigg| \geq \mathbb{E} \bigg| \bigg| \frac{1}{n} \sum_{k=1}^n X_k \bigg| \bigg| + t \bigg] \leq e^{-nt^2 / 2C^2} \]
      
      \item Show that 
      \[\mathbb{E} \bigg| \bigg| \frac{1}{n} \sum_{k=1}^n X_k \bigg| \bigg| \leq C n^{-1/2}\]
      
      \item Conclude that for all $t \geq C n^{-1/2}$, 
      \[\mathbb{P} \bigg[ \bigg| \bigg| \frac{1}{n} \sum_{k=1}^n X_k \bigg| \bigg| \geq t \bigg] \leq e^{-nt^2 / 8C^2}\]
      
      \item Finally, argue that for all $t \geq 0$, 
      \[\mathbb{P} \bigg[ \bigg| \bigg| \frac{1}{n} \sum_{k=1}^n X_k \bigg| \bigg| \geq t \bigg] \leq e^{-nt^2 / 8C^2}\]
  \end{enumerate}
  \end{exercise}

  \subsection{The Entropy Method}

  In order to develop more sophisticated concentration inequalities, let us introduce another term that is used to measure the deviation of a random variable. 

  \begin{definition}[Entropy]
  The \textbf{entropy} of a nonnegative random variable $Z$ is defined 
  \[\Ent[Z] \coloneqq \mathbb{E}[Z \log Z] - \mathbb{E}[Z] \log \mathbb{E}[Z]\]
  \end{definition}

  \begin{lemma}[Herbst]
  Suppose that random variable $X$ satisfies
  \[\Ent[e^{\lambda X}] \leq \frac{\lambda^2 \sigma^2}{2} \mathbb{E}[e^{\lambda X}]  \text{ for all } \lambda \geq 0\]
  Then, $X$ is $\sigma^2$-subgaussian. That is, 
  \[\psi(\lambda) \coloneqq \log\mathbb{E}[e^{\lambda (X - \mathbb{E}[X])}] \leq \frac{\lambda^2 \sigma^2}{2} \text{ for all } \lambda \geq 0\]
  \end{lemma}
  \begin{proof}
  As $\psi(\lambda) = \log\mathbb{E}[ e^{\lambda X}] - \lambda \mathbb{E}[X]$, we have 
  \[\frac{d}{d \lambda} \frac{\psi(\lambda)}{\lambda} = \frac{1}{\lambda} \frac{\mathbb{E}[X e^{\lambda X}]}{\mathbb{E}[e^{\lambda X}]} - \frac{1}{\lambda^2} \log \mathbb{E}[e^{\lambda X}] = \frac{1}{\lambda^2} \frac{\Ent [e^{\lambda X}]}{\mathbb{E}[e^{\lambda X}]} \leq \frac{\sigma^2}{2}\]
  where the last inequality yields from the assumption. By the fundamental theorem of calculus, we have 
  \[\frac{\psi (\lambda)}{\lambda} = \lim_{\lambda \downarrow 0} \frac{\psi(\lambda)}{\lambda} + \int_0^\lambda \frac{1}{t^2} \frac{\Ent[e^{t X}]}{\mathbb{E}[e^{t X}]} \,dt \leq \frac{\lambda \sigma^2}{2} \implies \psi(\lambda) \leq \frac{\lambda^2 \sigma^2}{2}\]
  \end{proof}

  \begin{exercise}
  It turns out that the converse is true up to a constant: If $X$ is $\frac{\sigma^2}{4}$-subgaussian, then 
  \[\Ent [e^{\lambda X}] \leq \frac{\lambda^2 \sigma^2}{2} \mathbb{E}[e^{\lambda X}]\]
  \end{exercise}
  \begin{solution}
  We know that by Jensen's inequality and concavity of the logarithm, 
  \[\log \mathbb{E}[e^{\lambda(X - \mathbb{E} X)}] \geq \mathbb{E}[\lambda (X - \mathbb{E} X)] = 0 \implies \mathbb{E}[e^{\lambda(X - \mathbb{E} X)}] \geq 1\]
  Furthermore, note that given $Z = e^{\lambda X} / \mathbb{E}[e^{\lambda X}]$, we have 
  \begin{align*}
      \mathbb{E}[Z \log{Z}] & = \mathbb{E} \bigg[ \frac{e^{\lambda X}}{\mathbb{E}[e^{\lambda X}]} \, \log \bigg( \frac{e^{\lambda X}}{\mathbb{E}[e^{\lambda X}]} \bigg) \bigg] \\
      & = \frac{1}{\mathbb{E}[e^{\lambda X}]} \mathbb{E}\big[ e^{\lambda X} \big( \log e^{\lambda X} - \log \mathbb{E}[e^{\lambda X}] \big) \big] \\
      & = \frac{1}{\mathbb{E}[e^{\lambda X}]} \mathbb{E} \big[ e^{\lambda X} \lambda X - e^{\lambda X} \log \mathbb{E}[e^{\lambda X}] \big] \\
      & = \frac{1}{\mathbb{E}[e^{\lambda X}]} \Big( \mathbb{E} [ e^{\lambda X} \lambda X ] - \mathbb{E}[ e^{\lambda X}] \, \log \mathbb{E}[e^{\lambda X}] \Big) \\
      & = \frac{\Ent [e^{\lambda X}]}{\mathbb{E}[e^[{\lambda X}]} 
  \end{align*}

  \end{solution}

  Since this theorem assumes a bound on $\Ent[e^{\lambda X}]$ rather than $\Ent[X]$, we will mainly be working with the entropy of exponentials of a random variable. 

  It turns out that entropy behaves very similarly to variance and extends nicely into the subgaussian setting. Just like variance, we define the partial entropy of function $f(x_1, \ldots, x_n)$ as 
  \[\Ent_k f (x_1, \ldots, x_n) \coloneqq \Ent[ f(x_1, \ldots, x_{k-1}, X_k , x_{k+1}, \ldots, x_n)]\]
  That is, $\Ent[f(X_1, \ldots, X_n)]$ is the entropy of $f(X_1, \ldots, X_n)$ with respect to the variable $X_k$ only, the remaining variables kept fixed. 

  \begin{theorem}[Tensorization of Entropy]
  Given that $X_1, \ldots, X_n$ are independent, 
  \[\Ent[ f(X_1, \ldots, X_n)] \leq \mathbb{E} \bigg[ \sum_{k=1}^n \Ent_k f (X_1, \ldots, X_n) \bigg]\]
  \end{theorem}

  Recall that the basic method for deriving Poincare inequalities is that we have some bound on the variance of a single random variable 
  \[\Var_\mu [g] \leq \mathbb{E}[|\nabla g|^2]\]
  and by tensorization, we can take the multivariate function $f$ and derive 
  \[\Var_\mu [f] \leq \mathbb{E}[ ||\nabla g||^2 ]\]
  In here, we derive modified log-Sobolev inequalities by bounding the entropy of the form 
  \[\Ent_\mu [e^g] \leq \mathbb{E}[ |\nabla g|^2 \, e^g ]\]
  and then using tensorization to bound 
  \[\Ent_\mu [e^{\lambda f}] \leq \mathbb{E} [ ||\nabla (\lambda f)||^2 \, e^{\lambda f} ]\]

  \begin{lemma}[Discrete Modified log-Sobolev]
  Let $D^- f \coloneqq f - \inf f$. Then, 
  \[\Ent[e^f] \leq \Cov[f, e^f] \leq \mathbb{E}[|D^- f|^2 e^f]\]
  \end{lemma}
  \begin{proof}
  Note that $\log \mathbb{E}[e^f] \geq \mathbb{E}[f]$ by Jensen's inequality. Therefore, 
  \[\Ent[e^f] = \mathbb{E}[f e^f] - \mathbb{E}[e^f] \, \log \mathbb{E}[e^f] \leq \mathbb{E}[f e^f] - \mathbb{E}[f] \mathbb{E}[e^f] = \Cov[f, e^f]\]
  To prove the second part, we have 
  \[\Cov[f, e^f] = \mathbb{E}[(f - \mathbb{E}[f]))(e^f - \mathbb{E}[e^f])] \leq \mathbb{E}[(f - \inf f)(e^f - e^{\inf f})] \]
  and since $e^x$ is convex, the first-order condition gives 
  \[e^{\inf f} \geq e^f + e^f (\inf f - f) \implies e^f - e^{\inf f} \leq e^f (f - \inf f)\]
  and substituting above gives the result. 
  \end{proof}

  Now, by defining the one-sided differences 
  \begin{align*}
      D_k^- f (x) & = f(x_1, \ldots, x_n) - \inf_z f (x_1, \ldots, x_{k-1}, z, x_{k+1}, \ldots, x_n) \\
      D_k^+ f (x) & = \sup_z f (x_1, \ldots, x_{k-1}, z, x_{k+1}, \ldots, x_n) - f(x_1, \ldots, x_n) 
  \end{align*}
  we can use the discrete modified log-Sobolev inequality on each of them and then tensorize to get the following. 

  \begin{theorem}[Bounded Difference Inequality]
  For all $t \geq 0$, 
  \begin{align*}
      \mathbb{P}[ f(X_1, \ldots, X_n) - \mathbb{E}[f(X_1, \ldots, X_n) \geq t] & \leq \exp \bigg( -\frac{t^2}{4 || \sum_{k=1}^n |D_k^- f|^2||_\infty} \bigg) \\
      \mathbb{P}[ f(X_1, \ldots, X_n) - \mathbb{E}[f(X_1, \ldots, X_n) \leq -t] & \leq \exp \bigg( -\frac{t^2}{4 || \sum_{k=1}^n |D_k^+ f|^2||_\infty} \bigg) 
  \end{align*}
  whenever $X_1, \ldots, X_n$ are independent. In particular, $f(X_1, \ldots, X_n)$ is subgaussian with variance proxy $2 ||\sum_{k=1}^n |D_k f|^2 ||_\infty$, where $D_k f = \sup_z f - \inf_z f$. 
  \end{theorem}

  \subsection{Modified log-Sobolev Inequalities}

  \begin{theorem}[Modified log-Sobolov Inequality]
  Let $P_t$ be a Markov semigroup with stationary measure $\mu$. The following are equivalent: 
  \begin{enumerate}
      \item $\Ent_\mu [f] \leq c \mathcal{E}(\log f, f)$ for all $f$ (modified log-Sobolev inequality). 
      \item $\Ent_\mu [P_t f] \leq e^{-t/c} \Ent_\mu [f]$ for all $f, t$ (entropic exponential ergodicity). 
  \end{enumerate}
  Moreover, if $\Ent_\mu [P_t f] \rightarrow 0$ as $t \rightarrow +\infty$, then 
  \[\mathcal{E}(\log P_t f, P_t f) \leq e^{-t/c} \mathcal{E}(\log f, f) \text{ for all } f, t\]
  implies $1$ and $2$ above. 
  \end{theorem}

\section{Lipschitz Concentration and Transportation Inequalities}

  \subsection{Concentration in Metric Spaces}

  Recall what a Lipschitz function is. 

  \begin{definition}[Lipschitz Function]
  Let $(X, d)$ be a matrix space. A function $f: X \rightarrow \mathbb{R}$ is called $L$-\textbf{Lipschitz} if $|f(x) - f(y)| \leq L \, d(x, y)$ for all $x, y \in X$. The family of all $1$-Lipschitz functions is denoted $\Lip(X)$. 
  \end{definition}

  Remember that given iid $X_1, \ldots, X_n \sim N(0, 1)$, Gaussian concentration states that the random variable is $|| ||\nabla f||^2 ||_\infty$-subgaussian. But we can write it in an equivalent way in terms of a Lipschitz property. 

  \begin{lemma}
  Let $f: \mathbb{R}^n \rightarrow \mathbb{R}$ be a $C^1$ function. Then, $|| ||\nabla f||^2 ||_\infty \leq L^2$ if and only if $f$ is $L$-lipschitz. 
  \end{lemma}

  Therefore, if given random vector $X \sim N(0, I)$, then $f(X)$ is $1$-subgaussian for every $f \in \Lip(\mathbb{R}^n, ||\cdot||)$. 


\end{document}

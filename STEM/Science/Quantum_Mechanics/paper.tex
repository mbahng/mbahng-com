\documentclass{article}

% packages
  \usepackage[letterpaper, top=1in, bottom=1in, left=1in, right=1in]{geometry}
  \usepackage[utf8]{inputenc}
  \usepackage[english]{babel}
  \usepackage{amsmath} 
  \usepackage{amssymb}
  % \usepackage{amsthm}
  \usepackage{tcolorbox}        % for better colored boxes in custom environments
  \tcbuselibrary{breakable}     % to allow tcolorboxes to break across pages
  \usepackage{float}            % for [H] figure placement
  \usepackage{fancyhdr}         % for headers and footers 
  \usepackage{centernot}        % for the centernot arrow 

  \usepackage{tikz-cd, bm, dcolumn, lastpage, pgfplots}
  \pgfplotsset{compat=1.18}
  \usetikzlibrary{arrows}
  \usepackage{mathrsfs, mathtools, hyperref, lastpage}
  \usepackage{graphicx, import, caption, subcaption}
  \usepackage{pdfpages, transparent}
  \usepackage{enumitem, tikz}
  \usepackage{fancyvrb,newverbs,xcolor}
  \usetikzlibrary{positioning}
  \usepackage[nottoc]{tocbibind}
  \definecolor{cverbbg}{gray}{0.93}
  \pdfsuppresswarningpagegroup=1

  \newenvironment{cverbatim}
    {\SaveVerbatim{cverb}}
    {\endSaveVerbatim
      \flushleft\fboxrule=0pt\fboxsep=.5em
      \colorbox{cverbbg}{%
        \makebox[\dimexpr\linewidth-2\fboxsep][l]{\BUseVerbatim{cverb}}%
      }
      \endflushleft
  }

  \newcommand{\incfig}[2][1]{%
    \def\svgwidth{#1\columnwidth}
    \import{./fig/}{#2.pdf_tex}
  }
  \renewcommand{\thispagestyle}[1]{}  % needed to include headers in title page

% New and replaced operators
  \DeclareMathOperator{\Tr}{Tr}
  \DeclareMathOperator{\Sym}{Sym}
  \DeclareMathOperator{\Span}{span}
  \DeclareMathOperator{\std}{std}
  \DeclareMathOperator{\Cov}{Cov}
  \DeclareMathOperator{\Var}{Var}
  \DeclareMathOperator{\Corr}{Corr}
  \DeclareMathOperator{\pos}{pos}
  \DeclareMathOperator*{\argmin}{\arg\!\min}
  \DeclareMathOperator*{\argmax}{\arg\!\max}
  \newcommand{\ket}[1]{\ensuremath{\left|#1\right\rangle}}
  \newcommand{\bra}[1]{\ensuremath{\left\langle#1\right|}}
  \newcommand{\braket}[2]{\langle #1 | #2 \rangle}
  \newcommand{\qed}{\hfill$\blacksquare$}     % I like QED squares to be black

% Custom Environments
  \newtcolorbox[auto counter, number within=section]{question}[1][]
  {
    colframe = orange!25,
    colback  = orange!10,
    coltitle = orange!20!black,  
    breakable, 
    title = \textbf{Question \thetcbcounter ~(#1)}
  }

  \newtcolorbox[auto counter, number within=section]{exercise}[1][]
  {
    colframe = teal!25,
    colback  = teal!10,
    coltitle = teal!20!black,  
    breakable, 
    title = \textbf{Exercise \thetcbcounter ~(#1)}
  }
  \newtcolorbox[auto counter, number within=section]{solution}[1][]
  {
    colframe = violet!25,
    colback  = violet!10,
    coltitle = violet!20!black,  
    breakable, 
    title = \textbf{Solution \thetcbcounter}
  }
  \newtcolorbox[auto counter, number within=section]{lemma}[1][]
  {
    colframe = red!25,
    colback  = red!10,
    coltitle = red!20!black,  
    breakable, 
    title = \textbf{Lemma \thetcbcounter ~(#1)}
  }
  \newtcolorbox[auto counter, number within=section]{theorem}[1][]
  {
    colframe = red!25,
    colback  = red!10,
    coltitle = red!20!black,  
    breakable, 
    title = \textbf{Theorem \thetcbcounter ~(#1)}
  } 
  \newtcolorbox[auto counter, number within=section]{proof}[1][]
  {
    colframe = orange!25,
    colback  = orange!10,
    coltitle = orange!20!black,  
    breakable, 
    title = \textbf{Proof. }
  } 
  \newtcolorbox[auto counter, number within=section]{definition}[1][]
  {
    colframe = yellow!25,
    colback  = yellow!10,
    coltitle = yellow!20!black,  
    breakable, 
    title = \textbf{Definition \thetcbcounter ~(#1)}
  } 
  \newtcolorbox[auto counter, number within=section]{example}[1][]
  {
    colframe = blue!25,
    colback  = blue!10,
    coltitle = blue!20!black,  
    breakable, 
    title = \textbf{Example \thetcbcounter ~(#1)}
  } 
  \newtcolorbox[auto counter, number within=section]{code}[1][]
  {
    colframe = green!25,
    colback  = green!10,
    coltitle = green!20!black,  
    breakable, 
    title = \textbf{Code \thetcbcounter ~(#1)}
  } 

% Page style
  \pagestyle{fancy}
  \fancyhead[L]{Quantum Mechanics}
  \fancyhead[C]{Muchang Bahng}
  \fancyhead[R]{Spring 2024} 
  \fancyfoot[C]{\thepage / \pageref{LastPage}}
  \renewcommand{\footrulewidth}{0.4pt}          % the footer line should be 0.4pt wide


\begin{document}
\title{Quantum Mechanics}
\author{Muchang Bahng}
\date{Spring 2024}

\maketitle
\tableofcontents
\pagebreak

\section{Systems} 

  Imagine a particle of mass $m$ constrained to move along the x-axis, subject to some applied force $F(x, t)$ (time-dependent). In classical mechanics, we want to determine the position of the particle at any given time $t$ by finding the function $x(t)$. How do we do this? We simply apply \textit{Newton's second law}. 
  \begin{equation} 
    \mathbf{F} = m \mathbf{a}
  \end{equation}
  and solve the ordinary differential equation (with some initial conditions), either analytically or numerically. Once we have $x(t)$, we can find other metrics of interest, such as the velocity $v(t)$, kinetic energy $T = \frac{1}{2} mv^2$, or others. For conservative systems (the only kind we'll consider, and fortunately the only kind that exists at the microscopic level), the force can be expressed as the gradient of a potential energy function $U(\mathbf{x})$. 

  \subsection{Schr\"odingers Equation}

    Quantum mechanics approaches the same problem differently. Rather than looking for position function $\mathbf{x}(t)$, we are looking for a \textbf{wave function} $\boldsymbol{\Psi}(\mathbf{x}, t)$ of the particle, which we can get by solving the \textit{Schr\"odinger equation}. 

    \begin{definition}[Schr\"odinger Equation]
      The \textbf{Schr\"odinger equation} is defined 
      \begin{equation} 
        i \hbar \frac{\partial \boldsymbol{\Psi}}{\partial t} = - \frac{\hbar^2}{2m} \frac{\partial^2 \boldsymbol{\Psi}}{\partial \mathbf{x}^2} + V \boldsymbol{\Psi}
      \end{equation}
      where $\hbar$ is the \textbf{reduced Planck's constant}, defined 
      \begin{equation} 
        \hbar = \frac{h}{2\pi} = 1.054573 \times 10^{-34} \mathrm{J}s
      \end{equation}
    \end{definition}

    Therefore, given suitable initial conditions $\boldsymbol{\Psi}(\mathbf{x}, 0)$, the Schr\"odinger equation determines $\boldsymbol{\Psi}(\mathbf{x}, t)$ for all future time. Now let's talk about this wave function and its physical interpretation, starting with \textbf{Born's statistical interpretation}. This says that the wavefunction $\boldsymbol{\Psi}(\mathbf{x}, t)$ determines the probability of finding th particle at point $\mathbf{x}$ at time $t$. That is, the probability density function of the particle's position at time $t$ is given by 
    \begin{equation} 
      f_{X, t} (\mathbf{x}, t) = |\boldsymbol{\Psi}(\mathbf{x}, t)|^2
    \end{equation}
    Unfortunately, Schr\"odinger's equation is a linear system, so the set of solutions to this equation forms a vector space. So if $\boldsymbol{\Psi}$ is a solution, then $c \boldsymbol{\Psi}$ is also a solution for all $c \in \mathbb{C}$. This is a problem since it must be normalized. This is why we have an extra condition that 
    \begin{equation} 
    \int |\boldsymbol{\Psi}(\mathbf{x}, t)|^2 \,d \mathbf{x} = 1
    \end{equation}
    which means that the probability of finding a particle somewhere in the space $X$ at a certain point $t$ must integrate to $1$. This gives us a normalization condition, and any functions that have an integral of infinity is not within our search space. Therefore, we're really just trying to find a function in the $L^2$ space of integrable functions. 

    \begin{definition}[Superposition]
      Now the solutions of Schrodinger's equation, as a linear PDE, forms a vector space,and so given two solutions $\psi_1, \psi_2$, any linear combination of them (after normalization), known as a superposition, is also a solution.
    \end{definition}

    There's two things that the reader may realize: First, it seems that multiple wavefunctions $\Psi$ may induce the same probability measure. It turns out that within this set of normalized wavefunctions, there is an equivalence class denoted by $\Psi \sim e^{i \theta} \Psi \text{ for all } \theta \in \mathbb{R}$, which means that $|e^{i \theta} \Psi|^2 = (e^{i \theta} \Psi)^\ast (e^{i \theta} \Psi) = \Psi^\ast e^{-i \theta} e^{i \theta} \Psi = |\Psi|^2$. This is a consequence of the fact that the probability of finding a particle at a certain point is not affected by the phase of the wavefunction.

    \begin{definition}[Global Phase Factor]
      Two wavefunctions are said to differ by a \textbf{global phase factor} iff they differ by a normalized complex scalar $e^{i \theta}$.  
    \end{definition}

    For now, we can think of two wavefunctions that differ by a global phase factor as being the same. The important distinction is the relative phase factor, which is the difference between two wavefunctions at a certain point. 

    \begin{definition}[Relative Phase]
      Two wavefunctions are said to differ by a \textbf{relative phase} if they differ by a normalized complex scalar $e^{i \theta(x)}$ that is dependent on the position $x$.
    \end{definition}

    Second, this normalization restriction may not be preserved in Shrodinger's equation. Fortunately for us, Schr\"odinger's equation keeps this normalization condition as time passes. Let's prove this. 

    \begin{theorem} 
      Given a solution $\boldsymbol{\Psi}$ that has been normalized, the function will stay normalized as time passes. 
    \end{theorem}
    \begin{proof} 
      Let us take the time-derivative of the total probability and show that is is $0$. We show that 
      \begin{equation} 
        \frac{d}{dt}  \int |\boldsymbol{\Psi} (\mathbf{x}, t)|^2 \,dx = \int \frac{\partial}{\partial t} |\boldsymbol{\Psi}(\mathbf{x}, t)|^2 \,dx
      \end{equation}
      and by the product rule we have 
      \begin{equation} 
        \frac{\partial}{\partial t} |\boldsymbol{\Psi}|^2 = \frac{\partial}{\partial t} (\boldsymbol{\Psi}^\ast \boldsymbol{\Psi}) = \boldsymbol{\Psi}^\ast \frac{\partial \boldsymbol{\Psi}}{\partial t} + \frac{\partial \boldsymbol{\Psi}^\ast}{\partial t} \boldsymbol{\Psi}
      \end{equation}
      We can take the complex conjugate of the Schr\"odinger equation to get 
      \begin{align} 
        \frac{\partial \boldsymbol{\Psi}}{\partial t} & = \frac{i \hbar}{2m} \frac{\partial^2 \boldsymbol{\Psi}}{\partial x^2} - \frac{i}{\hbar} V \boldsymbol{\Psi} \\
        \frac{\partial \boldsymbol{\Psi}^\ast}{\partial t} & = - \frac{i \hbar}{2m} \frac{\partial^2 \boldsymbol{\Psi}^\ast}{\partial x^2} + \frac{i}{\hbar} V \boldsymbol{\Psi}^\ast 
      \end{align}
      and substituting both equations into the product rule gives 
      \begin{equation} 
        \frac{\partial}{\partial t} |\boldsymbol{\Psi}|^2 = \frac{i \hbar}{2m} \bigg( \boldsymbol{\Psi}^\ast \frac{\partial^2 \boldsymbol{\Psi}}{\partial x^2} - \frac{\partial^2 \boldsymbol{\Psi}^\ast}{\partial x^2} \boldsymbol{\Psi} \bigg) = \frac{\partial}{\partial x} \bigg[ \frac{i \hbar}{2m} \bigg( \boldsymbol{\Psi}^\ast \frac{\partial \boldsymbol{\Psi}}{\partial x} - \frac{\partial \boldsymbol{\Psi}^\ast}{\partial x} \boldsymbol{\Psi} \bigg)\bigg]
      \end{equation}
      and now we can evaluate the integral to be 
      \begin{equation} 
        \frac{d}{dt} \int |\boldsymbol{\Psi}(x, t)|^2 \,dx = \frac{i \hbar}{2m} \big( \boldsymbol{\Psi}^\ast \frac{\partial \boldsymbol{\Psi}}{\partial x} - \frac{\partial \boldsymbol{\Psi}^\ast}{\partial x} \boldsymbol{\Psi} \bigg) \bigg|_{-\infty}^{+\infty} 
      \end{equation}
      which evaluates to $0$ since $\boldsymbol{\Psi}$ must go to $0$ as it is a probability density. 
    \end{proof}

    Now that we have settled on what a wavefunction is, we can now introduce the braket notation. 

    \subsection{Ket Notation} 

      We can talk about another paradigm of representing the states of a system: as a ket. We can equivalently say that the state of a system is represented by a normalized vector $\ket{\psi}$ in the Hilbert space $L^2$. Now it seems that both $\psi$ and $\ket{\psi}$ both live in $L^2$, so what is the difference between them? This is something that is not constructed very rigorously in introductory quantum mechanics courses, so let's try to settle this by constructing $\ket{\psi}$ from $\psi$. 

      Note that we have two spaces here: the function space $L^2$ and the state space $X$. In 1939, Dirac wanted to unify these 2 spaces in order to more easily work with them. He does this by first constructing some Hilbert space $\mathcal{H}$ and identifying two mappings $x \in X \mapsto \ket{x} \in \mathcal{H}$ and $\psi \in L^2 \mapsto \ket{\psi} \in \mathcal{H}$. The goal was to be able to evaluate $\psi(x)$ with an inner product $\braket{x}{\psi}$ in $\mathcal{H}$. Let's talk about each construction separately: 
      \begin{enumerate} 
        \item To do this, note that $\psi$ can be reduced to an uncountable sum of $\delta$-functions. 
          \begin{equation} 
            \psi(x) = \int_{x \in X} \psi(x^\prime) \delta_{x^\prime} (x) \,dx^\prime 
          \end{equation}
        This is analogous to an uncountable sum of basis vector functions $\delta_{x^\prime}$ and their coefficients $\psi(x^\prime)$. Each $\delta_{x^\prime}$ can be though of as an uncountably long vector 
        \begin{equation} 
          \ket{\delta_{x^\prime}} = (\ldots, 0, \ldots, 0, 1, 0, \ldots, 0, \ldots)
        \end{equation}
        where the $1$ is in the $x^\prime$ index, and each function is a vector 
        \begin{equation} 
          \ket{\psi} = (\ldots \psi(0) \ldots \psi(\pi) \ldots) = \int_{x \in X} \psi(x) \ket{\delta_x} \,dx 
        \end{equation}
        Therefore, we have defined the mapping $\psi \mapsto \ket{\psi}$. 

      \item To define $x \mapsto \ket{x}$, we can also analogously say that each $x \in X$ can also be represented as 
        \begin{equation} 
          \ket{x} = (\ldots, 0, \ldots, 0, 1, 0, \ldots, 0, \ldots) = \ket{\delta_x}
        \end{equation}
        like an uncountable version of one-hot encoding (each real number gets mapped to its own dimension). So, 
        \begin{equation} 
          \ket{\psi} = \int_{x \in X} \psi(x) \, \ket{x} \,dx 
        \end{equation}
      \end{enumerate}

      Therefore, we can represent 
      \begin{equation} 
        \braket{x}{\psi} = \braket{\delta_x}{\psi} = \int_{x^\prime \in X} \psi(x^\prime) \, \delta_{x^\prime}(x) \,dx^\prime = \psi(x) 
      \end{equation}

      We can verify that this is bilinear since
      \begin{equation} 
        (\psi + \phi)(x) = \braket{\delta_x}{\psi + \phi} = \braket{\delta_x}{\psi} + \braket{\delta_x}{\phi} = \psi(x) + \psi(x)
      \end{equation}
      and 
      \begin{equation} 
        \braket{\delta_x + \delta_y}{\psi} = \braket{\delta_x}{\psi} + \braket{\delta_y}{\psi} = \psi(x) + \psi(y)
      \end{equation}
      Note that in here $\delta_x + \delta_y \neq \delta_{x + y}$ since we are not talking addition in the reals. Rather, 
      \begin{equation} 
        (\delta_x + \delta_y)(q) = \begin{cases} 1 & \text{ if } x = q \text{ or } y = q \\ 0 & \text{ if else} \end{cases}
      \end{equation}

    \subsection{Evolution} 


    \subsection{Composite Systems}


\section{Measurements}

  When we have a (classical or quantum mechanical) system, there is a configuration space $M$ that describes the complete state of the system. In classical mechanics, this is usually the space $\mathcal{M} = \mathcal{X} \times \mathcal{P}$ where $\mathcal{X}$ is the position and $\mathcal{P}$ is the momentum. In quantum mechanics, $\mathcal{M} = L^2(\mathcal{X})$ is the space of wavefunctions defined over $\mathcal{X}$. 

  \begin{definition}[Observable in Classical Mechanics]
    In classical mechanics, an observable is a function $Q: \mathcal{M} \rightarrow \mathbb{R}$ of some dynamical quantity that you want to measure from the system.  
  \end{definition}

  \begin{example} 
    Some examples are 
    \begin{enumerate} 
      \item The position of the vector is simply the $x$ value of the tuple. 
      \item The kinetic energy is $K(x, p, t) = \frac{1}{2m} p^2$. 
      \item The potential energy is $U(x, p, t) = U(x)$, which is dependent only on $x$ for conservative systems. 
      \item The Hamiltonian is defined as the total (kinetic plus potential) energy, defined $H(x, p, t) = \frac{1}{2m} p^2 + V(x)$. 
    \end{enumerate}
  \end{example}

  In quantum mechanics, an observable is similar as in you want to define the specific type of quantity you want to measure, but now this is probabilistic: we now have a random variable $Q$. Broadly speaking, if we measure this observable, then it will cause $Q$ to realize into some real number $q$. 

  \begin{definition}[Observable in Quantum Mechanics]
    However, there is a slight catch: $\mathcal{X}$ must be some. A \textbf{quantum mechanical observable} $Q$ is a real valued random variable, and its \textbf{associated Hamiltonian} (often called the observable) is a Hermitian operator $\hat{Q}: L^2 (\mathcal{X}) \rightarrow L^2 (\mathcal{X})$.  
  \end{definition}

  Two things to note: First, the output of the operator doesn't necessarily have to be a wavefunction (doesn't need to be normalized, look at the position operator), but it should be normalizable. 

  Second, the Hermitian operator $\hat{H}$ is relevant since it turns out that its set of eigenvalues $\mathcal{E} = \{E\}$ of $\hat{Q}$ determines the support of $Q$, and the set of eigenvectors $\{\psi_E\}$ will determine the probability distribution of $Q$, which will be elaborated soon. Note that the Hermitian constraint happens because the outcomes must be real, since they are physical measurements that must be compared. Another technicality is that not all Hermitian operators are proper observables in the infinite dimensional case, but this is a technicality and can be ignored for now.  

  \begin{example} 
    Some examples of quantum mechanical observables can be obtained by the canonical substitution 
    \begin{equation} 
      p \rightarrow \frac{\hbar}{i} \frac{\partial}{\partial x} 
    \end{equation}
    from the classical observables. 
      \begin{enumerate} 
        \item Position: $\hat{Q}: \psi \rightarrow x \psi$ 
        \item Momentum: $\hat{Q}: \psi \rightarrow \frac{\partial}{\partial x} \psi $ 
        \item Kinetic energy: $\hat{Q}: \psi \rightarrow \frac{1}{2m} \frac{\hbar^2}{-1} \frac{\partial^2}{\partial x^2} \psi$

        \item Hamiltonian, or total energy, denoted with a special character $\hat{H} : \psi \mapsto \frac{\hbar^2}{2m} \frac{\partial^2}{\partial x^2} + V$. 
      \end{enumerate} 
  \end{example}

  Now an observable simply describes the type of quantity that you want to measure. A \textit{measurement}, on the other hand, is an action on the system, as if we would measure with a ruler or an instrument, and therefore is different from an observable. Once you do a measurement, the random variable $Q$ realizes onto a certain number $q \in \mathbb{R}$, and the state $\psi$ of the system is affected by a collapse of the wavefunction $\psi$ into the Delta function centered at $q$. Therefore, there are two evolutionary processes: the unitary evolution of $\psi$ according to Shrodinger's equation and measurement operators. 

  \begin{definition}[Measurement]
    A \textbf{measurement} of a system in state $\psi$, with respect to observable $Q$, is defined by two things. 
    \begin{enumerate} 
      \item First, a measurement will cause a realization of $Q$ into a certain real quantity $E \in \mathcal{E}$. The probability distribution of $Q$ is determined by taking the state $\psi$, expanding it into a linear combination of the eigenvectors 
      \begin{equation} 
        \psi = \int_{E \in \mathcal{E}}  \alpha_E \psi_E \,dE
      \end{equation}
      and finally taking $|\alpha_E|^2$ to be the probability (PMF for countable $\mathcal{E}$ and PDF for uncountable).  

      \item While $q$ is realized, the wavefunction $\psi$ will collapse into $\delta_q$. 
    \end{enumerate}
  \end{definition}

  Now let's put these things all together in one formal process. 
  \begin{enumerate} 
    \item You have a system in a state $\Psi$ and want to measure it in some way. But measuring something does not make sense without knowing what quantity you want to measure, so you choose an observable $\hat{Q}$, a Hermitian operator that corresponds to the value that you want to measure. 

    \item As soon as you choose this operator, by the spectral theorem it has some set of eigenvalues $\mathcal{E} \subset \mathbb{R}$ and corresponding orthonormal eigenfunctions $\{\psi_E\}_{E \in \mathcal{E}}$, which may be finite, countable, or uncountable. It turns out that the measurement outcome \textit{must} be one of these eigenvalues, and it is random, so you're essentially drawing from this set an $E$ with some random variable $Q$. This makes $\mathcal{E}$ a probability space with some measure $\mu$.

    \item To determine the actual probabilities of landing on each eigenvalue, you must first take the state $\Psi$ and expand it out into the eigenbasis. 
      \begin{equation} 
        \Psi = \sum_{E \in \mathcal{E}} \alpha_E \psi_E \text{ or } \Psi = \int_{E \in \mathcal{E}} \alpha_E \psi_E \,dE 
      \end{equation}

    and the amplitude squared of the coefficients gives you the respective probabilities. 

    \item Once you measure and get result $E$ at time $t$, the wavefunction  $\psi(t)$ collapses onto the delta at $E$ (similar to a realization of a random variable) and immediately begins to ``smear'' out again. If you measure really soon after, you can still sample $\psi(t + \delta t)$ with approximately probability $1$ at the point you just measured, since the state is so concentrated there and therefore has a very high coefficient corresponding to $\psi_E$ when you expand it out. 
  \end{enumerate}

  \begin{example}[Measurement of the Position Observable]
    We have said that given a wavefunction $\Psi (t)$, the probability distribution of the particle's position is determined by the PDF at time $t$ is $p_t (x) = |\Psi(x, t)|^2$. This is consistent with what we have described here if we talk about the position observable. Say that I have a wavefunction $\Psi$. I want to measure position with the observable $\hat{Q} : \psi \mapsto x \psi$. We first have to find the eigendecomposition of $\hat{Q}$. It turns out that the set of eigenfunctions is the set of all Dirac delta functions indexed by $\mathbb{R}$. 
    \begin{equation} 
      \delta_k (x) = \begin{cases} 1 & x = k \\ 0 & x \neq k \end{cases} 
    \end{equation}
    and we can see that every $k$ is an eignvalue with eigenfunction $\delta_k$ since 
    \begin{equation} 
      (\hat{Q} \delta_k)(x) = (k \delta_k) (x) = \begin{cases} k & x = k \\ 0 & x \neq k \end{cases} = k \delta_k 
    \end{equation}
    So the eigendecomposition is 
    \begin{equation} 
      \hat{Q} = \sum_{x \in X} k \delta_k \otimes \delta_k^\ast \text{ or } \hat{Q} = \int_{k \in \mathbb{R}} k \delta_k \otimes \delta_k^\ast \,dx 
    \end{equation}
    and so $\psi$ can be decomposed as the uncountable linear combination of the eigenfunctions 
    \begin{align*} 
      \psi & = \sum_x \alpha_x \delta_x \implies \mathbb{P}(Q = x) = |\alpha_x|^2  = | \psi(x) |^2\\  
      \psi & = \int_{x \in \mathbb{R}} \alpha_x \delta_x \,dx \implies p_Q (x) = |\alpha_x|^2 = |\psi(x)|^2 
    \end{align*}
  \end{example}   

  \begin{example}[Measurement of the Momentum Observable]
    Say that I have a wavefunction $\psi$. I want to measure momentum with the observable $Q$ and corresponding operator $\hat{Q}: \psi \rightarrow \frac{\hbar}{i} \frac{\partial}{\partial x} \psi$. We can verify that the spectrum consists of the eigenvalue/eigenfunction pair 
    \begin{equation} 
      \bigg\{ p, \psi_p (x) = \frac{1}{\sqrt{2 \pi \hbar}} e^{i p x /\hbar} \bigg\}_{p \in \mathbb{R}}
    \end{equation}
    and so 
    \begin{equation} 
      \hat{Q} = \int_{p \in \mathbb{R}} p \psi_p \otimes \psi_p^\ast \,dp 
    \end{equation}
    and so we can decompose 
    \begin{equation} 
      \psi = \int_{x \in \mathbb{R}} \alpha_x \psi_x \,dx \implies p_Q (x) = |\braket{\psi_x}{\psi}|^2 
    \end{equation}
  \end{example}

  Some properties of the these measurements are as follows. 

  \begin{exercise}
    Compute the commutator of the position and momentum observables (we will need this for the uncertainty principle).  What does the result mean? 
  \end{exercise}

  \subsection{Eigenvalues are just Symbols}

    Note that the probability measure of the measurement outcome is not dependent on the actual value of the eigenvalues themselves. We can think of them as some sort of symbol that we use to index the eigenfunctions. This is because the probability measure is determined by the coefficients of the expansion of the wavefunction into the eigenbasis, and not the eigenvalues themselves. Sometimes, there are natural quantities where it may be easy to assign the eigenvalues. The position operator in $\mathbb{R}^n$ gives a natural way since it has a natural ordering, but the following example does not.

    \begin{example}[Position Measurement of Qubit]
      Given a qubit in state $\ket{\psi} = \alpha \ket{0} + \beta \ket{1}$, let us measure it with two observables. 
      \begin{enumerate} 
        \item The observable 
          \begin{equation} 
            \hat{Q} = 1 \cdot \ket{0}\bra{0} + 0 \cdot \ket{1}\bra{1}
          \end{equation}
          gives $\mathbb{P}(Q = 1) = |\alpha|^2$ and $\mathbb{P}(Q = 0) = |\beta|^2$. 

        \item The observable 
          \begin{equation} 
            \hat{Q} = 1 \cdot \ket{0}\bra{0} + (-1) \cdot \ket{1}\bra{1}
          \end{equation}
          gives $\mathbb{P}(Q = 1) = |\alpha|^2$ and $\mathbb{P}(Q = -1) = |\beta|^2$. 
      \end{enumerate}
      Either way, these eigenvalues are just symbols that we use to index the eigenfunctions, and it does not matter whether we use $0$ or $-1$ to represent the collapse to the $\ket{1}$ state. The state of the system after measurement is also not affected, since it's only dependent on the eigenvector, not the value. 
    \end{example}

    Another observable is the spin, which was first found by the Stern-Gerlach experiment. 

  \subsection{Basis Orthogonality}

    Now let's talk a little about practical methodology. It turns out that in practice, preparing states of a quantum system is quite hard, and in other cases we may not even know what the state can be. This can be formulated in the language of density operators, but for now, let's talk about the problem of distinguishing quantum states.  We start off with an exercise. 

    \begin{exercise}[Neilson 2.58]
      If the state vector $\ket{\psi}$ is an eigenvector of the observable operator $\hat{Q}$ with eigenvalue, say $E$, then what can we say about the measurement outcome? Furthermore, what is the average observed value of $M$, and the standard deviation? 
    \end{exercise}
    \begin{solution} 
      The measurement outcome will always be $E$ with probability $1$. 
    \end{solution}

    Say that we are given some state that is one of $\{\ket{\psi}_i\}_{i=1}^n$ and we must construct some way to find which state it comes from. We would like to find some measurement operator $\hat{Q}$, measure the system, and deduce which state it came from based on what value $Q$ realized on. Can we do this perfectly? It turns out that we can under certain conditions. 

    \begin{theorem}[Distinguishing Quantum States]
      Given a set of states $\{\ket{\psi}_i\}_{i=1}^n$, there exists a measurement operator that can distinguish them perfectly if and only if they are orthogonal. If they are not orthogonal, then there exists not measurement operator. 
    \end{theorem}
    \begin{proof} 
      Assume that the states are orthogonal. Then, we can construct the measurement operator 
      \begin{equation} 
        \hat{Q} = \sum_{i=1}^n \lambda_i \ket{\psi_i}\bra{\psi_i}
      \end{equation}
      and so if $Q$ realizes onto $\lambda_i$, then we know that the state was $\ket{\psi_i}$ with probability $1$. 
    \end{proof}
    If they are not orthogonal, then we can't construct an orthonormal basis and cannot construct a measurement operator. Generally, if we have two non-orthogonal states $\ket{\psi}$ and $\ket{\phi}$, say that we want to construct $\hat{Q}$ with just two eigenvectors $\ket{u}, \ket{v}$ (the rest are irrelevant). 
    \begin{enumerate} 
      \item If none of its eigenvectors contain either $\ket{\psi}$ or $\ket{\phi}$. Then 
        \begin{align} 
           \mathbb{P}(\ket{u} \text{ after} \mid \ket{\psi} \text{ before}) & = |\braket{u}{\psi}|^2 > 0 \\
           \mathbb{P}(\ket{v} \text{ after} \mid \ket{\psi} \text{ before}) & = |\braket{v}{\psi}|^2 > 0 \\ 
           \mathbb{P}(\ket{u} \text{ after} \mid \ket{\phi} \text{ before}) & = |\braket{u}{\phi}|^2 > 0 \\ 
           \mathbb{P}(\ket{v} \text{ after} \mid \ket{\phi} \text{ before}) & = |\braket{v}{\phi}|^2 > 0 
        \end{align}
        and so no matter whatever eigenvector that the system realizes onto, there is a nonzero probability of it coming from either. So from using Bayes rule, we can't deduce anything. 

      \item Therefore, we must choose the eigenvectors to be the states themselves. Since they are not orthogonal, we can do it for one state, and let the eigenvectors be $\ket{\psi}, \ket{\psi^\perp}$. 
        \begin{align} 
          \mathbb{P}(\ket{\psi} \text{ after} \mid \ket{\psi} \text{ before}) & = 1 \\
          \mathbb{P}(\ket{\psi^\perp} \text{ after} \mid \ket{\psi} \text{ before}) & = 0 \\
          \mathbb{P}(\ket{\psi} \text{ after} \mid \ket{\phi} \text{ before}) & = |\braket{\psi}{\phi}|^2 \\
          \mathbb{P}(\ket{\psi^\perp} \text{ after} \mid \ket{\phi} \text{ before}) & = 1 - |\braket{\psi}{\phi}|^2
        \end{align}
        This is slightly better than before, since now when the system realizes onto $\ket{\psi^\perp}$, we can actually deduce where it came from. Using Bayes rule, 
        \begin{align} 
          \mathbb{P}(\ket{\phi} \text{ bef} \mid \ket{\psi^\perp} \text{ aft}) 
          & =  \frac{\mathbb{P}(\ket{\psi^\perp} \text{ aft} \mid \ket{\phi} \text{ bef}) \mathbb{P}(\ket{\phi} \text{ bef})}{\mathbb{P}(\ket{\psi^\perp} \text{ aft})} \\ 
          & =  \frac{\mathbb{P}(\ket{\psi^\perp} \text{ aft} \mid \ket{\phi} \text{ bef}) \mathbb{P}(\ket{\phi} \text{ bef})}{\mathbb{P}(\ket{\psi^\perp} \text{ aft} \mid \ket{\phi} \text{ bef}) \mathbb{P}(\ket{\phi} \text{ bef}) + \mathbb{P}(\ket{\psi^\perp} \text{ aft} \mid \ket{\psi} \text{ bef}) \mathbb{P}(\ket{\psi} \text{ bef})} \\
          & = \frac{(1 - |\braket{\psi}{\phi}|^2) \mathbb{P}(\ket{\phi} \text{ bef})}{(1 - |\braket{\psi}{\phi}|^2) \mathbb{P}(\ket{\phi} \text{ bef}) + 0 \cdot \mathbb{P}(\ket{\psi} \text{ bef})} = 1
        \end{align}
        but the problem comes from when the state is initialized at $\ket{\phi}$. Then, we have 
        \begin{align} 
          \mathbb{P}(\ket{\psi} \text{ before} \mid \ket{\psi} \text{ after}) 
          & = \frac{\mathbb{P}(\ket{\psi} \text{ before}, \ket{\psi} \text{ after})}{\mathbb{P}(\ket{\psi} \text{ after})} \\ 
          & = \frac{\mathbb{P}(\ket{\psi} \text{ aft} \mid \ket{\psi} \text{ bef}) \, \mathbb{P}(\ket{\psi} \text{ bef})}{\mathbb{P}(\ket{\psi} \text{ aft} \mid \ket{\psi} \text{ bef}) \mathbb{P}(\ket{\psi} \text{ bef})+ \mathbb{P}(\ket{\psi} \text{ aft} \mid \ket{\phi} \text{ bef}) \mathbb{P}(\ket{\phi} \text{ bef})} \\
          & = \frac{\mathbb{P}(\ket{\psi} \text{ bef})}{\mathbb{P}(\psi \text{ bef}) + |\braket{\psi}{\phi}|^2 \mathbb{P}(\phi \text{ bef})} 
        \end{align}
        which is neither $0$ nor $1$, given that our prior distributions are non-degenerate. 
    \end{enumerate}
    Therefore, there is no perfect way to distinguish non-orthogonal states. 

  \subsection{Expectation and Variance of Observables}

    Sometimes, we are interested in specific statistical properties of these distributions of observables. In this case, we can define the expectation and variance of the operators. 

    \begin{definition}[Expectation of Operator]
      The expectation value of the operator (as a function of time) is
      \begin{equation} 
        \langle Q \rangle (t) \coloneqq \int \Psi (x, t)^\ast (\hat{Q} \Psi) (x, t) \,dx = \braket{\Psi}{\hat{Q} \Psi}(t)
      \end{equation}
    \end{definition}

    Since physical observables must be real (to see why, look \href{https://physics.stackexchange.com/questions/436462/why-is-there-a-physical-preference-to-real-numbers}{here}), it must be the case that 
    \begin{equation} 
      \braket{\Psi}{\hat{Q} \Psi} = \braket{\Psi}{\hat{Q} \Psi}^\ast = \braket{\hat{Q} \Psi}{\Psi}
    \end{equation}
    for all vectors $\ket{\Psi}$, so it must follow that $\hat{Q}$ must be a Hermitian operator. 

    \begin{example}[Expectatation of Position Observable]
      The expected position of the particle can be written
      \begin{equation} 
        \langle \mathbf{x} \rangle \coloneqq \int \Psi (x, t)^\ast (\hat{Q} \Psi)(x, t) \,dx = \int \Psi(x, t)^\ast (x \Psi)(x, t) \,dx = \int x \Psi (x, t)^\ast \Psi(x, t) \,dx = \int x |\boldsymbol{\Psi}(\mathbf{x}, t)|^2 \,d \mathbf{x}
      \end{equation} 
      which is consistent with the original definition of expectation in probability. 
    \end{example}

    \begin{theorem} 
      The velocity of the expected value can be evaluated to: 
      \begin{equation} 
        \frac{d}{dt} \langle \mathbf{x} \rangle = -\frac{i \hbar}{m} \int \boldsymbol{\Psi}^\ast \frac{\partial \boldsymbol{\Psi}}{\partial x} \,dx
      \end{equation}
    \end{theorem}
    \begin{proof} 
      We can see that using integration by parts in the penultimate step, 
      \begin{align} 
        \frac{d \langle x \rangle}{dt} & = \int x \frac{\partial}{\partial t} |\boldsymbol{\Psi}|^2 \,dx \\
                                       & = \frac{i \hbar}{2m} \int x \frac{\partial}{\partial x} \bigg( \boldsymbol{\Psi}^\ast \frac{\partial \boldsymbol{\Psi}}{\partial x} - \frac{\partial \boldsymbol{\Psi}^\ast}{\partial x} \boldsymbol{\Psi} \bigg) \\
                                       & = - \frac{i \hbar}{2m} \int \bigg( \boldsymbol{\Psi}^\ast \frac{\partial \boldsymbol{\Psi}}{\partial x} - \frac{\partial \boldsymbol{\Psi}^\ast}{\partial x} \boldsymbol{\Psi} \bigg) \,dx \\
                                       & = -\frac{i \hbar}{m} \int \boldsymbol{\Psi}^\ast \frac{\partial \boldsymbol{\Psi}}{\partial x} \,dx
      \end{align}
    \end{proof}

    Note that this does not mean that if we take a single particle and measure it multiple times, we will get the expected value, since it will first evolve and second we will get the exact same measurement. Rather, if we take an \textit{ensemble} of particles all in the same state $\mathbf{\Psi}$ and measure them all at once, then the histogram of measurements can be used as an unbiased estimator of $\langle x \rangle$. Therefore, we can define momentum as the following. 

    \begin{definition}[Momentum]
      The momentum is defined as 
      \begin{equation} 
        \langle p \rangle \coloneqq m \frac{d \langle x \rangle}{dt} = - i\hbar \int \bigg( \boldsymbol{\Psi}^\ast \frac{\partial \boldsymbol{\Psi}}{\partial x} \bigg)  \,dx
      \end{equation}
    \end{definition}

    \begin{example}[Expectation of Total Energy]
      The expectation value of the total energy is 
      \begin{equation} 
        \langle H \rangle = \int \psi^\ast \hat{H} \psi \,dx = E \int |\psi|^2 \,dx = E
      \end{equation}      
    \end{example}

    \begin{theorem} 
      Observable quantities, $Q(x, p, t)$, are represented by Hermitian operators $\hat{Q}(x, \frac{\hbar}{i} \frac{\partial}{\partial x}, t)$. The expectation value of $Q$ in the state $\Psi$ at time $t$ is $\braket{\Psi}{\hat{Q} \Psi} (t)$. 
    \end{theorem}

    \begin{definition}[Variance]
      The variance of the observable $Q$ is defined as 
      \begin{equation} 
        \sigma_Q^2 \coloneqq \braket{\Psi}{\hat{Q}^2 \Psi} - \braket{\Psi}{\hat{Q} \Psi}^2
      \end{equation}
      and the standard deviation is 
      \begin{equation} 
        \sigma_Q = \Delta(Q) \coloneqq \sqrt{\sigma_Q^2}
      \end{equation}
    \end{definition}

  \subsection{Heisenberg Uncertainty Principle} 

    The Heisenberg uncertainty principle is often very misunderstood. It's blown up in the popular literature as some mysterious force of quantum mechanics, but under the things we have constructed, it is simply an inequality no more sophisiticated than the Cauchy-Schwarz inequality. It is also not the same thing as the observer effect. 

    \begin{theorem}[Heisenberg Uncertainty Principle]
      Suppose that $\hat{A}$ and $\hat{B}$ are Hermitian operators corresponding to observables $A$ and $B$. Then, the following inequality holds. 
      \begin{equation} 
        \sigma_A \sigma_B \geq \frac{|\langle \Psi \mid [\hat{A}, \hat{B}] \mid \Psi \rangle |}{2}
      \end{equation}
    \end{theorem}
    \begin{proof} 
      Given $\hat{A}, \hat{B}$, by linearity we can write 
      \begin{equation} 
        |\langle \Psi \mid [\hat{A}, \hat{B}] \mid \Psi \rangle |^2 + | \langle \Psi \mid \{\hat{A}, \hat{B} \} \mid \Psi \rangle |^2 = 4 | \langle \Psi \mid \hat{A} \hat{B} \mid \Psi \rangle |^2
      \end{equation}
      By Cauchy-Schwarz, 
      \begin{equation} 
        | \langle \Psi \mid \{\hat{A}, \hat{B} \} \mid \Psi \rangle |^2 \leq \langle \Psi \mid \hat{A}^2 \mid \Psi \rangle \langle \Psi \mid \hat{B}^2 \mid \Psi \rangle 
      \end{equation} 
      and substituting this to the first equation gives us 
      \begin{equation} 
        |\langle \Psi \mid [\hat{A}, \hat{B}] \mid \Psi \rangle |^2 \leq 4 \langle \Psi \mid \hat{A}^2 \mid \Psi \rangle \langle \Psi \mid \hat{B}^2 \mid \Psi \rangle 
      \end{equation}
      and substituting $A = A - \langle A \rangle$ and $B = B - \langle B \rangle$ gives us the desired result. 
    \end{proof}


 
\section{Time Independent Schr\"odinger Equation}

  Let's revisit Shrodinger's equation again. 
  \begin{equation} 
    i \hbar \frac{\partial \boldsymbol{\Psi}}{\partial t} = - \frac{\hbar^2}{2m} \frac{\partial^2 \boldsymbol{\Psi}}{\partial \mathbf{x}^2} + V \boldsymbol{\Psi}
  \end{equation}
  If we pay attention to the right hand side, the operator acting on $\Psi$ is really the Hamiltonian, and the left hand side is also another linear operator, say $A$. Then, the equation reduces to a linear equation 
  \begin{equation} 
    A \psi = \hat{H} \psi
  \end{equation}
  which we solve for $\psi$. 

  

  Let's talk about solving the Schr\"odinger function itself by looking at the simple case when the Schr\"odinger equation is time independent and we can thus solve it by the method of separation of variables. That is, we look for solutions of the form 
  \begin{equation} 
    \boldsymbol{\Psi}(x, t) = \psi(x) f(t)
  \end{equation}

  We have 
  \begin{equation} 
    \frac{\partial \Psi}{\partial t} = \psi \frac{d f}{\partial t} , \;\;\; \frac{\partial^2}{\partial x^2} = \frac{d^2 \psi}{\partial x^2} = \frac{d^2 \psi}{d x^2} f
  \end{equation}
  and the Schr\"odinger equation becomes 
  \begin{equation} 
    i \hbar \psi \frac{d f}{d t} = -\frac{\hbar^2}{2m}\frac{d^2 \psi}{d x^2} f + V \psi f
  \end{equation}
  and dividing by $f$ on both sides gives us 
  \begin{equation} 
    i \hbar \frac{1}{f} \frac{df}{dt} = -\frac{\hbar^2}{2m} \frac{1}{\psi} \frac{d^2 \psi}{d x^2} + V
  \end{equation}
  The LHS as a function of $t$ alone and RHS as a function of $x$ alone. This is only possible if both sides are constant (since we can simply change $t$ or $x$ to get different values). Now setting the LHS as the constant $E$, we can rewrite the partial differential equation as a system of two ODEs. 
  \begin{align} 
    \frac{df}{dt} = - \frac{i E}{\hbar} f \\
    - \frac{\hbar^2}{2m} \frac{d^2 \psi}{dx^2} + V \psi = E \psi
  \end{align}
  The first equation is easy to solve since we can just integrate, which gives $f(t) = e^{-i E t/ \hbar}$. The second equation is simply the Hamiltonian observable $\hat{H}$ acting on $\psi$, and so it reduces to an eigenvalue equation.  
  \begin{align} 
    \hat{H} \psi = E \psi
  \end{align}
  Therefore, once we solve by getting the eigenfunction $\psi_E$ corresponding to this eigenvalue $E$, the final solution of the time-independent Shrodinger equation is of form 
  \begin{equation} 
    \boldsymbol{\Psi}(x, t) = \psi(x) e^{-i E t/\hbar}  
  \end{equation}

  \begin{definition}[Stationary State]
    A wavefunction $\boldsymbol{\Psi}(x, t)$ is a \textbf{stationary state} if its corresponding probability density does not depend on $t$. That is, 
    \begin{equation} 
      |\boldsymbol{\Psi}(x, t)|^2 = |\boldsymbol{\Psi}(x, t^\prime)|^2  \text{ for all } t, t^\prime \in \mathbb{R}
    \end{equation}
    This means that the expected position is always constant since the probability density function never changes with time. 
  \end{definition}

  It turns out that solutions to separable equations all stationary states since we have 
  \begin{equation} 
    |\boldsymbol{\Psi}(x, t)|^2 = \boldsymbol{\Psi}^\ast \boldsymbol{\Psi} = \psi^\ast e^{+i E t/\hbar} \psi e^{- i E t/\hbar} = |\psi (x)|^2
  \end{equation}

\section{Other}

  \begin{definition}[Pauli Matrices]
    The \textbf{Pauli matrices} are defined 
      \[
        X = \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix},  
        Y = \begin{pmatrix} 0 & -i \\ i & 0 \end{pmatrix},   
        Z = \begin{pmatrix} 1 & 0 \\ 0 & -1 \end{pmatrix}  
      \]
  \end{definition}

  \begin{lemma}[Decomposition of Pauli Matrices]
    The Pauli matrices are self-adjoint (Hermitian) and unitary. 
  
    \begin{enumerate}
      \item They have the outer product decomposition of: 

        \[X = \ket{1} \bra{0} + \ket{0} \bra{1}, Y = i \ket{1} \bra{0} - i \ket{0} \bra{1}, Z = \ket{0} \bra{0} - \ket{1} \bra{1}\]

      \item They have an eigendecomposition of: 
        \begin{equation} 
          X = \ket{+} \bra{+} - \ket{-} \bra{-}, Y = i \ket{+} \bra{+} - i \ket{-} \bra{-}, Z = \ket{0} \bra{0} - \ket{1} \bra{1}   
        \end{equation}
    \end{enumerate}
  \end{lemma}

  \begin{theorem}
    These three matrices are very important because it turns out that

      \[\frac{1}{2} i X, \frac{1}{2} i Y, \frac{1}{2} i Z\]

    forms the basis for the Lie algebra $\mathfrak{u}(2)$, which exponentiates to the unitary group $\text{U}(2)$. Therefore, by exponentiating each Pauli matrix, we have

    \begin{align*}
      e^{-i \beta X /2} &= \cos \frac{\beta}{2} I - i \sin \frac{\beta}{2} X = \begin{pmatrix} \cos \frac{\beta}{2} & -i \sin \frac{\beta}{2} \\ -i \sin \frac{\beta}{2} & \cos \frac{\beta}{2} \end{pmatrix}, \\
      e^{-i \gamma Y/2} &= \cos \frac{\gamma}{2} I - i \sin \frac{\gamma}{2} Y = \begin{pmatrix} \cos \frac{\gamma}{2} & - \sin \frac{\gamma}{2} \\ \sin \frac{\gamma}{2} & \cos \frac{\gamma}{2} \end{pmatrix}, \\
      e^{-i \delta Z/2} &= \cos \frac{\delta}{2} I - i \sin \frac{\delta}{2} Z = \begin{pmatrix} e^{-i \delta /2} & 0 \\ 0 & e^{i\delta/2} \end{pmatrix}.
    \end{align*}

    and so every rotation matrix $U \in \text{U}(2)$, which represents single qubit operations, can be decomposed as the following products for real values of $\beta, \gamma, \delta$:

    \[
      U = \begin{pmatrix} \cos \frac{\beta}{2} & -i \sin \frac{\beta}{2} \\ -i \sin \frac{\beta}{2} & \cos \frac{\beta}{2} \end{pmatrix} \begin{pmatrix} \cos \frac{\gamma}{2} & - \sin \frac{\gamma}{2} \\ \sin \frac{\gamma}{2} & \cos \frac{\gamma}{2} \end{pmatrix} \begin{pmatrix} e^{-i \delta /2} & 0 \\ 0 & e^{i\delta/2} \end{pmatrix}.
    \]
  \end{theorem}

  \begin{theorem}[Commutation and Anti-Commutation]
    The Pauli matrices satisfy the following properties. 
    \begin{enumerate}
      \item Commutation properties: 
        \begin{align} 
          [X, Y] & = 2iZ, \\
          [Y, Z] & = 2iX, \\
          [Z, X] & = 2iY.
        \end{align}

      \item Anticommutation properties: 
        \begin{align} 
          \{X, Y\} & = 0, \\
          \{Y, Z\} & = 0, \\
          \{Z, X\} & = 0.
        \end{align}
    \end{enumerate}
  \end{theorem}



\end{document} 

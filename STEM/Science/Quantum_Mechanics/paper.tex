\documentclass{article}

  % preamble
  \usepackage[letterpaper, top=1in, bottom=1in, left=1in, right=1in]{geometry}
  \usepackage[utf8]{inputenc}
  \usepackage[english]{babel}
  \usepackage{amsmath, amssymb, amsthm, mathtools} % necessary
  \usepackage{lastpage} % insert last page number
  \usepackage{centernot} % for not slash

  \usepackage{pgfplots}
  \pgfplotsset{compat=1.18}
  \usepackage{hyperref} % hyperlinks
  \usepackage{fancyhdr} % fancy headers
  \usepackage{fancyvrb} % verbatim
  \usepackage{parskip}

  \usepackage{subcaption} % captions for figures
  \definecolor{cverbbg}{gray}{0.93}

  \setlength{\parindent}{0pt} % set no indent
  \hfuzz=5.002pt % ignore overfull hbox badness warnings below this limit

  \DeclareMathOperator{\Tr}{Tr}
  \DeclareMathOperator{\Sym}{Sym}
  \DeclareMathOperator{\Span}{span}
  \DeclareMathOperator{\std}{std}
  \DeclareMathOperator{\Cov}{Cov}
  \DeclareMathOperator{\Var}{Var}
  \DeclareMathOperator{\Corr}{Corr}
  \DeclareMathOperator*{\argmin}{\arg\!\min}
  \DeclareMathOperator*{\argmax}{\arg\!\max}
  \newenvironment{question}{\color{blue}}{\ignorespacesafterend}

  \newcommand{\ket}[1]{\ensuremath{\left|#1\right\rangle}}
  \newcommand{\bra}[1]{\ensuremath{\left\langle#1\right|}}
  \newcommand{\braket}[2]{\langle #1 | #2 \rangle}

  \theoremstyle{definition}
  \newtheorem{theorem}{Theorem}[section]
  \newtheorem{proposition}[theorem]{Proposition}
  \newtheorem{lemma}[theorem]{Lemma}
  \newtheorem{example}{Example}[section]
  \newtheorem{exercise}{Exercise}[section]
  \newtheorem{corollary}{Corollary}[theorem]
  \newtheorem{definition}{Definition}[section]
  \renewcommand{\qed}{\hfill$\blacksquare$}
  \renewcommand{\footrulewidth}{0.4pt}% default is 0pt

  \newenvironment{solution}{\noindent \textit{Solution.}}{}
  \newenvironment{cverbatim}
    {\SaveVerbatim{cverb}}
    {\endSaveVerbatim
    \flushleft\fboxrule=0pt\fboxsep=.5em
    \colorbox{cverbbg}{%
      \makebox[\dimexpr\linewidth-2\fboxsep][l]{\BUseVerbatim{cverb}}%
    }
    \endflushleft
  }

  \renewcommand{\thispagestyle}[1]{} % needed for including header in title page

\begin{document}
\pagestyle{fancy}

\lhead{Quantum Mechanics}
\chead{Muchang Bahng}
\rhead{\date{Spring 2024}}
\cfoot{\thepage / \pageref{LastPage}}

\title{Quantum Mechanics}
\author{Muchang Bahng}
\date{Spring 2024}

\maketitle
\tableofcontents
\pagebreak

\section{Wavefunctions} 

  Imagine a particle of mass $m$ constrained to move along the x-axis, subject to some applied force $F(x, t)$ (time-dependent). In classical mechanics, we want to determine the position of the particle at any given time $t$ by finding the function $x(t)$. How do we do this? We simply apply \textit{Newton's second law}. 
  \begin{equation} 
    \mathbf{F} = m \mathbf{a}
  \end{equation}
  and solve the ordinary differential equation (with some initial conditions), either analytically or numerically. Once we have $x(t)$, we can find other metrics of interest, such as the velocity $v(t)$, kinetic energy $T = \frac{1}{2} mv^2$, or others. For conservative systems (the only kind we'll consider, and fortunately the only kind that exists at the microscopic level), the force can be expressed as the gradient of a potential energy function $U(\mathbf{x})$. 

  \subsection{Schr\"odingers Equation}

    Quantum mechanics approaches the same problem differently. Rather than looking for position function $\mathbf{x}(t)$, we are looking for a \textbf{wave function} $\boldsymbol{\Psi}(\mathbf{x}, t)$ of the particle, which we can get by solving the \textit{Schr\"odinger equation}. 

    \begin{definition}[Schr\"odinger Equation]
      The \textbf{Schr\"odinger equation} is defined 
      \begin{equation} 
        i \hbar \frac{\partial \boldsymbol{\Psi}}{\partial t} = - \frac{\hbar^2}{2m} \frac{\partial^2 \boldsymbol{\Psi}}{\partial \mathbf{x}^2} + V \boldsymbol{\Psi}
      \end{equation}
      where $\hbar$ is the \textbf{reduced Planck's constant}, defined 
      \begin{equation} 
        \hbar = \frac{h}{2\pi} = 1.054573 \times 10^{-34} \mathrm{J}s
      \end{equation}
    \end{definition}

    Therefore, given suitable initial conditions $\boldsymbol{\Psi}(\mathbf{x}, 0)$, the Schr\"odinger equation determines $\boldsymbol{\Psi}(\mathbf{x}, t)$ for all future time. Now let's talk about this wave function and its physical interpretation, starting with \textbf{Born's statistical interpretation}. This says that the wavefunction $\boldsymbol{\Psi}(\mathbf{x}, t)$ determines the probability of finding th particle at point $\mathbf{x}$ at time $t$. That is, the probability density function of the particle's position at time $t$ is given by 
    \begin{equation} 
      f_{X, t} (\mathbf{x}, t) = |\boldsymbol{\Psi}(\mathbf{x}, t)|^2
    \end{equation}
    Unfortunately, Schr\"odinger's equation is a linear system, so the set of solutions to this equation forms a vector space. So if $\boldsymbol{\Psi}$ is a solution, then $c \boldsymbol{\Psi}$ is also a solution for all $c \in \mathbb{C}$. This is a problem since it must be normalized. This is why we have an extra condition that 
    \begin{equation} 
    \int |\boldsymbol{\Psi}(\mathbf{x}, t)|^2 \,d \mathbf{x} = 1
    \end{equation}
    which means that the probability of finding a particle somewhere in the space $X$ at a certain point $t$ must integrate to $1$. This gives us a normalization condition, and any functions that have an integral of infinity is not within our search space. Therefore, we're really just trying to find a function in the $L^2$ space of integrable functions. 

    There's two things that the reader may realize: First, it seems that multiple wavefunctions $\Psi$ may induce the same probability measure. It turns out that within this set of normalized wavefunctions, there is an equivalence class denoted by $\Psi \sim e^{i \theta} \Psi \text{ for all } \theta \in \mathbb{R}$, which means that $|e^{i \theta} \Psi|^2 = (e^{i \theta} \Psi)^\ast (e^{i \theta} \Psi) = \Psi^\ast e^{-i \theta} e^{i \theta} \Psi = |\Psi|^2$. This is a consequence of the fact that the probability of finding a particle at a certain point is not affected by the phase of the wavefunction.

    \begin{definition}[Global Phase Factor]
      Two wavefunctions are said to differ by a \textbf{global phase factor} iff they differ by a normalized complex scalar $e^{i \theta}$.  
    \end{definition}

    For now, we can think of two wavefunctions that differ by a global phase factor as being the same. The important distinction is the relative phase factor, which is the difference between two wavefunctions at a certain point. 

    \begin{definition}[Relative Phase]
      Two wavefunctions are said to differ by a \textbf{relative phase} if they differ by a normalized complex scalar $e^{i \theta(x)}$ that is dependent on the position $x$.
    \end{definition}

    Second, this normalization restriction may not be preserved in Shrodinger's equation. Fortunately for us, Schr\"odinger's equation keeps this normalization condition as time passes. Let's prove this. 

    \begin{theorem} 
      Given a solution $\boldsymbol{\Psi}$ that has been normalized, the function will stay normalized as time passes. 
    \end{theorem}
    \begin{proof} 
      Let us take the time-derivative of the total probability and show that is is $0$. We show that 
      \begin{equation} 
        \frac{d}{dt}  \int |\boldsymbol{\Psi} (\mathbf{x}, t)|^2 \,dx = \int \frac{\partial}{\partial t} |\boldsymbol{\Psi}(\mathbf{x}, t)|^2 \,dx
      \end{equation}
      and by the product rule we have 
      \begin{equation} 
        \frac{\partial}{\partial t} |\boldsymbol{\Psi}|^2 = \frac{\partial}{\partial t} (\boldsymbol{\Psi}^\ast \boldsymbol{\Psi}) = \boldsymbol{\Psi}^\ast \frac{\partial \boldsymbol{\Psi}}{\partial t} + \frac{\partial \boldsymbol{\Psi}^\ast}{\partial t} \boldsymbol{\Psi}
      \end{equation}
      We can take the complex conjugate of the Schr\"odinger equation to get 
      \begin{align} 
        \frac{\partial \boldsymbol{\Psi}}{\partial t} & = \frac{i \hbar}{2m} \frac{\partial^2 \boldsymbol{\Psi}}{\partial x^2} - \frac{i}{\hbar} V \boldsymbol{\Psi} \\
        \frac{\partial \boldsymbol{\Psi}^\ast}{\partial t} & = - \frac{i \hbar}{2m} \frac{\partial^2 \boldsymbol{\Psi}^\ast}{\partial x^2} + \frac{i}{\hbar} V \boldsymbol{\Psi}^\ast 
      \end{align}
      and substituting both equations into the product rule gives 
      \begin{equation} 
        \frac{\partial}{\partial t} |\boldsymbol{\Psi}|^2 = \frac{i \hbar}{2m} \bigg( \boldsymbol{\Psi}^\ast \frac{\partial^2 \boldsymbol{\Psi}}{\partial x^2} - \frac{\partial^2 \boldsymbol{\Psi}^\ast}{\partial x^2} \boldsymbol{\Psi} \bigg) = \frac{\partial}{\partial x} \bigg[ \frac{i \hbar}{2m} \bigg( \boldsymbol{\Psi}^\ast \frac{\partial \boldsymbol{\Psi}}{\partial x} - \frac{\partial \boldsymbol{\Psi}^\ast}{\partial x} \boldsymbol{\Psi} \bigg)\bigg]
      \end{equation}
      and now we can evaluate the integral to be 
      \begin{equation} 
        \frac{d}{dt} \int |\boldsymbol{\Psi}(x, t)|^2 \,dx = \frac{i \hbar}{2m} \big( \boldsymbol{\Psi}^\ast \frac{\partial \boldsymbol{\Psi}}{\partial x} - \frac{\partial \boldsymbol{\Psi}^\ast}{\partial x} \boldsymbol{\Psi} \bigg) \bigg|_{-\infty}^{+\infty} 
      \end{equation}
      which evaluates to $0$ since $\boldsymbol{\Psi}$ must go to $0$ as it is a probability density. 
    \end{proof}

    Now that we have settled on what a wavefunction is, we can now introduce the braket notation. 

    \subsubsection{Ket Notation} 

      We can talk about another paradigm of representing the states of a system: as a ket. We can equivalently say that the state of a system is represented by a normalized vector $\ket{\psi}$ in the Hilbert space $L^2$. Now it seems that both $\psi$ and $\ket{\psi}$ both live in $L^2$, so what is the difference between them? This is something that is not constructed very rigorously in introductory quantum mechanics courses, so let's try to settle this by constructing $\ket{\psi}$ from $\psi$. 

      Note that we have two spaces here: the function space $L^2$ and the state space $X$. In 1939, Dirac wanted to unify these 2 spaces in order to more easily work with them. He does this by first constructing some Hilbert space $\mathcal{H}$ and identifying two mappings $x \in X \mapsto \ket{x} \in \mathcal{H}$ and $\psi \in L^2 \mapsto \ket{\psi} \in \mathcal{H}$. The goal was to be able to evaluate $\psi(x)$ with an inner product $\braket{x}{\psi}$ in $\mathcal{H}$. Let's talk about each construction separately: 
      \begin{enumerate} 
        \item To do this, note that $\psi$ can be reduced to an uncountable sum of $\delta$-functions. 
          \begin{equation} 
            \psi(x) = \int_{x \in X} \psi(x^\prime) \delta_{x^\prime} (x) \,dx^\prime 
          \end{equation}
        This is analogous to an uncountable sum of basis vector functions $\delta_{x^\prime}$ and their coefficients $\psi(x^\prime)$. Each $\delta_{x^\prime}$ can be though of as an uncountably long vector 
        \begin{equation} 
          \ket{\delta_{x^\prime}} = (\ldots, 0, \ldots, 0, 1, 0, \ldots, 0, \ldots)
        \end{equation}
        where the $1$ is in the $x^\prime$ index, and each function is a vector 
        \begin{equation} 
          \ket{\psi} = (\ldots \psi(0) \ldots \psi(\pi) \ldots) = \int_{x \in X} \psi(x) \ket{\delta_x} \,dx 
        \end{equation}
        Therefore, we have defined the mapping $\psi \mapsto \ket{\psi}$. 

      \item To define $x \mapsto \ket{x}$, we can also analogously say that each $x \in X$ can also be represented as 
        \begin{equation} 
          \ket{x} = (\ldots, 0, \ldots, 0, 1, 0, \ldots, 0, \ldots) = \ket{\delta_x}
        \end{equation}
        like an uncountable version of one-hot encoding (each real number gets mapped to its own dimension). So, 
        \begin{equation} 
          \ket{\psi} = \int_{x \in X} \psi(x) \, \ket{x} \,dx 
        \end{equation}
      \end{enumerate}

      Therefore, we can represent 
      \begin{equation} 
        \braket{x}{\psi} = \braket{\delta_x}{\psi} = \int_{x^\prime \in X} \psi(x^\prime) \, \delta_{x^\prime}(x) \,dx^\prime = \psi(x) 
      \end{equation}

      We can verify that this is bilinear since
      \begin{equation} 
        (\psi + \phi)(x) = \braket{\delta_x}{\psi + \phi} = \braket{\delta_x}{\psi} + \braket{\delta_x}{\phi} = \psi(x) + \psi(x)
      \end{equation}
      and 
      \begin{equation} 
        \braket{\delta_x + \delta_y}{\psi} = \braket{\delta_x}{\psi} + \braket{\delta_y}{\psi} = \psi(x) + \psi(y)
      \end{equation}
      Note that in here $\delta_x + \delta_y \neq \delta_{x + y}$ since we are not talking addition in the reals. Rather, 
      \begin{equation} 
        (\delta_x + \delta_y)(q) = \begin{cases} 1 & \text{ if } x = q \text{ or } y = q \\ 0 & \text{ if else} \end{cases}
      \end{equation}

  \subsection{Observables and Measurements}

    When we have a (classical or quantum mechanical) system, there is a configuration space $M$ that describes the complete state of the system. In classical mechanics, this is usually the space $\mathcal{M} = \mathcal{X} \times \mathcal{P}$ where $\mathcal{X}$ is the position and $\mathcal{P}$ is the momentum. In quantum mechanics, $\mathcal{M} = L^2(\mathcal{X})$ is the space of wavefunctions defined over $\mathcal{X}$. 

    \begin{definition}[Observable in Classical Mechanics]
      In classical mechanics, an observable is a function $Q: \mathcal{M} \rightarrow \mathbb{R}$ of some dynamical quantity that you want to measure from the system.  
    \end{definition}

    \begin{example} 
      Some examples are 
      \begin{enumerate} 
        \item The position of the vector is simply the $x$ value of the tuple. 
        \item The kinetic energy is $K(x, p, t) = \frac{1}{2m} p^2$. 
        \item The potential energy is $U(x, p, t) = U(x)$, which is dependent only on $x$ for conservative systems. 
        \item The Hamiltonian is defined as the total (kinetic plus potential) energy, defined $H(x, p, t) = \frac{1}{2m} p^2 + V(x)$. 
      \end{enumerate}
    \end{example}

    In quantum mechanics, an observable is similar as in you want to define the specific type of quantity you want to measure, but now this is probabilistic: we now have a random variable $Q$. Broadly speaking, if we measure this observable, then it will cause $Q$ to realize into some real number $q$. 

    \begin{definition}[Observable in Quantum Mechanics]
      However, there is a slight catch: $\mathcal{X}$ must be some. A \textbf{quantum mechanical observable} $Q$ is a real valued random variable, and its \textbf{associated Hamiltonian} (often called the observable) is a Hermitian operator $\hat{Q}: L^2 (\mathcal{X}) \rightarrow L^2 (\mathcal{X})$.  
    \end{definition}

    Two things to note: First, the output of the operator doesn't necessarily have to be a wavefunction (doesn't need to be normalized, look at the position operator), but it should be normalizable. 

    Second, the Hermitian operator $\hat{H}$ is relevant since it turns out that its set of eigenvalues $\mathcal{E} = \{E\}$ of $\hat{Q}$ determines the support of $Q$, and the set of eigenvectors $\{\psi_E\}$ will determine the probability distribution of $Q$, which will be elaborated soon. Note that the Hermitian constraint happens because the outcomes must be real, since they are physical measurements that must be compared. Another technicality is that not all Hermitian operators are proper observables in the infinite dimensional case, but this is a technicality and can be ignored for now.  

    \begin{example} 
      Some examples of quantum mechanical observables can be obtained by the canonical substitution 
      \begin{equation} 
        p \rightarrow \frac{\hbar}{i} \frac{\partial}{\partial x} 
      \end{equation}
      from the classical observables. 
        \begin{enumerate} 
          \item Position: $\hat{Q}: \psi \rightarrow x \psi$ 
          \item Momentum: $\hat{Q}: \psi \rightarrow \frac{\partial}{\partial x} \psi $ 
          \item Kinetic energy: $\hat{Q}: \psi \rightarrow \frac{1}{2m} \frac{\hbar^2}{-1} \frac{\partial^2}{\partial x^2} \psi$

          \item Hamiltonian, or total energy, denoted with a special character $\hat{H} : \psi \mapsto \frac{\hbar^2}{2m} \frac{\partial^2}{\partial x^2} + V$. 
        \end{enumerate} 
    \end{example}

    Now an observable simply describes the type of quantity that you want to measure. A \textit{measurement}, on the other hand, is an action on the system, as if we would measure with a ruler or an instrument, and therefore is different from an observable. Once you do a measurement, the random variable $Q$ realizes onto a certain number $q \in \mathbb{R}$, and the state $\psi$ of the system is affected by a collapse of the wavefunction $\psi$ into the Delta function centered at $q$. Therefore, there are two evolutionary processes: the unitary evolution of $\psi$ according to Shrodinger's equation and measurement operators. 

    \begin{definition}[Measurement]
      A \textbf{measurement} of a system in state $\psi$, with respect to observable $Q$, is defined by two things. 
      \begin{enumerate} 
        \item First, a measurement will cause a realization of $Q$ into a certain real quantity $E \in \mathcal{E}$. The probability distribution of $Q$ is determined by taking the state $\psi$, expanding it into a linear combination of the eigenvectors 
        \begin{equation} 
          \psi = \int_{E \in \mathcal{E}}  \alpha_E \psi_E \,dE
        \end{equation}
        and finally taking $|\alpha_E|^2$ to be the probability (PMF for countable $\mathcal{E}$ and PDF for uncountable).  

        \item While $q$ is realized, the wavefunction $\psi$ will collapse into $\delta_q$. 
      \end{enumerate}
    \end{definition}

    Now let's put these things all together in one formal process. 
    \begin{enumerate} 
      \item You have a system in a state $\Psi$ and want to measure it in some way. But measuring something does not make sense without knowing what quantity you want to measure, so you choose an observable $\hat{Q}$, a Hermitian operator that corresponds to the value that you want to measure. 

      \item As soon as you choose this operator, by the spectral theorem it has some set of eigenvalues $\mathcal{E} \subset \mathbb{R}$ and corresponding orthonormal eigenfunctions $\{\psi_E\}_{E \in \mathcal{E}}$, which may be finite, countable, or uncountable. It turns out that the measurement outcome \textit{must} be one of these eigenvalues, and it is random, so you're essentially drawing from this set an $E$ with some random variable $Q$. This makes $\mathcal{E}$ a probability space with some measure $\mu$.

      \item To determine the actual probabilities of landing on each eigenvalue, you must first take the state $\Psi$ and expand it out into the eigenbasis. 
        \begin{equation} 
          \Psi = \sum_{E \in \mathcal{E}} \alpha_E \psi_E \text{ or } \Psi = \int_{E \in \mathcal{E}} \alpha_E \psi_E \,dE 
        \end{equation}

      and the amplitude squared of the coefficients gives you the respective probabilities. 

      \item Once you measure and get result $E$ at time $t$, the wavefunction  $\psi(t)$ collapses onto the delta at $E$ (similar to a realization of a random variable) and immediately begins to ``smear'' out again. If you measure really soon after, you can still sample $\psi(t + \delta t)$ with approximately probability $1$ at the point you just measured, since the state is so concentrated there and therefore has a very high coefficient corresponding to $\psi_E$ when you expand it out. 
    \end{enumerate}

    \begin{example}[Measurement of the Position Observable]
      We have said that given a wavefunction $\Psi (t)$, the probability distribution of the particle's position is determined by the PDF at time $t$ is $p_t (x) = |\Psi(x, t)|^2$. This is consistent with what we have described here if we talk about the position observable. Say that I have a wavefunction $\Psi$. I want to measure position with the observable $\hat{Q} : \psi \mapsto x \psi$. We first have to find the eigendecomposition of $\hat{Q}$. It turns out that the set of eigenfunctions is the set of all Dirac delta functions indexed by $\mathbb{R}$. 
      \begin{equation} 
        \delta_k (x) = \begin{cases} 1 & x = k \\ 0 & x \neq k \end{cases} 
      \end{equation}
      and we can see that every $k$ is an eignvalue with eigenfunction $\delta_k$ since 
      \begin{equation} 
        (\hat{Q} \delta_k)(x) = (k \delta_k) (x) = \begin{cases} k & x = k \\ 0 & x \neq k \end{cases} = k \delta_k 
      \end{equation}
      So the eigendecomposition is 
      \begin{equation} 
        \hat{Q} = \sum_{x \in X} k \delta_k \otimes \delta_k^\ast \text{ or } \hat{Q} = \int_{k \in \mathbb{R}} k \delta_k \otimes \delta_k^\ast \,dx 
      \end{equation}
      and so $\psi$ can be decomposed as the uncountable linear combination of the eigenfunctions 
      \begin{align*} 
        \psi & = \sum_x \alpha_x \delta_x \implies \mathbb{P}(Q = x) = |\alpha_x|^2  = | \psi(x) |^2\\  
        \psi & = \int_{x \in \mathbb{R}} \alpha_x \delta_x \,dx \implies p_Q (x) = |\alpha_x|^2 = |\psi(x)|^2 
      \end{align*}
    \end{example}   

    \begin{example}[Measurement of the Momentum Observable]
      Say that I have a wavefunction $\psi$. I want to measure momentum with the observable $Q$ and corresponding operator $\hat{Q}: \psi \rightarrow \frac{\hbar}{i} \frac{\partial}{\partial x} \psi$. We can verify that the spectrum consists of the eigenvalue/eigenfunction pair 
      \begin{equation} 
        \bigg\{ p, \psi_p (x) = \frac{1}{\sqrt{2 \pi \hbar}} e^{i p x /\hbar} \bigg\}_{p \in \mathbb{R}}
      \end{equation}
      and so 
      \begin{equation} 
        \hat{Q} = \int_{p \in \mathbb{R}} p \psi_p \otimes \psi_p^\ast \,dp 
      \end{equation}
      and so we can decompose 
      \begin{equation} 
        \psi = \int_{x \in \mathbb{R}} \alpha_x \psi_x \,dx \implies p_Q (x) = |\braket{\psi_x}{\psi}|^2 
      \end{equation}
    \end{example}

    \subsubsection{Expectation and Variance of Observables}

      Sometimes, we are interested in specific statistical properties of these distributions of observables. In this case, we can define the expectation and variance of the operators. 

      \begin{definition}[Expectation of Operator]
        The expectation value of the operator (as a function of time) is
        \begin{equation} 
          \langle Q \rangle (t) \coloneqq \int \Psi (x, t)^\ast (\hat{Q} \Psi) (x, t) \,dx = \braket{\Psi}{\hat{Q} \Psi}(t)
        \end{equation}
      \end{definition}

      Since physical observables must be real (to see why, look \href{https://physics.stackexchange.com/questions/436462/why-is-there-a-physical-preference-to-real-numbers}{here}), it must be the case that 
      \begin{equation} 
        \braket{\Psi}{\hat{Q} \Psi} = \braket{\Psi}{\hat{Q} \Psi}^\ast = \braket{\hat{Q} \Psi}{\Psi}
      \end{equation}
      for all vectors $\ket{\Psi}$, so it must follow that $\hat{Q}$ must be a Hermitian operator. 

      \begin{example}[Expectatation of Position Observable]
        The expected position of the particle can be written
        \begin{equation} 
          \langle \mathbf{x} \rangle \coloneqq \int \Psi (x, t)^\ast (\hat{Q} \Psi)(x, t) \,dx = \int \Psi(x, t)^\ast (x \Psi)(x, t) \,dx = \int x \Psi (x, t)^\ast \Psi(x, t) \,dx = \int x |\boldsymbol{\Psi}(\mathbf{x}, t)|^2 \,d \mathbf{x}
        \end{equation} 
        which is consistent with the original definition of expectation in probability. 
      \end{example}

      \begin{theorem} 
        The velocity of the expected value can be evaluated to: 
        \begin{equation} 
          \frac{d}{dt} \langle \mathbf{x} \rangle = -\frac{i \hbar}{m} \int \boldsymbol{\Psi}^\ast \frac{\partial \boldsymbol{\Psi}}{\partial x} \,dx
        \end{equation}
      \end{theorem}
      \begin{proof} 
        We can see that using integration by parts in the penultimate step, 
        \begin{align} 
          \frac{d \langle x \rangle}{dt} & = \int x \frac{\partial}{\partial t} |\boldsymbol{\Psi}|^2 \,dx \\
                                         & = \frac{i \hbar}{2m} \int x \frac{\partial}{\partial x} \bigg( \boldsymbol{\Psi}^\ast \frac{\partial \boldsymbol{\Psi}}{\partial x} - \frac{\partial \boldsymbol{\Psi}^\ast}{\partial x} \boldsymbol{\Psi} \bigg) \\
                                         & = - \frac{i \hbar}{2m} \int \bigg( \boldsymbol{\Psi}^\ast \frac{\partial \boldsymbol{\Psi}}{\partial x} - \frac{\partial \boldsymbol{\Psi}^\ast}{\partial x} \boldsymbol{\Psi} \bigg) \,dx \\
                                         & = -\frac{i \hbar}{m} \int \boldsymbol{\Psi}^\ast \frac{\partial \boldsymbol{\Psi}}{\partial x} \,dx
        \end{align}
      \end{proof}

      Note that this does not mean that if we take a single particle and measure it multiple times, we will get the expected value, since it will first evolve and second we will get the exact same measurement. Rather, if we take an \textit{ensemble} of particles all in the same state $\mathbf{\Psi}$ and measure them all at once, then the histogram of measurements can be used as an unbiased estimator of $\langle x \rangle$. Therefore, we can define momentum as the following. 

      \begin{definition}[Momentum]
        The momentum is defined as 
        \begin{equation} 
          \langle p \rangle \coloneqq m \frac{d \langle x \rangle}{dt} = - i\hbar \int \bigg( \boldsymbol{\Psi}^\ast \frac{\partial \boldsymbol{\Psi}}{\partial x} \bigg)  \,dx
        \end{equation}
      \end{definition}

      \begin{example}[Expectation of Total Energy]
        The expectation value of the total energy is 
        \begin{equation} 
          \langle H \rangle = \int \psi^\ast \hat{H} \psi \,dx = E \int |\psi|^2 \,dx = E
        \end{equation}      
      \end{example}

      \begin{theorem} 
        Observable quantities, $Q(x, p, t)$, are represented by Hermitian operators $\hat{Q}(x, \frac{\hbar}{i} \frac{\partial}{\partial x}, t)$. The expectation value of $Q$ in the state $\Psi$ at time $t$ is $\braket{\Psi}{\hat{Q} \Psi} (t)$. 
      \end{theorem}

    

\section{Time Independent Schr\"odinger Equation}

  Let's revisit Shrodinger's equation again. 
  \begin{equation} 
    i \hbar \frac{\partial \boldsymbol{\Psi}}{\partial t} = - \frac{\hbar^2}{2m} \frac{\partial^2 \boldsymbol{\Psi}}{\partial \mathbf{x}^2} + V \boldsymbol{\Psi}
  \end{equation}
  If we pay attention to the right hand side, the operator acting on $\Psi$ is really the Hamiltonian, and the left hand side is also another linear operator, say $A$. Then, the equation reduces to a linear equation 
  \begin{equation} 
    A \psi = \hat{H} \psi
  \end{equation}
  which we solve for $\psi$. 

  

  Let's talk about solving the Schr\"odinger function itself by looking at the simple case when the Schr\"odinger equation is time independent and we can thus solve it by the method of separation of variables. That is, we look for solutions of the form 
  \begin{equation} 
    \boldsymbol{\Psi}(x, t) = \psi(x) f(t)
  \end{equation}

  We have 
  \begin{equation} 
    \frac{\partial \Psi}{\partial t} = \psi \frac{d f}{\partial t} , \;\;\; \frac{\partial^2}{\partial x^2} = \frac{d^2 \psi}{\partial x^2} = \frac{d^2 \psi}{d x^2} f
  \end{equation}
  and the Schr\"odinger equation becomes 
  \begin{equation} 
    i \hbar \psi \frac{d f}{d t} = -\frac{\hbar^2}{2m}\frac{d^2 \psi}{d x^2} f + V \psi f
  \end{equation}
  and dividing by $f$ on both sides gives us 
  \begin{equation} 
    i \hbar \frac{1}{f} \frac{df}{dt} = -\frac{\hbar^2}{2m} \frac{1}{\psi} \frac{d^2 \psi}{d x^2} + V
  \end{equation}
  The LHS as a function of $t$ alone and RHS as a function of $x$ alone. This is only possible if both sides are constant (since we can simply change $t$ or $x$ to get different values). Now setting the LHS as the constant $E$, we can rewrite the partial differential equation as a system of two ODEs. 
  \begin{align} 
    \frac{df}{dt} = - \frac{i E}{\hbar} f \\
    - \frac{\hbar^2}{2m} \frac{d^2 \psi}{dx^2} + V \psi = E \psi
  \end{align}
  The first equation is easy to solve since we can just integrate, which gives $f(t) = e^{-i E t/ \hbar}$. The second equation is simply the Hamiltonian observable $\hat{H}$ acting on $\psi$, and so it reduces to an eigenvalue equation.  
  \begin{align} 
    \hat{H} \psi = E \psi
  \end{align}
  Therefore, once we solve by getting the eigenfunction $\psi_E$ corresponding to this eigenvalue $E$, the final solution of the time-independent Shrodinger equation is of form 
  \begin{equation} 
    \boldsymbol{\Psi}(x, t) = \psi(x) e^{-i E t/\hbar}  
  \end{equation}

  \begin{definition}[Stationary State]
    A wavefunction $\boldsymbol{\Psi}(x, t)$ is a \textbf{stationary state} if its corresponding probability density does not depend on $t$. That is, 
    \begin{equation} 
      |\boldsymbol{\Psi}(x, t)|^2 = |\boldsymbol{\Psi}(x, t^\prime)|^2  \text{ for all } t, t^\prime \in \mathbb{R}
    \end{equation}
    This means that the expected position is always constant since the probability density function never changes with time. 
  \end{definition}

  It turns out that solutions to separable equations all stationary states since we have 
  \begin{equation} 
    |\boldsymbol{\Psi}(x, t)|^2 = \boldsymbol{\Psi}^\ast \boldsymbol{\Psi} = \psi^\ast e^{+i E t/\hbar} \psi e^{- i E t/\hbar} = |\psi (x)|^2
  \end{equation}
 



\end{document} 

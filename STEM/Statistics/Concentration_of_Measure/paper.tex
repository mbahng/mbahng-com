\documentclass{article}

% packages
  % basic stuff for rendering math
  \usepackage[letterpaper, top=1in, bottom=1in, left=1in, right=1in]{geometry}
  \usepackage[utf8]{inputenc}
  \usepackage[english]{babel}
  \usepackage{amsmath} 
  \usepackage{amssymb}
  % \usepackage{amsthm}

  % extra math symbols and utilities
  \usepackage{mathtools}        % for extra stuff like \coloneqq
  \usepackage{mathrsfs}         % for extra stuff like \mathsrc{}
  \usepackage{centernot}        % for the centernot arrow 
  \usepackage{bm}               % for better boldsymbol/mathbf 
  \usepackage{enumitem}         % better control over enumerate, itemize
  \usepackage{hyperref}         % for hypertext linking
  \usepackage{fancyvrb}          % for better verbatim environments
  \usepackage{newverbs}         % for texttt{}
  \usepackage{xcolor}           % for colored text 
  \usepackage{listings}         % to include code
  \usepackage{lstautogobble}    % helper package for code
  \usepackage{parcolumns}       % for side by side columns for two column code
  

  % page layout
  \usepackage{fancyhdr}         % for headers and footers 
  \usepackage{lastpage}         % to include last page number in footer 
  \usepackage{parskip}          % for no indentation and space between paragraphs    
  \usepackage[T1]{fontenc}      % to include \textbackslash
  \usepackage{footnote}
  \usepackage{etoolbox}

  % for custom environments
  \usepackage{tcolorbox}        % for better colored boxes in custom environments
  \tcbuselibrary{breakable}     % to allow tcolorboxes to break across pages

  % figures
  \usepackage{pgfplots}
  \pgfplotsset{compat=1.18}
  \usepackage{float}            % for [H] figure placement
  \usepackage{tikz}
  \usepackage{tikz-cd}
  \usepackage{circuitikz}
  \usetikzlibrary{arrows}
  \usetikzlibrary{positioning}
  \usetikzlibrary{calc}
  \usepackage{graphicx}
  \usepackage{caption} 
  \usepackage{subcaption}

  % for tabular stuff 
  \usepackage{dcolumn}

  \usepackage[nottoc]{tocbibind}
  \pdfsuppresswarningpagegroup=1
  \hfuzz=5.002pt                % ignore overfull hbox badness warnings below this limit

% New and replaced operators
  \DeclareMathOperator{\Tr}{Tr}
  \DeclareMathOperator{\Sym}{Sym}
  \DeclareMathOperator{\Span}{span}
  \DeclareMathOperator{\std}{std}
  \DeclareMathOperator{\Cov}{Cov}
  \DeclareMathOperator{\Var}{Var}
  \DeclareMathOperator{\Corr}{Corr}
  \DeclareMathOperator{\Ent}{Ent}
  \DeclareMathOperator{\Lip}{Lip}
  \DeclareMathOperator*{\argmin}{\arg\!\min}
  \DeclareMathOperator*{\argmax}{\arg\!\max}
  \newcommand{\ket}[1]{\ensuremath{\left|#1\right\rangle}}
  \newcommand{\bra}[1]{\ensuremath{\left\langle#1\right|}}
  \newcommand{\braket}[2]{\langle #1 | #2 \rangle}
  \newcommand{\qed}{\hfill$\blacksquare$}     % I like QED squares to be black

% Custom Environments
  \newtcolorbox[auto counter, number within=section]{question}[1][]
  {
    colframe = orange!25,
    colback  = orange!10,
    coltitle = orange!20!black,  
    breakable, 
    title = \textbf{Question \thetcbcounter ~(#1)}
  }

  \newtcolorbox[auto counter, number within=section]{exercise}[1][]
  {
    colframe = teal!25,
    colback  = teal!10,
    coltitle = teal!20!black,  
    breakable, 
    title = \textbf{Exercise \thetcbcounter ~(#1)}
  }
  \newtcolorbox[auto counter, number within=section]{solution}[1][]
  {
    colframe = violet!25,
    colback  = violet!10,
    coltitle = violet!20!black,  
    breakable, 
    title = \textbf{Solution \thetcbcounter}
  }
  \newtcolorbox[auto counter, number within=section]{lemma}[1][]
  {
    colframe = red!25,
    colback  = red!10,
    coltitle = red!20!black,  
    breakable, 
    title = \textbf{Lemma \thetcbcounter ~(#1)}
  }
  \newtcolorbox[auto counter, number within=section]{theorem}[1][]
  {
    colframe = red!25,
    colback  = red!10,
    coltitle = red!20!black,  
    breakable, 
    title = \textbf{Theorem \thetcbcounter ~(#1)}
  } 
  \newtcolorbox[auto counter, number within=section]{proposition}[1][]
  {
    colframe = red!25,
    colback  = red!10,
    coltitle = red!20!black,  
    breakable, 
    title = \textbf{Proposition \thetcbcounter ~(#1)}
  } 
  \newtcolorbox[auto counter, number within=section]{corollary}[1][]
  {
    colframe = red!25,
    colback  = red!10,
    coltitle = red!20!black,  
    breakable, 
    title = \textbf{Corollary \thetcbcounter ~(#1)}
  } 
  \newtcolorbox[auto counter, number within=section]{proof}[1][]
  {
    colframe = orange!25,
    colback  = orange!10,
    coltitle = orange!20!black,  
    breakable, 
    title = \textbf{Proof. }
  } 
  \newtcolorbox[auto counter, number within=section]{definition}[1][]
  {
    colframe = yellow!25,
    colback  = yellow!10,
    coltitle = yellow!20!black,  
    breakable, 
    title = \textbf{Definition \thetcbcounter ~(#1)}
  } 
  \newtcolorbox[auto counter, number within=section]{example}[1][]
  {
    colframe = blue!25,
    colback  = blue!10,
    coltitle = blue!20!black,  
    breakable, 
    title = \textbf{Example \thetcbcounter ~(#1)}
  } 
  \newtcolorbox[auto counter, number within=section]{code}[1][]
  {
    colframe = green!25,
    colback  = green!10,
    coltitle = green!20!black,  
    breakable, 
    title = \textbf{Code \thetcbcounter ~(#1)}
  } 

  \BeforeBeginEnvironment{example}{\savenotes}
  \AfterEndEnvironment{example}{\spewnotes}
  \BeforeBeginEnvironment{lemma}{\savenotes}
  \AfterEndEnvironment{lemma}{\spewnotes}
  \BeforeBeginEnvironment{theorem}{\savenotes}
  \AfterEndEnvironment{theorem}{\spewnotes}
  \BeforeBeginEnvironment{corollary}{\savenotes}
  \AfterEndEnvironment{corollary}{\spewnotes}
  \BeforeBeginEnvironment{proposition}{\savenotes}
  \AfterEndEnvironment{proposition}{\spewnotes}
  \BeforeBeginEnvironment{definition}{\savenotes}
  \AfterEndEnvironment{definition}{\spewnotes}
  \BeforeBeginEnvironment{exercise}{\savenotes}
  \AfterEndEnvironment{exercise}{\spewnotes}
  \BeforeBeginEnvironment{proof}{\savenotes}
  \AfterEndEnvironment{proof}{\spewnotes}
  \BeforeBeginEnvironment{solution}{\savenotes}
  \AfterEndEnvironment{solution}{\spewnotes}
  \BeforeBeginEnvironment{question}{\savenotes}
  \AfterEndEnvironment{question}{\spewnotes}
  \BeforeBeginEnvironment{code}{\savenotes}
  \AfterEndEnvironment{code}{\spewnotes}

  \definecolor{dkgreen}{rgb}{0,0.6,0}
  \definecolor{gray}{rgb}{0.5,0.5,0.5}
  \definecolor{mauve}{rgb}{0.58,0,0.82}
  \definecolor{lightgray}{gray}{0.93}

  % default options for listings (for code)
  \lstset{
    autogobble,
    frame=ltbr,
    language=C,                           % the language of the code
    aboveskip=3mm,
    belowskip=3mm,
    showstringspaces=false,
    columns=fullflexible,
    keepspaces=true,
    basicstyle={\small\ttfamily},
    numbers=left,
    firstnumber=1,                        % start line number at 1
    numberstyle=\tiny\color{gray},
    keywordstyle=\color{blue},
    commentstyle=\color{dkgreen},
    stringstyle=\color{mauve},
    backgroundcolor=\color{lightgray}, 
    breaklines=true,                      % break lines
    breakatwhitespace=true,
    tabsize=3, 
    xleftmargin=2em, 
    framexleftmargin=1.5em, 
    stepnumber=1
  }

% Page style
  \pagestyle{fancy}
  \fancyhead[L]{Concentration of Measure}
  \fancyhead[C]{Muchang Bahng}
  \fancyhead[R]{Spring 2024} 
  \fancyfoot[C]{\thepage / \pageref{LastPage}}
  \renewcommand{\footrulewidth}{0.4pt}          % the footer line should be 0.4pt wide
  \renewcommand{\thispagestyle}[1]{}  % needed to include headers in title page

\begin{document}

\title{Concentration of Measure}
\author{Muchang Bahng}
\date{Spring 2024}

\maketitle
\tableofcontents
\pagebreak

An informal statement of concentration of measure is the following: \textit{If $X_1, \ldots, X_n$ are independent random variables, then the random variable $f(X_1, \ldots, X_n)$ is "close" to its mean $\mathbb{E}[f(X_1, \ldots, X_n)]$ provided that the function $f(x_1, \ldots, x_n)$ is not too "sensitive" to any of the coordinates $x_i$.} Intuitively, say that we have a bunch of independent random variables $X_i$ and sample from them, to get some values $x_i$. Calculating $f(x_1, \ldots, x_n)$, we have sampled from $f(X_1, \ldots, X_n)$. Since $f$ depends smoothly w.r.t. its arguments, to drastically change $f$, we must drastically change all the arguments. This is not likely, since all the $X_i$'s are independent. 

\section{High-Dimensional Geometry}

Most of our intuition about probability in low-dimensional spaces breaks down in high-dimensional ones (on the order of perhaps $10$ or $20$). We start off with two geometric examples in high-dimensional space. 

\begin{example}[Uniform Measure on Sphere]
Let $\mu_n$ be the uniform probability distribution on the $n$-sphere $\mathbf{S}^{n} \subset \mathbb{R}^{n+1}$. That is, let us consider any measurable set $A \subset \mathbb{S}^{n}$ such that $\mu_n (A) \geq 1/2$. Then, if we let $d(x, A)$ be the geodesic distance between $x \in \mathbb{S}^n$ and $A$ , we define the expanded set 
\[A_t = \{x \in \mathbb{S}^n \mid d(x, A) < t\}\]
and it turns out that 
\[\mu_n (A_t) \geq 1 - e^{- (n -1) t^2 / 2}\]
which states that given \textit{any} length $t > 0$, no matter how small, $A_t$ almost covers the whole space. Then, for large enough $n$, $\mu_n$ is highly concentrated around the equator. 
\end{example}

Note that the bounds decay \textit{exponentially} (or of greater order). 

\begin{example}[Uniform Measure on Cube]

\end{example}

\begin{example}[High Dimensional Gaussian]
Given iid $X_1, \ldots, X_n \sim \mathcal{N}(0, \sigma^2)$, then let $\mathbf{X}$ be the random $n$-vector of these random variables. Then, the random variable 
\[||\mathbf{X}|| = \sqrt{X_1^2 + \ldots, X_n^2}\]
has a distribution that is very concentrated around the expectation 
\[\mathbb{E}[||\mathbf{X}||] = \sqrt{\frac{n}{3}}\]
\end{example}

Naturally, this concentration phenomenon extends to random variables. 

\begin{example}
Let us have iid random variables $X_i$ with $\mathbb{P}(X_i = 1) = 1/2$ and $\mathbb{P}(X_i = -1) = 1/2$. Then, let's define $S_n = \sum_{i=1}^n X_i$. The strong law of large numbers tell us that 
\[\frac{S_n}{n} \xrightarrow{a.s.} 0\]
while the central limit theorem tells us that 
\[\frac{S_n}{\sqrt{n}} \xrightarrow{d} \mathcal{N}(0, 1)\]
since $\mathbb{E}[X_i] = 0$ and $\mathrm{Var}[X_i] = 1$. The CLT result shows us that the fluctuations (variance) of $S_n$ of are order $n$. However, note that $|S_n|$ can take values as large as $n$, so the maximum value of $S_n / n$ is of order $1$. If we measure $S_n$ using this scale, then $\frac{S_n}{n}$ is essentially $0$. The actual bound looks like 
\[\mathbb{P} \bigg( \frac{|S_n|}{n} \geq r \bigg) \leq 2 e^{-n r^2 / 2}\]
\end{example}

\section{Basic Concentration Inequalities}

\begin{lemma}[Markov's Inequality]
Given any random variable $X$, we have 
\[\mathbb{P}(X \geq \alpha) \leq \frac{\mathbb{E}[X]}{\alpha}\]
\end{lemma}

\begin{lemma}[Chebyshev's Inequality]
Given $X$ with finite variance and expectation, we have 
\[\mathbb{P}(|X - \mathbb{E}[X]| \geq \alpha) \leq \frac{\Var[X]}{\alpha^2}\]
\end{lemma}

An inequality that we will use often in proofs is Jensen's inequality. 

\begin{lemma}[Jensen's Inequality]
Given a convex function $g: \mathbb{R} \rightarrow \mathbb{R}$ and random variable $X$, we have 
\[g(\mathbb{E}[X]) \leq \mathbb{E}[g(X)]\]
\end{lemma}
\begin{proof}
We will assume that $f$ is differentiable for simplicity and let $\mathbb{E}[X] = \mu$. Define the linear function centered at $\mu$ to be $l(x) \coloneqq f(\mu) + f^\prime (\mu) (x - \mu)$. Then, we know that $f(x) \geq l(x)$ for all $x$, so 
\begin{align*}
    \mathbb{E}[f(X)] & \geq \mathbb{E}[ l(X)] \\ 
    & = \mathbb{E}[f(\mu) + f^\prime (\mu) \, (X - \mu)] \\
    & = \mathbb{E}[f(\mu)] + f^\prime (\mu) ( \mathbb{E}[X] - \mu) \\
    & = \mathbb{E}[f(\mu)] \\
    & = f(\mathbb{E}[X])
\end{align*}
\end{proof}

\begin{definition}[Lipschitz Continuity]
A function $f: (X, d_X) \longrightarrow (Y, d_Y)$ is \textbf{Lipschitz continuous}, with Lipschitz constant $A$, if it satisfies 
\[d_Y \big( f(\mathbf{x}), f(\mathbf{y})\big) \leq A \, d_X (\mathbf{x}, \mathbf{y})\]
for all $\mathbf{x}, \mathbf{y} \in X$. 
\end{definition}

\subsection{Talagrand's Gaussian Inequality}

\begin{lemma}[Gaussian Integration by Parts Formula]
For Gaussian random variables $x, x_1, \ldots, x_n$ and a function $F$ of moderate growth at infinity, we have 
\[\mathbb{E}\big[ x \, F(x_1, \ldots, x_n) \big] = \sum_{i=1}^n \mathbb{E}[x \, x_i] \; \mathbb{E}\bigg[ \frac{\partial F}{\partial x_i} (x_1, \ldots, x_n) \bigg]\]
\end{lemma}

\begin{theorem}[Talagrand's Gaussian Inequality]
Consider a Lipschitz function $F: \mathbb{R}^N \longrightarrow \mathbb{R}$ (with Lipschitz constant $A$). Let $x_1, \ldots, x_N \sim \mathcal{N}(0, 1)$ be iid, and let $\mathbf{x} = (x_1, \ldots, x_N)$. Then, for each $t > 0$, we have 
\[\mathbb{P} \big( | F(\mathbf{x}) - \mathbb{E} F(\mathbf{x}) | \geq t \big) \leq 2 \exp \bigg(- \frac{t^2}{4A^2} \bigg)\]
\end{theorem}
\begin{proof}
For this proof, we assume that $F$ is not only Lipschitz, but $C^2$. This is the case in most applications of this theorem, and if it is not the case, then we can regularize $F$ by convolving with a smooth function to solve the problem. We begin with a parameter $s$ and consider the function $G: \mathbb{R}^{2N} \longrightarrow \mathbb{R}$ defined 
\[G(z_1, \ldots, z_{2N}) = \exp \Big( s \big[ F ( z_1, \ldots, z_N) - F(z_{N+1}, \ldots, z_{2N}) \big] \Big)\]
For clarity, we will denote variables of $F$ with $x_i$ and variables of $G$ with $z_i$. Let $u_1, \ldots, u_{2N} \sim \mathcal{N}(0, 1)$ be iid, and let $v_1, \ldots, v_n \sim \mathcal{N}(0, 1)$ be iid, with $v_{N+1}, \ldots, v_{2N}$ copies of the first $N$. For shorthand, we can denote the collection as $\mathbf{u}$ and $\mathbf{v}$. Then, we have 
\[\mathbb{E} [u_i u_j] - \mathbb{E}[ v_i v_j] = 0\]
except when $j = i + M$ or $i = j + M$, in which case we have 
\[\mathbb{E} [u_i u_j] - \mathbb{E}[ v_i v_j] = 0 - 1 = -1\]
since $v_i v_j = X^2$, where $X \sim \mathcal{N}(0, 1) = \chi^2_1$, a Chi-Squared distribution with 1 degree of freedom. We consider the transformed random variable
\[\mathbf{f}(t) \coloneqq \sqrt{t} \, \mathbf{u} + \sqrt{1 - t} \, \mathbf{v} \sim \mathcal{N}(0, 1) \text{ for all } t\]
that is essentially some smooth path from $\mathbf{f}(0) = \mathbf{u}$ and $\mathbf{f}(1) = \mathbf{v}$. Note that given some $t \in [0, 1]$, $\mathbf{f}(t)$ is some random vector, $G ( \mathbf{f}(t))$ is some random variable, and $\mathbb{E}[ G(\mathbf{f}(t))]$ is some number. We can define the function $\phi: [0, 1] \longrightarrow \mathbb{R}$ as 
\begin{align*}
    \phi(t) = \mathbb{E} [G (\mathbf{f}(t))] & = \int_\mathbb{R} x \; p_{G(f(t))} (x) \,dx \\
    & = \int_{\mathbb{R}^{2N}} G(y) \; p_{f(t)} (y) \,dy 
\end{align*}
where $p_X$ is the PDF of the distribution $X$. Take the derivative with respect to $t$ to get the first line, and we can simplify using Gaussian integration by parts 
\begin{align*}
    \phi^\prime (t) & \mathbb{E}\bigg[ \sum_{i=1}^{2N} \frac{d}{dt} f_i (t) \; \frac{\partial G}{\partial z_i} \big( \mathbf{f}(t)\big) \bigg] \\
    & = \sum_{i=1}^{2N} \mathbb{E} \bigg[ \frac{d}{dt} f_i (t) \, \frac{\partial G}{\partial z_i} \big( \mathbf{f}(t)\big)\bigg] \\
    & = \sum_{i=1}^{2N} \sum_{j=1}^{2N} \mathbb{E} \bigg[ \Big( \frac{\partial}{\partial t} f_i (t) \Big) \, f_i (t) \bigg] \; \mathbb{E} \bigg[ \frac{\partial^2 G}{\partial z_i \partial z_{j}} \mathbf{f} (t) \bigg] 
\end{align*}
But we can simplify 
\begin{align*}
    \mathbb{E} \bigg[ \Big( \frac{\partial}{\partial t} f_i (t) \Big) \, f_i (t) \bigg] & = \mathbb{E} \bigg[ \Big( \frac{1}{2 \sqrt{t}} u_i - \frac{1}{2 \sqrt{1 - t}} v_i \Big) \big( \sqrt{t} u_j - \sqrt{1 - t} \, v_j \big) \bigg] \\
    & = \frac{1}{2} \big(\mathbb{E}[ u_i u_j] - \mathbb{E}[v_i v_j] \big) = \begin{cases} -1 & \text{ if } j = i + M , i = j + M \\
    0 & \text{ else} \end{cases} 
\end{align*}
So, we can simplify the above to
\[\phi^\prime (t) = - \mathbb{E} \bigg[  \sum_{i=1}^N \frac{\partial^2 G}{\partial z_i \, \partial z_{i + M}} \big( \mathbf{f}(t)\big) \bigg]\]
and computing the second derivative using the chain rule gives 
\begin{align*}
    \frac{\partial G}{\partial z_i} (\mathbf{z}) & = \frac{\partial G}{\partial F} \frac{\partial F}{\partial x_i} (z_1, \ldots, z_N) \\
    & = s \; G(\mathbf{z}) \, \frac{\partial F}{\partial x_i} (z_1, \ldots, z_N) \\
    \frac{\partial^2 G}{\partial z_i \partial z_{i + N}} (\mathbf{z}) & = - s^2 \, G(\mathbf{z}) \, \frac{\partial F}{\partial x_i} (z_1, \ldots, z_N) \, \frac{\partial F}{\partial x_i} (z_{N+1}, \ldots, z_{2N}) 
\end{align*}
for all $\mathbf{z}$. So we have for all $t \in [0, 1]$, 
\begin{align*}
    \phi^\prime (t) & = s^2 \, \mathbb{E} \bigg[ \sum_{i=1}^N G(\mathbf{f}(t)) \, \frac{\partial F}{\partial x_i} \big( f_1 (t), \ldots, f_N (t) \big) \, \frac{\partial F}{\partial x_i} \big( f_{N+1} (t), \ldots, f_{2N} (t) \big) \bigg] \\ 
    & \leq s^2 \mathbb{E} \bigg[ G(\mathbf{f}(t)) \sum_{i=1}^N \frac{\partial F}{\partial x_i} \big( f_1 (t), \ldots, f_N (t) \big) \, \frac{\partial F}{\partial x_i} \big( f_{N+1} (t), \ldots, f_{2N} (t) \big) \bigg] \\ 
    & \leq s^2 \mathbb{E} \big[ G(\mathbf{f}(t) \big) \, A^2 \big] \\
    & \leq s^2 A^2 \mathbb{E}[G(\mathbf{f}(t))] = s^2 A^2 \phi(t)
\end{align*}
Solving the inequality for $\phi$ gives 
\begin{align*}
    \phi^\prime (t) / \phi(t) \leq s^2 A^2 & \implies \int \phi^\prime (t) / \phi(t) \,dt \leq \int s^2 A^2 \,dt \\
    & \implies \log{\phi(t)} \leq s^2 A^2 t + C \\
    & \implies \phi(t) \leq e^{s^2 A^2 t} \leq e^{s^2 A^2}
\end{align*}
Recalling that $\mathbf{f}(1) = \mathbf{u}$, we have 
\[\mathbb{E}[\exp\{ s ( F(u_1, \ldots, u_N) - F(u_{N+1}, \ldots, u_{2N})) \}] \leq e^{s^2 A^2}\]
and by independence of the $u_i$'s, the LHS equals $\mathbb{E}[e^{s F(u_1, \ldots, u_N)}]\, \mathbb{E}[e^{-s F(u_{N+1}, \ldots, u_{2N})}]$ and by Jensen's inequality, we have $\mathbb{E}[e^{-s F(u_{N+1}, \ldots, u_{2N})}] \geq e^{-s \mathbb{E}[F(u_{N+1}, \ldots, u_{2N})]}$. We can derive as follows: 
\begin{align*}
    e^{s^2 A^2} & \geq \mathbb{E}[e^{s F(u_1, \ldots, u_N)}]\, \mathbb{E}[e^{-s F(u_{N+1}, \ldots, u_{2N})}] \\
    & \geq \mathbb{E}[e^{s F(u_1, \ldots, u_N)}]\, e^{-s \mathbb{E}[F(u_{N+1}, \ldots, u_{2N})]} \\
    & = \mathbb{E}[e^{s F(u_1, \ldots, u_N)}]\, \mathbb{E}[e^{-s \mathbb{E}[F(u_{N+1}, \ldots, u_{2N})]}] \\
    & = \mathbb{E}[e^{s F(u_1, \ldots, u_N) -s \mathbb{E}[F(u_{N+1}, \ldots, u_{2N})]}] \\
    & = \mathbb{E}[\exp \big( s F(u_1, \ldots, u_N) -s \mathbb{E}[F(u_{N+1}, \ldots, u_{2N})] \big) ]
\end{align*}
and by Markov's inequality, we get for a random vector of standard Gaussian random variables $\mathbf{x}$
\begin{align*}
    \mathbb{P} \big( F(\mathbf{x}) - \mathbb{E}[F(\mathbf{x})] \geq t) & = \mathbb{P} \big( e^{s( F(\mathbf{x}) - \mathbb{E}[F(\mathbf{x})]} \geq e^{st} \big) \\
    & \leq \frac{\mathbb{E}[e^{s( F(\mathbf{x}) - \mathbb{E}[F(\mathbf{x})]}]}{e^{st}} \\
    & \leq e^{s^2 A^2 - st} \\
    & = e^{- t^2 / 4A^2} \text{ when } s = t / 2A^2
\end{align*}
\end{proof}

\pagebreak 

\section{Variance Bounds and Poincare Inequalities}

Let us first describe this concentration phenomenon by investigating bounds on the variance 
\[\mathrm{Var}[f(x_1, \ldots, x_n)] \coloneqq \mathbb{E}\big[ \big( f(x_1, \ldots, x_n) - \mathbb{E}[f(x_1, \ldots, x_n)] \big)^2 \big] \]
We can first bound 
\[\Var[f(X_1, \ldots, X_n)] = \mathbb{E}\big[ \big( f(X_1, \ldots, X_n)\big)^2 \big] - \mathbb{E}\big[ f(X_1, \ldots, X_n) \big]^2 \leq \mathbb{E}\big[ \big( f(X_1, \ldots, X_n)\big)^2 \big]\]
and since adding a constant term to $f$ doesn't affect the variance, we can utilize this to get our first variance bound. 

\begin{lemma}
Let $\mathbf{X}$ be a random variable or vector. Then, 
\[\mathrm{Var}[f(\mathbf{X})] \leq \mathbb{E} \big[ \big( f(\mathbf{X}) - \inf f \big)^2 \big] \text{ and } \mathrm{Var}[f(\mathbf{X})] \leq \mathbb{E} \big[ \big(\sup f - f(\mathbf{X}) \big)^2 \big]\]
and 
\[\mathrm{Var}[ f(\mathbf{X})] \leq \frac{1}{4} ( \sup f - \inf f)^2\]
\end{lemma}
\begin{proof} 
Since $\mathrm{Var}[\mathbf{X}] = \mathbb{E}[\mathbf{X}^2] - \mathbb{E}[\mathbf{X}]^2$ from above, we have 
\[\mathrm{Var}[ f(\mathbf{X})] = \mathrm{Var}[f(\mathbf{X}) - a] = \mathbb{E}[(f(\mathbf{X}) - a)^2] - \mathbb{E}[f(\mathbf{X}) - a]^2 \leq \mathbb{E}[(f(\mathbf{X}) - a)^2]\]
By letting $a = \inf f$, we get the first inequality. By letting $a = (\sup f + \inf f) /2$ be the "middle" of $f$, we have $|f(\mathbf{X}) - a| \leq (\sup f - \inf f)/2 \implies [f(\mathbf{X}) - a]^2 \leq (\sup f - \inf f)^2/4$, and so 
\[\Var[ f(\mathbf{X})] \leq \mathbb{E}[(f(\mathbf{X}) - a)^2] \leq \frac{1}{4} (\sup f - \inf f)^2\]
which gives our third inequality. We can also see that 
\[\mathrm{Var}[ f(\mathbf{X})] = \Var[ -f (\mathbf{X})] = \Var[ b - f(\mathbf{X})] \leq \mathbb{E}[ (b - f(\mathbf{X}))^2]\]
to get our second. 
\end{proof} 

This allows us to bound the random vector $f(\mathbf{X})$ if $f$ itself is bounded, no matter what $\mathbf{X}$ is. But this generally turns out to be a very conservative bound, which is unsurprising since we assume so little about $\mathbf{X}$. For example, if we let $X_1, \ldots, X_n$ be iid random variables taking values in $[-1, 1]$, and let $f(x_1, \ldots, x_n) = \frac{1}{n} \sum_{i=1}^n x_i$. Then, $f$ takes values in $[-1, 1]$, and by the previous lemma, we have
\[\mathrm{Var}[f(X_1, \ldots, X_n)] \leq \frac{1}{4} (1 - (-1))^2 = 1\]
which looks good, until we see that we can derive a better bound from direct computation (which becomes much better as $n$ increases). 
\[\mathrm{Var}[f(X_1, \ldots, X_n)] = \frac{1}{n^2} \sum_{i=1}^n \mathrm{Var}[X_i] = \frac{1}{n}\]
However, this computation assumes independence of $X_i$'s, which the previous lemma doesn't. This is the reason we're able to get a better bound, since if we took $n$ copies of the same $X$, we would have 
\[\mathrm{Var}[f(X_1, \ldots, X_n)] = \mathrm{Var}[n X / n] = \mathrm{Var}[X] = 1\]
Therefore, we will capitalize on the independence of these random variables in high dimensions to obtain better bounds. Now in the next result, we shall show that the variance of a high dimensional $f(X_1, \ldots, X_n)$ can be bounded by the variances of each random variable. Those quantities, like the variance, that behave well in high dimensions is said to \textit{tensorize}. 

Consider independent random variables $X_1, \ldots, X_n$ and a function $f: \mathbb{R}^n \longrightarrow \mathbb{R}$. If we fix values $x_1, \ldots, x_n$, then we can define for all $k = 1, \ldots, n$ the function $g_{k}(x_1, \ldots, x_{k-1}, x_{k+1}, \ldots, x_n): \mathbb{R} \rightarrow \mathbb{R}$ as 
\[g_{k}(x_1, \ldots, x_{k-1}, x_{k+1}, \ldots, x_n)(z) = f(x_1, \ldots, x_{k-1}, z, x_{k+1}, \ldots, x_n)\]
where 
\[(g_{k}(x_1, \ldots, x_{k-1}, x_{k+1}, \ldots, x_n))^\prime (z) = \frac{\partial}{\partial x_k} f(x_1, \ldots, x_{k-1}, z, x_{k+1}, \ldots, x_n)\]
and $g_k (x_1, \ldots, x_{k-1}, x_{k+1}, \ldots, x_n) (X_k)$ is a random variable of $X_k$. Then, we can define 
\begin{align*}
    \Var_k f(x_1, \ldots, x_n) & = \Var_{X_k} [ f(x_1, \ldots, x_{k-1}, X_k, x_{k+1} \ldots, x_n)] \\ 
    & = \mathbb{E}_{X_k} \big[ \big( f (x_1, \ldots, x_{k-1}, X_k, x_{k+1}, \ldots, x_n) - \mathbb{E}_{X_k} [f (x_1, \ldots, x_{k-1}, X_k, x_{k+1}, \ldots, x_n)] \big)^2 \big] \\
    & = \Var[g_{k} (x_1, \ldots, x_{k-1}, x_{k+1}, \ldots, x_n) (X_k)] \\
    & = \Var_{X_k} [g(x_1, \ldots, x_{k-1}, x_{k+1}, \ldots, x_n)] 
\end{align*}
which takes the variance of $f$ with respect to $X_k$, keeping all other variables fixed. However, this value will change for different $x_1, \ldots, x_n$'s, and so we can loosen the restriction that they are fixed. We can take 
\[g_k (X_1, \ldots, X_{k-1}, X_{k+1}, \ldots, X_n) (z) = f(X_1, \ldots, X_{k-1}, z, X_{k+1}, \ldots, X_n)\]
where $g_k (X_1, \ldots, X_{k-1}, X_{k+1}, \ldots, X_n) (X_k)$ is a random variable of $X_1, \ldots, X_n$. Now if we calculate its partial variance, we get 
\begin{align*}
    \Var_k f(X_1, \ldots, X_n) & = \Var_{X_k} [f(X_1, \ldots, X_k, \ldots, X_n)]\\
    & = \Var [g_k (X_1, \ldots, X_{k-1}, X_{k+1}, \ldots, X_n) (X_k)] \\
    & = \Var_{X_k} [g_k (X_1, \ldots, X_{k-1}, X_{k+1}, \ldots, X_n)]
\end{align*}
which is now a random variable of all $X_i$'s, $i \neq k$, that outputs the variance of $f$ with respect to $X_k$. \textbf{But is it true that }
\[\mathbb{E}_{X_k} [ f(X_1, \ldots, X_n)] = \mathbb{E}[ f(X_1, \ldots, X_n) \mid X_1, \ldots, X_{k-1}, X_{k+1}, \ldots, X_n] ?\]


Now, we can show a very useful property of variance: that the variance of some arbitrary function can be bounded by the expected sum of the partial variances. 

\begin{theorem}[Tensorization of Variance]
That is, $\Var_i f(\mathbf{x})$ is the variance of $f(X_1, \ldots, X_n)$ w.r.t. the variable $X_i$ only, the remaining variables kept fixed. Then, we have 
\[\Var[f(X_1, \ldots, X_n)] \leq \mathbb{E} \bigg[ \sum_{i=1}^n \Var_i f(X_1, \ldots, X_n) \bigg] \]
\end{theorem}
\begin{proof}
We try to mimic the fact that the variance of the sum of independent random variables is the sum of the variances. At first sight, the general function $f(x_1, \ldots, x_n)$ need not look anything like a sum, but we can expand it as a telescoping sum of random variables. We will prove this using the \textit{martingale method}, which constructs this random variable $f(X_1, \ldots, X_n)$ as a sum of finer and finer increments starting from the "coarse" constant function $\mathbb{E}[f(X_1, \ldots, X_n)]$. We define the random variable 
\[\Delta_k \coloneqq \mathbf{E}[ f(X_1, \ldots, X_n) \mid X_1, \ldots X_k] - \mathbb{E}[ f(X_1, \ldots, X_n) \mid X_1, \ldots, X_{k-1}]\]
Then, we can express 
\[f( X_1, \ldots, X_n) - \mathbf{E}[ f(X_1, \ldots, X_n)] = \sum_{k=1}^n \Delta_k\]
Note that $\mathbb{E}[\Delta_k \mid X_1, \ldots, X_{k-1}] = 0$ (i.e. $\Delta_k$'s are martingale increments). In particular, even though the $\Delta_k$'s are not independent, if we have $l < k$, then 
\begin{align*}
    \mathbb{E}[ \Delta_k \Delta_l] & = \mathbb{E}[ \mathbb{E}[\Delta_k \Delta_l \mid X_1, \ldots, X_{k-1}]] \\
    & = \mathbb{E}[ \mathbb{E}[\Delta_k \mid X_1, \ldots X_{k-1} ] \, \mathbb{E}[\Delta_l \mid X_1, \ldots X_{k-1} ]] \\
    & = \mathbb{E}[ \mathbb{E}[\Delta_k \mid X_1, \ldots X_{k-1} ] \, \Delta_l] \\
    & = \mathbb{E}[0 \cdot \Delta_l] = 0
\end{align*}
and so, the variance can be expanded into terms that vanish. 
\begin{align*}
    \Var[ f(X_1, \ldots, X_n)] & = \mathbb{E} \big[ \big( f( X_1, \ldots, X_n) - \mathbf{E}[ f(X_1, \ldots, X_n)] \big)^2\big] \\
    & = \mathbb{E} \bigg[ \bigg( \sum_{k=1}^n \Delta_k \bigg)^2 \bigg] = \sum_{k=1}^n \mathbb{E}[ \Delta_k^2]
\end{align*}
Now it remains to show that $\mathbb{E}[\Delta_k^2] \leq \mathbb{E}[\Var_k f(X_1, \ldots, X_n)]$ for every $k$. Let us define 
\[\Tilde{\Delta}_k = f(X_1, \ldots, X_n) - \mathbb{E}[f(X_1, \ldots, X_n) \mid X_1, \ldots, X_{k-1}, X_{k+1}, \ldots, X_n]\]
to be the approximation of $f(X_1, \ldots, X_n)$ "one step" before the final increment. Then, we have 
\[\Delta_k = \mathbb{E}[\Tilde{\Delta}_k \mid X_1, \ldots, X_k]\]
and as $X_k$ and $X_1, \ldots, X_{k-1}, X_{k+1}, \ldots, X_n$ are independent, we have 
\[\Var_k f(X_1, \ldots, X_n) = \mathbb{E}[\Tilde{\Delta}_k^2 \mid X_1, \ldots, X_{k-1}, X_{k+1}, \ldots, X_n] \]
and therefore using Jensen's inequality we can prove 
\[\mathbb{E}[\Delta_k^2] = \mathbb{E}[\mathbb{E}[ \Tilde{\Delta}_k \mid X_1, \ldots, X_k]^2 ] \leq \mathbb{E}[\Tilde{\Delta}_k^2] = \mathbb{E}[\Var_k f(X_1, \ldots, X_n)]\]
\end{proof}

What we want to eventually do is prove an inequality of the form where for any function $h: \mathbb{R} \rightarrow \mathbb{R}$ and some $X \sim \mu$, 
\[\Var_\mu[h] = \Var [h(X)] \leq ||\mathcal{L}(h)||^2_{L^2 (\mu)}\]
where $\mathcal{L}$ is an operator on $h$. This will allow us to bound 
\[\Var [g_k (x_1, \ldots, x_{k-1}, x_{k+1}, \ldots, x_n)(X_k)] \leq ||\mathcal{L}(g_k (x_1, \ldots, x_{k-1}, x_{k+1}, \ldots, x_n))||^2\]
for all $x_1, \ldots, x_n$, simply by taking $h = g(x_1, \ldots, x_{k-1}, x_{k+1}, \ldots, x_n)$. Since this works for all $x_1, \ldots, x_n$, we can claim that this inequality holds for all $X_1 (\omega), \ldots, X_n (\omega)$ for all $\omega \in \Omega$. That is, we can loosen the fixed values into random variables. 
\begin{align*}
    \Var_{k} f(X_1, \ldots, X_n) & = \Var[g_k (X_1, \ldots, X_{k-1}, X_{k+1}, \ldots, X_n)(X_k)] \\
    & \leq || \mathcal{L}(g_k (X_1, \ldots, X_{k-1}, X_{k+1}, \ldots, X_n))||^2_{L^2 (\mu)} 
\end{align*}
Note that all terms are random variables of $X_1, \ldots, X_n$, and so the same inequality holds for their expectations over the entire joint measure. 
\[\mathbb{E}[ \Var_{k} f(X_1, \ldots, X_n) ] \leq \mathbb{E} \big[ || \mathcal{L}(g_k (X_1, \ldots, X_{k-1}, X_{k+1}, \ldots, X_n))||^2_{L^2 (\mu)} \big] \]
and so by tensorization (i.e. summing them up), we get 
\[\Var[f(X_1, \ldots, X_n)] \leq \sum_{i=1}^n \mathbb{E} \big[ \Var_i f(X_1, \ldots, X_n) \big] \leq \sum_{i=1}^n \mathbb{E} \big[ || \mathcal{L}(g_k (X_1, \ldots, X_{k-1}, X_{k+1}, \ldots, X_n))||^2_{L^2 (\mu)} \big] \]

Furthermore, this bound is sharp when $f$ is linear. Let us demonstrate this by letting $f(x_1, \ldots, x_n) = a_1 x_1 + \ldots + a_n x_n$. On the left hand side, we have 
\[\Var[ f(X_1, \ldots, X_n)] = \Var\bigg[ \sum_{i=1}^n a_i X_i \bigg] = \sum_{i=1}^n a_i^2 \Var[X_i] \]
and on the right hand side, each component divides up to 
\begin{align*}
    \Var_i f(x_1, \ldots, x_n ) & = \Var[ f(x_1, \ldots, X_i, \ldots, x_n)] \\
    & = \Var[ a_1 x_1 + \ldots + a_i X_i + \ldots a_n x_n] \\
    & = \Var[a_i X_i] \\
    & = a_i^2 \Var[X_i]
\end{align*}
\textbf{Then?} Note that since $f$ is linear, the values of all $x_j, j \neq i$ have no effect on the variance of $X_i$, and so $\Var_i f(X_1, \ldots, X_n)$, which is originally a random variable of $X_1, \ldots, X_{i-1}, X_{i+1}, \ldots, X_n$, is really just the constant (random variable) $a_i^2 \Var[X_i]$. This is because no matter what values $X_1, \ldots, X_{i-1}, X_{i+1}, \ldots, X_n$ are realized, these values will only contribute to a translation of the random variable $f(X_1, \ldots, X_n)$, and hence will not affect the variance w.r.t. $X_i$. So, the right hand side also becomes 
\[\mathbb{E} \bigg[ \sum_{i=1}^n \Var_i f(X_1, \ldots, X_n) \bigg] = \mathbb{E} \bigg[ \sum_{i=1}^n a_i^2 \Var[X_i] \bigg] = \sum_{i=1}^n a_i^2 \Var[X_i]\]
which is the same as the LHS. 

We can view the tensorization of the variance in itself as an expression of the concentration phenomenon. $\Var_i f (\mathbf{x})$ quantifies the sensitivity of the function $f(\mathbf{x})$ of the coordinate $x_i$ in a distribution-dependent manner. If this sensitivity w.r.t. each coordinate ($\mathbb{E}[ \Var_i f(X_1, \ldots, X_n)]$) is small, then $f(X_1, \ldots, X_n)$ is close to its mean. However, it might not be so straightforward to compute $\Var_i f$, since it depends on both the function $f$ and on the distribution of $X_i$. So, we can try combining this with a suitable bound on the component-wise variance. 

Let us define the quantities: 
\[D_i f (\mathbf{x}) \coloneqq \sup_z f(x_1, \ldots, x_{i-1}, z, x_{i+1}, \ldots, x_n) - \inf_z f(x_1, \ldots, x_{i-1}, z, x_{i+1}, \ldots, x_n)\]
and 
\[D_i^- f(\mathbf{x}) \coloneqq f(x_1, \ldots, x_n) - \inf_z f(x_1, \ldots, x_{i-1}, z, x_{i+1}, \ldots, x_n)\]
which quantifies the sensitivity of the function $f$ to the coordinate $x_i$ in a distribution-independent manner. Now we can introduce the following bounds. 

\begin{corollary}
We have 
\[\Var[ f(X_1, \ldots, X_n)] \leq \frac{1}{4} \mathbb{E} \bigg[ \sum_{i=1}^n \big( D_i f(X_1, \ldots, X_n) \big)^2 \bigg] \]
\end{corollary}
\begin{proof}
We start off with 
\begin{align*}
    \Var_i f (X_1, \ldots, X_n) & = \Var[ f(X_1, \ldots, X_i, \ldots, X_n)] \\
    & \leq \frac{1}{4} \big( D_i f (X_1, \ldots, X_n)\big)^2 
\end{align*}
Since these a random variables follow this inequality (for all $\omega \in \Omega$), we can attach an expectation on them to get 
\[\mathbb{E}[\Var_i f (X_1, \ldots, X_n)] \leq \mathbb{E} \bigg[ \frac{1}{4} \big( D_i f (X_1, \ldots, X_n)\big)^2\bigg] \]
and substituting in the previous theorem gives 
\begin{align*}
    \Var[f(X_1, \ldots, X_n)] & \leq \mathbb{E} \bigg[ \sum_{i=1}^n \Var_i f(X_1, \ldots, X_n) \bigg] \\
    & = \sum_{i=1}^n \mathbb{E}\big[ \Var_i f(X_1, \ldots, X_n) \big] \\
    & \leq \sum_{i=1}^n \mathbb{E} \bigg[ \frac{1}{4} \big( D_i f (X_1, \ldots, X_n)\big)^2\bigg] \\
    & = \frac{1}{4} \mathbb{E} \bigg[ \sum_{i=1}^n \big( D_i f(X_1, \ldots, X_n) \big)^2 \bigg] 
\end{align*}
\end{proof}

\begin{example}[Random Matrices]

\end{example}



\begin{exercise}[Banach-Valued Sums]
Let $X_1, X_2, \ldots, X_N$ be independent random variables with values in a Banach space $(B, ||\cdot ||_B)$. Suppose these random variables are bounded in the sense that $||X_i||_B \leq C$ a.s. for every $i$. Show that 
\[\Var\bigg( \bigg| \bigg| \frac{1}{n} \sum_{k=1}^n X_k \bigg| \bigg|_B \bigg) \leq \frac{C^2}{n}\]
This is a simple vector-valued variant of the elementary fact that the variance of $\frac{1}{n} \sum_{k=1}^n X_k$ for real-valued random variables $X_k$ is of order $\frac{1}{n}$. 
\end{exercise}
\begin{solution}
We can tensorize the variance to get 
\begin{align*}
    \Var_k \bigg| \bigg| \frac{1}{n} \sum_{k=1}^n X_k \bigg| \bigg|_B & = \Var \bigg| \bigg| \frac{1}{n} X_k \bigg| \bigg|_B = \frac{1}{n^2} \Var ||X_k||_B \\
    & \leq \frac{1}{n^2} \bigg( \frac{1}{4} (C - (-C))^2 \bigg) = \frac{C^2}{n^2} 
\end{align*}
and so letting $f(X_1, \ldots, X_n) = \big| \big| \frac{1}{n} \sum_{k=1}^n X_k \big| \big|_B$, we get 
\begin{align*}
    \Var [f(X_1, \ldots X_n)] & \leq \sum_{k=1}^n \mathbb{E}[ \Var_k f (X_1, \ldots, X_n)] \\
    & \leq \sum_{k=1}^n \frac{C^2}{n^2} = \frac{C^2}{n} 
\end{align*}
\end{solution}

\begin{exercise}[Rademacher Processes]
Let $\epsilon_1, \ldots, \epsilon_n$ be independent symmetric Bernoulli random variables $\mathbb{P}(\epsilon_i = \pm 1) = \frac{1}{2}$ (also called Rademacher variables), let $T \subset \mathbb{R}^n$. The following identity is completely trivial: 
\[\sup_{t \in T} \Var \bigg[ \sum_{k=1}^n \epsilon_k t_k \bigg] = \sup_{t \in T} \sum_{k=1}^n t_k^2\]
Prove the following nontrivial fact: 
\[\Var \bigg[ \sup_{t \in T} \sum_{k=1}^n \epsilon_k t_k \bigg] \leq 4 \sup_{t \in T} \sum_{k=1}^n t_k^2\]
\end{exercise}
\begin{solution}
Let us consider a fixed $\boldsymbol{\epsilon} = (\epsilon_1, \ldots, \epsilon_n)$ and index $i \in [n]$. Then, consider the random variable formed by taking the value $f(\epsilon_1, \ldots, \epsilon_n)$ and loosening $\epsilon_i$ to be an random variable. That is, 
\begin{align*}
    \mathbb{P} \Big[ f(\epsilon_1, \ldots, \epsilon_n) = \sup_{t \in T} \{\epsilon_1 t_1 + \ldots + 1 t_i + \ldots + \epsilon_n t_n\} \Big] = \frac{1}{2} \\
    \mathbb{P} \Big[ f(\epsilon_1, \ldots, \epsilon_n) = \sup_{t \in T} \{\epsilon_1 t_1 + \ldots - 1 t_i + \ldots + \epsilon_n t_n\} \Big] = \frac{1}{2} 
\end{align*}
Then, we compute 
\[D_i^- f (\epsilon_1, \ldots, \epsilon_n) = \inf_{\epsilon_i \in \{-1, 1\}} \sup_{t \in T} \sum_{k=1}^n \epsilon_k t_k\]
and we can estimate 
\begin{align*}
    D_i^- f(\boldsymbol{\epsilon}) & = f(\epsilon_1, \ldots, \epsilon_n) - D_i f (\epsilon_1, \ldots, \epsilon_n) \\
    & = \sup_{t \in T} \sum_{k=1}^n \epsilon_k t_k - \inf_{\epsilon_i \in \{-1, 1\}} \sup_{t \in T} \sum_{k=1}^n \epsilon_k t_k \\
    & \leq \sup_{t \in T} 2 |t_i| 
\end{align*}
We can finally bound 
\begin{align*}
    \Var[ f(\epsilon_1, \ldots, \epsilon_n)] & \leq \mathbb{E} \bigg[ \sum_{i=1}^n \big( D_i^- f(\boldsymbol{\epsilon})\big)^2 \bigg] \\
    & \leq 4 \mathbb{E} \bigg[ \sum_{i=1}^n \sup_{t \in T} t_i^2 \bigg] \\
    & = 4 \sup_{t \in T} \sum_{i=1}^n t_i^2 
\end{align*}
\end{solution}

\begin{exercise}[Bin Packing]
This is a classical application of bounded difference inequalities. Let $X_1, \ldots, X_n$ i.i.d. random variables with values in $[0, 1]$. Each $X_i$ represents the size of a package to be shipped. The shipping containers are bins of size $1$ (so each bin can hold a set packages whose sizes sum to at most $1$). Let $B_n = f(X_1, \ldots, X_n)$ be the minimal number of bins needed to store the packages. Note that computing $B_n$ is a hard combinatorial optimization problem, but we can bound its mean and variance by easy arguments. 
\begin{enumerate}
    \item Show that $\Var[B_n] \leq n/4$
    \item Show that $\mathbb{E}[B_n] \geq n \mathbb{E}[X_1]$
\end{enumerate}
Thus the fluctuations $\sim \sqrt{n}$ of $B_n$ are much smaller than its magnitude $\sim n$. 
\end{exercise}
\begin{solution}
Listed. 
\begin{enumerate}
    \item Given fixed sizes $X_1, \ldots, X_n$ and some $i \in [n]$, we can see that a property of $f$ is that 
    \[f(X_1, \ldots, X_{i-1}, 0, X_{i+1}, \ldots, X_n) + 1 = f(X_1, \ldots, X_{i-1}, 1, X_{i+1}, \ldots, X_n)\]
    since for an extra package with size $1$, you would for sure need one more bin. So the maximum difference of $f$ based on the $x_i$ value is the constant random variable 
    \begin{align*}
        D_i f(X_1, \ldots, X_n) & = \sup_{z \in [0, 1]} f(X_1, \ldots, z, \ldots, X_n) - \inf_{z \in [0, 1]} f(X_1, \ldots, z, \ldots, X_n)\\
        & = f(X_1, \ldots, 1, \ldots, X_n) - f(X_1, \ldots, 0, \ldots, X_n) = 1
    \end{align*}
    and so by the bounded difference inequalities, 
    \begin{align*}
        \Var[B_n] = \Var[f(X_1, \ldots, X_n)] & \leq \frac{1}{4} \mathbb{E} \bigg[ \sum_{i=1}^n \big( D_i f(X_1, \ldots, X_n) \big)^2 \bigg] \\
        & = \frac{1}{4} \sum_{i=1}^n \mathbb{E} \big[ \big( D_i f(X_1, \ldots, X_n) \big)^2 \big] \\
        & \leq \frac{n}{4} 
    \end{align*}
    \item Given the sizes $X_1, \ldots, X_n$, $B_n$ must satisfy 
    \[B_n = f(X_1, \ldots, X_n) \geq X_1 + \ldots + X_n\] 
    since the total volume of bins $B_n$ must exceed the total volume $X_1 + \ldots + X_n$ of packages. So, 
    \[\mathbb{E}[B_n] \geq \mathbb{E}\bigg[ \sum_{k=1}^n X_k \bigg] = n \mathbb{E}[X_1]\]
\end{enumerate}
\end{solution}

\begin{exercise}[Order Statistics and Spacings]
Let $X_1, \ldots, X_n$ be independent random variables, and denote by $X_{(1)} \geq \ldots \geq X_{(n)}$ their decreasing rearrangement ($X_{(1)} = \max_i X_i$, $X_{(n)} = \min_i X_i$, etc.). Show that 
\[\Var[X_{(k)}] \leq k \, \mathbb{E}[(X_{(k)} - X_{(k+1)})^2] \text{ for } 1 \leq k \leq n/2\]
and that 
\[\Var[X_{(k)}] \leq (n - k + 1)\, \mathbb{E}[(X_{(k-1)} - X_{(k)})^2] \text{ for } n/2 < k \leq n\]
\end{exercise}

\begin{exercise}[Convex Poincare Inequality]
Let $X_1, \ldots, X_n$ be independent random variables taking values in $[a, b]$. The bounded difference inequalities estimate the variance $\Var[f(X_1, \ldots, X_n)]$ in terms of \textit{discrete} derivatives $D_i f$ or $D_i^- f$ of the function $f$. The goal of this problem is to show that if the function $f$ is convex, then one can obtain a similar bound in terms of the ordinary notion of derivative $\nabla_i f(x) = \partial f(x)/\partial x_i$ in $\mathbb{R}^n$. 
\begin{enumerate}
    \item Show that if $g: \mathbb{R} \longrightarrow \mathbb{R}$ is convex, then 
    \[g(y) - g(x) \geq g^\prime (x)\, (y - x) \text{ for all } x, y \in \mathbb{R}\]
    
    \item Show using part (a) and the bounded difference inequalities that if $f: \mathbb{R}^n \rightarrow \mathbb{R}$ is convex, then 
    \[\Var[f(X_1, \ldots, X_n)] \geq (b - a)^2 \mathbb{E}[ ||\nabla f (X_1, \ldots, X_n)||^2]\]
    
    \item Conclude that if $f$ is convex and $L$-Lipschitz, i.e. $|f(x) - f(y)| \leq L ||x - y||$ for all $x, y \in [a, b]^n$, then $\Var[f(X_1, \ldots, X_n)] \geq L^2 (b - a)^2$. 
\end{enumerate}
\end{exercise}
\begin{solution}
Listed. 
\begin{enumerate}
    \item Assuming $g$ is differentiable, let us choose any $x, y \in \mathbb{R}$ and define some $z = \lambda x + (1 - \lambda)y$ in between. Then, pictorially, we would like to formally show that 
    \[\frac{f(z) - f(x)}{z - x} \leq \frac{f(y) - f(x)}{y - x}\]
    and take the limit as $z \rightarrow x$ to get $f^\prime(x)$ on the LHS. By definition, we have 
    \[f(z) = f\big( \lambda x + (1 - \lambda) y\big) \leq \lambda f(x) + (1 - \lambda) f(y)\]
    Subtracting $f(x)$ and then dividing by $1 - \lambda > 0$ on both sides gives 
    \[\frac{f(z) - f(x)}{1 - \lambda} \leq f(y) - f(x)\] 
    Note that $z - x = \lambda x + (1 - \lambda y) - x = (1 - \lambda)(y - x)$. So, dividing by $y - x > 0$ on both sides gives 
    \[\frac{f(z) - f(x)}{z - x} \leq \frac{f(y) - f(x)}{y - x}\]
    and taking the limit on the LHS gives 
    \[f^\prime (x) = \lim_{z \rightarrow x} \frac{f(z) - f(x)}{z - x} \leq \frac{f(y) - f(x)}{y - x}\]
    Since $y - x > 0$, we can multiply both on the same side to get 
    \[f(y) - f(x) \geq f^\prime (x) \, (y - x)\]
    If $y < x$, then the proof is the same, and the inequality sign ends up getting switched around twice, leading to the same conclusion. 
    
    \item Note that from the above result, we can multiply both sides by $-1$ to get that $g(x) - g(y) \leq g^\prime (x) (x - y)$ for all $x, y \in \mathbb{R}$, and then swap the two variables to get $g(y) - g(x) \leq g^\prime (y) (y - x)$. Let us consider fixed $x_1, \ldots, x_n$ and some $i \in [n]$. Given $f: \mathbb{R}^n \rightarrow \mathbb{R}$, we define $f_i (\mathbf{x}): \mathbb{R} \rightarrow \mathbb{R}$ by unfixing the $i$th variable. Then, given some $\alpha, \beta \in [a, b]$, 
    \[f_i (\mathbf{x}) (\beta) - f_i (\mathbf{x}) (\alpha) \leq g^\prime (\beta) (\beta - \alpha)\]
    or equivalently, 
    \[f(x_1, \ldots, \beta, \ldots, x_n) - f(x_1, \ldots, \alpha, \ldots, x_n) \leq \frac{\partial f}{\partial x_i} (x_1, \ldots, \beta, \ldots, x_n) \; (\beta - \alpha)\]
    Now let $z^\ast \in [a, b]$ be the value s.t. 
    \[z^\ast = \arg \min_{z \in [a, b]} f(x_1, \ldots, z, \ldots, x_n) \]
    Then, 
    \[D_i^- f(\mathbf{x}) = f(x_1, \ldots, x_i, \ldots x_n) - f(x_1, \ldots, z^\ast, \ldots, x_n) \leq \frac{\partial f}{\partial x_i} (x_1, \ldots, x_i, \ldots, x_n) \; (x_i - z^\ast)\]
    and so 
    \[\big( D_i^- f(\mathbf{X}) \big)^2 \leq \nabla_i f (\mathbf{x})^2 \, (x_i - z^\ast)^2 \leq \nabla_i f (\mathbf{x})^2 \, (b - a)^2\]
    which gives from the bounded difference inequality 
    \begin{align*}
        \Var[f(X_1, \ldots, X_n)] & \leq \mathbb{E} \bigg[ \sum_{i=1}^n \big( D_i^- f(X_1, \ldots, X_n) \big)^2 \bigg] \\
        & \leq \mathbb{E} \bigg[ \sum_{i=1}^n \nabla_i f (\mathbf{x})^2 \, (b - a)^2 \bigg] \\
        & = (b - a)^2 \mathbb{E} \big[ \big| \big| \nabla f(\mathbf{X})\big| \big|^2 \big]
    \end{align*}
    
    \item If $f$ is $L$-lipschitz, then $||\nabla f(\mathbf{X})|| \leq L$, and so  
    \[\Var[f(X_1, \ldots, X_n)] \leq (b - a)^2 L^2\]
\end{enumerate}
\end{solution}

\subsection{Markov Semigroups}

\begin{definition}[Markov Process]
Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space and $(S, \mathcal{S})$ be a measurable space. A homogeneous Markov process $\{X_t\}_{t \geq 0}$ is a stochastic process that satisfies the \textbf{Markov property}: for every bounded measurable function $f$ and $s, t \geq 0$, there exists a bounded measurable function $P_s f$ satisfying 
\[\mathbb{E}[f (X_{t + s}) \mid \{X_r\}_{r \leq t}] = (P_s f) (X_t) = \mathbb{E}[ f(X_{t + s}) \mid X_t]\]
\end{definition}

\begin{definition}[Stationary Measure]
A probability measure $\mu$ is called \textbf{stationary} or \textbf{invariant} if 
\[\mathbb{E}_\mu[f] = \mathbb{E}_\mu [P_t f] \text{ i.e. } \int_S f \,d \mu = \int_S P_t f d\mu\]
for all $t \geq 0$ and bounded measurable $f$. By abusing notation, this is conventionally written 
\[\mu(f) = \mu(P_t f)\]
\end{definition}

To interpret this notion, suppose that $X_0 \sim \mu$. Then, 
\[\mathbb{E}[f(X_t)] = \mathbb{E}[\mathbb{E}[f(X_t) \mid X_0]] = \mathbb{E}[P_t f (X_0)] = \mathbb{E}_\mu [P_t f]\]
and if $\mu$ is stationary, then we have $\mathbb{E}[f(X_t)] = \mathbb{E}_\mu [f]$. If $f = 1_A$ for some measurable $A \subset S$, then $\mathbb{E}[1_A (X_t)] = \mathbb{P}(X_t \in A)$, and 
\[\mathbb{P}(X_t \in A) = \mathbb{E}_\mu [1_A] = \int_S 1_A \,d\mu = \int_A d\mu = \mu(A) = \mathbb{P}(X_0 \in A)\]
which means that the probability that for all $A \in \mathcal{S}$ and all $t \geq 0$, the probability of $X_t$ realizing in $A$ is equivalent to the initial probability of $X_0$ realizing in $A$. This means that the process remains distributed according to the stationary measure $X_t \sim \mu$ for every time $t$. In summary, stationary measures describe the equilibrium or steady-state behavior of the Markov process.  

From now, given the state space $(S, \mathcal{S})$ we can put a measure $\mu$ on it to get a measure space $(S, \mathcal{S}, \mu)$. The Banach space of all $\mu$-measurable functions $f: (S, \mathcal{S}, \mu) \rightarrow (\mathbb{R}, \mathcal{R})$ (i.e. for every Borel $B \in \mathcal{R}$, $f^{-1}(B) \in \mathcal{S}$) will be denoted $L^p (\mu)$, equipped with the norm 
\[||f||_{L^p(\mu)} \coloneqq \mathbb{E}_\mu [f^p]^{1/p} = \bigg( \int_S |f|^p \,d\mu \bigg)^{1/p}\]
If $p = 2$, then we can define the inner product 
\[\langle f, g \rangle_\mu \coloneqq \mathbb{E}_\mu [f g] = \int_S f g \, d\mu\]

\begin{lemma}
Let $\mu$ be a stationary measure. Then, the following hold for all $p \geq 1$, $t, s \geq 1$, $\alpha, \beta \in \mathbb{R}$, and bounded measurable functions $f, g$. 
\begin{enumerate}
    \item Contraction: 
    \[||P_t f||_{L^p(\mu)} \leq ||f||_{L^p (\mu)} = \mathbb{E}_\mu [f^p]^{1/p}\]
    
    \item Linearity: 
    \[P_t (\alpha f + \beta g) = \alpha P_t f + \beta P_t g\] 
    
    \item Semigroup Property: 
    \[P_{t + s} f = P_t P_s f\]
    
    \item Conservativeness: 
    \[P_t 1 = 1\]
\end{enumerate}
\end{lemma}

\begin{lemma}
Let $\mu$ be a stationary measure. Then, $t \mapsto \Var_\mu [P_t f]$ is a decreasing function of time for every function $f \in L^2 (\mu)$. 
\end{lemma}
\begin{proof}
Note that 
\begin{align*}
    \Var_\mu [P_t f] & = ||P_t f - \mu f||^2_{L^2(\mu) } =  ||P_t (f - \mu f)||^2_{L^2 (\mu)} = ||P_{t - s} P_s (f - \mu f)||^2_{L^2 (\mu)} \\
    & \leq ||P_s (f - \mu f)||^2_{L^2 (\mu)} = ||P_s f - \mu f||^2_{L^2 (\mu)} = \Var_\mu (P_s f)
\end{align*}
\end{proof}

We now define the analogous operator to the transition rate matrix in discrete time chains with a finite state space. 

\begin{definition}[Generator]
The generator $\mathscr{L}$ is defined as 
\[\mathscr{L} f \coloneqq \lim_{t \downarrow 0} \frac{P_t f - f}{t}\]
for every $f \in L^2 (\mu)$ for which the above limit exists in $L^2 (\mu)$. The set of $f$ for which $\mathscr{L}f$ is defined is called the domain $\mathrm{Dom}(\mathscr{L})$ of the generator, and $\mathscr{L}$ defines a linear operator from $\mathrm{Dom}(\mathscr{L}) \subset L^2 (\mu)$ to $L^2 (\mu)$. 
\end{definition}

We have defined the generator $\mathscr{L}$ from the Markov semigroup $\{P_t\}_{t \geq 0}$. Now, let's try to define the semigroup in terms of the generator $\mathscr{L}$. Given that we have some map $\mathscr{L})$, can we define some semigroup $\{P_t\}$ satisfying the definition? To do this, we must solve the differential equation: 
\[\frac{d}{dt} P_t = \lim_{\delta \downarrow 0} \frac{P_{t + \delta} - P_t}{\delta} = \lim_{\delta \downarrow 0} \frac{P_t P_\delta - P_t}{\delta} = P_t \lim_{\delta \downarrow 0} \frac{P_\delta - I}{\delta} = P_t \mathscr{L}\]
For function $P_t$ to satisfy this differential equation, we have the solution 
\[P_t = e^{t \mathscr{L}}\]
which also implies that $\mathscr{L}$ and $P_t$ must commute. 

\begin{definition}[Reversibility]
The Markov semigroup $\{P_t\}_{t \geq 0}$ with stationary measure $\mu$ is called \textbf{reversible} if 
\[\langle f, P_t g \rangle_\mu = \langle P_t f, g \rangle_\mu\]
for every $f, g \in L^2 (\mu)$. Equivalently, we can say that $P_t$ is self-adjoint on $L^2 (\mu)$, or since $P_t = e^{t \mathscr{L}}$, we have $\mathscr{L}$ is self-adjoint. 
\end{definition}

\begin{definition}[Ergodicity]
The Markov semigroup $\{P_t\}_{t \geq 0}$ with stationary measure $\mu$ if called \textbf{ergodic} if 
\[P_t f \rightarrow \mu f\]
in $L^2 (\mu)$ as $t \rightarrow +\infty$ for every $f \in L^2 (\mu)$. Note that $\mu f = \mu(f)$ is the constant function in $L^2 (\mu)$. 
\end{definition}

\begin{exercise}[Elementary Identities]
Let $P_t$ be a Markov semigroup with generator $\mathscr{L}$ and stationary measure $\mu$. Prove the following elementary facts. 
\begin{enumerate}
    \item Show that $\mu( \mathscr{L} f) = 0$ for every $f \in L^2 (\mu)$ 
    \item If $\phi : \mathbb{R} \rightarrow \mathbb{R}$ is convex, then $P_t \phi (f) \geq \phi (P_t f)$ when $f, \phi(f) \in L^2(\mu)$ 
    \item If $\phi : \mathbb{R} \rightarrow \mathbb{R}$ is convex, then $\mathscr{L} \phi(f) \geq \phi^\prime (f) \mathscr{L} f$ when $f, \phi(f) \in L^2 (\mu)$ 
    \item Let $f \in L^2 (\mu)$. Show that the following process is a martingale. 
    \[M_t^f \coloneqq f(X_t) - \int_0^t \mathscr{L} f(X_s) \,ds\]
\end{enumerate}
\end{exercise}
\begin{solution}
Listed. 
\begin{enumerate}
    \item This is simply a property of the generator. Not worrying about interchanging limits and integrals, we have 
    \begin{align*}
        \mu(\mathscr{L} f) = \mathbb{E}_\mu [\mathscr{L} f] & = \int_S \lim_{t \downarrow 0} \frac{P_t f - P_0 f}{t} \,d\mu \\ 
        & = \lim_{t \downarrow 0} \int_S \frac{P_t f - P_0 f}{t} \,d\mu \\
        & = \lim_{t \downarrow 0} \frac{1}{t} \big( \mathbb{E}_\mu [P_t f] - \mathbb{E}_\mu [f] \big) = \lim_{t \downarrow 0} \frac{1}{t} \cdot 0 = 0 
    \end{align*}
    
    \item By Jensen's inequality, 
    \begin{align*}
        P_s \phi(f) & = \mathbb{E} [ \phi(f) (X_{t + s}) \mid X_t] \\
        & \geq \phi \bigg( \mathbb{E}[f(X_{t + s} \mid X_t] \big) = \phi(P_s f)
    \end{align*}

\end{enumerate}
\end{solution}


\subsection{Poincare Inequalities}

Recall that a Poincare inequality for $\mu$ is, informally, of the form 
\[\mathrm{variance}(f) \leq \mathbb{E}_\mu[ ||\mathrm{gradient}(f)||^2 ]\]
At first sight, such an inequality has nothing to do with Markov processes. However, the validity of a Poincare inequality for $\mu$ turns out to be related to the rate of convergence of an ergodic Markov process for which $\mu$ is the stationary distribution. That is, a measure $\mu$ satisfies a Poincare inequality for a certain notion of gradient if and only if an ergodic Markov semigroup associated to this gradient converges exponentially fast to $\mu$. 

\begin{definition}[Dirichlet Form]
Given a Markov process with generator $\mathscr{L}$ and stationary measure $\mu$, the corresponding Dirichlet form is defined as 
\[\mathcal{E}(f, g) \coloneqq - \langle f, \mathscr{L} g \rangle_\mu\]
\end{definition}

\begin{theorem}[Poincare Inequality]
Let $P_t$ be a reversible ergodic Markov semigroup with stationary measure $\mu$. The following are equivalent given $c \geq 0$. 
\begin{enumerate}
    \item $\mathrm{Var}_\mu (f) \leq c \mathcal{E}(f, f)$ for all $f$ (Poincare Inequality) 
    \item $||P_t f - \mu f||_{L^2 (\mu)} \leq e^{-t /c} ||f - \mu f||_{L^2 (\mu)}$
    \item $\mathcal{E}(P_t f, P_t f) \leq e^{-2t /c} \mathcal{E}(f, f)$ for all $f, t$
    \item For every $f$ there exists $\kappa (f)$ s.t. $||P_t f - \mu f||_{L^2 (\mu)} \leq \kappa(f) e^{-t/c}$
    \item For every $f$ there exists $\kappa (f)$ s.t. $\mathcal{E}(P_t f, P_t f) \leq \kappa(f) e^{-2t/c}$ 
\end{enumerate}
\end{theorem}

We should view properties 2 through 5 as different notions of exponential convergence of the Markov semigroup $P_t$ to the stationary measure $\mu$. Properties 2 and 4 directly measure the rate of convergence of $P_t f$ to $\mu f$ in $L^2 (\mu)$, while properties 3 and 5 measure the rate of convergence of the "gradient" (now depicted as $\mathcal{E}$) of $P_t f$ to $0$. 

\subsubsection{The Gaussian Poincare Inequality}

\begin{definition}[Ornstein-Uhlenbeck Process]
Given standard Brownian motion $(W_t)_{t \geq 0}$, the \textbf{Ornstein-Uhlenbeck process} is defined as 
\[X_t = e^{-t} X_0 + e^{-t} W_{e^{2t} - 1}\]
\end{definition}

\begin{lemma}[Gaussian Integration by Parts]
If $\xi \sim \mathcal{N}(0, 1)$, then 
\[\mathbb{E}[ \xi f(\xi)] = \mathbb{E}[f^\prime (\xi)]\]
\end{lemma}
\begin{proof}
Assuming that $f$ is smooth with compact support, we have by integration by parts 
\begin{align*}
    \mathbb{E}[f^\prime (\xi)] & = \int_{-\infty}^\infty f^\prime(x) \frac{e^{-x^2 / 2}}{\sqrt{2\pi}} \,dx \\ 
    & = \frac{e^{-x^2 / 2}}{\sqrt{2\pi}} \, f(x) \bigg|_{-\infty}^\infty - \int_{-\infty}^\infty f(x) \frac{d}{dx} \bigg(\frac{e^{-x^2 / 2}}{\sqrt{2\pi}}\bigg) \,dx \\
    & = - \int_{-\infty}^\infty -x f(x) \frac{e^{-x^2 / 2}}{\sqrt{2\pi}} \,dx \\
    & = \int_{-\infty}^\infty \big( x f(x)\big) \frac{e^{-x^2 / 2}}{\sqrt{2\pi}}\,dx = \mathbb{E}[\xi f(\xi)]
\end{align*}
\end{proof}

\begin{theorem}
The Ornstein-Uhlenbeck Process $(X_t)_{t \geq 0}$ 
\begin{enumerate}
    \item is a Markov process with semigroup 
    \[P_t f(x) = \mathbb{E} \big[ f(e^{-t} x + \sqrt{1 - e^{-2t}} \xi) \big] \text{ with } \xi \sim \mathcal{N}(0, 1)\]
    \item admits $\mu = \mathcal{N}(0, 1)$ as its stationary measure
    \item is ergodic
    \item has generator and Dirichlet form given by 
    \[\mathscr{L} f(x) = -x f^{\prime} (x) + f^{\prime\prime} (x), \;\;\;\;\; \mathcal{E}(f, g) = \langle f^\prime , g^\prime \rangle_\mu\]
    \item is reversible
\end{enumerate}
\end{theorem}
\begin{proof}
Let $s \geq t$. 
\begin{enumerate}
    \item By definition of $X_t$, we have $X_t = e^{-t} X_0 + e^{-t} W_{e^{2t - 1}}$ and 
    \[X_s = e^{-s} X_0 + e^{-s} W_{e^{2s} - 1} \implies X_0 = (X_s - e^{-s} W_{e^{2s} - 1} ) e^{s}\]
    Substituting in the equation for $X_s$ gives 
    \begin{align*}
        X_t & = e^{-(t - s)} X_s + e^{-t} (W_{e^{2t} - 1} - W_{e^{2s} - 1}) \\
        & = e^{-(t - s)} X_s + \sqrt{1 - e^{-2 (t - s)}} \xi
    \end{align*}
    where $\xi = (W_{e^{2t} - 1} - W_{e^{2s} - 1}) / \sqrt{e^{2t} - e^{2s}} \sim N(0, 1)$ is independent of $\{X_r\}_{r \leq s}$. Therefore, we can write 
    \[\mathbb{E}[ f(X_t) \mid \{X_r\}_{r \leq s}] = P_{t - s} f (X_s) = \mathbb{E}\big[f \big( e^{-(t - s)} X_s + \sqrt{1 - e^{-2 (t - s)}} \xi \big) \big]\]
    which proves the Markov property and gives the semigroup. 
    
    \item We can clearly see that if $X_t \sim N(0, 1)$, then $X_{t + s} = e^{-s} X_t + \sqrt{1 - e^{-2s}}\xi$ is a sum of Gaussians, one with variance $e^{-2s}$ and the other with variance $1 - e^{-2s}$, and so their sum has variance $1$. 
    
    \item We will take for granted that this is ergodic. 
    
    \item To compute the generator, we use the chain rule (and not worry about whether we take the derivative within the expectation integral) and then use Gaussian integration by parts to get 
    \begin{align*}
        \frac{d}{dt} P_t f(x) & = \mathbb{E} \bigg[ f^\prime (e^{-t} x + \sqrt{1 - e^{-2t}} \xi) \bigg( \frac{e^{-2t}}{\sqrt{1 - e^{-2t}}} \xi - e^{-t} x \bigg) \bigg] \\
        & = \mathbb{E} \big[ e^{-t} x f^\prime (e^{-t} x + \sqrt{1 - e^{-2t}} \xi) + e^{-2t} f^{\prime\prime} (e^{-t} x + \sqrt{1 - e^{-2t}} \xi ) \big]
    \end{align*}
    and therefore have 
    \[\frac{d}{dt} P_t f (x) = \bigg( -x \frac{d}{dx} + \frac{d^2}{dx^2} \bigg) P_t f (x)\]
    The Dirichlet form can be simplified using the Gaussian integration by parts as 
    \begin{align*}
        \mathcal{E} (f, g) & = - \langle f, \mathscr{L} g \rangle_\mu \\
        & = \mathbb{E}[ f(\xi) \big( x g^\prime (\xi) - g^{\prime\prime} (\xi) \big)] \\
        & = \mathbb{E}[\xi f(\xi) g^\prime(\xi)] - \mathbb{E}[f(\xi) g^{\prime\prime} (\xi)] \\
        & = \mathbb{E}[f^\prime (\xi) g^\prime (\xi) + f(\xi) g^{\prime\prime} (\xi)] - \mathbb{E}[f(\xi) g^{\prime\prime} (\xi)] \\
        & = \mathbb{E}[f^\prime (\xi) g^\prime (\xi) ]
    \end{align*} 
    
    \item Since $\mathcal{E}(f, g) = \mathbb{E}[f^\prime(\xi) g^\prime (\xi)]$, it is symmetric and so $\mathscr{L}$ is self-adjoint. 
\end{enumerate}

\end{proof}

From the previous theorem part 4, we can see that 
\[\mathcal{E}(f, f) = \langle f^\prime, f^\prime \rangle_\mu = ||f^\prime||_{L^2(\mu)}^2 = \mathbb{E}_\mu[ f^{\prime 2} ]\]
which means that the Dirichlet form of an Ornstein-Uhlenbeck process is precisely the expected square gradient of function $f$! Therefore, with the Poincare inequality, we can bound the variance of $f$ with the Dirichlet form, which is the expected square gradient of $f$. 

\begin{theorem}
Let $\mu = \mathcal{N}(0, 1)$. Then, 
\[\mathrm{Var}_\mu [f] \leq ||f^\prime||_{L^2(\mu)}^2\]
\end{theorem}
\begin{proof}
We have from the properties of the Ornstein-Uhlenbeck process that
\begin{align*}
    \frac{d}{dx} P_t f(x) & = \frac{d}{dx} \mathbb{E}[ f(e^{-t} x + \sqrt{1 - e^{-2t}} \xi)] \\
    & = \mathbb{E} \bigg[ \frac{d}{dx} f(e^{-t} x + \sqrt{1 - e^{-2t}} \xi)] \\
    & = \mathbb{E}[f^\prime (e^{-t} x + \sqrt{1 - e^{-2t}} \xi) \; e^{-t}] \\
    & = e^{-t} \mathbb{E}[f^\prime (e^{-t} x + \sqrt{1 - e^{-2t}} \xi)] \\
    & = e^{-t} P_t f^\prime (x) 
\end{align*}
Thus
\[\mathcal{E}(P_t f, P_t f) = ||(P_t f)^\prime||_{L^2 (\mu)}^2 = e^{-2t} || P_t f^\prime ||^2_{L^2(\mu)} \leq e^{-2t} ||f^\prime||^2_{L^2(\mu)} = e^{-2t} \mathcal{E}(f, f) \]
where the inequality follows from contraction. 
\end{proof}

By tensorization, we can prove the following. 

\begin{corollary}[Gaussian Poincare Inequality]
Let $X_1, \ldots, X_n \sim N(0, 1)$ be iid. Then, 
\[\Var[ f(X_1, \ldots, X_n)] \leq \mathbb{E}[ || \nabla f (X_1, \ldots, X_n)||^2 ]\]
\end{corollary}
\begin{proof}
Computation. 
\begin{align*}
    \mathrm{Var}[f(X_1, \ldots, X_n)] & \leq \mathbb{E} \bigg[ \sum_{i=1}^n \mathrm{Var}_i f(X_1, \ldots, X_n) \bigg] \\
    & \leq \mathbb{E} \bigg[ \sum_{i=1}^n \bigg| \bigg| \frac{d}{dx_i} f(X_1, \ldots, X_n)\bigg|\bigg|^2 \bigg] \\
    & = \mathbb{E}[ ||\nabla f (X_1, \ldots, X_n) ||^2 ]
\end{align*}
\end{proof}

So what have we done so far? If we have some distribution $\mu$ and want to prove an inequality that bounds $\Var_\mu [f]$, then we should choose some (reversible ergodic) Markov process that has a stationary distribution $\mu$. We can identify its semigroup, generator, and ultimately its Dirichlet form $\mathcal{E}(f, g)$, which will allow us to invoke the Poincare inequality to bound 
\[\Var_\mu [f] \leq c \mathcal{E}(f, f)\]
and since $\mu = N(0, 1)$, we have shown above using both the properties of the generator of the Ornstein-Uhlenbeck process and Gaussian integration by parts that this Dirichlet form is precisely the norm of $f^\prime$. This is clear since the Dirichlet form $\langle f, \mathscr{L} g\rangle_\mu$ only depends on $\mathscr{L}$ and $\mu$. However, the Dirichlet form does not have to be this form. 
\begin{enumerate}
    \item If $\mu$ is some other distribution, we would not be able to reduce $\mathcal{E}(f, f)$ to the norm of its derivative, and so it make take on a different form. 
    \item If we choose a different Markov process, even with the same stationary measure $\mu = N(0, 1)$, the generator may be different and so will the Dirichlet form. 
\end{enumerate}

\begin{exercise}[Carre du Champ]
We have interpreted the Dirichlet form $\mathcal{E}(f, f)$ as a general notion of “expected square gradient” that arises in the study of Poincare inequalities. There is an analogous quantity $\Gamma(f, f)$ that plays the role of “square gradient” in this setting (without the expectation). In good probabilistic tradition, it is universally known by its French name carre du champ (literally, “square of the field”). The carre du champ is defined as
\[\Gamma(f, g) \coloneqq \frac{1}{2} \big[ \mathscr{L}(f g) - f \mathscr{L} g - g \mathscr{L} f \big] \]
in terms of the generator $\mathscr{L}$ of a Markov process with stationary measure $\mu$. 
\begin{enumerate}
    \item Show that $\mathcal{E}(f, f) = \int \Gamma(f, f) \, d\mu$ and that $\mathcal{E}(f, g) = \int \Gamma(f, g) \,d\mu$ if the Markov process is in addition reversible. 
    \item Show that $\Gamma(f, f) \geq 0$ so it can indeed by interpreted as a square. 
    \item Prove the Cauchy-Schwartz inequality $\Gamma(f, g)^2 \leq \Gamma(f, f) \, \Gamma(g, g)$ 
    \item Compute the carre du champ of the Ornstein-Uhlenbeck process and confirm that it should indeed be interpreted as the appropriate notion of "square gradient." 
\end{enumerate}
\end{exercise}
\begin{solution}
Listed. 
\begin{enumerate}
    \item By stationarity, we have 
    \[\mu ( \mathscr{L} f) = \int_S \mathscr{L} f \, d\mu = 0\]
    for all $f \in L^2 (\mu)$, which reduces the first term below to $0$. So, we can reduce the carre du champ to 
    \begin{align*}
        \int_S \Gamma(f, f) \, d\mu & = \frac{1}{2} \bigg( \int_S \mathscr{L} (f^2) \, d\mu - 2 \int_S f \mathscr{L} f \, d\mu \bigg) \\
        & = - \int_S f \mathscr{L} f \, d\mu = - \langle f, \mathscr{L} f \rangle_\mu = \mathcal{E}(f, f)
    \end{align*}
    Furthermore, assuming that $P_t$ is reversible, we have 
    \[\mathcal{E}(f, g) = - \langle f, \mathscr{L} g \rangle_\mu = -\langle \mathscr{L} f, g \rangle_\mu = - \langle g, \mathscr{L} f \rangle_\mu = \mathcal{E}(g, f)\]
    and so 
    \begin{align*}
        \int \Gamma (f, g) \, d\mu & = \frac{1}{2} \bigg( \int \mathscr{L}(f g) \, d\mu - \int f \mathscr{L} g \, d\mu - \int g \mathscr{L} f \, d\mu \bigg) \\
        & = \frac{1}{2} \big( - \langle f, \mathscr{L} g \rangle_\mu - \langle g, \mathscr{L} f\rangle_\mu \big) \\
        & = - \langle f, \mathscr{L} g \rangle_\mu = \mathcal{E}(f, g)
    \end{align*}
    
    \item Since $\Gamma(f, f) = \frac{1}{2} \big( \mathscr{L} (f^2) - 2 f \mathscr{L}f \big)$, the problem now reduces to proving that $\mathscr{L} (f^2) \geq 2 f \mathscr{L}f$. By Jensen's inequality, we have $P_t (f^2) \geq (P_t f)^2$, and so 
    \begin{align*}
        \mathscr{L}(f^2) & = \lim_{t \downarrow 0} \frac{P_t (f^2) - f^2}{t} \geq \lim_{t \downarrow 0} \frac{(P_t f)^2 - f^2}{t} \\
        & = \frac{d}{dt} (P_t f)^2 \bigg|_{t = 0} = \bigg( 2 (P_t f) \cdot \frac{d}{dt} (P_t f) \bigg)\bigg|_{t = 0} = 2 f \mathscr{L} f
    \end{align*}
    
    \item We know that $\Gamma(f + t g, f + tg) \geq 0$ from above, and so if we expand out, we get
    \begin{align*}
        \Gamma(f + t g, f + tg) & = \frac{1}{2} \Big[ \mathscr{L} \big( (f + t g)^2 \big) - 2 (f + t g) \mathscr{L}(f + t g) \Big] \\
        & = \Gamma(g, g) t^2 + 2 \Gamma (f, g) t + \Gamma(f, f) \geq 0 
    \end{align*}
    for all $t$. Since this quadratic is nonnegative, its discriminant must be $\leq 0$, and so 
    \[\Delta = \big( 2 \Gamma (f, g) \big)^2 - 2 \Gamma(g, g) \Gamma(f, f) \leq 0 \implies \Gamma(f, g)^2 \leq \Gamma(f, f) \Gamma(g, g)\]
    
    \item The generator of the Ornstein-Uhlenbeck process is $\mathscr{L}f(x) = -x f^\prime(x) + f^{\prime\prime} (x)$. Therefore, 
    \begin{align*}
        \Gamma (f, g) (x) & = \frac{1}{2} \big[ \mathscr{L} (f g)(x) - f(x) \mathscr{L} g(x) - g(x) \mathscr{L} f (x) \big] \\
        & = \frac{1}{2} \Big[ \big( -x (f g)^\prime (x) + (f g)^{\prime\prime} (x) \big) - f(x) \big( -x g^\prime(x) + g^{\prime\prime} (x) \big) - g(x) \big( -x f^\prime(x) + f^{\prime\prime} (x) \big) \Big]
    \end{align*}
    which simplifies down to $f^\prime(x) g^\prime(x)$, and so $\Gamma(f, f) = [ f^\prime(x)]^2$ can be interpreted as the square gradient of $f$. 
\end{enumerate}
\end{solution}

\subsection{Variance Identities and Exponential Ergodicity}

Now, let us develop some intuition on the connection between Markov semigroups, $\Var_\mu [f]$ and the Dirichlet form $\mathcal{E}(f, f)$. 

\begin{lemma}
The following identity holds. 
\[\frac{d}{dt} \Var_\mu [P_t f] = -2 \mathcal{E} (P_t f, P_t f)\]
\end{lemma}
\begin{proof}
By stationarity, $\mu (P_t f) = \mu(f)$, and so 
\begin{align*}
    \frac{d}{dt} \Var_\mu [P_t f] & = \frac{d}{dt} \big\{ \mu((P_t f)^2) - \mu(P_t f)^2\big\} \\
    & = \frac{d}{dt} \big\{ \mu((P_t f)^2) - \mu( f)^2\big\} = \frac{d}{dt} \mu((P_t f)^2) \\
    & = \frac{d}{dt} \int_S (P_t f)^2 \, d\mu = \int_S \frac{d}{dt} (P_t f)^2 \,d\mu = 2 \int_S (P_t f) \, \frac{d}{dt} P_t f \, d\mu \\
    & = 2 \mathbb{E}_\mu [P_t f, \mathscr{L} (P_t f) ] = 2 \langle P_t f, \mathscr{L} P_t f \rangle_\mu = -2 \mathcal{E}(P_t f, P_t f) 
\end{align*}
\end{proof}

\begin{theorem}
$\mathcal{E}(f, f) \geq 0$ for every $f$. 
\end{theorem}
\begin{proof}
We know that $t \mapsto \Var_\mu [P_t f]$ is a decreasing function of $t$ (by contraction of $P_t$), so 
\[\frac{d}{dt} \Var_\mu [P_t f] = - 2 \mathcal{E}(P_t f, P_t f) \leq 0\]
\end{proof}

\begin{theorem}
Suppose that the Markov semigroup is ergodic. Then, we have for every $f$ 
\[\Var_\mu [f] = 2 \int_0^\infty \mathcal{E}(P_t f, P_t f) \, dt\]
\end{theorem}

\section{Subgaussian Concentration and log-Sobolev Inequalities}

\subsection{Subgaussian Variables and Chernoff Bounds}

We should first consider how one might go about proving that a random variable satisfies a Gaussian tail bound. Most tail bounds in probability theory are proved using some form of Markov's inequality. 
\begin{lemma}[Markov's Inequality]
Given a nonnegative random variable $X$, we have 
\[\mathbb{P}(X > \alpha) \leq \frac{\mathbb{E}[X]}{\alpha}\]
which means that the probability that $X > \alpha$ goes down at least as fast as $1/\alpha$. 
\end{lemma}

Markov's inequality is very conservative but very general, too. If we make further assumptions about the random variable $X$, we can often make stronger bounds. Chebyshev's inequality assumes a (possibly negative) random variable with finite variance and states that the probability will go down as $1/x^2$. 

\begin{theorem}[Chebyshev Inequality]
Given (possibly negative) random variable $X$, if $\mathbb{E}[X] = \mu < +\infty$ and $\Var(X) = \sigma^2 < +\infty$, then for all $\alpha > 0$, 
\[\mathbb{P} \big( |X - \mu| > k \sigma \big) \leq \frac{1}{k^2} \iff \mathbb{P}(|X - \mu| > \alpha) \leq \frac{\mathrm{Var}[X]}{\alpha^2}\]
That is, the probability that $X$ takes a value further than $k$ standard deviations away from $\mu$ goes down by $1/k^2$. Therefore, if $\sigma$ is small, then this bound will be small since there is more concentration in the mean. 
\end{theorem}
\begin{proof}
We apply Markov's inequality to the non-negative random variable $|X - \mu|$. 
\[\mathbb{P}(|X - \mu| > \alpha) = \mathbb{P}(|X - \mu|^2 > \alpha^2) \leq \frac{\mathbb{E}(|X - \mu|^2)}{\alpha^2} = \frac{\mathrm{Var}[X]}{\alpha^2}\]
since the numerator on the RHS is the definition of variance. 
\end{proof}

Using higher powers, we can obtain better and better bounds, but not exponential ones. To obtain these Gaussian tail bounds, we must use more sophisticated methods. 

\begin{lemma}[Chernoff Bound]
Define the log-moment generating function $\psi$ of a random variable $X$ and its Legendre dual $\psi^*$ as 
\[\psi_X (\lambda) \coloneqq \log \mathbb{E}[ e^{\lambda (X - \mathbb{E}[X])} ] = \mathbb{E}[ e^{\lambda X} ] - \lambda \mathbb{E}[X] \;\;\;\;\; \psi_X^* (t) = \sup_{\lambda \geq 0} \{ \lambda t - \psi_X(\lambda)\}\]
Then, the following is known as the \textbf{Chernoff bound}. 
\[\mathbb{P}[X - \mathbb{E}[X] \geq t] \leq e^{-\psi_X^* (t)}\]
for all $t \geq 0$. We can lower bound it too with 
\[\mathbb{P}[X - \mathbb{E}[X] \leq -t] \leq e^{-\psi_X^* (t)}\]
and union bounding them gives 
\[\mathbb{P}(|X - \mathbb{E}[X] | \geq t ] \leq 2e^{- \psi_X^* (t)}\]
\end{lemma}
\begin{proof}
We take some $\lambda \geq 0$ and given that the map $x \mapsto e^{\lambda x}$ is nondecreasing, we can exponentiate and then use Markov's inequality: 
\[\mathbb{P}[X - \mathbb{E}[X] \geq t ] = \mathbb{P}[ e^{\lambda(X - \mathbb{E}[X])} \geq e^{\lambda t}] \leq e^{-\lambda t} \mathbb{E}[e^{\lambda (X - \mathbb{E}[X])}] = e^{- (\lambda t - \psi_X (\lambda))} \leq e^{-\psi_X^* (t)}\]
as the left hand does not depend on the choice of $\lambda$, we have the additional flexibility of tuning $\lambda$ to get potentially better bounds. We can also use Chernoff bound on the random variable $-X$ to bound \begin{align*}
    \mathbb{P}(X - \mathbb{E}[X] \leq -t) & = \mathbb{P}(-X - \mathbb{E}[-X] \geq t) \\
    & = \mathbb{P}(e^{\lambda(-X + \mathbb{E}[X])} \geq e^{\lambda t} ] \\
    & \leq e^{-\lambda t} \mathbb{E}[ e^{\lambda (-X + \mathbb{E}[X])}] \\
    & = e^{-(\lambda t - \psi_{-X}(\lambda))} \leq e^{-\psi_{-X}^* (t)} 
\end{align*}
There seems to be a minor problem in the fact that $-\psi^*_X$ and $-\psi^*_{-X}$ are different, and so provide different bounds for the upper and lower tail. But note that $\psi_X (\lambda) = \psi_{-X}(-\lambda)$, and so their maximum will coincide and $\psi_X^* (t) = \psi_{-X}^* (t)$, allowing us to get the union bound. 
\[\mathbb{P}(|X - \mathbb{E}[X] | \geq t ] \leq 2e^{- \psi^* (t)}\]
\end{proof}

To observe how the Chernoff bound can give rise to Gaussian tail bounds, let us first consider the case of an actual Gaussian random variable. 

\begin{example}
Let $X \sim N(\mu, \sigma^2)$. Then, $\mathbb{E}[ e^{\lambda (X - \mathbb{E}[X])} ] = e^{\lambda^2 \sigma^2 / 2}$, so 
\[\psi(\lambda) = \frac{\lambda^2 \sigma^2}{2}, \;\;\;\;\; \psi^* (t) = \sup_{\lambda \geq 0} \big\{ \lambda t - \frac{\lambda^2 \sigma^2}{2} \big\} = \frac{t^2}{2 \sigma^2}\]
and by the Chernoff bound, we have $\mathbb{P}(X - \mathbb{E}[X] \geq t ] \leq e^{-t^2 / 2\sigma^2}$. 
\end{example}

Note that in order to get the tail bound, the fact that $X$ is Gaussian was not actually important. It would suffice to assume that the log-MGF is bouded from above by a Gaussian. 

\begin{definition}[Subgaussian Random Variables]
A random variable is called $\sigma^2$-\textbf{subgaussian} if its log-MGF satisfies 
\[\psi(\lambda) \leq \frac{\lambda^2 \sigma^2}{2}\]
for all $\lambda \in \mathbb{R}$. The constant $\sigma^2$ is called the \textbf{variance proxy}. 
\end{definition}

Remember that if $\psi(\lambda)$ is the log-MGF of a random variable $X$, then $\psi(-\lambda)$ is the log-MGF of the random variable $-X$. For a $\sigma^2$-subgaussian random variable $X$, we can therefore apply the Chernoff bound to both the upper and lower tails and union bound to obtain 
\[\mathbb{P}(|X - \mathbb{E}[X]| \geq t ) \leq 2 e^{-t/2\sigma^2}\]

We have only worked with Gaussians, which are trivially subgaussian. A nontrivial results is that every bounded random variable is subgaussian. 

\begin{lemma}[Hoeffding's Lemma]
Let $a \leq X \leq b$ a.s. for some $a, b \in \mathbb{R}$. Then, 
\[\mathbb{E}[e^{\lambda(X - \mathbb{E}[X])}] \leq \exp \bigg( \frac{\lambda^2 (b - a)^2}{8} \bigg)\]
That is, $X$ is $(b-a)^2 /4$-subgaussian. 
\end{lemma}
\begin{proof}
We assume without loss of generality that $\mathbb{E}[X] = 0$. Then, we have $\psi(\lambda) = \log \mathbb{E}[ e^{\lambda X}]$, and we can compute 
\[\psi^\prime (\lambda) = \frac{\mathbb{E}[X e^{\lambda X}]}{\mathbb{E}[e^{\lambda X}]}, \;\;\;\;\; \psi^{\prime\prime} (\lambda) = \frac{\mathbb{E}[X^2 e^{\lambda X}]}{\mathbb{E}[e^{\lambda X}]} - \bigg( \frac{\mathbb{E}[X e^{\lambda X}]}{\mathbb{E}[e^{\lambda X}]} \bigg)^2\]
and thus 
\[\psi^{\prime\prime} (\lambda) = \int_\Omega X^2 \, \frac{e^{\lambda X}}{\mathbb{E}[e^{\lambda X}]} \,d\mathbb{P} - \bigg( \int_\Omega X \, \frac{e^{\lambda X}}{\mathbb{E}[e^{\lambda X}]} \,d\mathbb{P} \bigg)^2 \] 
can be interpreted as the variance of the random variable $X$ under the twisted probability measure $d\mathbb{Q} = \frac{e^{\lambda X}}{\mathbb{E}[e^{\lambda X}]} \,d\mathbb{P}$. But $a \leq X \leq b$, so we can bound the variance by its infimum and suprememum $\psi^{\prime\prime} (\lambda) = \Var_\mathbb{Q} [X] \leq (b-a)^2 / 4$, and the fundamental theorem of calculus yields 
\[\psi(\lambda) = \int_0^\lambda \int_0^\mu \psi^{\prime\prime} (\rho) \, d\rho \, d\mu \leq \frac{\lambda^2 (b - a)^2}{8}\]
using $\psi(0) = 0$ and $\psi^\prime (0)$. 
\end{proof}

\begin{exercise}[Subgaussian Variables]
There are several different notions of random variables with a Gaussian tail that are all essentialy equivalent up to constants. The aim of this problem is to obtain some insight into these notions. 
\begin{enumerate}
    \item Show that if $X$ is $\sigma^2$-subgaussian, then $\Var[X] \leq \sigma^2$. 
    \item Show that for any increasing and differentiable function $\Phi$, 
    \[\mathbb{E}[ \Phi(|X|)] = \Phi(0) + \int_0^\infty \Phi^\prime (t) \, \mathbb{P}(|X| \geq t) \, dt\]
\end{enumerate}
In the following, we will assume for simplicity that $\mathbb{E}[X] = 0$. We now prove that the following three properties are equivalent for suitable constants $\sigma, b, c$: (1) $X$ is $\sigma^2$-subgaussian; (2) $\mathbb{P}(|X| \geq t) \leq 2 e^{-b t^2}$; and (3) $\mathbb{E}[e^{c X^2}] \leq 2$. 
\begin{enumerate}[resume]
    \item Show that if $X$ is $\sigma^2$-subgaussian , then $\mathbb{P}(|X| \geq t) \leq 2 e^{-t^2 / 2 \sigma^2}$ 
    \item Show that if $\mathbb{P}(|X| \geq t) \leq 2 e^{- t^2 / 2 \sigma^2}$, then $\mathbb{E}[e^{X^2 / 6 \sigma^2} ] \leq 2$. 
    \item Show that if $\mathbb{E}[e^{X^2 / 6 \sigma^2}] \leq 2$, then $X$ is $18 \sigma^2$-subgaussian. 
\end{enumerate}
In addition, the subgaussian property of $X$ is equivalent to the fact that the moments of $X$ scale as is the case for the Gaussian distribution. 
\begin{enumerate}[resume]
    \item Show that if $X$ is $\sigma^2$-subgaussian, then $\mathbb{E}[X^{2q}] \leq (4 \sigma^2)^q q!$ for all $q \in \mathbb{N}$. 
    \item Show that if $\mathbb{E}[X^{2q}] \leq (4 \sigma^2)^q q!$ for all $q \in \mathbb{N}$, then $\mathbb{E}[e^{X^2 / 8 \sigma^2}] \leq 2$. 
\end{enumerate}
\end{exercise}

\begin{solution}
Listed. 
\begin{enumerate}
    \item We can expand out 
    \begin{align*}
        \mathbb{E}[e^{\lambda (X - \mathbb{E} X}] & = \mathbb{E} \bigg[ 1 + \lambda( X - \mathbb{E}X) + \frac{\lambda^2}{2} (X - \mathbb{E} X)^2 + \ldots \bigg] \\
        & = 1 + \frac{\lambda^2}{2} \Var[X] + o(\lambda^2) \\
        & \leq e^{\lambda^2 \sigma^2 / 2} = 1 + \frac{\lambda^2 \sigma^2}{2} + o (\lambda^2)
    \end{align*}
    which is true for all $\lambda$. Setting $\lambda = 0$, we get $\Var[X] \leq \sigma^2$. 
    
    \item Unfinished. 
    
    \item Since $X$ is $\sigma^2$ subgaussian, its log-MGF satisfies $\psi(\lambda) = \log \mathbb{E}[e^{\lambda X}] \leq \frac{\lambda^2 \sigma^2}{2} \implies - \psi(\lambda) \geq - \frac{\lambda^2 \sigma^2}{2}$. Then, its Legendre dual is 
    \[\psi^\ast (t) = \sup_{\lambda \geq 0} \{ \lambda t - \psi(\lambda)\} \geq \sup_{\lambda \geq 0} \{ \lambda t - \frac{\lambda^2 \sigma^2}{2} \} = \frac{t^2}{2 \sigma^2}\]
    where we optimize the quadratic w.r.t. $\lambda$. Therefore, $-\psi^* (t) \leq - \frac{t^2}{2\sigma^2} \implies \mathbb{P}(X \geq t) \leq e^{ - \psi^* (t)} \leq e^{- t^2/ 2 \sigma^2}$. 
    
    
    \item By using the identity above with $\Phi(t) = e^{t^2 / 6 \sigma^2}$, we have 
    \begin{align*}
        \mathbb{E}[e^{X^2 / 6 \sigma^2}] & = \mathbb{E}[e^{|X|^2 / 6 \sigma^2}] \\
        & = e^{0^2 / 6 \sigma^2} + \int_0^\infty e^{t^2 / 6 \sigma^2} \, \frac{t}{3 \sigma^2} \mathbb{P}(|X| \geq t) \, dt \\
        & \leq 1 + \frac{1}{3t^2} \int_0^\infty t e^{t^2 / 6 \sigma^2} \, 2 e^{-t^2 / 2\sigma^2} \, dt\\
        & = 1 + \frac{2}{3 \sigma^2} \int_0^\infty t e^{-\frac{1}{3} \frac{t^2}{\sigma^2}} \, dt \\
        & = 1 - \frac{1}{\sigma^2} \int_0^\infty \Big( - \frac{2}{3 \sigma} t \Big) \, e^{- \frac{t^2}{3 \sigma^2}} \, dt \\
        & = 1 - e^{- \frac{t^2}{3 \sigma^2}} \bigg|_0^\infty \\
        & = 1 - (0 - 1) = 2 
    \end{align*}
    
    \item Unfinished. 
    
    \item We know $X^{2q} = |X|^{2q}$ for all $q \in \mathbb{N}$. By setting $\Phi(t) = t^{2q}$ from the identity above, we can get 
    \[\mathbb{E}[|X|^{2q}] = 0^{2q} + \int_0^\infty (2q) t^{2q - 1} \mathbb{P}(|X| \geq t) \,dt\]
    and from (3), we get the first line, where we can just keep doing integration by parts: 
    \begin{align*}
        \mathbb{E}[|X|^{2q}] & \leq \int_0^\infty (2q) t^{2q - 1} e^{- t^2 / 2 \sigma^2} \,dt \\
        & = 2 (4q \sigma^2) \int_0^\infty (2q - 2) t^{2q - 3} e^{-t^2 / 2 \sigma^2} \, dt \\
        & = 2 (4q \sigma^2) (4 (q - 1) \sigma^2) \int_0^\infty (2q - 4) t^{2q - 5} e^{-t^2 / 2\sigma^2} \,dt \\
        & = \ldots \\
        & = 2 (4q \sigma^2) \ldots (4 \cdot 2\sigma^2) \int_0^\infty 2t e^{-t^2 / 2 \sigma^2} \,dt \\
        & = \prod_{k=1}^q (4 k \sigma^2) = (4 \sigma^2)^q q! 
    \end{align*}
    
    \item We can expand and from the inequality above, we get 
    \begin{align*}
        \mathbb{E}[e^{X^2 / 8 \sigma^2}] = \mathbb{E} \bigg[ 1 + \frac{X^2}{8 \sigma^2} + \frac{1}{2} \bigg( \frac{X^2}{8 \sigma^2}\bigg)^2 + \ldots \bigg] \\
        & = 1 + \sum_{q = 1}^\infty \frac{1}{(8 \sigma^2)^q q!} \mathbb{E}[X^{2q}] \\
        & \leq 1 + \sum_{q = 1}^\infty \frac{1}{(8 \sigma^2)^q q!} (4 \sigma^2)^q q! \\
        & = 1 + \sum_{q=1}^\infty \frac{1}{2^q} = 2 
    \end{align*}
\end{enumerate}


\end{solution}

\begin{exercise}[Tightness of Hoeffding's Lemma]
Show that the bound on Hoeffding's lemma is the best possible by consider $\mathbb{P}(X = a) = \mathbb{P}(X = b) = \frac{1}{2}$. 
\end{exercise}
\begin{solution}
From computing the expectation 
\[\mathbb{E}[ e^{\lambda (X - \mathbb{E} X)}] = e^{\lambda (a - \frac{a + b}{2})} \mathbb{P}(X = a) + e^{\lambda (b - \frac{a + b}{2})} \mathbb{P}(X = b) = \frac{1}{2} e^{ \lambda \frac{a - b}{2}} + \frac{1}{2} e^{\lambda \frac{b - a}{2}}\]
we know that this is always less than $\lambda^2 (b - a)^2/ 8$ for all $\lambda$. But setting $\lambda = 0$ satisfies equality. 
\end{solution}

\subsection{The Martingale Method}

In this section, we will use the martingale method to derive useful results. Recall that in order to derive some property (like tensorization of variance) of $f(X_1, \ldots, X_n) - \mathbb{E}[f(X_1, \ldots, X_n)]$, we can expand it as a telescoping sum of martingale differences 
\[f(X_1, \ldots, X_n) - \mathbb{E}[f(X_1, \ldots, X_n)] = \sum_{k=1}^n \Delta_k\]
where 
\[\Delta_k = \mathbb{E}[f(X_1, \ldots, X_n) \mid X_1, \ldots, X_k] - \mathbb{E}[f(X_1, \ldots, X_n) \mid X_1, \ldots, X_{k-1}]\]
and then deriving bounds on each difference. Note that these are martingale differences because given the filtration $\mathbb{F} = \{\mathcal{F}_k = \sigma(X_1, \ldots, X_k)\}$, the stochastic process
\[Y_k = \sum_{i=1}^k \Delta_i = \mathbb{E}[f(X_1, \ldots, X_n) \mid X_1, \ldots, X_k] - \mathbb{E}[f(X_1, \ldots, X_n)]\]
is a martingale. 

\begin{lemma}[Azuma]
Let $\mathbb{F} = \{\mathcal{F}_k\}_{k \leq n}$ be any filtration, and $\Delta_1, \ldots, \Delta_n$ be random variables that satisfy the following properties for $k = 1, \ldots, n$. 
\begin{enumerate}
    \item Martingale Difference Property: $\Delta_k$ is $\mathcal{F}_k$-measurable and $\mathbb{E}[\Delta_k \mid \mathcal{F}_{k-1}] = 0$ 
    \item Conditional Subgaussian Property: $\mathbb{E}[e^{\lambda \Delta_k} \mid \mathcal{F}_{k-1}] \leq e^{\lambda^2 \sigma^2_k / 2}$ a.s. 
\end{enumerate}
Then, the sum $\sum_{k=1}^n \Delta_k$ is subgaussian with variance proxy $\sum_{k=1}^n \sigma_k^2$. 
\end{lemma}
\begin{proof}
For any $1 \leq k \leq n$, we can compute 
\[\mathbb{E}[ e^{\lambda \sum_{i=1}^k \Delta_i} ] = \mathbb{E}[e^{\lambda \sum_{i=1}^{k-1} \Delta_i} \mathbb{E}[e^{\lambda \Delta_k} \mid \mathcal{F}_{k-1}]] \leq e^{\lambda^2 \sigma_k^2 / 2} \, \mathbb{E}[e^{\lambda \sum_{i=1}^{k-1} \Delta_i}]\]
and by induction, this proof is finished. Note that $\mathbb{E}[e^{\lambda \Delta_k} \mid \mathcal{F}_{k-1}] \leq e^{\lambda^2 \sigma^2_k / 2}$ can only hold if $\mathbb{E}[\Delta_k \mid \mathcal{F}_{k-1}] = 0$. 
\end{proof}

What this lemma basically says is that if we decompose a random variable into martingale differences, and each martingale difference is conditionally subgaussian, then their sum is also subgaussian. Now, if we just assume that each of these martingale differences are bounded, then we can use Hoeffding's lemma on each of them to make them subgaussian, and then use Azuma's lemma to show that their sum is subgaussian. This is exactly what we do here. 

\begin{theorem}[Azuma-Hoeffding Inequality]
Let $\mathbb{F} = \{ \mathcal{F}_k \}_{k \leq n}$ be any filtration, and let $\Delta_k, A_k, B_k$ satisfy the following properties for $k = 1, \ldots, n$. 
\begin{enumerate}
    \item Martingale Difference Property: $\Delta_k$ is $\mathcal{F}_k$-measurable and $\mathbb{E}[\Delta_k \mid \mathcal{F}_{k-1}] = 0$ 
    \item Predictable bounds: $A_k, B_k$ are $\mathcal{F}_{k-1}$-measurable and $A_k \leq \Delta_k \leq B_k$ a.s. 
\end{enumerate}
Then, $\sum_{k=1}^n \Delta_k$ is subgaussian with variance proxy $\frac{1}{4} \sum_{k=1}^n ||B_k - A_k||^2_\infty$. In particular, we obtain for every $t \geq 0$ the tail bound 
\[\mathbb{P} \bigg( \sum_{k=1}^n \Delta_k \geq t \bigg) \leq \exp \bigg( - \frac{2t^2}{\sum_{k=1}^n ||B_k - A_k||_\infty^2} \bigg)\]
\end{theorem}

The Azuma-Hoeffding's inequality is often applied in the following setting. Let $X_1, \ldots, X_n$ be independent random variables s.t. $a \leq X_i \leq b$ for all $i$ (we can interpret $a$ and $b$ as simply constant random variables). Then, let $\Delta_k = (X_k - \mathbb{E}[X_k])/n$ be martingale differences, which we can show that $\Delta_k$ is clearly $\mathcal{F}_k$-measurable and that by independence of $X_i$'s,  $\mathbb{E}[\Delta_k \mid \mathcal{F}_{k-1}] = \mathbb{E}[\Delta_k] = 0$. Therefore, we can show that its sum satisfies
\[\mathbb{P} \bigg( \frac{1}{n} \sum_{k=1}^n \{X_k - \mathbb{E}[X_k]\} \geq t \bigg) \leq e^{-2n t^2 / (b - a)^2}\]
which is consistent with the central limit theorem. 

Now we can return to the case of functions $f(X_1, \ldots, X_n)$ of independent random variables. Recall that the discrete derivative is defined 
\[D_k f(x) = \sup_z f(x_1, \ldots, x_{k-1}, z, x_{k+1}, \ldots, x_n) - \inf_z f(x_1, \ldots, x_{k-1}, z, x_{k+1}, \ldots, x_n)\]

\begin{theorem}[McDiarmid]
For $X_1, \ldots, X_n$ independent, $f(X_1, \ldots, X_n)$ is subgaussian with variance proxy $\frac{1}{4} \sum_{k=1}^n ||D_k f||^2$. That is, 
\[\mathbb{P}\big[ f(X_1, \ldots, X_n) - \mathbb{E}[f(X_1, \ldots, X_n)] \geq t \big] \leq \exp \bigg( -\frac{2 t^2}{\sum_{k=1}^n ||D_k f||^2_\infty} \bigg)\]
\end{theorem}
\begin{proof}
We use the martingale method again to write 
\[f(X_1, \ldots, X_n) - \mathbb{E}[f(X_1, \ldots, X_n)] = \sum_{k=1}^n \Delta_k\]
where 
\[\Delta_k = \mathbb{E}[f(X_1, \ldots, X_n) \mid X_1, \ldots, X_k] - \mathbb{E}[f(X_1, \ldots, X_n) \mid X_1, \ldots, X_{k-1}]\]
What we want to do is set some upper and lower bound on $\mathbb{E}[f(X_1, \ldots, X_n) \mid X_1, \ldots, X_k]$, which will set bounds on $\Delta_k$. We can do this by bounding $f$ by the infimum and supremum w.r.t. each element, getting 
\begin{align*}
    &\mathbb{E}[ \inf_z f(X_1, \ldots, X_{k-1}, z, X_{k+1}, \ldots, X_n) \mid X_1, \ldots, X_k] \\
    &\;\;\;\;\;\leq \mathbb{E}[f (X_1, \ldots, X_n) \mid X_1, \ldots, X_k] \\
    &\;\;\;\;\;\;\;\;\;\; \leq \mathbb{E}[ \sup_z f(X_1, \ldots, X_{k-1}, z, X_{k+1}, \ldots, X_n) \mid X_1, \ldots, X_k]
\end{align*}
but by independence of $X_k$'s, we have 
\[\mathbb{E}[ \inf_z f(X_1, \ldots, z, \ldots, X_n) \mid X_1, \ldots, X_k] = \mathbb{E}[ \inf_z f(X_1, \ldots, X_{k-1}, z, X_{k+1}, \ldots, X_n) \mid X_1, \ldots, X_{k-1}]\]
So, setting 
\begin{align*}
    A_k & = \mathbb{E}[ \inf_z f(X_1, \ldots, X_{k-1}, z, X_{k+1}, \ldots, X_n) - f(X_1, \ldots, X_n) \mid X_1, \ldots, X_{k-1}] \\
    B_k & = \mathbb{E}[ \sup_z f(X_1, \ldots, X_{k-1}, z, X_{k+1}, \ldots, X_n) - f(X_1, \ldots, X_n) \mid X_1, \ldots, X_{k-1}]
\end{align*}
we have $A_k \leq \Delta_k \leq B_k$ for all $k$, and by Azuma-Hoeffding's inequality along with the fact that $||B_k - A_k|| \leq ||D_k f||_\infty$, we get 
\[\mathbb{P}[ f(X_1, \ldots, X_n) - \mathbb{E}[ f(X_1, \ldots, X_n)] \geq t] \leq \exp \bigg(- \frac{2t^2}{\sum_{k=1}^n ||B_k - A_k||^2_\infty} \bigg) \leq \exp \bigg(- \frac{2t^2}{\sum_{k=1}^n ||D_k f||^2_\infty} \bigg)\]
\end{proof}

We should treat McDiarmid's inequality as a subgaussian form of the bounded difference inequality 
\[\Var[ f(X_1, \ldots, X_n)] \leq \frac{1}{4} \mathbb{E} \bigg[ \sum_{k=1}^n \big(D_k f (X_1, \ldots, X_n)\big)^2 \bigg]\]
The bounded difference inequality says that the variance is controlled by the expectation of the square gradient of the function $f$. In contrast, McDiarmid's inequality asserts the stronger subgaussian inequality, but under the stronger condition that the variance proxy is controlled by a uniform upper bound on the square gradient rather than its expectation. This will be a recurring theme: 
\begin{enumerate}
    \item the expectation of the square gradient controls the variance 
    \item a uniform bound on the square gradient controls the subgaussian property
\end{enumerate}
Note that McDiarmid's theorem is not satisfactory. The appropriate notion of a square gradient in both inequalities is the random variable $\sum_{k=1}^n |D_k f|^2$. To control the variance, we want to take its expectation $\mathbb{E} [\sum_{k=1}^n |D_k f|^2]$, and to control the upper bound of the square gradient, we simply want to take its supremum $|| \sum_{k=1}^n |D_k f|^2||_\infty$. However, McDiarmid's inequality only yields control in terms of the larger quantity $\sum_{k=1}^n || D_k f||^2_\infty$ (by triangle inequality), which gets worse in higher dimensions. Rather than taking the supremum of square gradient, we just take the supremum of each (squared) component and add them up, which may be much greater than the actual upper bound. Therefore, the martingale method is far too crude to capture this idea, and we will need new techniques for more refined bounds. 

\begin{exercise}[Bin Packing]
For the Bin packing problem previoulsly, show that the variance bound $\Var[B_n] \leq n/4$ can be strengthened to a Gaussian tail bound 
\[\mathbb{P}(|B_n - \mathbb{E} B_n| \geq t) \leq 2e^{-2t^2/n}\]
\end{exercise}
\begin{solution}
We can see that 
\[D_k f(X_1, \ldots, X_n) = f(X_1, \ldots, X_{k-1}, 1, X_{k+1}, \ldots, X_n) - f(X_1, \ldots, X_{k-1}, 1, X_{k+1}, \ldots, X_n) = 1\]
and by McDiarmid's inequality, we are done. 
\end{solution}

\begin{exercise}[Rademacher Processes]

\end{exercise}

\begin{exercise}[Sums in Hilbert Space]
Let $X_1, \ldots, X_n$ be independent random variables with zero mean that map to a Hilbert space, and suppose that $||X_k|| \leq C$ a.s. for every $k$. 
\begin{enumerate}
    \item Show that for all $t \geq 0$, 
    \[\mathbb{P} \bigg[ \bigg| \bigg| \frac{1}{n} \sum_{k=1}^n X_k \bigg| \bigg| \geq \mathbb{E} \bigg| \bigg| \frac{1}{n} \sum_{k=1}^n X_k \bigg| \bigg| + t \bigg] \leq e^{-nt^2 / 2C^2} \]
    
    \item Show that 
    \[\mathbb{E} \bigg| \bigg| \frac{1}{n} \sum_{k=1}^n X_k \bigg| \bigg| \leq C n^{-1/2}\]
    
    \item Conclude that for all $t \geq C n^{-1/2}$, 
    \[\mathbb{P} \bigg[ \bigg| \bigg| \frac{1}{n} \sum_{k=1}^n X_k \bigg| \bigg| \geq t \bigg] \leq e^{-nt^2 / 8C^2}\]
    
    \item Finally, argue that for all $t \geq 0$, 
    \[\mathbb{P} \bigg[ \bigg| \bigg| \frac{1}{n} \sum_{k=1}^n X_k \bigg| \bigg| \geq t \bigg] \leq e^{-nt^2 / 8C^2}\]
\end{enumerate}
\end{exercise}

\subsection{The Entropy Method}

In order to develop more sophisticated concentration inequalities, let us introduce another term that is used to measure the deviation of a random variable. 

\begin{definition}[Entropy]
The \textbf{entropy} of a nonnegative random variable $Z$ is defined 
\[\Ent[Z] \coloneqq \mathbb{E}[Z \log Z] - \mathbb{E}[Z] \log \mathbb{E}[Z]\]
\end{definition}

\begin{lemma}[Herbst]
Suppose that random variable $X$ satisfies
\[\Ent[e^{\lambda X}] \leq \frac{\lambda^2 \sigma^2}{2} \mathbb{E}[e^{\lambda X}]  \text{ for all } \lambda \geq 0\]
Then, $X$ is $\sigma^2$-subgaussian. That is, 
\[\psi(\lambda) \coloneqq \log\mathbb{E}[e^{\lambda (X - \mathbb{E}[X])}] \leq \frac{\lambda^2 \sigma^2}{2} \text{ for all } \lambda \geq 0\]
\end{lemma}
\begin{proof}
As $\psi(\lambda) = \log\mathbb{E}[ e^{\lambda X}] - \lambda \mathbb{E}[X]$, we have 
\[\frac{d}{d \lambda} \frac{\psi(\lambda)}{\lambda} = \frac{1}{\lambda} \frac{\mathbb{E}[X e^{\lambda X}]}{\mathbb{E}[e^{\lambda X}]} - \frac{1}{\lambda^2} \log \mathbb{E}[e^{\lambda X}] = \frac{1}{\lambda^2} \frac{\Ent [e^{\lambda X}]}{\mathbb{E}[e^{\lambda X}]} \leq \frac{\sigma^2}{2}\]
where the last inequality yields from the assumption. By the fundamental theorem of calculus, we have 
\[\frac{\psi (\lambda)}{\lambda} = \lim_{\lambda \downarrow 0} \frac{\psi(\lambda)}{\lambda} + \int_0^\lambda \frac{1}{t^2} \frac{\Ent[e^{t X}]}{\mathbb{E}[e^{t X}]} \,dt \leq \frac{\lambda \sigma^2}{2} \implies \psi(\lambda) \leq \frac{\lambda^2 \sigma^2}{2}\]
\end{proof}

\begin{exercise}
It turns out that the converse is true up to a constant: If $X$ is $\frac{\sigma^2}{4}$-subgaussian, then 
\[\Ent [e^{\lambda X}] \leq \frac{\lambda^2 \sigma^2}{2} \mathbb{E}[e^{\lambda X}]\]
\end{exercise}
\begin{solution}
We know that by Jensen's inequality and concavity of the logarithm, 
\[\log \mathbb{E}[e^{\lambda(X - \mathbb{E} X)}] \geq \mathbb{E}[\lambda (X - \mathbb{E} X)] = 0 \implies \mathbb{E}[e^{\lambda(X - \mathbb{E} X)}] \geq 1\]
Furthermore, note that given $Z = e^{\lambda X} / \mathbb{E}[e^{\lambda X}]$, we have 
\begin{align*}
    \mathbb{E}[Z \log{Z}] & = \mathbb{E} \bigg[ \frac{e^{\lambda X}}{\mathbb{E}[e^{\lambda X}]} \, \log \bigg( \frac{e^{\lambda X}}{\mathbb{E}[e^{\lambda X}]} \bigg) \bigg] \\
    & = \frac{1}{\mathbb{E}[e^{\lambda X}]} \mathbb{E}\big[ e^{\lambda X} \big( \log e^{\lambda X} - \log \mathbb{E}[e^{\lambda X}] \big) \big] \\
    & = \frac{1}{\mathbb{E}[e^{\lambda X}]} \mathbb{E} \big[ e^{\lambda X} \lambda X - e^{\lambda X} \log \mathbb{E}[e^{\lambda X}] \big] \\
    & = \frac{1}{\mathbb{E}[e^{\lambda X}]} \Big( \mathbb{E} [ e^{\lambda X} \lambda X ] - \mathbb{E}[ e^{\lambda X}] \, \log \mathbb{E}[e^{\lambda X}] \Big) \\
    & = \frac{\Ent [e^{\lambda X}]}{\mathbb{E}[e^[{\lambda X}]} 
\end{align*}

\end{solution}

Since this theorem assumes a bound on $\Ent[e^{\lambda X}]$ rather than $\Ent[X]$, we will mainly be working with the entropy of exponentials of a random variable. 

It turns out that entropy behaves very similarly to variance and extends nicely into the subgaussian setting. Just like variance, we define the partial entropy of function $f(x_1, \ldots, x_n)$ as 
\[\Ent_k f (x_1, \ldots, x_n) \coloneqq \Ent[ f(x_1, \ldots, x_{k-1}, X_k , x_{k+1}, \ldots, x_n)]\]
That is, $\Ent[f(X_1, \ldots, X_n)]$ is the entropy of $f(X_1, \ldots, X_n)$ with respect to the variable $X_k$ only, the remaining variables kept fixed. 

\begin{theorem}[Tensorization of Entropy]
Given that $X_1, \ldots, X_n$ are independent, 
\[\Ent[ f(X_1, \ldots, X_n)] \leq \mathbb{E} \bigg[ \sum_{k=1}^n \Ent_k f (X_1, \ldots, X_n) \bigg]\]
\end{theorem}

Recall that the basic method for deriving Poincare inequalities is that we have some bound on the variance of a single random variable 
\[\Var_\mu [g] \leq \mathbb{E}[|\nabla g|^2]\]
and by tensorization, we can take the multivariate function $f$ and derive 
\[\Var_\mu [f] \leq \mathbb{E}[ ||\nabla g||^2 ]\]
In here, we derive modified log-Sobolev inequalities by bounding the entropy of the form 
\[\Ent_\mu [e^g] \leq \mathbb{E}[ |\nabla g|^2 \, e^g ]\]
and then using tensorization to bound 
\[\Ent_\mu [e^{\lambda f}] \leq \mathbb{E} [ ||\nabla (\lambda f)||^2 \, e^{\lambda f} ]\]

\begin{lemma}[Discrete Modified log-Sobolev]
Let $D^- f \coloneqq f - \inf f$. Then, 
\[\Ent[e^f] \leq \Cov[f, e^f] \leq \mathbb{E}[|D^- f|^2 e^f]\]
\end{lemma}
\begin{proof}
Note that $\log \mathbb{E}[e^f] \geq \mathbb{E}[f]$ by Jensen's inequality. Therefore, 
\[\Ent[e^f] = \mathbb{E}[f e^f] - \mathbb{E}[e^f] \, \log \mathbb{E}[e^f] \leq \mathbb{E}[f e^f] - \mathbb{E}[f] \mathbb{E}[e^f] = \Cov[f, e^f]\]
To prove the second part, we have 
\[\Cov[f, e^f] = \mathbb{E}[(f - \mathbb{E}[f]))(e^f - \mathbb{E}[e^f])] \leq \mathbb{E}[(f - \inf f)(e^f - e^{\inf f})] \]
and since $e^x$ is convex, the first-order condition gives 
\[e^{\inf f} \geq e^f + e^f (\inf f - f) \implies e^f - e^{\inf f} \leq e^f (f - \inf f)\]
and substituting above gives the result. 
\end{proof}

Now, by defining the one-sided differences 
\begin{align*}
    D_k^- f (x) & = f(x_1, \ldots, x_n) - \inf_z f (x_1, \ldots, x_{k-1}, z, x_{k+1}, \ldots, x_n) \\
    D_k^+ f (x) & = \sup_z f (x_1, \ldots, x_{k-1}, z, x_{k+1}, \ldots, x_n) - f(x_1, \ldots, x_n) 
\end{align*}
we can use the discrete modified log-Sobolev inequality on each of them and then tensorize to get the following. 

\begin{theorem}[Bounded Difference Inequality]
For all $t \geq 0$, 
\begin{align*}
    \mathbb{P}[ f(X_1, \ldots, X_n) - \mathbb{E}[f(X_1, \ldots, X_n) \geq t] & \leq \exp \bigg( -\frac{t^2}{4 || \sum_{k=1}^n |D_k^- f|^2||_\infty} \bigg) \\
    \mathbb{P}[ f(X_1, \ldots, X_n) - \mathbb{E}[f(X_1, \ldots, X_n) \leq -t] & \leq \exp \bigg( -\frac{t^2}{4 || \sum_{k=1}^n |D_k^+ f|^2||_\infty} \bigg) 
\end{align*}
whenever $X_1, \ldots, X_n$ are independent. In particular, $f(X_1, \ldots, X_n)$ is subgaussian with variance proxy $2 ||\sum_{k=1}^n |D_k f|^2 ||_\infty$, where $D_k f = \sup_z f - \inf_z f$. 
\end{theorem}

\subsection{Modified log-Sobolev Inequalities}

\begin{theorem}[Modified log-Sobolov Inequality]
Let $P_t$ be a Markov semigroup with stationary measure $\mu$. The following are equivalent: 
\begin{enumerate}
    \item $\Ent_\mu [f] \leq c \mathcal{E}(\log f, f)$ for all $f$ (modified log-Sobolev inequality). 
    \item $\Ent_\mu [P_t f] \leq e^{-t/c} \Ent_\mu [f]$ for all $f, t$ (entropic exponential ergodicity). 
\end{enumerate}
Moreover, if $\Ent_\mu [P_t f] \rightarrow 0$ as $t \rightarrow +\infty$, then 
\[\mathcal{E}(\log P_t f, P_t f) \leq e^{-t/c} \mathcal{E}(\log f, f) \text{ for all } f, t\]
implies $1$ and $2$ above. 
\end{theorem}


\section{Lipschitz Concentration and Transportation Inequalities}

\subsection{Concentration in Metric Spaces}

Recall what a Lipschitz function is. 

\begin{definition}[Lipschitz Function]
Let $(X, d)$ be a matrix space. A function $f: X \rightarrow \mathbb{R}$ is called $L$-\textbf{Lipschitz} if $|f(x) - f(y)| \leq L \, d(x, y)$ for all $x, y \in X$. The family of all $1$-Lipschitz functions is denoted $\Lip(X)$. 
\end{definition}

Remember that given iid $X_1, \ldots, X_n \sim N(0, 1)$, Gaussian concentration states that the random variable is $|| ||\nabla f||^2 ||_\infty$-subgaussian. But we can write it in an equivalent way in terms of a Lipschitz property. 

\begin{lemma}
Let $f: \mathbb{R}^n \rightarrow \mathbb{R}$ be a $C^1$ function. Then, $|| ||\nabla f||^2 ||_\infty \leq L^2$ if and only if $f$ is $L$-lipschitz. 
\end{lemma}

Therefore, if given random vector $X \sim N(0, I)$, then $f(X)$ is $1$-subgaussian for every $f \in \Lip(\mathbb{R}^n, ||\cdot||)$. 

\end{document}

\section{Generative Adversarial Networks} 

  The year after Kigma published his paper on VAEs, Ian Goodfellow in 2014 published \cite{gans} on \textit{generative adversarial networks}. Unlike the current generative models, which relied on ``fancy'' methods like variational Bayes or contrastive divergence, Goodfellow claimed that we can just do this with vanilla MLPs. The general idea is to approximate the true density $p^\ast (x)$, you are trying to build a function (a neural network) $G_\alpha$ that is a transformation of a simple latent variable, e.g. $Z \sim \mathcal{N}(0, 1)$, such that the pdf of $G_\alpha (Z)$ is similar to $p^\ast (x)$. 

  Remember that for the generative models that we have so far, we were able to \textit{explicitly model} the true pdf. 
  \begin{enumerate}
    \item \textit{Explicit and Analytical}. The simplest types of densities can be analytically written and can be sampled from directly (e.g. Gaussian with inverse CDF, Box-Muller transform, and even GMMs). 

    \item \textit{Explicit and Approximate}. More complicated models such as the RBM and VAE cannot be analytically written, but can still be directly sampled from with probabilities. Boltzmann machines approximate the explicit density as $p_\theta (x) = \frac{1}{Z} e^{-E(x, z)}$ for some quadratic term $-E(x, z)$. VAEs construct another family of posterior distributions $\{q_\lambda\}$ where each $\lambda$ is the output of an encoder neural network $E_\alpha$. Later on, we will see that normalizing flows and autoregressive models also fall in this category. 

    \item \textit{Implicit}. GANs approximate the true pdf through a transformation of random variables and are implicit in the way that you can't estimate the probabilities but you can still sample from them.\footnote{Given $Z \sim \mathcal{N}(0, 1)$ and the sine function $f(z) = \sin(z)$, it's hard to analytically compute the pdf of $X = f(Z)$. However, to sample from this, we can just sample $z$ from $Z$ and then transform it through $f$. }

    \begin{figure}[H]
      \centering 
      \includegraphics[scale=0.4]{img/09_GAN/implicit_gen_models.png}
      \caption{ You essentially want to take some simple latent distribution and construct a differentiable a \textbf{generator model} that maps it to a more complicated distribution in the observable space. } 
      \label{fig:implicit_gen_models}
    \end{figure}
  \end{enumerate}

  The problem with VAEs is that they tend to generate blurry images, which is the result of optimizing a variational lower bound rather than the true objective. This makes it easy to identify whether a given image is from the true dataset or has been artificially generated. In contrast, GANs generate high-resolution images because we are directly optimizing the true objective. 

  \begin{definition}[Generative Adversarial Networks]
    A \textbf{generative adversarial network (GAN)} is a nonlinear latent-variable model consisting of 
    by optimizing a pair of generator and disciminator neural networks, which play a game where one tries to bea the other. 
    \begin{enumerate} 
      \item A latent random variable $Z$ with pdf $p(z)$. 
      \item The \textbf{generator} tries to generate fake samples to fool the discriminator. We sample from a latent space $\mathbf{z}$ and run that through the neural network to get $\mathbf{x} = \mathcal{G}_{\theta_g} (\mathbf{z})$.i It should be differentiable, but does not have to be invertible.   
      \item The \textbf{discriminator} tries to distinguish between real and fake samples, like a critic which can tell from real from fake. It should also be differentiable, and its output is essentially $0 \leq \mathcal{D}_{\theta_d}(\mathbf{x}) \leq 1$, with a value of $1$ if real, $0$ if fake.  
    \end{enumerate}
    We want to train these two models against each other, and in the end, we throw $\mathcal{D}$ away, since it's only role is to force $\mathcal{G}_{\theta_g}$ to work harder, which leaves us with a really good generative $\mathcal{D}_{\theta_g}$.  
    
    \begin{figure}[H]
      \centering 
      \includegraphics[scale=0.4]{img/09_GAN/general_gan_architecture.png}
      \caption{} 
      \label{fig:}
    \end{figure}
  \end{definition}

  \begin{definition}[GAN Loss]
    Now each of these networks will have its own set of parameters which we have to optimize. We want to optimize them by maximizing the likelihood such that the model says ``real" to the samples from the world and ``fake" to the generated samples. This leads to 
    \begin{equation}
      \mathcal{L}(\theta_d, \theta_g) = V(\mathcal{D}, \mathcal{G}) = \underbrace{\mathbb{E}_{x \sim real} \big[ \log \mathcal{D}_{\theta_d}(\mathbf{x}) \big]}_{\substack{\text{log-prob that } \mathcal{D} \text{ correctly} \\ \text{predicts real data as real}}} + \underbrace{\mathbb{E}_{\mathbf{z}} \big[ \log \big( 1 - \mathcal{D}_{\theta_d} ( \mathcal{G}_{\theta_g} (\mathbf{z}))\big)\big]}_{\substack{{log-prob that } \mathcal{D} \text{ correctly} \\ \text{predicts generated data as fake} }}
    \end{equation}
    Therefore, the discrimiator is trying to maximize its reward (to get max value of $0$), and the generator is trying to minimize the disciminator's reward (pulling this log probability down to $-\infty$). This is known as a \textbf{minimax} optimization problem, and we have to find 
    \begin{equation}
      \min_{\theta_g} \max_{\theta_d} V(\mathcal{D}_{\theta_d}, \mathcal{G}_{\theta_g})
    \end{equation}
    which is some saddle point. 
  \end{definition}

  Before we even talk about evaluating the gradients for this, let's prove a result on the properties of such an optimum.   

  \begin{theorem}[Global Optimality] 
    Given the minimax loss above, for a fixed $\mathcal{G}$, the optimal discriminator $\mathcal{D}_G^\ast$ is given by 
    \begin{equation}
      \mathcal{D}_G^\ast (\mathbf{x}) = \frac{p(\mathbf{x} \mid \text{real})}{p(\mathbf{x} \mid \text{real}) + p(\mathbf{x} \mid \text{synthesized})}
    \end{equation}
    Therefore, the global minimum of the training criterion, $\max_{\mathcal{D}} V(\mathcal{D}, \mathcal{G})$ is achieved if and only if 
    \begin{equation}
      p(\mathbf{x} \mid \text{real}) = p(\mathbf{x} \mid \text{synthesized})
    \end{equation}
  \end{theorem}
  \begin{proof} 
    We first have 
    \begin{equation}
      V(\mathcal{D}, \mathcal{G}) = \mathbb{E}_{\mathbf{x} \sim real} \log \mathcal{D}_{\theta_d} (\mathbf{x}) + \mathbb{E}_{\mathbf{z}} \log \big( 1 - \mathcal{D}_{\theta_d} (\mathcal{G}_{\theta_g} (\mathcal{z}))\big) 
    \end{equation}
    Since $\mathcal{G}$ is fixed (i.e. $\theta_g$ is fixed) and acting, we can write the second expectation with respect to the probability measure induced by $\mathcal{G}$.
    \begin{align*} 
      V(\mathcal{D}, \mathcal{G}) & = \mathbb{E}_{\mathbf{x} \sim real} \log \mathcal{D}_{\theta_d} (\mathbf{x}) + \mathbb{E}_{\mathbf{w} \sim \text{fake}} \log \big( 1 - \mathcal{D}_{\theta_d} (\mathbf{w}) \big) \\   
                                      & = \int p(\mathbf{x} \mid \text{real}) \log \mathcal{D}_{\theta_d}(\mathbf{x}) \,d\mathbf{x}+ \int p(\mathbf{x} \mid \text{fake}) \, \log \big( 1 - \mathcal{D}_{\theta_d} (\mathbf{x}) \big) \,d\mathbf{x} 
    \end{align*}
    where the $\mathbf{x}$ in the second integral is a dummy variable. Taking the derivative w.r.t. $\mathbf{x}$ and setting equal to $0$ gives 
    \begin{equation}
      p(\mathbf{x} \mid \text{real}) \frac{1}{\mathcal{D}_{\theta_d} (\mathbf{x})} + p(\mathbf{x} \mid \text{fake}) \cdot \frac{-1}{1 - \mathcal{D}_{\theta_d}(\mathbf{x})} = 0
    \end{equation}
    implies that 
    \begin{equation}
      \mathcal{D}_{\theta_d}( \mathbf{x}) = \frac{p(\mathbf{x} \mid \text{real})}{p(\mathbf{x} \mid \text{real}) + p(\mathbf{x} \mid \text{fake})}
    \end{equation}
  \end{proof} 

  If the discriminator $\mathcal{D}$ is optimal, then the generator is minimizing the \textbf{Jensen-Shannon divergence} between the real and generated (model distributions). However, $\mathcal{D}$ is not optimal in practice since we have limited computational resources, the loss is non-convex, etc. 

  \begin{figure}[H]
    \centering 
    \includegraphics[scale=0.3]{img/equil.png}
    \caption{} 
    \label{fig:nash_equilibrium}
  \end{figure}

  Now we can talk about evaluating the gradients and optimizing it. What is the gradient of such a loss function? This is quite easy since the expectations are over the true data distribution and the known prior $z$, and so we can swap gradients and expectations accordingly. 

  \begin{lemma}[Gradients of GAN Loss]
    
  \end{lemma} 
  \begin{proof}
    
  \end{proof}

  Note that the generator has no effect on the probability of $\mathcal{D}$ correctly identifying real images, so it only focuses on the latter term. 


  \begin{algo}[Alternative Gradient Descent]
    The idea is to train both models simultaneously via SGD using mini-batches consisting of some generated samples and some real-world samples, which is called the \textbf{alternative gradient descent algorithm}. It is usually the case that the generator does better than the disciminator, so we sometimes make $k > 1$ to allow more steps for training. 
    \begin{algorithm}[H]
      \caption{Minibatch stochastic gradient descent training of generative adversarial nets. The number of steps to apply to the discriminator, $k$, is a hyperparameter. We used $k = 1$, the least expensive option, in our experiments.}
      \label{alg:alt_gd}
      \begin{algorithmic}[1] 
        \For{number of training iterations}
            \For{$k$ steps}
                \State Sample minibatch of $m$ noise samples $\{z^{(1)}, \ldots, z^{(m)}\}$ from noise prior $p_g(z)$.
                \State Sample minibatch of $m$ examples $\{x^{(1)}, \ldots, x^{(m)}\}$ from data generating distribution $p_{\text{data}}(x)$.
                \State Update the discriminator by ascending its stochastic gradient:
                \[
                    \nabla_{\theta_d} \frac{1}{m} \sum_{i=1}^m \left[\log D\left(x^{(i)}\right) + \log\left(1-D\left(G\left(z^{(i)}\right)\right)\right)\right].
                \]
            \EndFor
            \State Sample minibatch of $m$ noise samples $\{z^{(1)}, \ldots, z^{(m)}\}$ from noise prior $p_g(z)$.
            \State Update the generator by descending its stochastic gradient:
            \[
                \nabla_{\theta_g} \frac{1}{m} \sum_{i=1}^m \log\left(1-D\left(G\left(z^{(i)}\right)\right)\right).
            \]
        \EndFor
      \end{algorithmic}
    \end{algorithm}
    The gradient-based updates can use any standard gradient-based learning rule. We used momentum in our experiments.
  \end{algo} 

  There is a vanishing gradient problem in GANs. For instance, assume that the model $\mathcal{G}_{\theta_g}$ has very bad parameters, and it generates very bad samples that the disciminator can detect very well. Then, $\mathbb{E}_{\mathbf{z}} \big[ \log \big( 1 - \mathcal{D}_{\theta_d} (\mathcal{G}_{\theta_g}))\big) \big]$ will be very close to $0$, and the generator's cost will be very flat. Therefore, the gradient would die out and the generator can't improve! For example, if $\mathcal{D}$ was just a sigmoid function, then we can approximate the gradient of the expectation with a sample of the gradient, which would die out. 

  \begin{align*} 
    \nabla_{\theta_g} V(\mathcal{D}, \mathcal{G}) & = \nabla_{\theta_g} \mathbb{E}_{z \sim q(\mathbf(\mathbf{z})} \big[ \log \big( 1 - \mathcal{D}(\mathcal{G}(\mathbf{z}))\big) \big] \\
                                                  & \approx \nabla_a \log \big( 1 - \sigma(a)\big) \\
                                                  & = - \frac{\sigma(a) \, \big( 1 - \sigma(a)\big)}{1 - \sigma(a)} = - \sigma(a) = - \mathcal{D}(\mathcal{G}(\mathbf{z})) 
  \end{align*} 

  and so the gradient goes to $0$ if $\mathcal{D}$ is confident, i.e. $\mathcal{D}(\mathcal{G}(\mathbf{z})) \rightarrow 0$. Therefore, we can modify the cost for the generator term by changing the cost to 
  \begin{equation}
    \mathbb{E}_{\mathbf{z}} \log \big( 1 - \mathcal{D}_{\theta_d} ( \mathcal{G}_{\theta_g} (\mathbf{z}))\big)
  \end{equation}
  and trying to minimize it. 


  \begin{theorem}[Convergence]
    
  \end{theorem}

\subsection{Conditional GAN} 

\subsection{CycleGAN}

\subsection{Deep Convolution GAN (DCGAN)} 


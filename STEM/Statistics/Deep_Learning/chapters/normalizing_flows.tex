\section{Normalizing Flows} 

  We have seen many examples of generative models that attempt to produce a probability distribution $p$ that approximates the true pdf $p^\ast$ of the data samples. Some are given by an explicit model (e.g. GMMs, RBMs, VAEs) which approximate with parameterized form $p^\ast \sim p_\theta$, while in others (GANs) the model is implicit since we model the random variable as a transformation $X = G_{g_\theta} (Z)$ through a neural network with $Z \sim \mathcal{N}(0, I)$. If we focus on VAEs---specifically variational inference---for a second, recall that the performance of the model really just depends on the fact that we can approximate the intractable posterior $p(z \mid x)$ with some parameterized family $q_\phi (z)$\footnote{This can be written $q_\phi (z) = q_\phi (z \mid x)$, but since the encoder network produces the $\phi = E_\alpha (x)$, the $q_\phi(z)$ really represents a conditional distribution given $x$.} 

  \begin{equation}
    \elbo(x^{(i)}, \phi, \theta) = \underbrace{\mathbb{E}_{q_\phi (z \mid x^{(i)})} [\log p_{\theta} (x^{(i)} \mid z)]}_{\substack{\text{likelihood term} \\ \text{(reconstruction part)}}}- \underbrace{KL(q_{\phi} (z \mid x^{(i)}) \mid\mid p(z))}_{\substack{\text{closeness of encoding to } p(z) \\ \text{(typically Gaussian)}}}
  \end{equation}
  This may not be advantageous and flexible enough to capture the true posterior if $q_\phi(z)$ is far away from $p(z \mid x)$

  To address this problem, in 2015, Google Deepmind through \cite{flow} introduced the idea of \textit{flow-based} models \footnote{This has nothing to do with flow graphs and the max-flow-min-cut theorem in graph theory.} which---like GANs---want to map simple distributions (e.g. a Gaussian) to complex densities representing the data. This would normally result in an implicitly defined pdf, but by making the transformation to be \textit{invertible}, we can use the change of basis formula to get a closed-form of the pdf, making flow models an \textit{explicit} model of the pdf. Recall the lemma below from multivariate calculus. 
  
  \begin{lemma}[Jacobi] 
    Let $X, Z$ be absolutely continuous random variables in $\mathbb{R}^n$. 
    Given that $f: \mathbb{R}^n \rightarrow \mathbb{R}^n$ is invertible and differentiable everywhere, with $X = f(Z), Z = f^{-1} (X)$, we claim 
    \begin{equation}
      p_X (x) = p_Z (z) \cdot \bigg| \det \frac{\partial f^{-1}}{\partial x} \bigg| = p_Z (z) \cdot \bigg| \det \frac{\partial f}{\partial z}  \bigg|^{-1}
    \end{equation} 
    where $\det$ is the determinant of the total derivative (Jacobian). 
  \end{lemma}
  \begin{proof}
    For $n = 1$, we have 
    \begin{align}
      p_X (x) & = \frac{d}{dx} F_X (x) \\
              & = \frac{d}{dx} F_Z (f^{-1}(x)) \\
              & = p_z (f^{-1} (x)) \cdot \frac{d}{dx} f^{-1} (x)  \\
              & = p_z 
    \end{align}
  \end{proof} 

  Therefore, if we parameterize $f$ with some $\theta$, then the marginal likelihood of $x$ given $\theta$ can be written as 
  \begin{equation}
    p_X (x;\; \theta) = p_Z (f^{-1}_\theta (x)) \cdot \bigg| \det \frac{\partial f_\theta^{-1}}{\partial x} \bigg|
  \end{equation} 
  Therefore, if $X$ is a complex distribution and $Z$ is a simple one (e.g. uniform), we might have hope to efficiently compute $p_X (x)$ for any $x \in \mathbb{R}^n$ if 
  \begin{enumerate}
    \item we can efficiently compute $f^{-1}_\theta (x)$, which allows us to compute $p_Z (f^{-1} (x))$ efficiently since $Z$ is simple. 
    \item we can efficiently compute the determinant of the Jacobian $|\det (\partial f_\theta^{-1}/\partial x)|$.\footnote{Note that computing determinants are approximately $O(n^{2.4})$, which may not be practical. } 
  \end{enumerate}  

\subsection{Finite Normalizing Flows}

  It looks like we have split this enormously hard problem of modeling $X$ with two slightly less difficult problems. However, with some tricks, we may be able to get a simple enough parameterization of $f$ to be able to compute its inverse plus the determinant of the Jacobian. Note that the construction of a neural network was to take a simple function (one layer) and construct a complex family of functions composed of multiple layers. This is what we can do as well. 

  \begin{corollary}[Finite Normalizing Flow]
    Given a sequence of transformations 
    \begin{equation}
      Z = Z_0 \xrightarrow{f_1} Z_1 \xrightarrow{f_2} \ldots \xrightarrow{f_{K-1}} Z_{K-1} \xrightarrow{f_K} Z_K = X
    \end{equation} 
    where each $f_k$ is invertible and differentiable everywhere, let us denote $f = f_K \circ \ldots \circ f_1: Z \rightarrow X$, which is invertible. Then
    \begin{equation}
      p_X (x) = p_Z (f^{-1} (x)) \cdot \prod_{k=1}^K \bigg| \det \frac{\partial f_k}{ \partial z_{k-1}} \bigg|^{-1}
    \end{equation}
    This sequence of random variables $Z_k$ is called a \textbf{flow}, and the sequence of the corresponding pdfs $p_{Z_k}$ is called the \textbf{normalizing flow}. As for how we parameterize this, our notation will assume that $\theta = (\theta_1, \ldots, \theta_M)$ and each $\theta_m$ parameterizes $f_m$.
  \end{corollary} 
  \begin{proof}
    This can automatically be proved by induction. For an example, consider the sequence of invertible functions $Z \xrightarrow{f} Y \xrightarrow{g} \rightarrow X$, where $Z$ is a simple distribution that gets transformed to a slightly more complicated distribution $Y$ that then gets transformed to a complex distribution $X$. We can apply the Jacobi theorem above to see $p_Y (y) = p_Z (f^{-1} (y)) \cdot | (Df^{-1})(x)|$, and so we have 
    \begin{align}
      p_X (x) & = p_Y (g^{-1} (x)) \cdot | (D g^{-1}) (x) | \\
              & = p_Z (f^{-1} (g^{-1} (x))) \cdot | (Df^{-1}) (y)| \cdot | (D g^{-1}) (x) | \\ 
              & = p_Z ((g \circ f)^{-1} (x))) \cdot | (Df^{-1}) (y)| \cdot | (D g^{-1}) (x) | 
    \end{align}
  \end{proof}

  Therefore, by taking a sequence these functions $f_m$, which may each have a simple parameterization, we may construct a very complex composition $f_\theta$ that may result in a very expressive $Z$. Again, this is very similar to how a composition of linear mappings plus an activation gives us a very expressive neural network, and unsurprisingly, there is an analogue of the universal approximation theorem for transformations of this form. 

  \begin{theorem}[Probability Integral Transform] 
    Any $n$-dimensional random variable in $\mathbb{R}^n$ that is absolutely continuous w.r.t. the Lebesgue measure can be constructed from the uniform distribution $U$ on $[0, 1]^n$. Since we can map to and back from invertibility, any two such random variables $X$ and $Y$ can be mapped from each other, 
    \begin{equation}
      X \mapsto U \mapsto Y 
    \end{equation}
  \end{theorem} 

  Now given this, our job is to maximize the log-likelihood. Since $x = z_K$ and $z = z_0$, we substitute this in 
  \begin{equation}
    \ln p_{Z_K} (x) = \ln p_{Z_0} (f^{-1}(x)) - \sum_{k=1}^K \ln \bigg| \det \frac{\partial f_k}{\partial z_{k-1}} \bigg| 
  \end{equation} 
  and over the dataset we want to find 
  \begin{equation}
    f^\ast = \argmax_{f} \sum_{i=1}^N \ln p_{Z_0} (f^{-1}(x^{(i)})) - \sum_{k=1}^K \ln \bigg| \det \frac{\partial f_k}{\partial z_{k-1}} \bigg| 
  \end{equation}
  where we are really optimizing over $f_1, \ldots, f_K$, i.e. their parameters $\theta_1, \ldots, \theta_K$. Great, now let's define the parametric form of $f$. There are two different types of invertible transformations that we can calculate in linear time. 

  \begin{definition}[Planar Contractions]
    Let us have $\theta = \{w \in \mathbb{R}^D, u \in \mathbb{R}^D, b \in \mathbb{R}\}$ and define the family of \textbf{planar contractions}
    \begin{equation}
      \{f(z) = z + u \cdot h( w^T z + b)\}_\theta
    \end{equation}
    for some smooth nonlinearity $h$.\footnote{As the paper states, this flow can be visualized as modifying the initial density by applying a series of contractions and expansions in the direction perpendicular to the hyperplane $w^T z + b$.} This is not always invertible, but the paper uses $h(x) = \tanh(x)$ with the fact that $w^T u \geq -1$ is sufficient for $f$ to be invertible. 
  \end{definition} 
  
  This already looks like a single layer, so you might wonder why one can't just directly make an invertible neural net? There is already literature on this, but the typical computational complexity of computing the determinant (plus the gradient of the determinant) is of scale $O(LD^3)$, where $L$ is the number of hidden layers and $D$ is the dimension of each hidden layer. It turns out that this decomposition gives us a very cute formula for computing determinants. 

  \begin{lemma}
    The determinant of a planar contraction is 
    \begin{equation}
      \bigg| \det \frac{\partial f}{\partial z} \bigg| = \big| \det (I + u \psi (z)^T ) \big| = \big| 1 + u^T \psi(z) \big| 
    \end{equation} 
    where $\psi(z) = h^\prime (w^T z + b) \cdot w$. This can be computed in $O(D)$ time. 
  \end{lemma}
  \begin{proof}
    Use block matrix multiplication and the \href{https://en.wikipedia.org/wiki/Matrix_determinant_lemma}{matrix determinant lemma}.
  \end{proof}

  The other type mentioned is a radial contraction. 

  \begin{definition}[Radial Contraction]
    Let us have $\theta = \{ c \in \mathbb{R}^D, \alpha \in \mathbb{R}^+, \beta \in \mathbb{R} \}$ and define the family of \textbf{radial contractions} 
    \begin{equation}
      \{ f(z) = z + \beta h(\alpha, r) (z - c) \}_\theta
    \end{equation}
    where $r = |z - c|$ and $h(\alpha, r) = \frac{1}{\alpha + r}$.\footnote{It applies radial contractions and expansions around the reference point $c$ and are referred to as radial flows. }
  \end{definition}

  \begin{lemma} 
    The determinant of a radial contraction is 
    \begin{equation}
      \bigg| \det \frac{\partial f}{\partial z} \bigg| = \big[ 1 + \beta h(\alpha, r)\big]^{d-1} \big[ 1 + \beta h(\alpha, r) + \beta h^\prime (\alpha, r) r \big]
    \end{equation}
  \end{lemma}

  \begin{figure}[H]
    \centering 
    \includegraphics[scale=0.4]{img/contractions.png}
    \caption{The effects of normalizing flow on two distributions. For example, for $k=2$ we can transform a spherical Gaussian into a bimodal distribution by applying 2 successive transformations. } 
    \label{fig:contractions}
  \end{figure} 

  With this form, we can now substitute this into the variational lower bound. Note that we have not just simplified it with the following theorem, but we have also set it up as an expectation over the initial density $p(z)$, analogous to the reparamaterization trick that we have used for VAEs. 

  \begin{theorem}
    The variational lower bound can be written in the simplified form where the 
    \begin{equation}
      \elbo(x) = \mathbb{E}_{q_0 (z_0)} [ \log q_0 (z_0)] - \mathbb{E}_{q_0 (z_0)} [ \log p(x, z_K)] - \mathbb{E}_{q_0 (z_0)} \bigg[ \sum_{k=1}^K \log \big| 1 + u_k^T \psi_k (z_{k-1}) \big| \bigg]
    \end{equation}
    for flow models. 
  \end{theorem}
  \begin{proof}
    By definition $q_\phi(z \mid x) \coloneqq q_K (z_k)$, and substituting this in along with our simplified formula for the log likelihood gives us 
    \begin{align}
      \elbo(x) & \coloneqq \mathbb{E}_{q_\phi (z \mid x)} \big[ \log q_\phi (z \mid x) - \log p(x, z) \big] \\
               & = \mathbb{E}_{q_K (z_K)} \big[ \log q_K (z_K) \big] - \mathbb{E}_{q_K (z_K)} \big[ \log p(x, z_K) \big]  \\ 
               & = \mathbb{E}_{q_K (z_K)}  \bigg[ \log q_0 (z_0) - \sum_{k=1}^K \log \big| 1 + u_k^T \psi_k (z_{k-1}) \big| \bigg] - \mathbb{E}_{q_K (z_K)} \big[ \log p(x, z_K) \big] 
    \end{align}
    But since $z_K = f(z_0)$, using LOTUS we can get 
    \begin{equation}
      \elbo(x) = \mathbb{E}_{q_0 (z_0)} [\log q_0 (z_0)] - \mathbb{E}_{q_0 (z_0)}  \bigg[ \sum_{k=1}^K \log \big| 1 + u_k^T \psi_k (z_{k-1}) \big| \bigg] - \mathbb{E}_{q_0 (z_0)} \big[ \log p(x, z_K) \big] 
    \end{equation}
  \end{proof} 

  At this point we can take the gradients, freely swap them with the expectations, and compute them. 

\subsection{Coupling-Layer Flows} 

    This general method allowing linear-time computation of Jacobian determinants is an example of a \textbf{general normalizing flow}. Before, we have just constructed some family of invertible functions $\{f\}$ to transform the data, such that it was simple enough to calculate the Jacobian determinants in linear time. But note that in general, computing determinants requires you to have an triangular matrix (multiply all the diagonals). Therefore, if we can create a neural network $f$ such that its Jacobian is triangular, we will be done. In order to have such an architecture to support this, we should introduce a special type of layer.  

    \begin{definition}[Coupling Layer]
      Let us have an input $z \in \mathbb{R}^D$ such that it can be partitioned into its first $d$ elements and the rest. 
      \begin{equation}
        z = [z_{1:d}, z_{d+1:D}]
      \end{equation}
      Now let's have 2 neural networks (not necessarily invertible) $F: \mathbb{R}^d \rightarrow \mathbb{R}^{D-d}$ and $H: \mathbb{R}^d \rightarrow \mathbb{R}^{D-d}$ defined 
      \begin{align}
        F(z^\prime) = \beta, \qquad G(z^\prime) = \gamma
      \end{align}
      Then the \textbf{coupling layer} $g: \mathbb{R}^D \rightarrow \mathbb{R}^D$ with parameters $\theta = (\theta_F, \theta_H)$ is defined as the transformation $x = g(z)$, where
      \begin{align}
        x_{1:d} & = z_{1:d} \\ 
        x_{d+1:D} & = z_{d+1:D} \odot \beta + \gamma = z_{d+1:D} \odot F(x_{1:d}) + H(x_{d+1:D})
      \end{align}
    \end{definition}

    So this ``layer'' really composes of two neural networks, and it has both properties that we want. It is invertible, with inverse $z = g^{-1} (x)$ defined 
    \begin{equation}
      z^\prime = x^\prime, \qquad \bar{z} = \frac{\bar{z} - \gamma}{\beta} = \frac{\bar{z} - H(z^\prime)}{F(z^\prime)} 
    \end{equation} 

    \begin{figure}[H]
      \centering
      \begin{subfigure}[b]{0.48\textwidth}
      \centering
        \includegraphics[width=\textwidth]{img/couple_forward.png}
        \caption{Forward pass of the coupling layer.}
        \label{fig:for}
      \end{subfigure}
      \hfill 
      \begin{subfigure}[b]{0.48\textwidth}
      \centering
        \includegraphics[width=\textwidth]{img/couple_backward.png}
        \caption{Backward pass of the coupling layer}
        \label{fig:back}
      \end{subfigure}
      \caption{Coupling layers are easily invertible. For both the forward and backward pass, we must do a forward pass through $F$ and $H$. }
      \label{fig:coupling_layer}
    \end{figure}

    Second, the determinant of this is easy to calculate since it is simply $\beta_1 \times \ldots \times \beta_{D-d}$. Therefore, we can stack these layers on top of each other, parameterized by different sequences of neural nets. 

  \subsubsection{RealNVP}

    By setting each function $f_i$ to be a cascading layer, we can model $f = f_K \circ \ldots \circ f_1$ and do the exact same normalizing flow model as mentioned before. This is pretty much NVP. 

    \begin{figure}[H]
      \centering 
      \includegraphics[scale=0.4]{img/cascading_layers.png}
      \caption{Cascading coupling layers represents $f$, where each layer is $f_i$.} 
      \label{fig:cascading_layers}
    \end{figure}

  \subsubsection{NICE}

    \textbf{Volume preserving flows} design the flow such that the Jacobian determinant is equal to $1$ (which is more restrictive but allows $O(1)$ computation) but still allows for rich posteriors. Therefore, even though computing the transformation $f_K \circ \ldots f_1$ will take longer due to the forward pass of neural nets, the lack of need to calculate the determinant keeps this fast. An example of such a volume-preserving coupling-layer flow model was introduced in \cite{nice} by Dinh in 2014. 

    \begin{definition}[NICE]
      The \textbf{nonlinear independent components estimation (NICE)} model uses \textbf{additive coupling layers} of the form 
      \begin{align}
        x_{1:d} & = z_{1:d} \\
        x_{d+1:n} & = z_{d+1:n} + H(x_{1:d})
      \end{align}
      which we can see has a Jacobian determinant of one. The final layer of NICE is a multiplication by a diagonal matrix with all diagonal elements nonzeros, i.e. $x_i = \beta_i z_i$, which is invertible and has the absolute value of the determinant $\prod_i |\beta_i|$. 
    \end{definition}

    \begin{figure}[H]
      \centering 
      \includegraphics[scale=0.3]{img/nice_coupling.png}
      \caption{An additive coupling layer. } 
      \label{fig:nice_coupling}
    \end{figure}

\subsection{GLOW}

  A lot of hacks to improve it (no math). Improved normalizing flow dramatically. 
  You have a flow and squeeze a bunch of times is GLOW. 

\subsection{Augoregressive Flows} 

\subsection{Infinitesimal Flows} 




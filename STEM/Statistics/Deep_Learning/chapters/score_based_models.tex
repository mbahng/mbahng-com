\section{Score Based Models}

  We have built several types of generative models so far that estimated densities. While calculating PDFs is called \textit{density estimation}, we can indirectly estimate it by calculating the gradient of the PDF (with respect to the sample), which is known as \textbf{score matching}. 

  \begin{definition}[Score]
    Let $X$ be a continuous random variable defined on $\mathbb{R}^n$, and let $p$ be its pdf, parameterized by $\theta$. The score function of $p$ is the gradient of the log-pdf with respect to the sample.\footnote{Note that this is w.r.t. the sample, not the parameter, unlike what we do usually in machine learning.}
    \begin{equation}
      \psi (x; \theta) = \begin{bmatrix} \frac{\partial \log{p(x;\theta)}}{\partial x_1} \\ \vdots \\ \frac{\partial \log{p(x;\theta)}}{\partial x_n} \end{bmatrix} =  
      \begin{bmatrix}
        \psi_1 (x;\theta) \\ \vdots \\ \psi_n (x;\theta)
      \end{bmatrix} = 
      \nabla_x \log{p (x; \theta)}
    \end{equation}
    Note that the score is a function $\psi: \mathbb{R}^n \rightarrow \mathbb{R}^n$. 
  \end{definition} 

  The reason Hyvarinen introduced this score function in 2005 is because we want to have such a score is that it does not depend on the normalizing constant $Z$ (since the log derivative gets rid of it). \cite{orig_score} Therefore, rather than maximizing the likelihood, we want to minimize the L2 distance between the score functions. 

  \begin{equation}
    R(\theta) = \mathbb{E}_{X} \big[ || \psi(x\;\theta) - \psi_{\mathcal{D}} (x) ||^2 \big]
  \end{equation}

  \cite{score}. 

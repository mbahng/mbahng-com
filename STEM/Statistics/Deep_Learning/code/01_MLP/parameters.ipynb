{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In here, we will explore what we can do with a basic neural net. This will apply to future architectures. Let's first create some simple functions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input x : torch.Size([100])\n",
      "Shape of output y : torch.Size([50])\n"
     ]
    }
   ],
   "source": [
    "# create a linear map from R^100 to R^50 with bias \n",
    "linear1 = nn.Linear(\n",
    "    in_features = 100, \n",
    "    out_features= 50, \n",
    "    bias = True\n",
    ")\n",
    "\n",
    "x = torch.randn(100) \n",
    "print(f\"Shape of input x : {x.size()}\")\n",
    "y = linear1(x) \n",
    "print(f\"Shape of output y : {y.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the parameters of this function, through three ways: \n",
    "- Call the `.parameters()` method, which returns a generator of `torch.nn.parameter.Parameter` objects. \n",
    "- Call the `.named_parameters()` method, which returns a generator of tuples of type (`str`, `torch.nn.parameter.Parameter`) \n",
    "- Call the `.state_dict()` method, which returns a generater of all model states. As of this point, the state dict is equivalent, but it is more general and gives more information for more complicated models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just calling Parameters ========\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([[ 0.0254,  0.0228, -0.0663,  ..., -0.0025,  0.0983, -0.0226],\n",
      "        [ 0.0070,  0.0402,  0.0142,  ..., -0.0476, -0.0482,  0.0760],\n",
      "        [-0.0830, -0.0249, -0.0068,  ...,  0.0219,  0.0149, -0.0563],\n",
      "        ...,\n",
      "        [ 0.0198, -0.0488, -0.0460,  ...,  0.0835,  0.0892,  0.0963],\n",
      "        [ 0.0608,  0.0494, -0.0246,  ..., -0.0522, -0.0519,  0.0348],\n",
      "        [-0.0501,  0.0029,  0.0696,  ..., -0.0722,  0.0799,  0.0588]],\n",
      "       requires_grad=True)\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([-0.0206, -0.0592, -0.0231,  0.0037, -0.0225,  0.0966,  0.0424, -0.0877,\n",
      "        -0.0550, -0.0859,  0.0200,  0.0045,  0.0856,  0.0892,  0.0064,  0.0447,\n",
      "        -0.0982, -0.0598,  0.0698,  0.0447, -0.0645, -0.0610, -0.0348, -0.0928,\n",
      "         0.0738, -0.0765, -0.0180, -0.0069,  0.0815,  0.0230, -0.0525,  0.0228,\n",
      "         0.0038, -0.0225,  0.0542,  0.0087,  0.0344,  0.0931, -0.0794,  0.0746,\n",
      "         0.0421, -0.0776,  0.0883, -0.0080, -0.0152,  0.0780,  0.0745,  0.0306,\n",
      "         0.0832, -0.0274], requires_grad=True)\n",
      "Calling Named Parameters ========\n",
      "weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0254,  0.0228, -0.0663,  ..., -0.0025,  0.0983, -0.0226],\n",
      "        [ 0.0070,  0.0402,  0.0142,  ..., -0.0476, -0.0482,  0.0760],\n",
      "        [-0.0830, -0.0249, -0.0068,  ...,  0.0219,  0.0149, -0.0563],\n",
      "        ...,\n",
      "        [ 0.0198, -0.0488, -0.0460,  ...,  0.0835,  0.0892,  0.0963],\n",
      "        [ 0.0608,  0.0494, -0.0246,  ..., -0.0522, -0.0519,  0.0348],\n",
      "        [-0.0501,  0.0029,  0.0696,  ..., -0.0722,  0.0799,  0.0588]],\n",
      "       requires_grad=True)\n",
      "bias\n",
      "Parameter containing:\n",
      "tensor([-0.0206, -0.0592, -0.0231,  0.0037, -0.0225,  0.0966,  0.0424, -0.0877,\n",
      "        -0.0550, -0.0859,  0.0200,  0.0045,  0.0856,  0.0892,  0.0064,  0.0447,\n",
      "        -0.0982, -0.0598,  0.0698,  0.0447, -0.0645, -0.0610, -0.0348, -0.0928,\n",
      "         0.0738, -0.0765, -0.0180, -0.0069,  0.0815,  0.0230, -0.0525,  0.0228,\n",
      "         0.0038, -0.0225,  0.0542,  0.0087,  0.0344,  0.0931, -0.0794,  0.0746,\n",
      "         0.0421, -0.0776,  0.0883, -0.0080, -0.0152,  0.0780,  0.0745,  0.0306,\n",
      "         0.0832, -0.0274], requires_grad=True)\n",
      "Calling Model State Dict ========\n",
      "weight\n",
      "tensor([[ 0.0254,  0.0228, -0.0663,  ..., -0.0025,  0.0983, -0.0226],\n",
      "        [ 0.0070,  0.0402,  0.0142,  ..., -0.0476, -0.0482,  0.0760],\n",
      "        [-0.0830, -0.0249, -0.0068,  ...,  0.0219,  0.0149, -0.0563],\n",
      "        ...,\n",
      "        [ 0.0198, -0.0488, -0.0460,  ...,  0.0835,  0.0892,  0.0963],\n",
      "        [ 0.0608,  0.0494, -0.0246,  ..., -0.0522, -0.0519,  0.0348],\n",
      "        [-0.0501,  0.0029,  0.0696,  ..., -0.0722,  0.0799,  0.0588]])\n",
      "bias\n",
      "tensor([-0.0206, -0.0592, -0.0231,  0.0037, -0.0225,  0.0966,  0.0424, -0.0877,\n",
      "        -0.0550, -0.0859,  0.0200,  0.0045,  0.0856,  0.0892,  0.0064,  0.0447,\n",
      "        -0.0982, -0.0598,  0.0698,  0.0447, -0.0645, -0.0610, -0.0348, -0.0928,\n",
      "         0.0738, -0.0765, -0.0180, -0.0069,  0.0815,  0.0230, -0.0525,  0.0228,\n",
      "         0.0038, -0.0225,  0.0542,  0.0087,  0.0344,  0.0931, -0.0794,  0.0746,\n",
      "         0.0421, -0.0776,  0.0883, -0.0080, -0.0152,  0.0780,  0.0745,  0.0306,\n",
      "         0.0832, -0.0274])\n"
     ]
    }
   ],
   "source": [
    "print(\"Just calling Parameters ========\")\n",
    "for param in linear1.parameters(): \n",
    "    print(type(param))\n",
    "    print(param) \n",
    "\n",
    "print(\"Calling Named Parameters ========\")\n",
    "for name, param in linear1.named_parameters(): \n",
    "    print(name)\n",
    "    print(param) \n",
    "\n",
    "print(\"Calling Model State Dict ========\")\n",
    "for param in linear1.state_dict(): \n",
    "    print(param) \n",
    "    print(linear1.state_dict()[param])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important attributes and methods in parameter objects that you will deal with is the following : \n",
    "- `.data` attribute, which gives you the actual parameter matrix. \n",
    "- `.requires_grad` attribute, which tells you whether the parameter is storing gradients. \n",
    "- `.`\n",
    "\n",
    "You can also filter using the state_dict or the named parameters to find parameters that are only weights or only biases, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor([[ 0.0254,  0.0228, -0.0663,  ..., -0.0025,  0.0983, -0.0226],\n",
      "        [ 0.0070,  0.0402,  0.0142,  ..., -0.0476, -0.0482,  0.0760],\n",
      "        [-0.0830, -0.0249, -0.0068,  ...,  0.0219,  0.0149, -0.0563],\n",
      "        ...,\n",
      "        [ 0.0198, -0.0488, -0.0460,  ...,  0.0835,  0.0892,  0.0963],\n",
      "        [ 0.0608,  0.0494, -0.0246,  ..., -0.0522, -0.0519,  0.0348],\n",
      "        [-0.0501,  0.0029,  0.0696,  ..., -0.0722,  0.0799,  0.0588]])\n"
     ]
    }
   ],
   "source": [
    "for name, param in linear1.named_parameters(): \n",
    "    if 'bias' not in name: \n",
    "        print(param.requires_grad)\n",
    "        print(param.data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how they are initialized, let's make a histogram of the parameters, which shows us that the parameters are uniformly distributed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "param_list = [] \n",
    "\n",
    "for param in linear1.parameters(): \n",
    "    # first flatten it  and then put it into a list\n",
    "    param_list += param.reshape(-1).tolist()\n",
    "    \n",
    "plt.hist(param_list)\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say that we want to have a custom mapping, say the identity and adding a vector of all $2$'s onto our input vector. We can modify this mapping by iterating through the parameters and replacing it with what we want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Weights ========\n",
      "weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0299, -0.0558, -0.0585,  ..., -0.0816, -0.0070,  0.0219],\n",
      "        [ 0.0929,  0.0220,  0.0127,  ...,  0.0034,  0.0754, -0.0020],\n",
      "        [-0.0011,  0.0257,  0.0006,  ...,  0.0357, -0.0484,  0.0411],\n",
      "        ...,\n",
      "        [ 0.0917,  0.0243,  0.0226,  ...,  0.0048,  0.0701,  0.0683],\n",
      "        [-0.0701,  0.0980, -0.0791,  ..., -0.0411,  0.0728,  0.0270],\n",
      "        [-0.0212,  0.0067, -0.0193,  ..., -0.0230, -0.0925,  0.0084]],\n",
      "       requires_grad=True)\n",
      "bias\n",
      "Parameter containing:\n",
      "tensor([ 0.0995, -0.0656,  0.0001, -0.0185, -0.0895,  0.0707, -0.0920,  0.0730,\n",
      "        -0.0846,  0.0945,  0.0118,  0.0152,  0.0142,  0.0434,  0.0172,  0.0894,\n",
      "         0.0638, -0.0862, -0.0055, -0.0566,  0.0945, -0.0462, -0.0587, -0.0815,\n",
      "         0.0314, -0.0407,  0.0321, -0.0175, -0.0699, -0.0792,  0.0049, -0.0699,\n",
      "        -0.0688, -0.0340, -0.0549,  0.0776,  0.0769,  0.0975,  0.0257,  0.0721,\n",
      "         0.0225, -0.0197,  0.0704,  0.0628, -0.0495,  0.0455,  0.0504,  0.0254,\n",
      "         0.0283, -0.0868], requires_grad=True)\n",
      "\n",
      "Evaluted : tensor([-0.1220,  0.1625,  0.2353,  0.5414, -0.8405, -0.4898, -0.3993,  0.4034,\n",
      "        -0.7933,  0.6353, -0.9556, -0.2367,  0.3651,  0.3393,  0.2207, -0.1597,\n",
      "         0.5482,  0.4888, -0.2635, -0.3560,  0.4007,  0.9232, -0.0935,  0.2263,\n",
      "        -0.4687,  0.4280,  0.3136, -0.0238, -0.4300,  0.0548,  0.5991,  0.1211,\n",
      "         0.0372,  0.5088,  0.1595, -0.4023, -0.5419,  1.0390, -0.2350,  0.6097,\n",
      "        -0.1665, -0.0709,  0.8688,  0.5184, -0.1613, -0.2287, -0.1152,  0.4499,\n",
      "         0.1438, -0.8168], grad_fn=<ViewBackward0>)\n",
      "=========================\n",
      "Updated Weights =========\n",
      "weight\n",
      "Parameter containing:\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True)\n",
      "bias\n",
      "Parameter containing:\n",
      "tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],\n",
      "       requires_grad=True)\n",
      "\n",
      "Evaluated : tensor([3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
      "        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
      "        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(100)\n",
    "\n",
    "linear2 = nn.Linear(100, 50, bias=True)\n",
    "print(\"Original Weights ========\")\n",
    "for name, param in linear2.named_parameters(): \n",
    "    print(name)\n",
    "    print(param)\n",
    "print(f\"\\nEvaluted : {linear2(x)}\")\n",
    "print(\"=========================\")\n",
    "\n",
    "\n",
    "new_weight = torch.eye(n=50, m=100)\n",
    "new_bias = 2 * torch.ones(50)\n",
    "\n",
    "for name, param in linear2.named_parameters(): \n",
    "    if \"weight\" in name: \n",
    "        param.data = new_weight \n",
    "    elif \"bias\" in name: \n",
    "        param.data = new_bias \n",
    "    else: \n",
    "        pass \n",
    "\n",
    "print(\"Updated Weights =========\")    \n",
    "# now look at the update module \n",
    "for name, param in linear2.named_parameters(): \n",
    "    print(name)\n",
    "    print(param)\n",
    "print(f\"\\nEvaluated : {linear2(x)}\")\n",
    "print(\"=========================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's talk about different types of activations functions. These have no parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0.8653,  15.9285,  -1.4185,  -9.7655, -10.4387])\n",
      "tensor([ 0.8653, 15.9285,  0.0000,  0.0000,  0.0000])\n",
      "tensor([7.0378e-01, 1.0000e+00, 1.9490e-01, 5.7395e-05, 2.9278e-05])\n"
     ]
    }
   ],
   "source": [
    "relu = nn.ReLU() \n",
    "sigmoid = nn.Sigmoid() \n",
    "\n",
    "x = 10 * torch.randn(5) \n",
    "print(x) \n",
    "print(relu(x)) \n",
    "print(sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequential class allows us to create compositions of functions easily. For example, we can make a simple sequential module by taking a linear map, plus a relu, plus a linear, plus a sigmoid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6161, 0.5707, 0.5686, 0.4921, 0.4395, 0.4861, 0.4634, 0.4874, 0.5312,\n",
      "        0.5061], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sequential = nn.Sequential(\n",
    "    nn.Linear(30, 20), \n",
    "    nn.ReLU(), \n",
    "    nn.Linear(20, 10), \n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "x = torch.randn(30) \n",
    "print(sequential(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we take a look through its parameters, we can see that each unique parameter is indexed properly, so we can update them accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight\n",
      "0.bias\n",
      "2.weight\n",
      "2.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in sequential.named_parameters(): \n",
    "    print(name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2.,\n",
      "        -2., -2., -2., -2., -2., -2.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "new_params = {\n",
    "    \"0.weight\" : torch.eye(20, 30, dtype=torch.float32), \n",
    "    \"0.bias\" : torch.ones(20, dtype=torch.float32) * -2, \n",
    "    \"2.weight\" : torch.ones(10, 20, dtype=torch.float32), \n",
    "    \"2.bias\" : torch.arange(10, dtype=torch.float32)\n",
    "}\n",
    "\n",
    "for name, param in sequential.named_parameters(): \n",
    "    param.data = new_params[name] \n",
    "    \n",
    "for name, param in sequential.named_parameters(): \n",
    "    print(param) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(32*32*3, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can explore its layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_relu_stack.0.weight torch.Size([512, 3072])\n",
      "linear_relu_stack.0.bias torch.Size([512])\n",
      "linear_relu_stack.2.weight torch.Size([512, 512])\n",
      "linear_relu_stack.2.bias torch.Size([512])\n",
      "linear_relu_stack.4.weight torch.Size([10, 512])\n",
      "linear_relu_stack.4.bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters(): \n",
    "    print(name, param.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

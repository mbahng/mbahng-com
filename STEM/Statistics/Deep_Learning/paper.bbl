\begin{thebibliography}{10}

\bibitem{berthelot2020remixmatch}
David Berthelot, Nicholas Carlini, Ekin~D. Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, and Colin Raffel.
\newblock Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring, 2020.

\bibitem{berthelot2019mixmatch}
David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin Raffel.
\newblock Mixmatch: A holistic approach to semi-supervised learning, 2019.

\bibitem{symmetry}
An~Mei Chen, Haw-minn Lu, and Robert Hecht-Nielsen.
\newblock On the geometry of feedforward neural network error surfaces.
\newblock {\em Neural Computation}, 5(6):910--927, 11 1993.

\bibitem{ImageNet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em 2009 IEEE Conference on Computer Vision and Pattern Recognition}, pages 248--255, 2009.

\bibitem{frankle2019lottery}
Jonathan Frankle and Michael Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural networks, 2019.

\bibitem{Lecun1998ConvNets}
Y.~Lecun, L.~Bottou, Y.~Bengio, and P.~Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324, 1998.

\bibitem{misra2020mish}
Diganta Misra.
\newblock Mish: A self regularized non-monotonic activation function, 2020.

\bibitem{sohn2020fixmatch}
Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin~D. Cubuk, Alex Kurakin, Han Zhang, and Colin Raffel.
\newblock Fixmatch: Simplifying semi-supervised learning with consistency and confidence, 2020.

\bibitem{score}
Yang Song, Jascha Sohl{-}Dickstein, Diederik~P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.
\newblock Score-based generative modeling through stochastic differential equations.
\newblock {\em CoRR}, abs/2011.13456, 2020.

\bibitem{srivastava14a}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock {\em Journal of Machine Learning Research}, 15(56):1929--1958, 2014.

\bibitem{tam2020fiedler}
Edric Tam and David Dunson.
\newblock Fiedler regularization: Learning neural networks with graph sparsity, 2020.

\end{thebibliography}

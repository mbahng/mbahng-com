\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{BCG{\etalchar{+}}19}

\bibitem[BCC{\etalchar{+}}20]{berthelot2020remixmatch}
David Berthelot, Nicholas Carlini, Ekin~D. Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, and Colin Raffel.
\newblock Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring, 2020.

\bibitem[BCG{\etalchar{+}}19]{berthelot2019mixmatch}
David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin Raffel.
\newblock Mixmatch: A holistic approach to semi-supervised learning, 2019.

\bibitem[CLHN93]{symmetry}
An~Mei Chen, Haw-minn Lu, and Robert Hecht-Nielsen.
\newblock On the geometry of feedforward neural network error surfaces.
\newblock {\em Neural Computation}, 5(6):910--927, 11 1993.

\bibitem[DDS{\etalchar{+}}09]{ImageNet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em 2009 IEEE Conference on Computer Vision and Pattern Recognition}, pages 248--255, 2009.

\bibitem[FC19]{frankle2019lottery}
Jonathan Frankle and Michael Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural networks, 2019.

\bibitem[Hyv05]{orig_score}
Aapo Hyv{{\"a}}rinen.
\newblock Estimation of non-normalized statistical models by score matching.
\newblock {\em Journal of Machine Learning Research}, 6(24):695--709, 2005.

\bibitem[KW22]{vae}
Diederik~P Kingma and Max Welling.
\newblock Auto-encoding variational bayes, 2022.

\bibitem[LBBH98]{Lecun1998ConvNets}
Y.~Lecun, L.~Bottou, Y.~Bengio, and P.~Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324, 1998.

\bibitem[Mis20]{misra2020mish}
Diganta Misra.
\newblock Mish: A self regularized non-monotonic activation function, 2020.

\bibitem[SBL{\etalchar{+}}20]{sohn2020fixmatch}
Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin~D. Cubuk, Alex Kurakin, Han Zhang, and Colin Raffel.
\newblock Fixmatch: Simplifying semi-supervised learning with consistency and confidence, 2020.

\bibitem[SHK{\etalchar{+}}14]{srivastava14a}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock {\em Journal of Machine Learning Research}, 15(56):1929--1958, 2014.

\bibitem[SSK{\etalchar{+}}20]{score}
Yang Song, Jascha Sohl{-}Dickstein, Diederik~P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.
\newblock Score-based generative modeling through stochastic differential equations.
\newblock {\em CoRR}, abs/2011.13456, 2020.

\bibitem[TD20]{tam2020fiedler}
Edric Tam and David Dunson.
\newblock Fiedler regularization: Learning neural networks with graph sparsity, 2020.

\end{thebibliography}

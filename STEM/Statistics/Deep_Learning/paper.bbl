\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{GPAM{\etalchar{+}}14}

\bibitem[BCC{\etalchar{+}}20]{berthelot2020remixmatch}
David Berthelot, Nicholas Carlini, Ekin~D. Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, and Colin Raffel.
\newblock Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring, 2020.

\bibitem[BCG{\etalchar{+}}19]{berthelot2019mixmatch}
David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin Raffel.
\newblock Mixmatch: A holistic approach to semi-supervised learning, 2019.

\bibitem[CLHN93]{symmetry}
An~Mei Chen, Haw-minn Lu, and Robert Hecht-Nielsen.
\newblock On the geometry of feedforward neural network error surfaces.
\newblock {\em Neural Computation}, 5(6):910--927, 11 1993.

\bibitem[DDS{\etalchar{+}}09]{ImageNet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em 2009 IEEE Conference on Computer Vision and Pattern Recognition}, pages 248--255, 2009.

\bibitem[DKB15]{nice}
Laurent Dinh, David Krueger, and Yoshua Bengio.
\newblock Nice: Non-linear independent components estimation, 2015.

\bibitem[FC19]{lottery}
Jonathan Frankle and Michael Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural networks, 2019.

\bibitem[GPAM{\etalchar{+}}14]{gans}
Ian~J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial networks, 2014.

\bibitem[Hin02]{cd}
Geoffrey~E. Hinton.
\newblock Training products of experts by minimizing contrastive divergence.
\newblock {\em Neural Computation}, 14(8):1771--1800, 2002.

\bibitem[Hyv05]{orig_score}
Aapo Hyv{{\"a}}rinen.
\newblock Estimation of non-normalized statistical models by score matching.
\newblock {\em Journal of Machine Learning Research}, 6(24):695--709, 2005.

\bibitem[KD18]{glow}
Diederik~P. Kingma and Prafulla Dhariwal.
\newblock Glow: Generative flow with invertible 1x1 convolutions, 2018.

\bibitem[KW22]{vae}
Diederik~P Kingma and Max Welling.
\newblock Auto-encoding variational bayes, 2022.

\bibitem[LBBH98]{cnn}
Y.~Lecun, L.~Bottou, Y.~Bengio, and P.~Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324, 1998.

\bibitem[LH20]{cglow}
You Lu and Bert Huang.
\newblock Structured output learning with conditional generative flows, 2020.

\bibitem[Mis20]{mish}
Diganta Misra.
\newblock Mish: A self regularized non-monotonic activation function, 2020.

\bibitem[RM16]{flow}
Danilo~Jimenez Rezende and Shakir Mohamed.
\newblock Variational inference with normalizing flows, 2016.

\bibitem[SBL{\etalchar{+}}20]{sohn2020fixmatch}
Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin~D. Cubuk, Alex Kurakin, Han Zhang, and Colin Raffel.
\newblock Fixmatch: Simplifying semi-supervised learning with consistency and confidence, 2020.

\bibitem[SHK{\etalchar{+}}14]{dropout}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock {\em Journal of Machine Learning Research}, 15(56):1929--1958, 2014.

\bibitem[SK21]{ebm_train}
Yang Song and Diederik~P. Kingma.
\newblock How to train your energy-based models.
\newblock {\em CoRR}, abs/2101.03288, 2021.

\bibitem[SSK{\etalchar{+}}20]{score}
Yang Song, Jascha Sohl{-}Dickstein, Diederik~P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.
\newblock Score-based generative modeling through stochastic differential equations.
\newblock {\em CoRR}, abs/2011.13456, 2020.

\bibitem[TD20]{fiedler}
Edric Tam and David Dunson.
\newblock Fiedler regularization: Learning neural networks with graph sparsity, 2020.

\end{thebibliography}

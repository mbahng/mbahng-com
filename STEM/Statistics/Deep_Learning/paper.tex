\documentclass{article}

  % packages
    % basic stuff for rendering math
    \usepackage[letterpaper, top=1in, bottom=1in, left=1in, right=1in]{geometry}
    \usepackage[utf8]{inputenc}
    \usepackage[english]{babel}
    \usepackage{amsmath} 
    \usepackage{amssymb}
    % \usepackage{amsthm}

    % extra math symbols and utilities
    \usepackage{mathtools}        % for extra stuff like \coloneqq
    \usepackage{mathrsfs}         % for extra stuff like \mathsrc{}
    \usepackage{centernot}        % for the centernot arrow 
    \usepackage{bm}               % for better boldsymbol/mathbf 
    \usepackage{enumitem}         % better control over enumerate, itemize
    \usepackage{hyperref}         % for hypertext linking
    \usepackage{fancyvrb}          % for better verbatim environments
    \usepackage{newverbs}         % for texttt{}
    \usepackage{xcolor}           % for colored text 
    \usepackage{listings}         % to include code
    \usepackage{lstautogobble}    % helper package for code
    \usepackage{parcolumns}       % for side by side columns for two column code
    
    % page layout
    \usepackage{fancyhdr}         % for headers and footers 
    \usepackage{lastpage}
    \usepackage{parskip}          % for no indentation and space between paragraphs    
    \usepackage[T1]{fontenc}      % to include \textbackslash
    \usepackage{footnote}
    \usepackage{footnote}
    \usepackage{etoolbox}

    % for custom environments
    \usepackage{tcolorbox}        % for better colored boxes in custom environments
    \tcbuselibrary{breakable}     % to allow tcolorboxes to break across pages
    \tcbuselibrary{theorems}      % to allow tcolorboxes to be theorems

    % figures
    \usepackage{pgfplots}
    \pgfplotsset{compat=1.18}
    \usepackage{float}            % for [H] figure placement
    \usepackage{tikz}
    \usepackage{tikz-cd}
    \usetikzlibrary{arrows}
    \usetikzlibrary{positioning}
    \usepackage{graphicx}
    \usepackage{caption} 
    \usepackage{subcaption}
    \captionsetup{font=small}

    % for tabular stuff 
    \usepackage{dcolumn}

    \usepackage[nottoc]{tocbibind}
    \pdfsuppresswarningpagegroup=1
    \hfuzz=5.002pt                % ignore overfull hbox badness warnings below this limit

  % New and replaced operators
    \DeclareMathOperator{\Tr}{Tr}
    \DeclareMathOperator{\Sym}{Sym}
    \DeclareMathOperator{\Span}{span}
    \DeclareMathOperator{\std}{std}
    \DeclareMathOperator{\Cov}{Cov}
    \DeclareMathOperator{\Var}{Var}
    \DeclareMathOperator{\Corr}{Corr}
    \DeclareMathOperator{\pos}{pos}
    \DeclareMathOperator*{\argmin}{\arg\!\min}
    \DeclareMathOperator*{\argmax}{\arg\!\max}
    \newcommand{\qed}{\hfill$\blacksquare$}     % I like QED squares to be black

  % Custom Environments
    \newtcolorbox[auto counter, number within=section]{question}[1][]
    {
      colframe = orange!25,
      colback  = orange!10,
      coltitle = orange!20!black,  
      breakable, 
      title = \textbf{Question \thetcbcounter ~(#1)}
    }

    \newtcolorbox[auto counter, number within=section]{exercise}[1][]
    {
      colframe = teal!25,
      colback  = teal!10,
      coltitle = teal!20!black,  
      breakable, 
      title = \textbf{Exercise \thetcbcounter ~(#1)}
    }
    \newtcolorbox[auto counter, number within=section]{solution}[1][]
    {
      colframe = violet!25,
      colback  = violet!10,
      coltitle = violet!20!black,  
      breakable, 
      title = \textbf{Solution \thetcbcounter}
    }
    \newtcolorbox[auto counter, number within=section]{lemma}[1][]
    {
      colframe = red!25,
      colback  = red!10,
      coltitle = red!20!black,  
      breakable, 
      title = \textbf{Lemma \thetcbcounter ~(#1)}
    }
    \newtcolorbox[auto counter, number within=section]{theorem}[1][]
    {
      colframe = red!25,
      colback  = red!10,
      coltitle = red!20!black,  
      breakable, 
      title = \textbf{Theorem \thetcbcounter ~(#1)}
    } 
    \newtcolorbox[auto counter, number within=section]{corollary}[1][]
    {
      colframe = red!25,
      colback  = red!10,
      coltitle = red!20!black,  
      breakable, 
      title = \textbf{Corollary \thetcbcounter ~(#1)}
    } 
    \newtcolorbox[auto counter, number within=section]{proof}[1][]
    {
      colframe = orange!25,
      colback  = orange!10,
      coltitle = orange!20!black,  
      breakable, 
      title = \textbf{Proof. }
    } 
    \newtcolorbox[auto counter, number within=section]{definition}[1][]
    {
      colframe = yellow!25,
      colback  = yellow!10,
      coltitle = yellow!20!black,  
      breakable, 
      title = \textbf{Definition \thetcbcounter ~(#1)}
    } 
    \newtcolorbox[auto counter, number within=section]{example}[1][]
    {
      colframe = blue!25,
      colback  = blue!10,
      coltitle = blue!20!black,  
      breakable, 
      title = \textbf{Example \thetcbcounter ~(#1)}
    } 
    \newtcolorbox[auto counter, number within=section]{code}[1][]
    {
      colframe = green!25,
      colback  = green!10,
      coltitle = green!20!black,  
      breakable, 
      title = \textbf{Code \thetcbcounter ~(#1)}
    } 
    \definecolor{cverbbg}{gray}{0.93}
    \newenvironment{cverbatim}
      {\SaveVerbatim{cverb}}
      {\endSaveVerbatim
        \flushleft\fboxrule=0pt\fboxsep=.5em
        \colorbox{cverbbg}{%
          \makebox[\dimexpr\linewidth-2\fboxsep][l]{\BUseVerbatim{cverb}}%
        }
        \endflushleft
    }

    \BeforeBeginEnvironment{example}{\savenotes}
    \AfterEndEnvironment{example}{\spewnotes}
    \BeforeBeginEnvironment{lemma}{\savenotes}
    \AfterEndEnvironment{lemma}{\spewnotes}
    \BeforeBeginEnvironment{theorem}{\savenotes}
    \AfterEndEnvironment{theorem}{\spewnotes}
    \BeforeBeginEnvironment{corollary}{\savenotes}
    \AfterEndEnvironment{corollary}{\spewnotes}
    \BeforeBeginEnvironment{definition}{\savenotes}
    \AfterEndEnvironment{definition}{\spewnotes}
    \BeforeBeginEnvironment{exercise}{\savenotes}
    \AfterEndEnvironment{exercise}{\spewnotes}
    \BeforeBeginEnvironment{proof}{\savenotes}
    \AfterEndEnvironment{proof}{\spewnotes}
    \BeforeBeginEnvironment{solution}{\savenotes}
    \AfterEndEnvironment{solution}{\spewnotes}
    \BeforeBeginEnvironment{question}{\savenotes}
    \AfterEndEnvironment{question}{\spewnotes}
    \BeforeBeginEnvironment{code}{\savenotes}
    \AfterEndEnvironment{code}{\spewnotes}

    \definecolor{dkgreen}{rgb}{0,0.6,0}
    \definecolor{gray}{rgb}{0.5,0.5,0.5}
    \definecolor{mauve}{rgb}{0.58,0,0.82}
    \definecolor{lightgray}{gray}{0.93}

    % default options for listings (for code)
    \lstset{
      autogobble,
      frame=ltbr,
      language=Python,                           % the language of the code
      aboveskip=3mm,
      belowskip=3mm,
      showstringspaces=false,
      columns=fullflexible,
      keepspaces=true,
      basicstyle={\small\ttfamily},
      numbers=left,
      firstnumber=1,                        % start line number at 1
      numberstyle=\tiny\color{gray},
      keywordstyle=\color{blue},
      commentstyle=\color{dkgreen},
      stringstyle=\color{mauve},
      backgroundcolor=\color{lightgray}, 
      breaklines=true,                      % break lines
      breakatwhitespace=true,
      tabsize=3, 
      xleftmargin=2em, 
      framexleftmargin=1.5em, 
      stepnumber=1
    }

  % Page style
    \pagestyle{fancy}
    \fancyhead[L]{Deep Learning}
    \fancyhead[C]{Muchang Bahng}
    \fancyhead[R]{Summer 2023} 
    \fancyfoot[C]{\thepage / \pageref{LastPage}}
    \renewcommand{\footrulewidth}{0.4pt}          % the footer line should be 0.4pt wide
    \renewcommand{\thispagestyle}[1]{}  % needed to include headers in title page
    \renewcommand{\thefootnote}{\arabic{footnote}}

\begin{document}

\title{Deep Learning}
\author{Muchang Bahng}
\date{Summer 2023}

\maketitle
\tableofcontents
\pagebreak 

This requires you to know both the supervised and unsupervised learning notes, especially on generalized linear models. We will use PyTorch.  


\include{chapters/multilayered_perceptrons} 
\include{chapters/training_and_control} 
\include{chapters/convolutional_neural_networks}
\include{chapters/recurrent_neural_networks}
\include{chapters/encoder_decoder_models} 
\include{chapters/linear_factor_models} 
\include{chapters/boltzmann_machines} 
\include{chapters/variational_autoencoders} 
\include{chapters/generative_adversarial_networks} 

\section{Normalizing Flows} 

  We have seen many examples of generative models that attempt to produce a probability distribution $p$ that approximates that of the data samples. Some are given by an explicit model (e.g. RBMs) while in others the model is implicit. The key idea for flow-based models\footnote{This has nothing to do with flow graphs and the max-flow-min-cut theorem in graph theory.} is that we want to map simple distributions (e.g. a Gaussian) to complex densities representing the data through an \textit{invertible transformation}. Recall the lemma below from multivariate calculus.  

  \begin{lemma}[Jacobi] 
    Let $X, Z$ be absolutely continuous random variables in $\mathbb{R}^n$. 
    Given that $f: \mathbb{R}^n \rightarrow \mathbb{R}^n$ is invertible and differentiable everywhere, with $X = f(Z), Z = f^{-1} (X)$, we claim 
    \begin{equation}
      p_X (x) = p_Z (f^{-1} (x)) \cdot \big| (D f^{-1}) (x) \big|
    \end{equation} 
    where $|(D f^{-1}) (x)|$ is the determinant of total derivative of $f^{-1}$ at $x$. 
  \end{lemma}
  \begin{proof}
    For $n = 1$, we have 
    \begin{align}
      p_X (x) & = \frac{d}{dx} F_X (x) \\
              & = \frac{d}{dx} F_Z (f^{-1}(x)) \\
              & = p_z (f^{-1} (x)) \cdot \frac{d}{dx} f^{-1} (x)  \\
              & = p_z 
    \end{align}
  \end{proof} 

  Therefore, if we parameterize $f$ with some $\theta$, then the marginal likelihood of $x$ given $\theta$ can be written as 
  \begin{equation}
    p_X (x;\; \theta) = p_Z (f^{-1}_\theta (x)) \cdot | (D f^{-1}_\theta) (x) |
  \end{equation} 
  Therefore, if $X$ is a complex distribution and $Z$ is a simple one (e.g. uniform), we might have hope to efficiently compute $p_X (x)$ for any $x \in \mathbb{R}^n$ if 
  \begin{enumerate}
    \item we can efficiently compute $f^{-1}_\theta (x)$, which allows us to compute $p_Z (f^{-1} (x))$ efficiently since $Z$ is simple. 
    \item we can efficiently compute the Jacobian $Df^{-1} (x)$, and furthermore be able to compute the determinant efficiently.\footnote{Note that computing determinants are approximately $O(n^{2.4})$, which may not be practical. } 
  \end{enumerate} 
  It looks like we have split this enormously hard problem of modeling $X$ with three slightly less difficult problems. However, with some tricks, we may be able to get a simple enough parameterization of $f$ to be able to compute its inverse plus the determinant of the Jacobian. As a first step, what if we add more intermediate functions? Consider the sequence of invertible functions $Z \xrightarrow{f} Y \xrightarrow{g} \rightarrow X$, where $Z$ is a simple distribution that gets transformed to a slightly more complicated distribution $Y$ that then gets transformed to a complex distribution $X$. We can apply the Jacobi theorem above to see $p_Y (y) = p_Z (f^{-1} (y)) \cdot | (Df^{-1})(x)|$, and so we have 
  \begin{align}
    p_X (x) & = p_Y (g^{-1} (x)) \cdot | (D g^{-1}) (x) | \\
            & = p_Z (f^{-1} (g^{-1} (x))) \cdot | (Df^{-1}) (y)| \cdot | (D g^{-1}) (x) | \\ 
            & = p_Z ((g \circ f)^{-1} (x))) \cdot | (Df^{-1}) (y)| \cdot | (D g^{-1}) (x) | 
  \end{align}
  This leads to the following corollary. 

  \begin{corollary}[Jacobi]
    Given a sequence of transformations 
    \begin{equation}
      Z = Z_0 \xrightarrow{f^1} Z_1 \xrightarrow{f^2} \ldots \xrightarrow{f^{M-1}} Z_{M-1} \xrightarrow{f^M} Z_M = X
    \end{equation} 
    where each $f^m$ is invertible and differentiable everywhere, let us denote $f = f^M \circ \cdot \circ f^1: Z \rightarrow X$, which is invertible. Then
    \begin{equation}
      p_X (x) = p_Z (f^{-1} (x)) \cdot \prod_{m=1}^M \big|([D f^m]^{-1})(z_m) \big|
    \end{equation}
  \end{corollary} 

  Therefore, by taking a sequence these functions $f^m_\theta$, which may each have a simple parameterization, we may construct a very complex composition $f_\theta$ that may result in a very expressive $Z$. As for how we parameterize this, our notation will assume that $\theta = (\theta_1, \ldots, \theta_M)$ and each $\theta_m$ parameterizes $f^m$. This is very similar to how a composition of linear mappings plus an activation gives us a very expressive neural network, and unsurprisingly, there is an analogue of the universal approximation theorem for transformations of this form. 

  \begin{theorem}[Probability Integral Transform] 
    Any $n$-dimensional random variable in $\mathbb{R}^n$ that is absolutely continuous w.r.t. the Lebesgue measure can be constructed from the uniform distribution $U$ on $[0, 1]^n$. Since we can map to and back from invertibility, any two such random variables $X$ and $Y$ can be mapped from each other, 
    \begin{equation}
      X \mapsto U \mapsto Y 
    \end{equation}
  \end{theorem} 

  The composition $f_\theta$ is pretty much our neural network, and we maximize the log-likelihood over the samples. 
  \begin{align}
    \theta^\ast & = \argmax_{\theta} \sum_{x \in \mathcal{D}} \log p_X (x) \\  
                & = \argmax_{\theta} \sum_{x \in \mathcal{D}} \log p_Z (f^{-1} (x)) \cdot \prod_{m=1}^M \big|([D f^m]^{-1})(z_m) \big|
  \end{align}

\include{chapters/attention_models} 
\include{chapters/learning_methodologies} 
\include{chapters/adversarial_learning} 
\include{chapters/semi_supervised_learning} 

\bibliographystyle{plain}
\bibliography{./bibfile}
\end{document}

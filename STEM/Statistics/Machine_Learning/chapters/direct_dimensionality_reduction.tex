\section{Direct Dimensionality Reduction} 

  Dimensionality reduction is used for many purposes, such as preprocessing data, visualizing it, or encoding it in a sparser, more efficient way. 

  \subsection{Principal Component Analysis} 
    
    PCA finds low dimensional approximations to the data by projecting the data onto linear subspaces. To begin with some motivation, let a linear map $A: \mathbb{R}^D \rightarrow \mathbb{R}^D$ be full rank, which maps some set of $n$ data points to the space of features. Then it is injective, and therefore for all data $x \in \mathbb{R}^D$ there exists a feature vector $z \in \mathbb{R}^D$ such that $z = Ax$. Generally, real-world data does not span the full space of $D$ dimensions.\footnote{The \textit{manifold hypothesis} that real-world data in high-dimensions actually lies on a lower-dimensional manifold. } In fact, if we further assume that the data lies in a linear subspace, we want to compress it into a lower-dimensional vector such that the covariates in this lower dimensional space are also orthogonal, i.e. uncorrelated. We tackle both problems in 2 steps. 
    \begin{enumerate}
      \item To compress this representation, we can take a data point $x \in \mathbb{R}^D$ and \textit{approximate} it as a point $\hat{x} \in L_k$ for some $k$-dimensional subspace $L_k \subset \mathbb{R}^d$ (say that this is done with some function $P: \mathbb{R}^D \rightarrow L_k \subset \mathbb{R}^D$). 
      \item After this projection, we then want to extract the $k$ features such that they are \textit{orthogonal} (i.e. no correlation). This is done with a simple change of basis, which we denote $T: L_k \rightarrow \mathbb{R}^k$, giving us $\hat{z} = T \hat{x} = T (P(x))$. We can invert this map $T^{-1} : \mathbb{R}^k \rightarrow L_k$ to go from the orthogonalized compressed version $\hat{z}$ to the approximate full version $\hat{x}$.
    \end{enumerate}
    We are done! But which subspace do we choose? Let's formalize what the optimal subspace should be. 

    \begin{definition}[Principal Subspace]
      Let $x \sim X \in \mathbb{R}^d$, with its normalized $\tilde{x} = x - \mathbb{E}[x]$, and $\mathcal{L}_k$ denote all $k$-dimensional linear subspaces of $\mathbb{R}^n$. The \textbf{$k$th principal subspace} is defined as 
      \begin{equation}
        \ell_k = \argmin_{\ell \in \mathcal{L}_k} \mathbb{E}_{\Tilde{x}} \big( \min_{y \in \ell} ||\Tilde{x} - y||^2 \big)
      \end{equation}

      To parse this, let's fix a subspace $\ell$. Then, the normalized data $\Tilde{x}$ is a random vector and the minimum distance of $\Tilde{x}$ onto the subspace $\ell$ is the inner min term. Taking the expectation of that gives us the expected distance of the data onto the subspace. The principal subspace is the subspace that minimizes this expected distance. The dimension reduced version of $x$ is then $P_k (x) = \mu + \mathrm{proj}_{\ell_k} x$. 

      \begin{figure}[H]
        \centering
        \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \begin{tikzpicture}[scale=0.7]
          % Draw axes
          \draw[black, line width=0.8pt] (-0.5,0) -- (7,0);
          \draw[black, line width=0.8pt] (0,-0.5) -- (0,6);
          
          % Define trend line coordinates (y = mx + b)
          \def\m{0.4}  % slope
          \def\b{1.5}  % y-intercept
          \draw[blue!50, line width=0.8pt] (-0.5,{-0.5*\m + \b}) -- (7,{7*\m + \b});
          
          % Calculate perpendicular direction vector (-m, 1) normalized
          \def\dx{-\m/sqrt(1+\m*\m)}
          \def\dy{1/sqrt(1+\m*\m)}
          
          % Points and their perpendicular projections
          % For point (x,y), intersection with line is:
          % x' = (x + m*y - m*b)/(1 + m^2)
          % y' = m*x' + b
          
          % Define and draw points with their projections
          \foreach \x/\y in {
            0.5/5,    % p1
            1.5/4,    % p2
            2/2,      % p3
            2.5/3.5,  % p4
            3/1.5,    % p5
            3.5/2.8,  % p6
            4.5/3,    % p7
            5/2,      % p8
            5.5/4,    % p9
            6.5/1     % p10
          } {
            % Calculate projection point
            \pgfmathsetmacro{\px}{(\x + \m*\y - \m*\b)/(1 + \m*\m)}
            \pgfmathsetmacro{\py}{\m*\px + \b}
            
            % Draw dashed line and point
            \draw[dashed, blue!30] (\x,\y) -- (\px,\py);
            \fill[blue] (\x,\y) circle (3pt);
          }
        \end{tikzpicture}
          \caption{PCA minimizes the orthogonal distance to the subspace. }
        \end{subfigure}
        \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \begin{tikzpicture}[scale=0.7]
          % Draw axes
          \draw[black, line width=0.8pt] (-0.5,0) -- (7,0);
          \draw[black, line width=0.8pt] (0,-0.5) -- (0,6);
          
          % Define trend line function: y = 0.4x + 1.5
          \draw[blue!50, line width=0.8pt] (-0.5,1.3) -- (7,4.3);
          
          % Points with their actual trend line intersections
          \coordinate (p1) at (0.5,5);
          \path (0.5,1.7) coordinate (t1);  % trend point
          
          \coordinate (p2) at (1.5,4);
          \path (1.5,2.1) coordinate (t2);
          
          \coordinate (p3) at (2,2);
          \path (2,2.3) coordinate (t3);
          
          \coordinate (p4) at (2.5,3.5);
          \path (2.5,2.5) coordinate (t4);
          
          \coordinate (p5) at (3,1.5);
          \path (3,2.7) coordinate (t5);
          
          \coordinate (p6) at (3.5,2.8);
          \path (3.5,2.9) coordinate (t6);
          
          \coordinate (p7) at (4.5,3);
          \path (4.5,3.3) coordinate (t7);
          
          \coordinate (p8) at (5,2);
          \path (5,3.5) coordinate (t8);
          
          \coordinate (p9) at (5.5,4);
          \path (5.5,3.7) coordinate (t9);
          
          \coordinate (p10) at (6.5,1);
          \path (6.5,4.1) coordinate (t10);
          
          % Draw dashed lines from points to their trend line intersections
          \foreach \i in {1,...,10} {
            \draw[dashed, blue!30] (p\i) -- (t\i);
            \fill[blue] (p\i) circle (3pt);
          }
        \end{tikzpicture}
          \caption{Linear regression minimizes the residual distance to the subspace.} 
        \end{subfigure}
        \caption{Note that this is in fact different from linear regression as it minimizes the expected \textit{orthogonal distance} to the subspace, rather than the residual distance to the subspace as in linear regression.}
        \label{fig:pca_lin_reg}
      \end{figure}
    \end{definition}

    We can see that by definition the properties of the principal subspace allows us to construct the best approximation of the points in a lower-dimensional subspace. This seems like a hard optimization problem, but it turns out that the theorem gives a simple solution. Note that we need to do 3 things: 
    \begin{enumerate}
      \item Find such a subspace $L_k \subset \mathbb{R}^D$. 
      \item Find the projection $P_k: \mathbb{R}^D \rightarrow L_k \subset \mathbb{R}^D$. Note that by definition of the principal subspace $P_k$ should be an \textit{orthogonal} projection. 
      \item Find the bijection $T_k: L_k \rightarrow \mathbb{R}^k$. 
    \end{enumerate}
    It turns out that we can solve all three problems with the singular value decomposition. 

    \begin{theorem}[Construction of the kth Principle Subspace] 
      Given covariates $x^{(1)}, \ldots, x^{(n)} \in \mathbb{R}^{d}$, let $X \in \mathbb{R}^{n \times d}$ be our random matrix representing our data, and its normalized form to $\Tilde{X} = X - \mu$. We take the SVD of it. 
      \begin{equation}
        \Tilde{X} = U \Sigma V^T
      \end{equation}
      where $U \in U(n) \subset \mathbb{R}^{n \times n}, V \in U(d) \subset \mathbb{R}^{d \times d}$ are orthogonal and $\Sigma \in \mathbb{R}^{n \times d}$ is diagonal that represents the singular values of $X$ in decreasing order\footnote{We can make it decreasing by permuting the rows/columns of the unitary matrices $U, V$.}. The columns of $V$ (rows of $V^T$) denoted $v_1, \ldots, v_d$ are called the \textbf{principal axes} and the columns of $U\Sigma$ are called the \textbf{principal components}. 
      \begin{enumerate}
        \item Then $\ell_k = \mathrm{span}\{v_1, \ldots, v_k\} \subset \mathbb{R}^d$, i.e. is the subspace spanned by the columns of $V$. By shifting it by $\mu$, $\mu + \ell_k$ is the best affine subspace approximation of the $x_i$'s. 

        \item The projection $P$ is defined 
        \begin{equation}
          \hat{x} = P_k (x) = \mu + \sum_{j=1}^k \langle x - \mu, v_j \rangle \, v_j = \sum_{j=1}^k \proj_{v_j} (x - \mu) = \mu + \proj_{\ell_k} (x - \mu)
        \end{equation}
        where we can rewrite it as the projection operator since the $v_j$'s are orthonormal. 

        \item The change of basis $T$ is defined with the mapping $v_j \in \mathcal{L}_k \mapsto \sigma_j e_j \in \mathbb{R}^k$. Note that the $v_j$'s form an orthogonal basis of $\mathcal{L}_k$. 
      \end{enumerate} 
      Now let $V_k \in \mathbb{R}^{d \times k}$ represent the first $k$ columns of $V$, $U_k \in \mathbb{R}^{n \times k}$ represent the first $k$ columns of $U$, and $\Sigma_k \in \mathbb{R}^{k \times k}$ represent the upper-left $k \times k$ matrix of $\Sigma$.\footnote{Note that $V^T$, which was originally surjective, is now just injective.} The matrix $X_k = U_k \Sigma_k V^T$, which is the low-rank approximation of $X$, is called the \textbf{denoised matrix} of $X$. As an immediate consequence, since $T^{-1}$ maps $e_j$ to $\sigma_j v_j$, we can interpret $U_k \Sigma_k V_k^T$ with the decomposition 
      \begin{center}
        \begin{tikzpicture}[
          node distance=2.5cm,
          every node/.style={font=\small}
        ]
          % Nodes
          \node (ej) {$e_j \in \mathbb{R}^k$};
          \node[right of=ej] (vj) {$v_j \in \mathcal{L}_k$};
          \node[right of=vj] (sigmavj) {$\Sigma_k v_j \in \mathcal{L}_k$};
          \node[right of=sigmavj] (xhat) {$\hat{x} \in \mathbb{R}^d$};
          \node[right of=xhat] (x) {$x \in \mathbb{R}^d$};
          
          % Arrows
          \draw[->] (ej) -- node[above] {$V_k^T$} (vj);
          \draw[->] (vj) -- node[above] {$\Sigma_k$} (sigmavj);
          \draw[->] (sigmavj) -- node[above] {$U_k$} (xhat);
          \draw[<-] (xhat) -- node[above] {$P$} (x);
          
          % Braces with increased thickness
          \draw[line width=1pt] [decoration={brace,mirror,raise=5pt,amplitude=5pt},decorate] (ej.south west) -- 
                node[below=6pt] {$T^{-1}$} (xhat.south east);
          \draw[line width=1pt] [decoration={brace,raise=5pt,amplitude=5pt},decorate] (xhat.north west) -- 
                node[above=7pt] {$P$} (x.north east);
        \end{tikzpicture}
      \end{center}

      Therefore, we have two maps. The projection $x \mapsto \hat{x}$ and the transformation from $z$ to $\hat{x}$. Therefore, we can think of the transformation of the independent components $e_i \in \mathbb{R}^k$ to the independent components living in $\mathbb{R}^d$. 
      \begin{equation}
        \hat{x} = \mu + U_k \Sigma_k V_k^T z \implies \hat{x}_i = \mu + U_k \Sigma_k V_k^T e_i = \mu + U_k \Sigma_k v_i 
      \end{equation}
    \end{theorem}
    \begin{proof}
      We see that 
      \begin{equation}
        X^T X =  V \Sigma^T \Sigma V^T
      \end{equation}
      Note that $X \neq V \Sigma^T$ in general. Now let $v_1, \ldots, v_d$ be the columns of $V$. Then 
      \begin{equation}
        X^T X [v_1, \ldots, v_d] = X^T X V = V \Sigma^T \Sigma = [\sigma_1^2 v_1, \ldots, \sigma_d^2 v_d]
      \end{equation}
      Therefore, we can see that the way $X^T X$ acts on $V$ That the $v_i$'s are the eigenvectors of $X^T X$, with $\sigma_i^2$ the associated eigenvalues. 
    \end{proof}

    \begin{algo}[Fitting] 
      Given a dataset $X \in \mathbb{R}^{n \times d}$, let us denote the rows as $x_i$, and say that we are looking for a subspace of dimension $k$. 
      \begin{enumerate}
        \item Compute the mean 
        \begin{equation}
          \mu = \frac{1}{n} \sum_{i=1}^n x_i  \in \mathbb{R}^d
        \end{equation} 

        \item Standardize the data $\Tilde{X} = X - \mu$, i.e. $\Tilde{x}_i = x_i - \mu$.  

        \item Compute the SVD $\Tilde{X} = U \Sigma V^T$.

        \item Compute the submatrices $V_k \in \mathbb{R}^{k \times k}$ and $\Sigma_k \in \mathbb{R}^{D \times k}$. 

        \item Define the projection operator $P_k (x) = \mu + \sum_{j=1}^k \langle x - \mu, v_j \rangle \, v_j$, the change of basis operator $T$, and the embedding operator $T^{-1} (z) = \mu + V_k \Sigma_k z$. 
      \end{enumerate} 
      A demonstration is done \href{code/pca.html}{here}.
    \end{algo}

    Now a question arises: how do we know that this sample decomposition is a good approximation to the true decomposition? It comes from the fact that the sample covariance $\hat{\Sigma}$ is a good approximation of the true covariance $\Sigma$, which we will later prove using concentration of measure. 

    \begin{theorem}[Risk]
      The risk satisfies 
      \begin{equation}
        R(k) = \mathbb{E}[|| x - P_k (x) ||^2 ] = \sum_{j=k+1}^D \lambda_j 
      \end{equation}
    \end{theorem}

    It is essential that you plot the spectrum in decreasing order. This allows you to analyze how well PCA is working. People often use the ``elbow'' technique to determine where to choose $K$, and we value 
    \begin{equation}
      \frac{\sum_{j=1}^k \lambda_j}{\sum_{j=1}^d \lambda_j} 
    \end{equation}
    accounts for the \textbf{variance explained}, which should be high with $K$ low. If you have to go out to dimension $K=50$ to explain $90\%$ of the variance, then PCA is not working. It may not work because of many reasons, such as there being nonlinear structure within the data. 

    It turns out that the elements of $\hat{\Sigma}$ are close entry-wise to those of $\Sigma$. But if this is true, then does it mean that the eigenvalues of the sample covariance matrix are close to the true eigenvalues of the covariance matrix? It turns out that the answer is no, and we need a proper metric to satisfy this assumption. The metric, as we can guess from linear algebra, is the operator norm, and we will show some results from matrix perturbation theory. 

    \begin{definition}[Operator Norm]
      The \textbf{operator norm} of a matrix $A$ is defined as 
      \begin{equation}
        ||A|| = \sup_{x \neq 0} \frac{||Ax||}{||x||} 
      \end{equation}
    \end{definition}

    \begin{lemma}[]
      It turns out that 
      \begin{equation}
        ||\hat{\Sigma} - \Sigma|| = O_p \bigg( \frac{1}{\sqrt{n}} \bigg)
      \end{equation}
    \end{lemma}

    \begin{theorem}[Weyl's Theorem]
      If $\hat{\Sigma}$ and $\Sigma$ are close in the operator norm, then their eigenvalues are close. 
      \begin{equation}
        ||\hat{\Sigma} - \Sigma|| = O_p \bigg( \frac{1}{\sqrt{n}} \bigg) \implies |\hat{\lambda}_j - \lambda_j| = O_p \bigg( \frac{1}{\sqrt{n}} \bigg) 
      \end{equation}
    \end{theorem}

    This only talks about their eigenvalues, but this does not necessarily imply that the eigenvalues are close. We need an extra condition. 

    \begin{theorem}[David-Kahan Theorem]
      If $\hat{\Sigma}$ and $\Sigma$ are close in the operator norm, and if the eigenvectors of $\Sigma$ are well-conditioned, then the eigenvectors of $\hat{\Sigma}$ are close to the eigenvectors of $\Sigma$. More specifically, 
      \begin{equation}
        ||\hat{v}_j - v_j|| \leq \frac{2^{3/2} ||\hat{\Sigma} - \Sigma||}{\lambda_j - \lambda_{j+1}}
      \end{equation}
    \end{theorem}

    \begin{example}[Eigenfaces]
      In 1991, Turk and Pentland presented an eigenface method of face recognition by taking the low-rank approximation of a dataset of face images. 

      \begin{figure}[H]
        \centering 
        \includegraphics[scale=0.3]{img/eigenfaces.png}
        \caption{Some eigenfaces from AT\&T Labs. } 
        \label{fig:eigenfaces}
      \end{figure}
    \end{example}

    \subsubsection{Kernel PCA}

      \begin{definition}[Kernel PCA]
        Let $N_i$ be the neighborhood around $X_i$. Then, we want to find a mapping $W: \mathbb{R}^n \rightarrow \mathbb{R}^k$ that minimizes 
        \begin{equation}
          \min_{W} \sum_{i=1}^n \bigg| \bigg| X_i - \sum_{j \in N_i} W_{ij} X_j \bigg| \bigg|^2 \text{ where } \sum_{j} W_{ij} = 1
        \end{equation}
        We can constrain the weights in $W$ so that anything that is not in the neighborhoods are $0$. 
      \end{definition}

  \subsection{Multi-Dimensional Scaling}

    Again, we want to reduce our dimension, but the goal is slightly different from PCA. 

    \begin{definition}[Multi-Dimensional Scaling]
      Given our data $X \in \mathbb{R}^d$, we want to construct a linear map $T: \mathbb{R}^d \rightarrow \mathbb{R}^k$ such that it preserves the pairwise differences between the data points. That is, we want to minimize the following loss function 
      \begin{equation}
        \min_{T} \sum_{i \neq j} \big( d_{\mathbb{R}^k}(T(x_i), T(x_j)) - d_{\mathbb{R}^d}(x_i, x_j) \big)
      \end{equation}
      where $d_{V}$ is a distance metric in the space $V$. 
    \end{definition}

    Note that we can easily modify this formulation to preserve other structures, such as dot products, weights between distances, or different types of metrics in each space. It turns out that when the distance metric is the Euclidean L2 distance, then the solution to this linear map turns out to be PCA. This may be a more intuitive way to think about PCA, since we're trying to preserve the pairwise distances between the data points. 

    \begin{theorem}[Equivalence of Classical MDS and PCA]
      If the distance metric is the Euclidean L2 distance, then the solution to the MDS problem is equivalent to PCA. That is, 
      \begin{equation}
        T_k = \argmin_{T} \sum_{i \neq j} \big( ||T(x_i) - T(x_j)||^2 - ||x_i - x_j||^2 \big)
      \end{equation}
    \end{theorem}

    Generally, if you don't use classical MDS, then you will get a different answer than PCA and there doesn't exist a closed form solution, so you'll have to minimize this numerically. 

    \begin{example}[Non Classical MDS]
      The loss 
      \begin{equation}
        \sum_{i \neq j}  \big( ||T(x_i) - T(x_j)|| - ||x_i - x_j|| \big)^2 
      \end{equation}
      does not give the same solution as PCA. 
    \end{example}

  \subsection{Isomap} 

    Isomap is a bit different in the way that it tries to capture more of the global structure of the data, which brings advantages and disadvantages. It is simply a modification of MDS but with geodesic distances. 

    \begin{definition}[Isomap]
      You start off with the point cloud, but with every point, $X_i$, you find the local neighborhood $N_i$ and you make a weighted graph over the whole dataset in the high dimensional space. Then, the distance between any two arbitrary points is the weighted sum of the path between them, calculated by Dijkstra's algorithm. Intuitively, this is an approximation of the geodesic distance between these two points on a manifold. Call this distance $d_G$. Then, we simply do MDS by minimizing 
      \begin{equation}
        \min_{T} \sum_{i \neq j} \big( d_{\mathbb{R}^k}(T(x_i), T(x_j)) - d_G(x_i, x_j) \big)
      \end{equation}

      \begin{figure}[H]
        \centering 
        \includegraphics[scale=0.4]{img/isomap.png}
        \caption{The classical example is the spiral manifold. The data lies in this manifold, and the geodesic distance helps us gain an accurate distance metric within this data. } 
        \label{fig:isomap}
      \end{figure}
    \end{definition}

    The problem with this is that it is very sensitive to noise. For example, if we had a few points lying between the spirals, then the geodesic distance between the two spirals would be very small, and so the MDS would try to bring them closer together.  

    \begin{figure}[H]
      \centering 
      \includegraphics[scale=0.4]{img/isomap_problem.png}
      \caption{With extra noisy points (red), the geodesic distance can get corrupted.} 
      \label{fig:isomap_problem}
    \end{figure}

    To fix this, we use \textit{diffusion maps}, which looks at all possible paths between two points and looks at some average of them, which increases robustness. 

  \subsection{Local Linear Embedding} 

    PCA and MDS are linear embedding methods. Let's move onto nonlinear ones. The first nonlinear models that we work with again use the idea of locality (remember kernel regression). You have data that is globally nonlinear, but if you look at a point and its local neighborhood around it, then it is approximately linear since we assume that it lives in some smooth manifold. 

    \begin{figure}[H]
      \centering 
      \includegraphics[scale=0.4]{img/local_linear_embedding.png}
      \caption{Local linear embedding assumes that the data is locally linear. } 
      \label{fig:local_linear_embedding}
    \end{figure}

    The concept of neighborhood can be defined in two ways. You can either just fix an $\epsilon$ and take the $\epsilon$-ball around each point $x_i$. Or you can fix a $k$ and take the $k$ nearest neighbors of each point. The general idea of using kernel PCA is to take a local neighborhood of the data and construct some linear approximation of it. 

  \subsection{UMAP}

  \subsection{t-SNE}


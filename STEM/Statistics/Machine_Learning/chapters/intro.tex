Machine learning in the 1980s have been focused on developing a rigorous theory of learning algorithms, and the field has been dominated by statisticians. They strived to develop the theoretical foundation of algorithms that can be implemented and applied to real-world data. With the advent of deep learning, the theory behind state-of-the-art algorithms, mostly black-box models, has slowed down, but their applications have exploded. It is now a field of trying out a bunch of things and sticking to what works. 

These set of notes focuses on trying to provide the theoretical foundations of classical, interpretable machine learning algorithms, while my deep learning notes skim over the theory to focus on model architectures and applications (which is plenty to write about on its own). I've spent a good amount of time trying to create a map of machine learning, but after rewriting these notes multiple times. I've come to the conclusion that it is impossible to create a nice chronological timeline. Like math, you keep on revisiting the same topics over and over again, but at a higher level, and it's not as simple to organize everything into, say parametric vs nonparametric\footnote{K nearest neighbors is a nonparameteric model given that the data is not fixed. When the data is fixed, then our function search space is finite.}, supervised vs unsupervised\footnote{There are semi-supervised or weakly supervised models, and models like autoencoders use a supervised algorithm without any labels.}, or discriminative vs generative models.\footnote{Using Bayes rule, we can always reduce generative models into discriminative models.} Therefore, I've settled (for now) on the following structure. Before you begin, the direct prerequisites are my notes in probability theory, functional analysis, frequenist and Bayesian statistics, and information theory. If you are new to machine learning, go over my notes on Stanford CS229, which simply covers basic algorithms and their implementation. 

First, we will establish the theoretical foundations of ML by introducing the most abstract and general formulation of learning: statistical learning theory. In here, we will talk about function classes, what it means for a model to \textit{learn}, what \textit{overfitting} means in a mathematical sense, and some basic probability theorems that give us enough confidence that we can achieve good theoretical results. 

Then we move on to supervised learning, which provides review from statistics and gives us ideas to build upon when talking about unsupervised learning. 
\begin{enumerate}
  \item Just like in every other class, we begin with low-dimensional linear regression, introducing it both in the frequentist and Bayesian perspectives. We expand a bit into modified versions of these models, through weighted least squares and different loss functions. 

  \item Then we consider the need for regularization in high-dimensional linear regression, where the covariates may be much lager than the samples. This leads us to introduce both lasso and ridge as convex approximations of forward stepwise regression. We interpret them in both the frequentist and Bayesian ways. 

  \item Now that we have covered linear regression, we can move onto \textit{nonparametric regression}. 

  \item We complete regression and move to \textit{linear classification}, with logistic/softmax regression, SVMs, and DLA, followed by \textit{nonparametric classification}, such as decision trees. 

  \item It turns out that all these models are a specific instance of a more general model called \textit{generalized linear models (GLMs)}. We develop the theory behind this model in general. 

  \item We conclude supervised learning by talking about \textit{ensemble methods}, where we can take any combination of the models that we have learned so far to create new models that may have better performance. 
\end{enumerate}

Now we move onto unsupervised learning, which consists of every other problem dealing with a dataset without labels. The simplest algorithms are direct-clustering (K-means), direct density estimation (kernel density estimation), or direct dimensionality-reduction (PCA, UMAP). I call them \textit{direct} because they interact with the data directly, as opposed to \textit{latent variable} models that attempts to model the data by adding hidden random variables. 
\begin{enumerate}
  \item Direct clustering and density estimation models cover most algorithms you would learn in a introduction to machine learning course. Besides simple clustering models and kernel density estimation, there is not a lot to learn here. However, I do focus on the high-dimensional case where we may need some tricks for computational efficiency. 

  \item Direct dimensionality reduction models tend to have more variants, and I go through the most widely used algorithms. 

  \item Now we start getting into latent variable models. Just as linear regression is the simplest supervised regression model, linear factor models are the unsupervised analogue, which attempts to model the latent and the visible variables through a linear relationship.   

  \item We expand on this into nonlinear latent variable models, such as latent variables living in a discrete space. The most popular is Gaussian mixture models, which uses a multinomial latent variable representing which ``cluster'' a sample lives in. 

  \item Finally we talk about a generalization of these models by modeling probability distributions with a \textit{graph}, or a \textit{Bayesian network}. This allows for high-dimensional distributions to be represented with much fewer parameters. 
\end{enumerate}

At this point we are done, and all that is left is to talk about some meta-training techniques, such as cross validation, information criterion, and practical methods for real-world problems. 

For clarification, $\mathcal{D}$ can represent different things depending on the problem: 
\begin{enumerate}
  \item In a density estimation problem, where we have a single dataset $\mathbf{X}$, $\mathcal{D} = \mathbf{X}$ since this data tells us information about which distribution it could come from. 

  \item In a regression problem, $\mathcal{D} = \mathbf{Y}$, that is, $\mathcal{D}$ will always be the output data, not the input data $\mathbf{X}$. We can think of the input data $\mathbf{X}$ as always being fixed, and it is upon observation of the \textit{outputs} $\mathbf{Y}$ on these inputs that gives us information. 
\end{enumerate}
In both the frequentist and Bayesian settings, the likelihood $p(\mathcal{D} \mid \mathbf{w})$ plays a central role. In the frequentist setting, the process is divided into two steps: 
\begin{enumerate}
  \item We optimize $\mathbf{w}$ with some \textbf{estimator}, with a popular one being the \textbf{maximum likelihood estimator}. A popular estimator is \textbf{maximum likelihood}, which seeks to maximize $p(\mathcal{D} \mid \mathbf{w})$ w.r.t. $\mathbf{w}$. 
  \item We optimize $\mathbf{w}$ with some \textbf{estimator}, with a popular one being the \textbf{maximum likelihood estimator}. A popular estimator is \textbf{maximum likelihood}, which seeks to maximize $p(\mathcal{D} \mid \mathbf{w})$ w.r.t. $\mathbf{w}$. 
  \item We fix the optimized $\mathbf{w}^\ast$ and error bars on this estimate are obtained by considering the distribution of possible datasets $\mathcal{D}$. One approach is \textbf{bootstrapping}, which goes as follows. Given our original dataset $\mathbf{X} = \{x^{(1)}, \ldots, x^{(N)}\}$, we can create a new dataset $\mathbf{X}^\prime$ by sampling $N$ points at random from $\mathbf{X}$, with replacement, so that some points in $\mathbf{X}$ may be replicated in $\mathbf{X}^\prime$, whereas other points in $\mathbf{X}$ may be absent in $\mathbf{X}^\prime$. This process is repeated $L$ times to generate $L$ different datasets. Then, we can look at the variability of prediction between the different bootstrap data sets. 
\end{enumerate}
In a Bayesian setting, there is only a single dataset $\mathcal{D}$ and the uncertainity in the parameters is expressed through a probability distribution over $\mathbf{w}$. It also includes prior knowledge naturally in the form of prior distributions. 


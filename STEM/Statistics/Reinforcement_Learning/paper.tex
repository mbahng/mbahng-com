\documentclass{article}

% packages
  % basic stuff for rendering math
  \usepackage[letterpaper, top=1in, bottom=1in, left=1in, right=1in]{geometry}
  \usepackage[utf8]{inputenc}
  \usepackage[english]{babel}
  \usepackage{amsmath} 
  \usepackage{amssymb}
  % \usepackage{amsthm}

  % extra math symbols and utilities
  \usepackage{mathtools}        % for extra stuff like \coloneqq
  \usepackage{mathrsfs}         % for extra stuff like \mathsrc{}
  \usepackage{centernot}        % for the centernot arrow 
  \usepackage{bm}               % for better boldsymbol/mathbf 
  \usepackage{enumitem}         % better control over enumerate, itemize
  \usepackage{hyperref}         % for hypertext linking
  \usepackage{fancyvrb}          % for better verbatim environments
  \usepackage{newverbs}         % for texttt{}
  \usepackage{xcolor}           % for colored text 
  \usepackage{listings}         % to include code
  \usepackage{lstautogobble}    % helper package for code
  \usepackage{parcolumns}       % for side by side columns for two column code
  

  % page layout
  \usepackage{fancyhdr}         % for headers and footers 
  \usepackage{lastpage}         % to include last page number in footer 
  \usepackage{parskip}          % for no indentation and space between paragraphs    
  \usepackage[T1]{fontenc}      % to include \textbackslash
  \usepackage{footnote}
  \usepackage{etoolbox}

  % for custom environments
  \usepackage{tcolorbox}        % for better colored boxes in custom environments
  \tcbuselibrary{breakable}     % to allow tcolorboxes to break across pages

  % figures
  \usepackage{pgfplots}
  \pgfplotsset{compat=1.18}
  \usepackage{float}            % for [H] figure placement
  \usepackage{tikz}
  \usepackage{tikz-cd}
  \usepackage{circuitikz}
  \usetikzlibrary{arrows}
  \usetikzlibrary{positioning}
  \usetikzlibrary{calc}
  \usepackage{graphicx}
  \usepackage{caption} 
  \usepackage{subcaption}

  % for tabular stuff 
  \usepackage{dcolumn}

  \usepackage[nottoc]{tocbibind}
  \pdfsuppresswarningpagegroup=1
  \hfuzz=5.002pt                % ignore overfull hbox badness warnings below this limit

% New and replaced operators
  \DeclareMathOperator{\Tr}{Tr}
  \DeclareMathOperator{\Sym}{Sym}
  \DeclareMathOperator{\Span}{span}
  \DeclareMathOperator{\std}{std}
  \DeclareMathOperator{\Cov}{Cov}
  \DeclareMathOperator{\Var}{Var}
  \DeclareMathOperator{\Corr}{Corr}
  \DeclareMathOperator{\pos}{pos}
  \DeclareMathOperator*{\argmin}{\arg\!\min}
  \DeclareMathOperator*{\argmax}{\arg\!\max}
  \newcommand{\ket}[1]{\ensuremath{\left|#1\right\rangle}}
  \newcommand{\bra}[1]{\ensuremath{\left\langle#1\right|}}
  \newcommand{\braket}[2]{\langle #1 | #2 \rangle}
  \newcommand{\qed}{\hfill$\blacksquare$}     % I like QED squares to be black

% Custom Environments
  \newtcolorbox[auto counter, number within=section]{question}[1][]
  {
    colframe = orange!25,
    colback  = orange!10,
    coltitle = orange!20!black,  
    breakable, 
    title = \textbf{Question \thetcbcounter ~(#1)}
  }

  \newtcolorbox[auto counter, number within=section]{exercise}[1][]
  {
    colframe = teal!25,
    colback  = teal!10,
    coltitle = teal!20!black,  
    breakable, 
    title = \textbf{Exercise \thetcbcounter ~(#1)}
  }
  \newtcolorbox[auto counter, number within=section]{solution}[1][]
  {
    colframe = violet!25,
    colback  = violet!10,
    coltitle = violet!20!black,  
    breakable, 
    title = \textbf{Solution \thetcbcounter}
  }
  \newtcolorbox[auto counter, number within=section]{lemma}[1][]
  {
    colframe = red!25,
    colback  = red!10,
    coltitle = red!20!black,  
    breakable, 
    title = \textbf{Lemma \thetcbcounter ~(#1)}
  }
  \newtcolorbox[auto counter, number within=section]{theorem}[1][]
  {
    colframe = red!25,
    colback  = red!10,
    coltitle = red!20!black,  
    breakable, 
    title = \textbf{Theorem \thetcbcounter ~(#1)}
  } 
  \newtcolorbox[auto counter, number within=section]{proposition}[1][]
  {
    colframe = red!25,
    colback  = red!10,
    coltitle = red!20!black,  
    breakable, 
    title = \textbf{Proposition \thetcbcounter ~(#1)}
  } 
  \newtcolorbox[auto counter, number within=section]{corollary}[1][]
  {
    colframe = red!25,
    colback  = red!10,
    coltitle = red!20!black,  
    breakable, 
    title = \textbf{Corollary \thetcbcounter ~(#1)}
  } 
  \newtcolorbox[auto counter, number within=section]{proof}[1][]
  {
    colframe = orange!25,
    colback  = orange!10,
    coltitle = orange!20!black,  
    breakable, 
    title = \textbf{Proof. }
  } 
  \newtcolorbox[auto counter, number within=section]{definition}[1][]
  {
    colframe = yellow!25,
    colback  = yellow!10,
    coltitle = yellow!20!black,  
    breakable, 
    title = \textbf{Definition \thetcbcounter ~(#1)}
  } 
  \newtcolorbox[auto counter, number within=section]{example}[1][]
  {
    colframe = blue!25,
    colback  = blue!10,
    coltitle = blue!20!black,  
    breakable, 
    title = \textbf{Example \thetcbcounter ~(#1)}
  } 
  \newtcolorbox[auto counter, number within=section]{code}[1][]
  {
    colframe = green!25,
    colback  = green!10,
    coltitle = green!20!black,  
    breakable, 
    title = \textbf{Code \thetcbcounter ~(#1)}
  } 

  \BeforeBeginEnvironment{example}{\savenotes}
  \AfterEndEnvironment{example}{\spewnotes}
  \BeforeBeginEnvironment{lemma}{\savenotes}
  \AfterEndEnvironment{lemma}{\spewnotes}
  \BeforeBeginEnvironment{theorem}{\savenotes}
  \AfterEndEnvironment{theorem}{\spewnotes}
  \BeforeBeginEnvironment{corollary}{\savenotes}
  \AfterEndEnvironment{corollary}{\spewnotes}
  \BeforeBeginEnvironment{proposition}{\savenotes}
  \AfterEndEnvironment{proposition}{\spewnotes}
  \BeforeBeginEnvironment{definition}{\savenotes}
  \AfterEndEnvironment{definition}{\spewnotes}
  \BeforeBeginEnvironment{exercise}{\savenotes}
  \AfterEndEnvironment{exercise}{\spewnotes}
  \BeforeBeginEnvironment{proof}{\savenotes}
  \AfterEndEnvironment{proof}{\spewnotes}
  \BeforeBeginEnvironment{solution}{\savenotes}
  \AfterEndEnvironment{solution}{\spewnotes}
  \BeforeBeginEnvironment{question}{\savenotes}
  \AfterEndEnvironment{question}{\spewnotes}
  \BeforeBeginEnvironment{code}{\savenotes}
  \AfterEndEnvironment{code}{\spewnotes}

  \definecolor{dkgreen}{rgb}{0,0.6,0}
  \definecolor{gray}{rgb}{0.5,0.5,0.5}
  \definecolor{mauve}{rgb}{0.58,0,0.82}
  \definecolor{lightgray}{gray}{0.93}

  % default options for listings (for code)
  \lstset{
    autogobble,
    frame=ltbr,
    language=C,                           % the language of the code
    aboveskip=3mm,
    belowskip=3mm,
    showstringspaces=false,
    columns=fullflexible,
    keepspaces=true,
    basicstyle={\small\ttfamily},
    numbers=left,
    firstnumber=1,                        % start line number at 1
    numberstyle=\tiny\color{gray},
    keywordstyle=\color{blue},
    commentstyle=\color{dkgreen},
    stringstyle=\color{mauve},
    backgroundcolor=\color{lightgray}, 
    breaklines=true,                      % break lines
    breakatwhitespace=true,
    tabsize=3, 
    xleftmargin=2em, 
    framexleftmargin=1.5em, 
    stepnumber=1
  }

% Page style
  \pagestyle{fancy}
  \fancyhead[L]{Reinforcement Learning}
  \fancyhead[C]{Muchang Bahng}
  \fancyhead[R]{Spring 2024} 
  \fancyfoot[C]{\thepage / \pageref{LastPage}}
  \renewcommand{\footrulewidth}{0.4pt}          % the footer line should be 0.4pt wide
  \renewcommand{\thispagestyle}[1]{}  % needed to include headers in title page

\begin{document}

\title{Reinforcement Learning}
\author{Muchang Bahng}
\date{Spring 2024}

\maketitle
\tableofcontents
\pagebreak

The following sources were used to make these notes. 
\begin{enumerate}
  \item David Silver's Google Deepmind Lectures on Reinforcement Learning 
\end{enumerate}

\section{Markov Decision Processes}

  \begin{definition}[State Space]
    The formulations for reinforcement learning is very different from that of supervised learning. It is formulated as follows. 
    \begin{enumerate}
      \item We start off with an agent, who's state lives in a finite set $\mathcal{S}$ of states. 
      \item At each discrete time step $t$, the agent can take some action $a_t \in \mathcal{A}$. Let $\mathcal{A}$ be the set of all actions and $\mathcal{A}(s_t)$ be the set of all actions available in state $s_t$. 
      \item There are probabilities $P_{a, s, s^\prime}$ that determine the probability of transitioning to state $s^\prime$ from state $s$ after taking action $a$. It must be normalized, so the transition probabilities satisfy 
        \begin{equation}
          \sum_{a \in \mathcal{A}(s), s^\prime} P_{a, s, s^\prime} = 1 \text{ for all } s \in \mathcal{S}
        \end{equation}
      \item There is also a reward function $R(s, a): \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ for taking action $a$ in state $s$. 
    \end{enumerate}
  \end{definition}

  Note that this is really just a directed graph with some transition probability matrix, and therefore the tools of Markov chains can be applied here. We restate some common definitions. Sometimes, the reward function can output to $\mathbb{R}^n$, which corresponds to a \textit{multi-objective} function that tries to maximize multiple rewards. 

  \begin{example}[Autonomous Helicopter Flight]
    To define an MDP for autonomous helicopter flight, we have 
    \begin{enumerate}
      \item The state space $\mathcal{S}$ is the set of all possible positions and orientations of the helicopter, which is isomorphic to $\mathrm{Tran}\mathbb{R}^3 \times \mathrm{SO}(3)$. 
      \item The set of actions $A$ represents the set of all possible configurations of the helicopter. Assuming that we have 3 switches that we can flick on and off, 2 meters that can be moved from 0 to 10, and two control sticks that can move 360 degrees, the configuration of all controls is really the space: 
        \begin{equation}
          \mathcal{A} = \{0, 1\}^3 \times [0, 10]^2 \times \mathrm{SO}(2)^2
        \end{equation}
    \end{enumerate}
  \end{example}

  \begin{definition}[Terminal State]
    A state is \textbf{terminal} if it only transitions to itself and yield $0$ reward. 
  \end{definition}

  \begin{definition}[Trajectory]
    At each time step $t$, the agent observes state $s_t \in \mathcal{S}$, takes action $a_t \in \mathcal{A}(s_t)$, and receives a reward $r_t = R(s_t, a_t) \in \mathbb{R}$. The environment, in turn, transitions to $s_{t+1}$ with probability $P_{a_t, s_t, s_{t+1}}$. This sequence of states, actions, and rewards is called a \textbf{trajectory}. 
    \begin{equation}
      \tau = (s_1, a_1, r_1), (s_2, a_2, r_2), \ldots, (s_T, a_T, r_T)
    \end{equation}
    A trajectory that begins in a starting state $s_1$ and ends in a terminal state is called an \textbf{episode}. 
  \end{definition}

  \begin{definition}[Return]
    The return should be defined as the sum of all rewards that the agent gets within an episode, but there a discount factor that comes with time. Therefore, given a discounting factor $\gamma \in [0, 1)$, the \textbf{discounted cumulative return} of a trajectory is the vector 
    \begin{equation}
      G(\tau) = \sum_{t=1}^\infty \gamma^{t - 1} R(s_t, a_t)
    \end{equation}
    Since the reward at timestep $t$ is discounted by a factor of $\gamma^t$, we would like to accrue positive rewards as soon as possible.
  \end{definition}

  So far, we have not defined how the agent should act given that it is on state $s_t$. This is defined by the \textit{policy}, which can be arbitrary defined stochastic process (i.e. a sequence of random variables) and does not need to be Markov. It just needs to depend on the trajectory up until that state. 

  \begin{definition}[Policy]
    A \textbf{policy} is a family of conditional measures over $\mathcal{A} \times \mathcal{S}^\infty$ that tells the agent what action to take given its current state $S_t$ and the trajectory $\tau$ (the history prior to reaching state $s$).  
    \begin{equation}
      \mathbb{P}_\pi (A_t = a \mid S_t = s, \tau) = \mathbb{P}_{\pi} (A_t = a \mid S_t = s, S_{t-1} = s_{t-1}, \ldots, S_1 = s_1)
    \end{equation}
  \end{definition}

  \begin{definition}[Stationary Policy]
    However, if it is Markov, then it is called a \textit{stationary policy}, which depends only on the current state. Therefore, $\pi$ is defined over $\mathcal{A} \times \mathcal{S}$. 
    \begin{equation}
      \mathbb{P}_\pi (A_t = a \mid S_t = s)
    \end{equation}
  \end{definition}

  With this policy in place, we should define some sort of total reward function. 

  \begin{definition}[Value Function]
    The \textbf{value function} is the expected total reward starting from $s$ and following policy $\pi$. 
    \begin{equation}
      V_\pi (s) \coloneqq \mathbb{E}_{\tau \sim \pi} [G(\tau) \mid S_1 = s]
    \end{equation}
    The \textbf{action value function} is the same thing but we fix an action $A_1 = a$. 
    \begin{equation}
      Q_{\pi} (s, a) \coloneqq \mathbb{E}_{\tau \sim \pi} [G(\tau) \mid S_1 = s, A_1 = a]
    \end{equation}
    Therefore, 
    \begin{equation}
      V_\pi (s) = \mathbb{E}_{A_1} [Q_\pi] = \sum_{a \in \mathcal{A}(s)} \pi(a \mid s) Q_\pi (s, a)
    \end{equation}
  \end{definition}

  

\section{Multi Armed Bandits}


\end{document}

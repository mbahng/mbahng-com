{
  "snapshot_time": "2025-12-23T23:11:59.041661+00:00",
  "objects": {
    "paper:arxiv.2511.09744": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2511.09744",
        "url": "https://arxiv.org/pdf/2511.09744",
        "title": "Computing parametric weighted Ehrhart polynomials of smooth polytopes",
        "authors": [
          "Daniel Hwang",
          "Juliet Whidden",
          "Josephine Yu"
        ],
        "abstract": "We show that when integral polytopes are deformed while keeping the same facet normal vectors, the coefficients of weighted Ehrhart and $h^*$-polynomials are piecewise polynomial functions in the ``right hand sides'' of the linear inequalities defining the polytopes. We give an algorithm and an implementation in SageMath for computing these polynomials for smooth polytopes, such as type $A$ alcoved polytopes, using a weighted Euler-Maclaurin type formula by Khovanski\u01d0 and Pukhlikov. We discuss some natural questions concerning signs of the coefficients of the weighted $h^*$-polynomials.",
        "timestamp": "2025-12-21T06:10:03.453Z",
        "rating": "novote",
        "publishedDate": "2025-11-12",
        "tags": [
          "math.CO"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url",
        "arxiv_tags": [
          "math.CO"
        ]
      },
      "meta": {
        "issue_number": 7,
        "object_id": "paper:arxiv.2511.09744",
        "created_at": "2025-12-21T06:10:03+00:00",
        "updated_at": "2025-12-21T19:49:06+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2210.05846": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2210.05846",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-12-21T07:37:55.265Z",
            "data": {
              "session_id": "session_1766302675253_l788cpw",
              "source_id": "arxiv",
              "paper_id": "2210.05846",
              "start_time": "2025-12-21T07:37:40.215Z",
              "end_time": "2025-12-21T07:37:55.253Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 0,
              "total_elapsed_seconds": 15
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-12-21T07:46:28.708Z",
            "data": {
              "session_id": "session_1766303188695_2r0em9o",
              "source_id": "arxiv",
              "paper_id": "2210.05846",
              "start_time": "2025-12-21T07:46:11.238Z",
              "end_time": "2025-12-21T07:46:28.695Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 2,
              "total_elapsed_seconds": 17
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-12-21T08:11:01.730Z",
            "data": {
              "session_id": "session_1766304661522_yuwoaey",
              "source_id": "arxiv",
              "paper_id": "2210.05846",
              "start_time": "2025-12-21T08:10:43.211Z",
              "end_time": "2025-12-21T08:11:01.522Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 3,
              "total_elapsed_seconds": 18
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-12-21T08:55:48.943Z",
            "data": {
              "session_id": "session_1766307348939_asapyqf",
              "source_id": "arxiv",
              "paper_id": "2210.05846",
              "start_time": "2025-12-21T08:55:43.720Z",
              "end_time": "2025-12-21T08:55:48.939Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 5
            }
          }
        ],
        "title": "FasterRisk: Fast and Accurate Interpretable Risk Scores",
        "authors": [
          "Jiachang Liu",
          "Chudi Zhong",
          "Boxuan Li",
          "Margo Seltzer",
          "Cynthia Rudin"
        ],
        "abstract": "Over the last century, risk scores have been the most popular form of predictive model used in healthcare and criminal justice. Risk scores are sparse linear models with integer coefficients; often these models can be memorized or placed on an index card. Typically, risk scores have been created either without data or by rounding logistic regression coefficients, but these methods do not reliably produce high-quality risk scores. Recent work used mathematical programming, which is computationally slow. We introduce an approach for efficiently producing a collection of high-quality risk scores learned from data. Specifically, our approach produces a pool of almost-optimal sparse continuous solutions, each with a different support set, using a beam-search algorithm. Each of these continuous solutions is transformed into a separate risk score through a \"star ray\" search, where a range of multipliers are considered before rounding the coefficients sequentially to maintain low logistic loss. Our algorithm returns all of these high-quality risk scores for the user to consider. This method completes within minutes and can be valuable in a broad variety of applications.",
        "publishedDate": "2022-10-12"
      },
      "meta": {
        "issue_number": 9,
        "object_id": "interactions:arxiv.2210.05846",
        "created_at": "2025-12-21T06:42:27+00:00",
        "updated_at": "2025-12-21T08:56:37+00:00",
        "version": 1
      }
    },
    "paper:arxiv.1706.03762": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1706.03762",
        "url": "https://arxiv.org/abs/1706.03762",
        "title": "Attention Is All You Need",
        "authors": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin",
        "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
        "timestamp": "2025-12-21T07:46:33.198Z",
        "rating": "novote",
        "publishedDate": "2017/06/12",
        "tags": [
          "Computation and Language (cs.CL)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 18,
        "object_id": "paper:arxiv.1706.03762",
        "created_at": "2025-12-21T07:46:33+00:00",
        "updated_at": "2025-12-21T07:47:33+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2502.06709": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.06709",
        "url": "https://arxiv.org/pdf/2502.06709",
        "title": "Talagrand Meets Talagrand: Upper and Lower Bounds on Expected Soft Maxima of Gaussian Processes with Finite Index Sets",
        "authors": [
          "Yifeng Chu",
          "Maxim Raginsky"
        ],
        "abstract": "Analysis of extremal behavior of stochastic processes is a key ingredient in a wide variety of applications, including probability, statistical physics, theoretical computer science, and learning theory. In this paper, we consider centered Gaussian processes on finite index sets and investigate expected values of their smoothed, or ``soft,'' maxima. We obtain upper and lower bounds for these expected values using a combination of ideas from statistical physics (the Gibbs variational principle for the equilibrium free energy and replica-symmetric representations of Gibbs averages) and from probability theory (Sudakov minoration). These bounds are parametrized by an inverse temperature $\u03b2> 0$ and reduce to the usual Gaussian maximal inequalities in the zero-temperature limit $\u03b2\\to \\infty$. We provide an illustration of our methods in the context of the Random Energy Model, one of the simplest models of physical systems with random disorder.",
        "timestamp": "2025-12-21T07:00:23.584Z",
        "rating": "novote",
        "publishedDate": "2025-02-10",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 10,
        "object_id": "paper:arxiv.2502.06709",
        "created_at": "2025-12-21T07:00:23+00:00",
        "updated_at": "2025-12-21T07:35:40+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2502.06709": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.06709",
        "interactions": [],
        "title": "Talagrand Meets Talagrand: Upper and Lower Bounds on Expected Soft Maxima of Gaussian Processes with Finite Index Sets",
        "authors": [
          "Yifeng Chu",
          "Maxim Raginsky"
        ],
        "abstract": "Analysis of extremal behavior of stochastic processes is a key ingredient in a wide variety of applications, including probability, statistical physics, theoretical computer science, and learning theory. In this paper, we consider centered Gaussian processes on finite index sets and investigate expected values of their smoothed, or ``soft,'' maxima. We obtain upper and lower bounds for these expected values using a combination of ideas from statistical physics (the Gibbs variational principle for the equilibrium free energy and replica-symmetric representations of Gibbs averages) and from probability theory (Sudakov minoration). These bounds are parametrized by an inverse temperature $\u03b2> 0$ and reduce to the usual Gaussian maximal inequalities in the zero-temperature limit $\u03b2\\to \\infty$. We provide an illustration of our methods in the context of the Random Energy Model, one of the simplest models of physical systems with random disorder.",
        "publishedDate": "2025-02-10"
      },
      "meta": {
        "issue_number": 11,
        "object_id": "interactions:arxiv.2502.06709",
        "created_at": "2025-12-21T07:00:39+00:00",
        "updated_at": "2025-12-21T07:35:40+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2210.05846": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2210.05846",
        "url": "https://arxiv.org/pdf/2210.05846",
        "title": "FasterRisk: Fast and Accurate Interpretable Risk Scores",
        "authors": [
          "Jiachang Liu",
          "Chudi Zhong",
          "Boxuan Li",
          "Margo Seltzer",
          "Cynthia Rudin"
        ],
        "abstract": "Over the last century, risk scores have been the most popular form of predictive model used in healthcare and criminal justice. Risk scores are sparse linear models with integer coefficients; often these models can be memorized or placed on an index card. Typically, risk scores have been created either without data or by rounding logistic regression coefficients, but these methods do not reliably produce high-quality risk scores. Recent work used mathematical programming, which is computationally slow. We introduce an approach for efficiently producing a collection of high-quality risk scores learned from data. Specifically, our approach produces a pool of almost-optimal sparse continuous solutions, each with a different support set, using a beam-search algorithm. Each of these continuous solutions is transformed into a separate risk score through a \"star ray\" search, where a range of multipliers are considered before rounding the coefficients sequentially to maintain low logistic loss. Our algorithm returns all of these high-quality risk scores for the user to consider. This method completes within minutes and can be valuable in a broad variety of applications.",
        "timestamp": "2025-12-21T07:08:01.847Z",
        "rating": "novote",
        "publishedDate": "2022-10-12",
        "tags": [
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url",
        "arxiv_tags": [
          "cs.LG"
        ]
      },
      "meta": {
        "issue_number": 13,
        "object_id": "paper:arxiv.2210.05846",
        "created_at": "2025-12-21T07:08:02+00:00",
        "updated_at": "2025-12-21T19:49:04+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2511.09744": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2511.09744",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-12-21T08:10:43.405Z",
            "data": {
              "session_id": "session_1766304643180_o3cxxk6",
              "source_id": "arxiv",
              "paper_id": "2511.09744",
              "start_time": "2025-12-21T08:10:23.410Z",
              "end_time": "2025-12-21T08:10:43.180Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 5,
              "total_elapsed_seconds": 20
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-12-21T19:46:09.482Z",
            "data": {
              "session_id": "session_1766346369023_c8x2yrm",
              "source_id": "arxiv",
              "paper_id": "2511.09744",
              "start_time": "2025-12-21T19:45:01.685Z",
              "end_time": "2025-12-21T19:46:09.023Z",
              "heartbeat_count": 13,
              "duration_seconds": 65,
              "idle_seconds": 2,
              "total_elapsed_seconds": 67
            }
          }
        ],
        "title": "Computing parametric weighted Ehrhart polynomials of smooth polytopes",
        "authors": [
          "Daniel Hwang",
          "Juliet Whidden",
          "Josephine Yu"
        ],
        "abstract": "We show that when integral polytopes are deformed while keeping the same facet normal vectors, the coefficients of weighted Ehrhart and $h^*$-polynomials are piecewise polynomial functions in the ``right hand sides'' of the linear inequalities defining the polytopes. We give an algorithm and an implementation in SageMath for computing these polynomials for smooth polytopes, such as type $A$ alcoved polytopes, using a weighted Euler-Maclaurin type formula by Khovanski\u01d0 and Pukhlikov. We discuss some natural questions concerning signs of the coefficients of the weighted $h^*$-polynomials.",
        "publishedDate": "2025-11-12"
      },
      "meta": {
        "issue_number": 12,
        "object_id": "interactions:arxiv.2511.09744",
        "created_at": "2025-12-21T07:04:09+00:00",
        "updated_at": "2025-12-21T19:47:16+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2510.23866": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.23866",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-12-21T17:57:33.275Z",
            "data": {
              "session_id": "session_1766339852711_1xcok4t",
              "source_id": "arxiv",
              "paper_id": "2510.23866",
              "start_time": "2025-12-21T17:55:29.315Z",
              "end_time": "2025-12-21T17:57:32.711Z",
              "heartbeat_count": 24,
              "duration_seconds": 120,
              "idle_seconds": 3,
              "total_elapsed_seconds": 123
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-12-21T17:59:33.801Z",
            "data": {
              "session_id": "session_1766339973316_ln3tnxq",
              "source_id": "arxiv",
              "paper_id": "2510.23866",
              "start_time": "2025-12-21T17:57:54.949Z",
              "end_time": "2025-12-21T17:59:33.316Z",
              "heartbeat_count": 19,
              "duration_seconds": 95,
              "idle_seconds": 3,
              "total_elapsed_seconds": 98
            }
          }
        ]
      },
      "meta": {
        "issue_number": 25,
        "object_id": "interactions:arxiv.2510.23866",
        "created_at": "2025-12-21T09:40:23+00:00",
        "updated_at": "2025-12-21T18:00:33+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2510.23866": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.23866",
        "url": "https://arxiv.org/abs/2510.23866",
        "title": "A PDE-Informed Latent Diffusion Model for 2-m Temperature Downscaling",
        "authors": "Paul Rosu, Muchang Bahng, Erick Jiang, Rico Zhu, Vahid Tarokh",
        "abstract": "This work presents a physics-conditioned latent diffusion model tailored for dynamical downscaling of atmospheric data, with a focus on reconstructing high-resolution 2-m temperature fields. Building upon a pre-existing diffusion architecture and employing a residual formulation against a reference UNet, we integrate a partial differential equation (PDE) loss term into the model's training objective. The PDE loss is computed in the full resolution (pixel) space by decoding the latent representation and is designed to enforce physical consistency through a finite-difference approximation of an effective advection-diffusion balance. Empirical observations indicate that conventional diffusion training already yields low PDE residuals, and we investigate how fine-tuning with this additional loss further regularizes the model and enhances the physical plausibility of the generated fields. The entirety of our codebase is available on Github, for future reference and development.",
        "timestamp": "2025-12-21T09:40:14.789Z",
        "rating": "novote",
        "publishedDate": "2025/10/27",
        "tags": [
          "Machine Learning (cs.LG)",
          "Artificial Intelligence (cs.AI)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 24,
        "object_id": "paper:arxiv.2510.23866",
        "created_at": "2025-12-21T09:40:15+00:00",
        "updated_at": "2025-12-21T09:41:16+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2506.22947": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.22947",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-12-21T09:46:44.916Z",
            "data": {
              "session_id": "session_1766310404731_nc534t6",
              "source_id": "arxiv",
              "paper_id": "2506.22947",
              "start_time": "2025-12-21T09:46:34.608Z",
              "end_time": "2025-12-21T09:46:44.731Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 0,
              "total_elapsed_seconds": 10
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-12-21T09:51:33.939Z",
            "data": {
              "session_id": "session_1766310693460_w7tj227",
              "source_id": "arxiv",
              "paper_id": "2506.22947",
              "start_time": "2025-12-21T09:48:11.956Z",
              "end_time": "2025-12-21T09:51:33.460Z",
              "heartbeat_count": 40,
              "duration_seconds": 200,
              "idle_seconds": 2,
              "total_elapsed_seconds": 202
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-12-21T17:14:33.462Z",
            "data": {
              "session_id": "session_1766337273153_3nta5vf",
              "source_id": "arxiv",
              "paper_id": "2506.22947",
              "start_time": "2025-12-21T17:14:25.187Z",
              "end_time": "2025-12-21T17:14:33.153Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-12-21T17:15:36.538Z",
            "data": {
              "session_id": "session_1766337335942_7af9cqr",
              "source_id": "arxiv",
              "paper_id": "2506.22947",
              "start_time": "2025-12-21T17:15:04.790Z",
              "end_time": "2025-12-21T17:15:35.942Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 1,
              "total_elapsed_seconds": 31
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-12-21T17:17:48.894Z",
            "data": {
              "session_id": "session_1766337468591_74jfp3f",
              "source_id": "arxiv",
              "paper_id": "2506.22947",
              "start_time": "2025-12-21T17:17:04.158Z",
              "end_time": "2025-12-21T17:17:48.591Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 4,
              "total_elapsed_seconds": 44
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-12-21T17:49:05.721Z",
            "data": {
              "session_id": "session_1766339345230_x4dvdys",
              "source_id": "arxiv",
              "paper_id": "2506.22947",
              "start_time": "2025-12-21T17:47:33.543Z",
              "end_time": "2025-12-21T17:49:05.230Z",
              "heartbeat_count": 18,
              "duration_seconds": 90,
              "idle_seconds": 2,
              "total_elapsed_seconds": 92
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-12-21T18:10:19.144Z",
            "data": {
              "session_id": "session_1766340618633_3nj9dk4",
              "source_id": "arxiv",
              "paper_id": "2506.22947",
              "start_time": "2025-12-21T18:08:05.905Z",
              "end_time": "2025-12-21T18:10:18.633Z",
              "heartbeat_count": 26,
              "duration_seconds": 130,
              "idle_seconds": 3,
              "total_elapsed_seconds": 133
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-12-21T18:24:35.107Z",
            "data": {
              "session_id": "session_1766341475100_46yx4s0",
              "source_id": "arxiv",
              "paper_id": "2506.22947",
              "start_time": "2025-12-21T18:24:25.015Z",
              "end_time": "2025-12-21T18:24:35.100Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 0,
              "total_elapsed_seconds": 10
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-12-21T18:25:41.930Z",
            "data": {
              "session_id": "session_1766341541497_x9mr9ra",
              "source_id": "arxiv",
              "paper_id": "2506.22947",
              "start_time": "2025-12-21T18:24:35.194Z",
              "end_time": "2025-12-21T18:25:41.497Z",
              "heartbeat_count": 13,
              "duration_seconds": 65,
              "idle_seconds": 1,
              "total_elapsed_seconds": 66
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-12-21T18:27:22.128Z",
            "data": {
              "session_id": "session_1766341641662_9t8tw11",
              "source_id": "arxiv",
              "paper_id": "2506.22947",
              "start_time": "2025-12-21T18:25:43.458Z",
              "end_time": "2025-12-21T18:27:21.662Z",
              "heartbeat_count": 19,
              "duration_seconds": 95,
              "idle_seconds": 3,
              "total_elapsed_seconds": 98
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-12-21T18:44:22.155Z",
            "data": {
              "session_id": "session_1766342661879_37pigqp",
              "source_id": "arxiv",
              "paper_id": "2506.22947",
              "start_time": "2025-12-21T18:43:29.937Z",
              "end_time": "2025-12-21T18:44:21.879Z",
              "heartbeat_count": 10,
              "duration_seconds": 50,
              "idle_seconds": 2,
              "total_elapsed_seconds": 52
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-12-21T18:45:31.838Z",
            "data": {
              "session_id": "session_1766342731358_zgwk4k1",
              "source_id": "arxiv",
              "paper_id": "2506.22947",
              "start_time": "2025-12-21T18:44:30.425Z",
              "end_time": "2025-12-21T18:45:31.358Z",
              "heartbeat_count": 12,
              "duration_seconds": 60,
              "idle_seconds": 1,
              "total_elapsed_seconds": 61
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-12-21T19:06:56.274Z",
            "data": {
              "session_id": "session_1766344016271_kjudoau",
              "source_id": "arxiv",
              "paper_id": "2506.22947",
              "start_time": "2025-12-21T19:06:41.518Z",
              "end_time": "2025-12-21T19:06:56.271Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 5,
              "total_elapsed_seconds": 15
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-12-21T19:22:31.303Z",
            "data": {
              "session_id": "session_1766344950779_oyy06g1",
              "source_id": "arxiv",
              "paper_id": "2506.22947",
              "start_time": "2025-12-21T19:19:45.908Z",
              "end_time": "2025-12-21T19:22:30.779Z",
              "heartbeat_count": 32,
              "duration_seconds": 160,
              "idle_seconds": 5,
              "total_elapsed_seconds": 165
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-12-21T19:31:29.797Z",
            "data": {
              "session_id": "session_1766345489299_rxkmnev",
              "source_id": "arxiv",
              "paper_id": "2506.22947",
              "start_time": "2025-12-21T19:29:49.195Z",
              "end_time": "2025-12-21T19:31:29.299Z",
              "heartbeat_count": 20,
              "duration_seconds": 100,
              "idle_seconds": 0,
              "total_elapsed_seconds": 100
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-12-21T19:33:08.782Z",
            "data": {
              "session_id": "session_1766345588239_orl4m82",
              "source_id": "arxiv",
              "paper_id": "2506.22947",
              "start_time": "2025-12-21T19:31:34.417Z",
              "end_time": "2025-12-21T19:33:08.239Z",
              "heartbeat_count": 18,
              "duration_seconds": 90,
              "idle_seconds": 4,
              "total_elapsed_seconds": 94
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-12-21T19:39:56.852Z",
            "data": {
              "session_id": "session_1766345996674_ujk4228",
              "source_id": "arxiv",
              "paper_id": "2506.22947",
              "start_time": "2025-12-21T19:39:44.959Z",
              "end_time": "2025-12-21T19:39:56.674Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 2,
              "total_elapsed_seconds": 12
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-12-21T20:29:27.190Z",
            "data": {
              "session_id": "session_1766348967007_f1itd4w",
              "source_id": "arxiv",
              "paper_id": "2506.22947",
              "start_time": "2025-12-21T20:29:11.146Z",
              "end_time": "2025-12-21T20:29:27.007Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 1,
              "total_elapsed_seconds": 16
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-12-22T04:16:18.291Z",
            "data": {
              "session_id": "session_1766376978083_q6q09h0",
              "source_id": "arxiv",
              "paper_id": "2506.22947",
              "start_time": "2025-12-22T04:16:05.597Z",
              "end_time": "2025-12-22T04:16:18.083Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 2,
              "total_elapsed_seconds": 12
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-12-22T06:46:50.384Z",
            "data": {
              "session_id": "session_1766386009847_vxqtp1k",
              "source_id": "arxiv",
              "paper_id": "2506.22947",
              "start_time": "2025-12-22T06:45:34.532Z",
              "end_time": "2025-12-22T06:46:49.847Z",
              "heartbeat_count": 15,
              "duration_seconds": 75,
              "idle_seconds": 0,
              "total_elapsed_seconds": 75
            }
          }
        ]
      },
      "meta": {
        "issue_number": 23,
        "object_id": "interactions:arxiv.2506.22947",
        "created_at": "2025-12-21T09:37:38+00:00",
        "updated_at": "2025-12-22T06:47:57+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.22947": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.22947",
        "url": "https://arxiv.org/pdf/2506.22947",
        "title": "Monotone Multispecies Flows",
        "authors": [
          "Lauren Conger",
          "Franca Hoffmann",
          "Eric Mazumdar",
          "Lillian J. Ratliff"
        ],
        "abstract": "We present a novel notion of $\u03bb$-monotonicity for an $n$-species system of partial differential equations governed by mass-preserving flow dynamics, extending monotonicity in Banach spaces to the Wasserstein-2 metric space. We show that monotonicity implies the existence of and convergence to a unique steady state, convergence of the velocity fields and second moments, and contraction in the Wasserstein-2 metric, at rates dependent on $\u03bb$. In the special setting of Wasserstein-2 gradient descent of different energies for each species, we prove convergence to the unique Nash equilibrium of the associated energies and delineate the relationship between monotonicity and displacement convexity. This extends known zero-sum results in infinite-dimensional game theory to the general-sum setting. We provide a number of examples of monotone coupled gradient flow systems, including cross-diffusion, gradient flows with potentials, nonlocal interaction, linear and nonlinear diffusion, and min-max systems, and draw connections to a class of mean-field games. Numerically, we demonstrate convergence of a four-player economic model for service providers and strategic users competing in a market, and a degenerately monotone game.",
        "timestamp": "2025-12-21T09:37:17.804Z",
        "rating": "novote",
        "publishedDate": "2025-06-28",
        "tags": [
          "math.AP"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url",
        "arxiv_tags": [
          "math.AP"
        ]
      },
      "meta": {
        "issue_number": 22,
        "object_id": "paper:arxiv.2506.22947",
        "created_at": "2025-12-21T09:37:18+00:00",
        "updated_at": "2025-12-21T19:49:02+00:00",
        "version": 1
      }
    },
    "interactions:url.2A46BCB2": {
      "data": {
        "sourceId": "url",
        "paperId": "2A46BCB2",
        "interactions": []
      },
      "meta": {
        "issue_number": 21,
        "object_id": "interactions:url.2A46BCB2",
        "created_at": "2025-12-21T09:27:04+00:00",
        "updated_at": "2025-12-21T09:27:05+00:00",
        "version": 1
      }
    },
    "paper:url.2A46BCB2": {
      "data": {
        "sourceId": "url",
        "paperId": "2A46BCB2",
        "url": "https://arxiv.org/pdf/physics/0605057",
        "title": "2A46BCB2",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-12-21T09:26:45.932Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 20,
        "object_id": "paper:url.2A46BCB2",
        "created_at": "2025-12-21T09:26:46+00:00",
        "updated_at": "2025-12-21T09:27:45+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2112.10752": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2112.10752",
        "url": "https://arxiv.org/abs/2112.10752",
        "title": "High-Resolution Image Synthesis with Latent Diffusion Models",
        "authors": "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj\u00f6rn Ommer",
        "abstract": "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at this https URL .",
        "timestamp": "2025-12-21T20:19:28.145Z",
        "rating": "novote",
        "publishedDate": "2021/12/20",
        "tags": [
          "Computer Vision and Pattern Recognition (cs.CV)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 26,
        "object_id": "paper:arxiv.2112.10752",
        "created_at": "2025-12-21T20:19:28+00:00",
        "updated_at": "2025-12-21T20:20:27+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2112.10752": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2112.10752",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-12-21T20:28:22.502Z",
            "data": {
              "session_id": "session_1766348901785_ma830kq",
              "source_id": "arxiv",
              "paper_id": "2112.10752",
              "start_time": "2025-12-21T20:19:29.958Z",
              "end_time": "2025-12-21T20:28:21.785Z",
              "heartbeat_count": 106,
              "duration_seconds": 530,
              "idle_seconds": 2,
              "total_elapsed_seconds": 532
            }
          }
        ]
      },
      "meta": {
        "issue_number": 27,
        "object_id": "interactions:arxiv.2112.10752",
        "created_at": "2025-12-21T20:28:23+00:00",
        "updated_at": "2025-12-21T20:29:21+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2512.10047": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2512.10047",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-12-22T05:32:59.159Z",
            "data": {
              "session_id": "session_1766381578580_mvbsh7c",
              "source_id": "arxiv",
              "paper_id": "2512.10047",
              "start_time": "2025-12-22T05:27:56.093Z",
              "end_time": "2025-12-22T05:32:58.580Z",
              "heartbeat_count": 60,
              "duration_seconds": 300,
              "idle_seconds": 2,
              "total_elapsed_seconds": 302
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-12-22T05:46:46.627Z",
            "data": {
              "session_id": "session_1766382406060_eiy3auh",
              "source_id": "arxiv",
              "paper_id": "2512.10047",
              "start_time": "2025-12-22T05:39:29.308Z",
              "end_time": "2025-12-22T05:46:46.060Z",
              "heartbeat_count": 87,
              "duration_seconds": 435,
              "idle_seconds": 2,
              "total_elapsed_seconds": 437
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-12-22T06:50:45.123Z",
            "data": {
              "session_id": "session_1766386244900_xm8c359",
              "source_id": "arxiv",
              "paper_id": "2512.10047",
              "start_time": "2025-12-22T06:50:37.390Z",
              "end_time": "2025-12-22T06:50:44.900Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "issue_number": 29,
        "object_id": "interactions:arxiv.2512.10047",
        "created_at": "2025-12-21T20:35:39+00:00",
        "updated_at": "2025-12-22T06:51:56+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2512.10047": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2512.10047",
        "url": "https://arxiv.org/pdf/2512.10047",
        "title": "Detailed balance in large language model-driven agents",
        "authors": [
          "Zhuo-Yang Song",
          "Qing-Hong Cao",
          "Ming-xing Luo",
          "Hua Xing Zhu"
        ],
        "abstract": "Large language model (LLM)-driven agents are emerging as a powerful new paradigm for solving complex problems. Despite the empirical success of these practices, a theoretical framework to understand and unify their macroscopic dynamics remains lacking. This Letter proposes a method based on the least action principle to estimate the underlying generative directionality of LLMs embedded within agents. By experimentally measuring the transition probabilities between LLM-generated states, we statistically discover a detailed balance in LLM-generated transitions, indicating that LLM generation may not be achieved by generally learning rule sets and strategies, but rather by implicitly learning a class of underlying potential functions that may transcend different LLM architectures and prompt templates. To our knowledge, this is the first discovery of a macroscopic physical law in LLM generative dynamics that does not depend on specific model details. This work is an attempt to establish a macroscopic dynamics theory of complex AI systems, aiming to elevate the study of AI agents from a collection of engineering practices to a science built on effective measurements that are predictable and quantifiable.",
        "timestamp": "2025-12-21T20:34:11.989Z",
        "rating": "novote",
        "publishedDate": "2025-12-10",
        "tags": [
          "cs.LG",
          "cond-mat.stat-mech",
          "cs.AI",
          "nlin.AO",
          "physics.data-an"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url",
        "arxiv_tags": [
          "cs.LG",
          "cond-mat.stat-mech",
          "cs.AI",
          "nlin.AO",
          "physics.data-an"
        ]
      },
      "meta": {
        "issue_number": 28,
        "object_id": "paper:arxiv.2512.10047",
        "created_at": "2025-12-21T20:34:12+00:00",
        "updated_at": "2025-12-21T20:38:18+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2506.16668": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.16668",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-12-21T22:20:04.284Z",
            "data": {
              "session_id": "session_1766355604018_dzox8wa",
              "source_id": "arxiv",
              "paper_id": "2506.16668",
              "start_time": "2025-12-21T22:19:48.272Z",
              "end_time": "2025-12-21T22:20:04.018Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 1,
              "total_elapsed_seconds": 16
            }
          }
        ]
      },
      "meta": {
        "issue_number": 31,
        "object_id": "interactions:arxiv.2506.16668",
        "created_at": "2025-12-21T22:05:32+00:00",
        "updated_at": "2025-12-21T22:21:06+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.16668": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.16668",
        "url": "https://arxiv.org/pdf/2506.16668",
        "title": "Bayesian Semiparametric Orthogonal Tucker Factorized Mixed Models for Multi-dimensional Longitudinal Functional Data",
        "authors": [
          "Arkaprava Roy",
          "Abhra Sarkar"
        ],
        "abstract": "We introduce a novel longitudinal mixed model for analyzing complex multidimensional functional data, addressing challenges such as high-resolution, structural complexities, and computational demands. Our approach integrates dimension reduction techniques, including basis function representation and Tucker tensor decomposition, to model complex functional (e.g., spatial and temporal) variations, group differences, and individual heterogeneity while drastically reducing model dimensions. The model accommodates multiplicative random effects whose marginalization yields a novel Tucker-decomposed covariance-tensor framework. To ensure scalability, we employ semi-orthogonal mode matrices implemented via a novel graph-Laplacian-based smoothness prior with low-rank approximation, leading to an efficient posterior sampling method. A cumulative shrinkage strategy promotes sparsity and enables semiautomated rank selection. We establish theoretical guarantees for posterior convergence and demonstrate the method's effectiveness through simulations, showing significant improvements over existing techniques. Applying the method to Alzheimer's Disease Neuroimaging Initiative (ADNI) neuroimaging data reveals novel insights into local brain changes associated with disease progression, highlighting the method's practical utility for studying cognitive decline and neurodegenerative conditions.",
        "timestamp": "2025-12-21T22:05:11.204Z",
        "rating": "novote",
        "publishedDate": "2025-06-20",
        "tags": [
          "stat.ME"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url",
        "arxiv_tags": [
          "stat.ME"
        ]
      },
      "meta": {
        "issue_number": 30,
        "object_id": "paper:arxiv.2506.16668",
        "created_at": "2025-12-21T22:05:11+00:00",
        "updated_at": "2025-12-21T22:08:13+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2508.12551": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2508.12551",
        "url": "https://arxiv.org/abs/2508.12551",
        "title": "OS-R1: Agentic Operating System Kernel Tuning with Reinforcement Learning",
        "authors": "Hongyu Lin, Yuchen Li, Haoran Luo, Kaichun Yao, Libo Zhang, Mingjie Xing, Yanjun Wu",
        "abstract": "Linux kernel tuning is essential for optimizing operating system (OS) performance. However, existing methods often face challenges in terms of efficiency, scalability, and generalization. This paper introduces OS-R1, an agentic Linux kernel tuning framework powered by rule-based reinforcement learning (RL). By abstracting the kernel configuration space as an RL environment, OS-R1 facilitates efficient exploration by large language models (LLMs) and ensures accurate configuration modifications. Additionally, custom reward functions are designed to enhance reasoning standardization, configuration modification accuracy, and system performance awareness of the LLMs. Furthermore, we propose a two-phase training process that accelerates convergence and minimizes retraining across diverse tuning scenarios. Experimental results show that OS-R1 significantly outperforms existing baseline methods, achieving up to 5.6% performance improvement over heuristic tuning and maintaining high data efficiency. Notably, OS-R1 is adaptable across various real-world applications, demonstrating its potential for practical deployment in diverse environments. Our dataset and code are publicly available at this https URL.",
        "timestamp": "2025-12-21T23:02:28.928Z",
        "rating": "novote",
        "publishedDate": "2025/08/18",
        "tags": [
          "Machine Learning (cs.LG)",
          "Artificial Intelligence (cs.AI)",
          "Operating Systems (cs.OS)",
          "Software Engineering (cs.SE)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 32,
        "object_id": "paper:arxiv.2508.12551",
        "created_at": "2025-12-21T23:02:29+00:00",
        "updated_at": "2025-12-21T23:03:29+00:00",
        "version": 1
      }
    },
    "paper:arxiv.1905.02199": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1905.02199",
        "url": "https://arxiv.org/abs/1905.02199",
        "title": "Nonlinear Approximation and (Deep) ReLU Networks",
        "authors": "I. Daubechies, R. DeVore, S. Foucart, B. Hanin, G. Petrova",
        "abstract": "This article is concerned with the approximation and expressive powers of deep neural networks. This is an active research area currently producing many interesting papers. The results most commonly found in the literature prove that neural networks approximate functions with classical smoothness to the same accuracy as classical linear methods of approximation, e.g. approximation by polynomials or by piecewise polynomials on prescribed partitions. However, approximation by neural networks depending on n parameters is a form of nonlinear approximation and as such should be compared with other nonlinear methods such as variable knot splines or n-term approximation from dictionaries. The performance of neural networks in targeted applications such as machine learning indicate that they actually possess even greater approximation power than these traditional methods of nonlinear approximation. The main results of this article prove that this is indeed the case. This is done by exhibiting large classes of functions which can be efficiently captured by neural networks where classical nonlinear methods fall short of the task. The present article purposefully limits itself to studying the approximation of univariate functions by ReLU networks. Many generalizations to functions of several variables and other activation functions can be envisioned. However, even in this simplest of settings considered here, a theory that completely quantifies the approximation power of neural networks is still lacking.",
        "timestamp": "2025-12-21T23:20:55.922Z",
        "rating": "novote",
        "publishedDate": "2019/05/05",
        "tags": [
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 33,
        "object_id": "paper:arxiv.1905.02199",
        "created_at": "2025-12-21T23:20:56+00:00",
        "updated_at": "2025-12-21T23:21:53+00:00",
        "version": 1
      }
    },
    "interactions:url.27C1C870": {
      "data": {
        "sourceId": "url",
        "paperId": "27C1C870",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-12-23T03:36:14.296Z",
            "data": {
              "session_id": "session_1766460973785_ly32h6z",
              "source_id": "url",
              "paper_id": "27C1C870",
              "start_time": "2025-12-23T03:35:34.479Z",
              "end_time": "2025-12-23T03:36:13.784Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 4,
              "total_elapsed_seconds": 39
            }
          }
        ]
      },
      "meta": {
        "issue_number": 35,
        "object_id": "interactions:url.27C1C870",
        "created_at": "2025-12-23T03:36:14+00:00",
        "updated_at": "2025-12-23T03:37:18+00:00",
        "version": 1
      }
    },
    "paper:url.27C1C870": {
      "data": {
        "sourceId": "url",
        "paperId": "27C1C870",
        "url": "https://jack-clark.net/about/",
        "title": "About",
        "authors": "",
        "abstract": "The greatest challenge of the 21st century is to make an increasingly fast-moving technical world \u2018legible\u2019 to a large number of people. My belief is that by solving these information a\u2026",
        "timestamp": "2025-12-23T03:35:33.754Z",
        "rating": "novote",
        "publishedDate": "2009-11-30T12:12:55+00:00",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 34,
        "object_id": "paper:url.27C1C870",
        "created_at": "2025-12-23T03:35:33+00:00",
        "updated_at": "2025-12-23T03:36:31+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.1406.2661": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1406.2661",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-12-23T18:45:27.418Z",
            "data": {
              "session_id": "session_1766515527054_3h2t1mt",
              "source_id": "arxiv",
              "paper_id": "1406.2661",
              "start_time": "2025-12-23T18:36:24.607Z",
              "end_time": "2025-12-23T18:45:27.054Z",
              "heartbeat_count": 108,
              "duration_seconds": 540,
              "idle_seconds": 2,
              "total_elapsed_seconds": 542
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-12-23T20:53:11.456Z",
            "data": {
              "session_id": "session_1766523191255_p7q4h0v",
              "source_id": "arxiv",
              "paper_id": "1406.2661",
              "start_time": "2025-12-23T20:53:00.003Z",
              "end_time": "2025-12-23T20:53:11.255Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-12-23T20:58:59.409Z",
            "data": {
              "session_id": "session_1766523539167_dbs4l4i",
              "source_id": "arxiv",
              "paper_id": "1406.2661",
              "start_time": "2025-12-23T20:58:28.399Z",
              "end_time": "2025-12-23T20:58:59.167Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 1,
              "total_elapsed_seconds": 31
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-12-23T21:01:28.472Z",
            "data": {
              "session_id": "session_1766523688466_3dknjyu",
              "source_id": "arxiv",
              "paper_id": "1406.2661",
              "start_time": "2025-12-23T21:01:12.228Z",
              "end_time": "2025-12-23T21:01:28.466Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 1,
              "total_elapsed_seconds": 16
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-12-23T21:16:10.723Z",
            "data": {
              "session_id": "session_1766524570553_sv4t9xo",
              "source_id": "arxiv",
              "paper_id": "1406.2661",
              "start_time": "2025-12-23T21:15:14.823Z",
              "end_time": "2025-12-23T21:16:10.553Z",
              "heartbeat_count": 11,
              "duration_seconds": 55,
              "idle_seconds": 1,
              "total_elapsed_seconds": 56
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-12-23T21:27:52.809Z",
            "data": {
              "session_id": "session_1766525272804_5r9i2tq",
              "source_id": "arxiv",
              "paper_id": "1406.2661",
              "start_time": "2025-12-23T21:27:34.101Z",
              "end_time": "2025-12-23T21:27:52.804Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 4,
              "total_elapsed_seconds": 19
            }
          }
        ]
      },
      "meta": {
        "issue_number": 37,
        "object_id": "interactions:arxiv.1406.2661",
        "created_at": "2025-12-23T18:36:23+00:00",
        "updated_at": "2025-12-23T21:28:09+00:00",
        "version": 1
      }
    },
    "paper:arxiv.1406.2661": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1406.2661",
        "url": "https://arxiv.org/abs/1406.2661",
        "title": "Generative Adversarial Networks",
        "authors": "Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio",
        "abstract": "We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.",
        "timestamp": "2025-12-23T18:35:56.249Z",
        "rating": "novote",
        "publishedDate": "2014/06/10",
        "tags": [
          "Machine Learning (stat.ML)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 36,
        "object_id": "paper:arxiv.1406.2661",
        "created_at": "2025-12-23T18:35:56+00:00",
        "updated_at": "2025-12-23T18:36:55+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.1503.03585": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1503.03585",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-12-23T23:04:04.018Z",
            "data": {
              "session_id": "session_1766531043763_y70ft1t",
              "source_id": "arxiv",
              "paper_id": "1503.03585",
              "start_time": "2025-12-23T23:03:48.654Z",
              "end_time": "2025-12-23T23:04:03.763Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 0,
              "total_elapsed_seconds": 15
            }
          }
        ]
      },
      "meta": {
        "issue_number": 39,
        "object_id": "interactions:arxiv.1503.03585",
        "created_at": "2025-12-23T21:43:44+00:00",
        "updated_at": "2025-12-23T23:05:14+00:00",
        "version": 1
      }
    },
    "paper:arxiv.1503.03585": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1503.03585",
        "url": "https://arxiv.org/abs/1503.03585",
        "title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics",
        "authors": "Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, Surya Ganguli",
        "abstract": "A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.",
        "timestamp": "2025-12-23T21:41:50.039Z",
        "rating": "novote",
        "publishedDate": "2015/03/12",
        "tags": [
          "Machine Learning (cs.LG)",
          "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
          "Neurons and Cognition (q-bio.NC)",
          "Machine Learning (stat.ML)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 38,
        "object_id": "paper:arxiv.1503.03585",
        "created_at": "2025-12-23T21:41:50+00:00",
        "updated_at": "2025-12-23T21:42:48+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.1701.07875": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1701.07875",
        "interactions": []
      },
      "meta": {
        "issue_number": 41,
        "object_id": "interactions:arxiv.1701.07875",
        "created_at": "2025-12-23T23:09:09+00:00",
        "updated_at": "2025-12-23T23:09:10+00:00",
        "version": 1
      }
    },
    "paper:arxiv.1701.07875": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1701.07875",
        "url": "https://arxiv.org/abs/1701.07875",
        "title": "Wasserstein GAN",
        "authors": "Martin Arjovsky, Soumith Chintala, L\u00e9on Bottou",
        "abstract": "We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.",
        "timestamp": "2025-12-23T23:08:59.657Z",
        "rating": "novote",
        "publishedDate": "2017/01/26",
        "tags": [
          "Machine Learning (stat.ML)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 40,
        "object_id": "paper:arxiv.1701.07875",
        "created_at": "2025-12-23T23:08:59+00:00",
        "updated_at": "2025-12-23T23:09:53+00:00",
        "version": 1
      }
    }
  }
}
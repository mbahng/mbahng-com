{
  "snapshot_time": "2025-12-21T08:19:34.616644+00:00",
  "objects": {
    "paper:arxiv.2511.09744": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2511.09744",
        "url": "https://arxiv.org/abs/2511.09744",
        "title": "Computing parametric weighted Ehrhart polynomials of smooth polytopes",
        "authors": "Daniel Hwang, Juliet Whidden, Josephine Yu",
        "abstract": "We show that when integral polytopes are deformed while keeping the same facet normal vectors, the coefficients of weighted Ehrhart and $h^*$-polynomials are piecewise polynomial functions in the ``right hand sides'' of the linear inequalities defining the polytopes. We give an algorithm and an implementation in SageMath for computing these polynomials for smooth polytopes, such as type $A$ alcoved polytopes, using a weighted Euler-Maclaurin type formula by Khovanski\u01d0 and Pukhlikov. We discuss some natural questions concerning signs of the coefficients of the weighted $h^*$-polynomials.",
        "timestamp": "2025-12-21T05:18:59.031Z",
        "rating": "novote",
        "publishedDate": "2025/11/12",
        "tags": [
          "Combinatorics (math.CO)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 1,
        "object_id": "paper:arxiv.2511.09744",
        "created_at": "2025-12-21T05:18:59+00:00",
        "updated_at": "2025-12-21T07:35:37+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2210.05846": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2210.05846",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-12-21T07:37:55.265Z",
            "data": {
              "session_id": "session_1766302675253_l788cpw",
              "source_id": "arxiv",
              "paper_id": "2210.05846",
              "start_time": "2025-12-21T07:37:40.215Z",
              "end_time": "2025-12-21T07:37:55.253Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 0,
              "total_elapsed_seconds": 15
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-12-21T07:46:28.708Z",
            "data": {
              "session_id": "session_1766303188695_2r0em9o",
              "source_id": "arxiv",
              "paper_id": "2210.05846",
              "start_time": "2025-12-21T07:46:11.238Z",
              "end_time": "2025-12-21T07:46:28.695Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 2,
              "total_elapsed_seconds": 17
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-12-21T08:11:01.730Z",
            "data": {
              "session_id": "session_1766304661522_yuwoaey",
              "source_id": "arxiv",
              "paper_id": "2210.05846",
              "start_time": "2025-12-21T08:10:43.211Z",
              "end_time": "2025-12-21T08:11:01.522Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 3,
              "total_elapsed_seconds": 18
            }
          }
        ],
        "title": "FasterRisk: Fast and Accurate Interpretable Risk Scores",
        "authors": [
          "Jiachang Liu",
          "Chudi Zhong",
          "Boxuan Li",
          "Margo Seltzer",
          "Cynthia Rudin"
        ],
        "abstract": "Over the last century, risk scores have been the most popular form of predictive model used in healthcare and criminal justice. Risk scores are sparse linear models with integer coefficients; often these models can be memorized or placed on an index card. Typically, risk scores have been created either without data or by rounding logistic regression coefficients, but these methods do not reliably produce high-quality risk scores. Recent work used mathematical programming, which is computationally slow. We introduce an approach for efficiently producing a collection of high-quality risk scores learned from data. Specifically, our approach produces a pool of almost-optimal sparse continuous solutions, each with a different support set, using a beam-search algorithm. Each of these continuous solutions is transformed into a separate risk score through a \"star ray\" search, where a range of multipliers are considered before rounding the coefficients sequentially to maintain low logistic loss. Our algorithm returns all of these high-quality risk scores for the user to consider. This method completes within minutes and can be valuable in a broad variety of applications.",
        "publishedDate": "2022-10-12"
      },
      "meta": {
        "issue_number": 9,
        "object_id": "interactions:arxiv.2210.05846",
        "created_at": "2025-12-21T06:42:27+00:00",
        "updated_at": "2025-12-21T08:12:13+00:00",
        "version": 1
      }
    },
    "paper:arxiv.1706.03762": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1706.03762",
        "url": "https://arxiv.org/abs/1706.03762",
        "title": "Attention Is All You Need",
        "authors": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin",
        "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
        "timestamp": "2025-12-21T07:46:33.198Z",
        "rating": "novote",
        "publishedDate": "2017/06/12",
        "tags": [
          "Computation and Language (cs.CL)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 18,
        "object_id": "paper:arxiv.1706.03762",
        "created_at": "2025-12-21T07:46:33+00:00",
        "updated_at": "2025-12-21T07:47:33+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2502.06709": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.06709",
        "url": "https://arxiv.org/pdf/2502.06709",
        "title": "Talagrand Meets Talagrand: Upper and Lower Bounds on Expected Soft Maxima of Gaussian Processes with Finite Index Sets",
        "authors": [
          "Yifeng Chu",
          "Maxim Raginsky"
        ],
        "abstract": "Analysis of extremal behavior of stochastic processes is a key ingredient in a wide variety of applications, including probability, statistical physics, theoretical computer science, and learning theory. In this paper, we consider centered Gaussian processes on finite index sets and investigate expected values of their smoothed, or ``soft,'' maxima. We obtain upper and lower bounds for these expected values using a combination of ideas from statistical physics (the Gibbs variational principle for the equilibrium free energy and replica-symmetric representations of Gibbs averages) and from probability theory (Sudakov minoration). These bounds are parametrized by an inverse temperature $\u03b2> 0$ and reduce to the usual Gaussian maximal inequalities in the zero-temperature limit $\u03b2\\to \\infty$. We provide an illustration of our methods in the context of the Random Energy Model, one of the simplest models of physical systems with random disorder.",
        "timestamp": "2025-12-21T07:00:23.584Z",
        "rating": "novote",
        "publishedDate": "2025-02-10",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 10,
        "object_id": "paper:arxiv.2502.06709",
        "created_at": "2025-12-21T07:00:23+00:00",
        "updated_at": "2025-12-21T07:35:40+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2502.06709": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.06709",
        "interactions": [],
        "title": "Talagrand Meets Talagrand: Upper and Lower Bounds on Expected Soft Maxima of Gaussian Processes with Finite Index Sets",
        "authors": [
          "Yifeng Chu",
          "Maxim Raginsky"
        ],
        "abstract": "Analysis of extremal behavior of stochastic processes is a key ingredient in a wide variety of applications, including probability, statistical physics, theoretical computer science, and learning theory. In this paper, we consider centered Gaussian processes on finite index sets and investigate expected values of their smoothed, or ``soft,'' maxima. We obtain upper and lower bounds for these expected values using a combination of ideas from statistical physics (the Gibbs variational principle for the equilibrium free energy and replica-symmetric representations of Gibbs averages) and from probability theory (Sudakov minoration). These bounds are parametrized by an inverse temperature $\u03b2> 0$ and reduce to the usual Gaussian maximal inequalities in the zero-temperature limit $\u03b2\\to \\infty$. We provide an illustration of our methods in the context of the Random Energy Model, one of the simplest models of physical systems with random disorder.",
        "publishedDate": "2025-02-10"
      },
      "meta": {
        "issue_number": 11,
        "object_id": "interactions:arxiv.2502.06709",
        "created_at": "2025-12-21T07:00:39+00:00",
        "updated_at": "2025-12-21T07:35:40+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2210.05846": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2210.05846",
        "url": "https://arxiv.org/pdf/2210.05846",
        "title": "FasterRisk: Fast and Accurate Interpretable Risk Scores",
        "authors": [
          "Jiachang Liu",
          "Chudi Zhong",
          "Boxuan Li",
          "Margo Seltzer",
          "Cynthia Rudin"
        ],
        "abstract": "Over the last century, risk scores have been the most popular form of predictive model used in healthcare and criminal justice. Risk scores are sparse linear models with integer coefficients; often these models can be memorized or placed on an index card. Typically, risk scores have been created either without data or by rounding logistic regression coefficients, but these methods do not reliably produce high-quality risk scores. Recent work used mathematical programming, which is computationally slow. We introduce an approach for efficiently producing a collection of high-quality risk scores learned from data. Specifically, our approach produces a pool of almost-optimal sparse continuous solutions, each with a different support set, using a beam-search algorithm. Each of these continuous solutions is transformed into a separate risk score through a \"star ray\" search, where a range of multipliers are considered before rounding the coefficients sequentially to maintain low logistic loss. Our algorithm returns all of these high-quality risk scores for the user to consider. This method completes within minutes and can be valuable in a broad variety of applications.",
        "timestamp": "2025-12-21T06:40:11.761Z",
        "rating": "novote",
        "publishedDate": "2022-10-12",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 8,
        "object_id": "paper:arxiv.2210.05846",
        "created_at": "2025-12-21T06:40:12+00:00",
        "updated_at": "2025-12-21T07:35:39+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2511.09744": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2511.09744",
        "interactions": [],
        "title": "Computing parametric weighted Ehrhart polynomials of smooth polytopes",
        "authors": [
          "Daniel Hwang",
          "Juliet Whidden",
          "Josephine Yu"
        ],
        "abstract": "We show that when integral polytopes are deformed while keeping the same facet normal vectors, the coefficients of weighted Ehrhart and $h^*$-polynomials are piecewise polynomial functions in the ``right hand sides'' of the linear inequalities defining the polytopes. We give an algorithm and an implementation in SageMath for computing these polynomials for smooth polytopes, such as type $A$ alcoved polytopes, using a weighted Euler-Maclaurin type formula by Khovanski\u01d0 and Pukhlikov. We discuss some natural questions concerning signs of the coefficients of the weighted $h^*$-polynomials.",
        "publishedDate": "2025-11-12"
      },
      "meta": {
        "issue_number": 2,
        "object_id": "interactions:arxiv.2511.09744",
        "created_at": "2025-12-21T05:19:24+00:00",
        "updated_at": "2025-12-21T07:35:38+00:00",
        "version": 1
      }
    }
  }
}
